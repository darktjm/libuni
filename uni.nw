% -*- mode: Noweb; noweb-code-mode: c-mode; -*-
% Build with noweb:
%  notangle -t8 build.nw > makefile
%  make
\documentclass[twoside,english]{report}
\usepackage[letterpaper,rmargin=1.5in,bmargin=1in]{geometry}
%%% latex preamble
% Begin-doc uni-rcs
\RCS $Id$
\RCS $Revision$
\RCS $Date$
% End-doc uni-rcs

%%% requires tjm-ext

% shut l2h up about the font changes
% l2h ignore newfontfamily A[{
% l2h ignore unimono

% There are very few monospace Unicode fonts, and even fewer
% that support U+2620 SKULL AND CROSSBONES.
% GNU Unifont isn't pretty, but supports it: http://unifoundry.com/
%\newfontfamily\unimono[Path = /usr/share/fonts/unifont/]{unifont.ttf}
% DejaVu Sans Mono looks OK: http://dejavu-fonts.org/
% It's installed by texlive by fontsextra, so it doesn't need a path.
\newfontfamily\unimono[Scale = 0.8]{DejaVuSansMono}

% You could just replace LMMono with unimono globally for consistency:
%\let\Tt\unimono

% or even replace all 3:
%\setmainfont{DejaVuSerif}
%\setsansfont{DejaVuSans}
%\setmonofont{DejaVuSansMono}

\begin{document}

\title{Unicode Support Routines}
\author{Thomas J. Moore}
% Begin-doc uni-version
\date{Version 0.\RCSRevision\\\RCSDate}
% End-doc uni-version
\maketitle

\begin{rawhtml}
<!-->
\end{rawhtml}
\iffalse
<<Sources>>=
$Id$
@

<<Version Strings>>=
"$Id$\n"
@

<<Common NoWeb Warning>>=
# $Id$
@
\fi
\begin{rawhtml}
<-->
\end{rawhtml}

\begin{abstract}

This document describes and implements a library which provides
basic Unicode (\url{http://www.unicode.org}) support in a convenient
manner.  Libraries already exist that do this:  GLib, ICU, and others.
However, this library does things my way, which allows me to e.g.
build a regular expression library and a compiler using these routines.

\end{abstract}

% Begin-doc Introduction
\tableofcontents
\listoftables

\chapter{Introduction}

The Unicode standard is a freely available standard describing a large
character set, and the features of those characters.  It also
describes several common transformations and processing methods for
those characters.  The ISO 10646 standard is also freely available at
this time, but like most ISO standards, this was not always so.  The
ISO standard describes the same character set, but has a much smaller
scope overall.  The Unicode Consortium also provides an electronically
readable form of its attribute information: the Unicode Character
Database.%
\footnote{\url{http://www.unicode.org/Public/zipped}}
The purpose of this library is to provide convenient access to that
information in C programs.  The ISO 10646 standard also provides such
information, but again, it is of more limited scope.  To the most
part, the ISO 10646 standard is completely ignored from this point on.

The Unicode consortium also provides a set of rules for interpreting
characters in different locales.  ISO also has standards to this
effect.  Both sets of standards and their data files are freely
available.  However, once again, the ISO data is ignored in favor of
the Unicode data.  The Unicode locale information is available in
electronically processable form in the Unicode Common Locale Data
Repository (CLDR).%
\footnote{\url{http://cldr.unicode.org},
\url{http://unicode.org/Public/cldr/}}
This library provides selected parts of the CLDR data to C programs as
well.

% End-doc Introduction
<<uni_all.h>>=
<<Common C Warning>>
#ifndef UNI_ALL_H
#define UNI_ALL_H

<<Library [[uni]] headers>>

#endif /* UNI_ALL_H */
@

% Begin-doc Introduction
This document does not contain the UCD or CLDR itself; it is expected
that the user download them manually and place them where they can be
found.  Some Linux distributions may already provide at least the UCD
as a package, although the CLDR is probably less common.  This package
was tested with versions 6.0.0 and 6.2.0 of the UCD, but the file
formats are relatively stable and it should work with later versions
as well.  I recommend at least version 6.1.0, as that is the first
version where the control character names were finally made official.
An earlier version of this library actually had hard-coded names for
ASCII control characters optionally added to the name database.  This
package was tested against CLDR version 22.1; I recommend always
getting the latest version available.  In particular, care must be
taken that the CLDR and UCD versions are compatible.

% End-doc Introduction
\lstset{language=make}
<<makefile.config>>=
# The location of the Unicode Character Database (unzipped)
# http://www.unicode.org/Public/zipped/<version>/UCD.zip
UCD_LOC = /usr/share/unicode-data
# The location of the Unicode Han Database (unzipped)
# http://www.unicode.org/Public/zipped/<version>/Unihan.zip
UNIHAN_LOC = $(UCD_LOC)
# The location of the Unicode Common Locale Data Repository (unzipped)
# http://www.unicode.org/Public/cldr/latest/core.zip
CLDR_LOC = /usr/share/unicode-data/cldr
@

% Begin-doc Introduction
Speaking of names, the Unicode character names are rather verbose.%
\footnote{It is recommended in some places that common names be
permitted in a less verbose manner, but that is locale-dependent and
will not be supported by this library.}
The XML standard, on the other hand, has fairly short names.  This
library can use either, or even both (where they do not conflict).  To
do this, the XML entity database is needed.  This is defined by the
W3C XML Entity Definitions for Characters.%
\footnote{\url{http://www.w3.org/TR/xml-entity-names/}}
This was tested with version 01 April 2010.%
\footnote{\url{http://www.w3.org/TR/2010/REC-xml-entity-names-20100401/}}
The only data file needed from that standard is unicode.xml,%
\footnote{\url{http://www.w3.org/2003/entities/2007xml/unicode.xml}}
which is linked from the standard.

% End-doc Introduction
<<makefile.config>>=
# The XML entity database http://www.w3.org/2003/entities/2007xml/unicode.xml
XMLUNI = /usr/share/unicode-data/unicode.xml
@

% Begin-doc Introduction
Other libraries exist which provide similar functionality; I have
looked at GLib\footnote{\url{http://www.gtk.org/}} and
ICU\footnote{\url{http://www.icu-project.org/}} in particular.  The
GLib library provides a number of utility functions for Unicode
character strings.  It provides normalization, classification, and
general string utilities. Of these, this library does not provide
general utf-8 string utilities.  It might have been useful to make
this a GLib extension instead, providing the missing properties (in
particular, the string properties such as character names).  However,
providing all functionality in a consistent way is easier than making
a GLib extension.

The ICU library is primarily C++, with a C wrapper.  While there is
nothing inherently wrong with this, I prefer C with a C++ wrapper over
C++ with a C wrapper in general.  The ICU does everything this library
does, and more.  Unfortunately, it is not very well documented.  Like
many modern library projects, it simply provides a basic API document
auto-generated from source.  This sort of documentation is necessary,
but insufficient.  Another part of the reason with not just using ICU
is that some operations I want to do in other code requires too much
overhead in ICU.

A lesser reason for not using either of these libraries is that I want
to use this library to support my own regular expression library, and
it seems redundant when both provide their own regular expression
implementation as well.  In addition, this library can be trimmed to
the minimal necessary code with static linking, and does not include
heavy wrapper code and additional utilities that the other libraries
include.

% End-doc Introduction
\chapter{Unicode Character Processing}

% Begin-doc unitypes
The supported character types (encoding forms) are UTF-8, UTF-16, and
UTF-32.  Technically, a UTF-32 character can be stored in 21 bits.
The other two require an unsigned storage type of the given width.
Instead of providing aliases for standard types, the standard sized C
types from \texttt{stdint.h} are used: [[uint8_t]], [[uint16_t]], and
[[uint32_t]], respectively.  All functions which take or return a
single character use the UTF-32 type.  If an error could be returned
for UTF-32 returning functions, a signed integer ([[int32_t]]) may be
used instead.

The stream encodings (encoding schemes) for these types are named
similarly.  The 8-bit encoding is UTF-8.  The 16-bit big-endian
encoding is UTF-16BE, and the little-endian encoding is UTF-16LE.  The
32-bit big-endian encoding is UTF-32BE, and the 32-bit little-endian
encoding is UTF-32LE.  UTF-16 and UTF-32 character streams must either
agree on a byte order ahead of time, or begin the stream with a Byte
Order Mark (U+FEFF).

% End-doc unitypes

\section{File I/O}

Basic I/O support is provided in the form of UTF decoders and
encoders.  More advanced I/O is provided in the form of arbitrary
encoding input.

<<Library [[uni]] Members>>=
uni_io.o
@

\lstset{language=C}
<<uni_io.c>>=
<<Common C Header>>
#include "uni_io.h"

<<Unicode I/O local definitions>>

<<Unicode I/O functions>>
@

<<Library [[uni]] headers>>=
#include "uni_io.h"
@

<<uni_io.h>>=
<<Common C Warning>>
#ifndef UNI_IO_H
#define UNI_IO_H

#include <stdint.h> /* for basic data types */
#include <stdio.h> /* it's I/O, right? */

<<Unicode I/O Exports>>
#endif
@

\section{Basic Unicode I/O}

Unicode defines several file formats:  UTF-8, UTF-16, and UTF-32.  No
special conversion needs to be made from UTF-32 to integer code
points, other than to filter out invalid values and possibly byte-swap
the code point.

<<Common C Includes>>=
#include <stdint.h>
/* FIXME: this is glibc-specific */
/* BSD apparently uses <sys/endian.h> */
/* OpenBSD additionally uses different fn names */
/* others may not even have any equivalent functions */
/* at least non-glibc users should get compilation errors that point here */
#include <endian.h>
@

\lstset{language=make}
<<makefile.vars>>=
# for endian
EXTRA_CFLAGS += -D_BSD_SOURCE
@

\lstset{language=C}
<<Unicode I/O Exports>>=
int uni_utf32_enclen(uint32_t cp);
#define uni_utf32_enclen(cp) 1
int uni_utf32_encode(uint32_t *buf, uint32_t cp, int bige);
@

<<Unicode I/O functions>>=
int uni_utf32_encode(uint32_t *buf, uint32_t cp, int bige)
{
  if(cp > 0x10ffff || (cp >= 0xd800 && cp < 0xe000))
    return 0;
  *buf = bige ? htobe32(cp) : htole32(cp);
  return 1;
}
@

<<Unicode I/O Exports>>=
int32_t uni_utf32_decode(const uint32_t *s, int bige);
@

<<Unicode I/O functions>>=
int32_t uni_utf32_decode(const uint32_t *s, int bige)
{
  uint32_t c = bige ? be32toh(*s) : le32toh(*s);
  if(c > 0x10ffff || (c >= 0xd800 && c < 0xe000))
    return -1;
  return c;
}
@

<<Unicode I/O Exports>>=
int uni_utf32_putc(uint32_t c, FILE *f, int bige);
@

<<Unicode I/O functions>>=
int uni_utf32_putc(uint32_t c, FILE *f, int bige)
{
  uint32_t w;

  if(!uni_utf32_encode(&w, c, bige))
    return 0;
  return fwrite(&w, 4, 1, f);
}
@

<<Unicode I/O Exports>>=
int32_t uni_utf32_getc(FILE *f, int bige); /* always reads 4 bytes */
@

<<Unicode I/O functions>>=
int32_t uni_utf32_getc(FILE *f, int bige)
{
  uint32_t w;
  int32_t ret;

  if((ret = fread(&w, 1, 4, f)) != 4)
    return ret ? -2 : -1;
  ret = uni_utf32_decode(&w, bige);
  return ret < 0 ? -3 : ret;
}
@

For UTF-16, a slightly more complex scheme is used, involving the
surrogate code points (D800 through DFFF) in pairs.  The first half of
the surrogate code points (D800 through DBFF) is always used for the
first member of the pair, and the second half is used for the other.
Each member gives 10 bits.  The maximum code point is 10FFFF,
requiring 21 bits, but since the extensions start at 10000, the
maximum etension is 10FFFF$-$10000 = FFFFF, requiring 20 bits.

<<Unicode I/O Exports>>=
int uni_utf16_enclen(uint32_t cp);
#define uni_utf16_enclen(cp) (1 + ((cp) >= 0x10000))
/* make sure at least 2 words avail in buf, or use enclen */
/* returns # of words written */
int uni_utf16_encode(uint16_t *buf, uint32_t cp, int bige);
@

<<Unicode I/O functions>>=
int uni_utf16_encode(uint16_t *buf, uint32_t cp, int bige)
{
  if((cp >= 0xD800 && cp < 0xE000) || cp > 0x10FFFF)
    return 0;
  if(cp < 0x10000) {
    *buf = bige ? htobe16(cp) : htole16(cp);
    return 1;
  }
  cp -= 0x10000;
  uint16_t w = 0xD800 + (cp >> 10);
  *buf++ = bige ? htobe16(w) : htole16(w);
  w = 0xDC00 + (cp & 0x3ff);
  *buf = bige ? htobe16(w) : htole16(w);
  return 2;
}
@

<<Unicode I/O Exports>>=
/* this function returns -1 on errors */
/* if the buffer may be invalid, ensure at least 2 words available */
int32_t uni_utf16_decode(const uint16_t *s, unsigned int *nread, int bige);
@

<<Unicode I/O functions>>=
int32_t uni_utf16_decode(const uint16_t *s, unsigned int *nread, int bige)
{
  uint16_t w = bige ? be16toh(*s) : le16toh(*s);
  if(nread)
    *nread = 1;
  if(w < 0xd800 || w >= 0xe000)
    return w;
  if(w >= 0xdc00)
    return -1;
  uint16_t w2 = bige ? be16toh(s[1]) : le16toh(s[1]);
  if(w2 >= 0xe000 || w2 < 0xdc00)
    return -1;
  if(nread)
    *nread = 2;
  return (w2 & 0x3ff) + ((uint32_t)(w & 0x3ff) << 10) + 0x10000;
}
@

<<Unicode I/O Exports>>=
int uni_utf16_putc(uint32_t c, FILE *f, int bige);
@

<<Unicode I/O functions>>=
int uni_utf16_putc(uint32_t c, FILE *f, int bige)
{
  unsigned char buf[4];
  uint16_t enc[2], encl;
  int wlen;
  
  encl = uni_utf16_encode(enc, c, bige);
  if(!encl)
    return 0;
  if(bige) {
    buf[0] = enc[0] >> 8;
    buf[1] = enc[0];
    if(encl > 1) {
      buf[2] = enc[1] >> 8;
      buf[3] = enc[1];
    }
  } else {
    buf[1] = enc[0] >> 8;
    buf[0] = enc[0];
    if(encl > 1) {
      buf[3] = enc[1] >> 8;
      buf[2] = enc[1];
    }
  }
  wlen = fwrite(buf, 2, encl, f);
  return wlen == encl ? wlen : -wlen;
}
@

The input function needs to read ahead to get the second member of a
pair.  If that character is not what was expected, the correct
behavior is to undo the readahead and return the current code point as
an error.  However, it is not possible in C standard I/O to push more
than one character back into the stream.  For now, this is going to
have to be erroneous behavior.

<<Unicode I/O Exports>>=
int32_t uni_utf16_getc(FILE *f, int bige, unsigned int *nread);
@

<<Unicode I/O functions>>=
int32_t uni_utf16_getc(FILE *f, int bige, unsigned int *nread)
{
  int c = fgetc(f), c2;
  if(c == EOF) {
    if(nread)
      *nread = 0;
    return -1;
  }
  c2 = fgetc(f);
  if(c == EOF) {
    if(nread)
      *nread = 1;
    return -2;
  }
  if(bige)
    c = c2 + (c << 8);
  else
    c += c2 << 8;
  if(c < 0xd800 || c >= 0xE000) {
    if(nread)
      *nread = 2;
    return c;
  }
  if(c >= 0xdc00) {
    if(nread)
      *nread = 2;
    return -3;
  }
  uint32_t res = 0x10000 + ((c & 0x3ff) << 10);
  if((c = fgetc(f)) == EOF) {
    if(nread)
      *nread = 2;
    return -2;
  }
  if((c2 = fgetc(f)) == EOF) {
    if(nread)
      *nread = 3;
    return -2;
  }
  if(bige)
    c = c2 + (c << 8);
  else
    c += c2 << 8;
  if(c < 0xdc00 || c >= 0xe000) {
    ungetc(bige ? c >> 8 : c & 0xff, f);
    /* POSIX only guarantees 1 ungetc */
    if(ungetc(c2, f) == EOF) {
      c2 = getc(f);
      if(nread)
        *nread = 4;
    } else if(nread)
      *nread = 2;
    return -3;
  }
  if(nread)
    *nread = 4;
  return res + (c & 0x3ff);
}
@

For UTF-8, an even more complex encoding scheme is used.  Again, a
standalone memory codec is provided.  All code points over 007F are
encoded using a multi-byte sequence.  All characters in a multi-byte
sequence have their high bit set; the first non-zero bit determines
the byte's role.  All but the first byte have only one high bit set,
and encode 6 bits.  The first byte determines how many trailing bytes
there are, and also encode 3--5 bits.  The number of high bits set in
the first byte is the total number of bytes in the sequence.

<<Unicode I/O Exports>>=
/* warning: cp is evaluated multiple times */
int uni_utf8_enclen(uint32_t cp);
#define uni_utf8_enclen(cp) \
  (1 + ((cp) >= 0x80) + ((cp) >= 0x800) + ((cp) >= 0x10000))
/* make sure at least 4 bytes avail in buf, or use enclen */
/* returns # of bytes written */
int uni_utf8_encode(uint8_t *buf, uint32_t cp);
@

<<Unicode I/O functions>>=
int uni_utf8_encode(uint8_t *buf, uint32_t cp)
{
    if((cp >= 0xD800 && cp < 0xE000) || cp > 0x10FFFF)
        return 0;
    if(cp < 128) {
        *buf = cp;
        return 1;
    } else if(cp < 0x800) {
        *buf = 0xc0 + (cp >> 6);
        buf[1] = 0x80 + (cp & 0x3f);
        return 2;
    } else if(cp < 0x10000) {
        *buf = 0xe0 + (cp >> 12);
        buf[1] = 0x80 + ((cp >> 6) & 0x3f);
        buf[2] = 0x80 + (cp & 0x3f);
        return 3;
    } else {
        *buf = 0xf0 + (cp >> 18);
        buf[1] = 0x80 + ((cp >> 12) & 0x3f);
        buf[2] = 0x80 + ((cp >> 6) & 0x3f);
        buf[3] = 0x80 + (cp & 0x3f);
        return 4;
    }
}
@

<<Unicode I/O Exports>>=
/* this function returns -1 on errors */
/* if the buffer may be invalid, ensure at least 4 chars available */
int32_t uni_utf8_decode(const uint8_t *s, unsigned int *nread);
@

<<Unicode I/O functions>>=
int32_t uni_utf8_decode(const uint8_t *s, unsigned int *nread)
{
  if(nread)
    *nread = 1;
  if(*s < 0x7f)
    return *s;
  uint32_t c = *s++, ec;
  if((c & 0xf8) == 0xf0) {
    c &= 0x07;
    ec = *s++;
    if((ec & 0xc0) != 0x80)
      return -1;
    if(nread)
      *nread = 2;
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if((ec & 0xc0) != 0x80)
      return -1;
    if(nread)
      *nread = 3;
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if((ec & 0xc0) != 0x80)
      return -1;
    if(nread)
      *nread = 4;
    c = (c << 6) + (ec & 0x3f);
    if(c < 0x10000 || c > 0x10FFFF)
      return -1;
    return c;
  } else if((c & 0xf0) == 0xe0) {
    c &= 0x0f;
    ec = *s++;
    if((ec & 0xc0) != 0x80)
      return -1;
    if(nread)
      *nread = 2;
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if((ec & 0xc0) != 0x80)
      return -1;
    if(nread)
      *nread = 3;
    c = (c << 6) + (ec & 0x3f);
    if(c < 0x800 || (c >= 0xD800 && c < 0xE000))
      return -1;
    return c;
  } else if((c & 0xe0) == 0xc0) {
    c &= 0x1f;
    ec = *s++;
    if((ec & 0xc0) != 0x80)
      return -1;
    if(nread)
      *nread = 2;
    c = (c << 6) + (ec & 0x3f);
    if(c < 0x80)
      return -1;
    return c;
  } else
    return -1;
}
@

<<Unicode I/O Exports>>=
int uni_utf8_putc(uint32_t c, FILE *f);
@

<<Unicode I/O functions>>=
int uni_utf8_putc(uint32_t c, FILE *f)
{
    uint8_t obuf[4];
    int nout = uni_utf8_encode(obuf, c);
    int nwr = fwrite(obuf, 1, nout, f);
    return nout == nwr ? nwr : -nwr;
}
@

UTF-8 output is common enough to warrant a string version of the
output function as well.

<<Unicode I/O Exports>>=
int uni_utf8_fputs(uint32_t *buf, int len, FILE *f);
@

<<Unicode I/O functions>>=
int uni_utf8_fputs(uint32_t *buf, int len, FILE *f)
{
  int ret = 0;

  while(len-- > 0) {
    int ret1 = uni_utf8_putc(*buf++, f);
    if(ret1 <= 0)
      return -ret + ret1;
    ret += ret1;
  }
  return ret;
}
@

Like UTF-16 input, UTF-8 input requires some readahead.  For anything
more than one character, C once again disallows returning the
characters to the stream.  However, it is relatively easy to abort
input as soon as an invalid character occurs.  The price to pay is the
extremely poor performance of [[getc]], but using the direct
decoder function above can be used to get around that.

<<Unicode I/O Exports>>=
int32_t uni_utf8_getc(FILE *f, unsigned int *nread);
@

<<Unicode I/O functions>>=
int32_t uni_utf8_getc(FILE *f, unsigned int *nread)
{
  int c;
  int chrread = 1;

  if((c = getc(f)) == EOF) {
    c = -1;
    chrread = 0;
  } else if(c >= 0x80) {
    int ec;
    chrread = 1;
    if((c & 0xf8) == 0xf0) {
      <<Read 4-char utf-8>>
      if(c < 0x10000 || c > 0x10ffff)
        c = -3;
    } else if((c & 0xf0) == 0xe0) {
      <<Read 3-char utf-8>>
      if(c < 0x800 || (c >= 0xD800 && c < 0xE000))
        c = -3;
    } else if((c & 0xe0) == 0xc0) {
      <<Read 2-char utf-8>>
      if(c < 0x80)
        c = -3;
    } else
      c = -3;
  }
  if(nread)
    *nread = chrread;
  return c;
}
@

<<Read 4-char utf-8>>=
c &= 0x07;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
  if((ec = getc(f)) == EOF)
    c = -2;
  else if((ec & 0xc0) != 0x80) {
    ungetc(ec, f);
    c = -3;
  } else {
    chrread = 3;
    c = (c << 6) + (ec & 0x3f);
    if((ec = getc(f)) == EOF)
      c = -2;
    else if((ec & 0xc0) != 0x80) {
      ungetc(ec, f);
      c = -3;
    } else {
      chrread = 4;
      c = (c << 6) + (ec & 0x3f);
    }
  }
}
@

<<Read 3-char utf-8>>=
c &= 0x0f;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
  if((ec = getc(f)) == EOF)
    c = -2;
  else if((ec & 0xc0) != 0x80) {
    ungetc(ec, f);
    c = -3;
  } else {
    chrread = 3;
    c = (c << 6) + (ec & 0x3f);
  }
}
@

<<Read 2-char utf-8>>=
c &= 0x1f;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
}
@

\section{General File Input}

A more general UTF reader would have to scan for a byte order mark and
remember what it said.  This requires retaining state.

<<Unicode I/O Exports>>=
typedef struct uni_file_t uni_file_t;
@

<<[[uni_file_t]]>>=
struct uni_file_t {
  FILE *f;
  char *name;
  <<Unicode file buffer state>>
};
@

<<Unicode I/O local definitions>>=
<<Unicode file buffer state deps>>
<<[[uni_file_t]]>>
@

<<Known Data Types>>=
uni_file_t,%
@

Rather than automatically attaching state to a file on first access
and never really knowing when to free it, explicit routines are
provided to create and remove the state.   

<<Unicode I/O Exports>>=
/* encoding = NULL for automatic detection */
/* automatically detects utf16/utf32 if encoding == UTF */
/* automatically detects endianness if encoding == UTF or UTF-16 or UTF-32 */
uni_file_t *uni_fopen(const char *name, const char *encoding);
void uni_fclose(uni_file_t *uf);
@

<<Unicode I/O functions>>=
uni_file_t *uni_fopen(const char *name, const char *encoding)
{
  uni_file_t *uf = calloc(sizeof(*uf), 1);

  if(!uf)
    return NULL;
  uf->name = strdup(name);
  if(!uf->name) {
    uni_fclose(uf);
    return NULL;
  }
  uf->f = fopen(name, "rb");
  if(!uf->f) {
    uni_fclose(uf);
    return NULL;
  }
  <<Initialize unicode file buffer state>>
  return uf;
}
@

<<Unicode I/O functions>>=
void uni_fclose(uni_file_t *uf)
{
  /* all frees are conditional so uni_fopen can uni_fclose partial progress */
  if(uf->name)
    free(uf->name);
  if(uf->f)
    fclose(uf->f);
  <<Free unicode file buffer state>>
  free(uf);
}
@

One limitation of standard I/O is that only one character may be
pushed back into the stream.  To correct this, a readahead buffer is
kept.  While C provides its own large buffers, preventing I/O overhead
on every read, the read itself can be expensive anyway if called too
often.  To avoid this, the buffer is much larger than needed for
simple UTF lookahead.

<<Unicode file buffer state deps>>=
#define UNIF_BUF_LEN 16384 /* about 4x my old usual buf size */
@

<<Unicode file buffer state>>=
char readahead[UNIF_BUF_LEN];
unsigned short raptr, ralen;
@

The first thing to do is to decide the file's encoding.  ASCII and
ISO-Latin-1 can be read raw, since they map directly to Unicode.
UTF is taken to mean automatic Unicode detection, with UTF-8 as a
default.  UTF-8, UTF-16, UTF-32, UTF-16LE, UTF-16BE, UTF-32LE, and
UTF-32BE are processed directly, with UTF-16 and UTF-32 using native
encoding if a byte order mark is not found.  For all other input
types (including the above-mentioned ones which are in other formats,
such as lower-case), the [[iconv]] function should be used.%
\footnote{The C99 library provides something similar in the
[[mbstowcs]] function, but the POSIX [[iconv]] function is commonly
available (perhaps even as a third party library), and could be
provided as a wrapper around [[mbstowcs]] if all else fails.  Unlike
[[mbstowcs]], the type of the output can always be set to Unicode
encodings, and the input type can be controlled more easily as well.}
This function should be in the standard C library, but may be provided
externally as well.  If so, [[CFLAGS]] and [[LDFLAGS]] may need to be
modified appropriately.  In order to accomodate the case where a
minimal C library has no working [[iconv]], or to otherwise avoid code
bloat, this can be disabled with a configuration variable.

\lstset{language=make}
<<makefile.config>>=
# Set to non-empty to enable iconv input support
#USE_ICONV=y

@

<<makefile.vars>>=
ifneq ($USE_ICONV,)
EXTRA_CFLAGS += -DUNI_USE_ICONV
endif
@

\lstset{language=C}
<<Unicode file buffer state deps>>=
#ifdef UNI_USE_ICONV
#include <langinfo.h>
#include <locale.h>
#include <iconv.h>
#endif
@

<<Unicode file buffer state>>=
/*
 * 0 = ASCII/Latin-1
 * 1 = iconv (if UNI_USE_ICONV)
 * 8/16/32 = utf-8/16/32
 */
unsigned char enctype;
unsigned char bige;
@

<<Initialize unicode file buffer state>>=
#ifdef UNI_USE_ICONV
if(!encoding) {
  setlocale(LC_CTYPE, "");
  encoding = nl_langinfo(CODESET);
}
#endif
if(encoding && (!strcmp(encoding, "ISO-8859-1") || /* Latin-1 */
                !strcmp(encoding, "ANSI_X3.4-1968"))) /* ASCII */
  return uf; /* enctype == 0 -> raw bytes */
if(encoding && !strncmp(encoding, "UTF", 3)) {
  if(!encoding[3])
    encoding = NULL;
  else if(encoding[3] == '-') {
    if(encoding[4] == '8' && !encoding[5]) {
      uf->enctype = 8;
      return uf;
    } else if(encoding[4] == '1' && encoding[5] == '6') {
      uf->enctype = 16;
      <<Check if [[encoding]] is has [[BE]] or [[LE]] suffix>>
    } else if(encoding[4] == '3' && encoding[5] == '2') {
      uf->enctype = 32;
      <<Check if [[encoding]] is has [[BE]] or [[LE]] suffix>>
    }
  }
}
@

<<Check if [[encoding]] is has [[BE]] or [[LE]] suffix>>=
if(encoding[6] == 'B' && encoding[7] == 'E' && !encoding[8]) {
  uf->bige = 1;
  return uf;
} else if(encoding[6] == 'L' && encoding[7] == 'E' && !encoding[8])
  return uf;
else if(!encoding[6]) {
  uf->bige = __BYTE_ORDER == __BIG_ENDIAN;
  encoding = NULL; /* need to determine endianness still */
} /* else use iconv */
@

<<Unicode file buffer state>>=
#ifdef UNI_USE_ICONV
iconv_t ic;
uint32_t *obuf; /* only allocated when needed */
uint16_t optr, olen;
#endif
@

<<Initialize unicode file buffer state>>=
#ifdef UNI_USE_ICONV
if(encoding) {
  uf->obuf = malloc(UNIF_BUF_LEN); /* max len == UNIF_BUF_LEN / 4 */
  if(!uf->obuf) {
    uni_fclose(uf);
    return NULL;
  }
#if __BYTE_ORDER == __BIG_ENDIAN
#define bo "BE"
#else
#define bo "LE"
#endif
  uf->ic = iconv_open("UTF-32" bo, encoding);
  if(uf->ic == (iconv_t)-1) {
    uni_fclose(uf);
    return NULL;
  }
  uf->enctype = 1;
  return uf;
}
#else
if(encoding) {
  errno = EINVAL;
  uni_fclose(uf);
  return NULL;
}
#endif
@

<<Free unicode file buffer state>>=
#ifdef UNI_USE_ICONV
if(uf->obuf)
  free(uf->obuf);
if(uf->enctype == 1)
  iconv_close(uf->ic);
#endif
@

The first thing to do on opening a new Unicode file is to check for
the byte order mark (U+FEFF).  Flags are kept to indicate what was
found.  Note that the byte order mark is still returned as the first
character, so all read characters must be backed out.

<<Initialize unicode file buffer state>>=
if(!uf->enctype)
  uf->enctype = 8; /* default is UTF-8 */
/* Read and process BOM (FEFF) */
int c;
<<Read and store a char from [[uf]]>>
if(c == 0xfe) {
  <<Read and store a char from [[uf]]>>
  if(c == 0xff && uf->enctype != 32) { /* FEFF UTF-16BE */
    uf->bige = 1;
    uf->enctype = 16;
  }
} else if(c == 0xff) {
  <<Read and store a char from [[uf]]>>
  if(c == 0xfe) { /* FFFE UTF-16LE or UTF-32LE (or native UTF-32) */
    if(uf->enctype == 16)
      uf->bige = 0;
    else {
      <<Read and store a char from [[uf]]>>
      if(!c) {
        <<Read and store a char from [[uf]]>>
        if(!c) { /* FFFE0000 UTF-32LE */
          uf->enctype = 32;
	  uf->bige = 0;
        } else if(!uf->enctype)
          uf->enctype = 16;
	/* else UTF-32 native-endian */
      } else if(!uf->enctype)
        uf->enctype = 16;
      /* else UTF-32 native-endian */
    }
  }
} else if(!c && uf->enctype != 16) {
  <<Read and store a char from [[uf]]>>
  if(!c) {
    <<Read and store a char from [[uf]]>>
    if(c == 0xfe) {
      <<Read and store a char from [[uf]]>>
      if(c == 0xff) { /* 0000FEFF UTF-32BE */
        uf->enctype = 32;
	uf->bige = 1;
      }
    }
  }
}
<<Unread all chars read from [[uf]]>>
@

<<Read and store a char from [[uf]]>>=
if(uf->raptr < uf->ralen)
  c = uf->readahead[uf->raptr++];
else if(!uf->ralen) {
  uf->ralen = fread(uf->readahead, 1, UNIF_BUF_LEN, uf->f);
  if(uf->raptr)
    c = uf->readahead[uf->raptr++];
  else
    c = EOF;
} else
  c = EOF;
@

<<Unread all chars read from [[uf]]>>=
uf->raptr = 0;
@

The reader then uses the appropriate method to read a character based
on the [[enctype]].

<<Unicode I/O Exports>>=
int32_t uni_fgetc(uni_file_t *uf);
@

<<Unicode I/O functions>>=
int32_t uni_fgetc(uni_file_t *uf)
{
  if(!uf->enctype) {
    return getc(uf->f);
#ifdef UNI_USE_ICONV
  } else if(uf->enctype == 1) {
    <<Return a character using iconv>>
#endif
  }
  <<Return a Unicode character>>
}
@

Rather than using the potentially flawed direct I/O routines, the
Unicode reader always reads raw bytes and then calls the decoder
instead.  That way, the readahead issue disappears.  The readahead
buffer is refilled whenever it has less than 4 characters, since all
encodings require at most that many.

<<Return a Unicode character>>=
if(uf->ralen < uf->raptr + 4 && (!uf->ralen || uf->ralen == UNIF_BUF_LEN)) {
  memcpy(uf->readahead, uf->readahead + uf->raptr, uf->ralen - uf->raptr);
  uf->ralen -= uf->raptr;
  uf->raptr = 0;
  uf->ralen += fread(uf->readahead + uf->ralen, 1, UNIF_BUF_LEN - uf->ralen, uf->f);
}
if(uf->raptr == uf->ralen)
  return -1;
int cp;
unsigned int l;
uint8_t *ra = (uint8_t *)uf->readahead + uf->raptr;
if(uf->enctype == 32) {
  cp = uni_utf32_decode((uint32_t *)ra, uf->bige);
  l = 4;
} else if(uf->enctype == 16) {
  cp = uni_utf16_decode((uint16_t *)ra, &l, uf->bige);
  l *= 2;
} else
  cp = uni_utf8_decode(ra, &l);
if(l > uf->ralen - uf->raptr) {
  uf->ralen = uf->raptr = 0;
  return -2;
}
if(l < 4)
  movebuf(uf->readahead, uf->readahead + l, 4 - l);
uf->raptr += l;
return cp < 0 ? -2 : cp;
@

The [[iconv]] conversion proceeds much differently.  If a character is
availble in the output buffer, it validates and returns that character.
Otherwise, it fills up the input buffer from the file, and then
attempts to fill the output buffer using [[iconv]].

The [[iconv]] routine needs to be called one more time at end-of-file.
Detection of end-of-file is easy, but detection of whether or not this
last step has been done is not.  A flag is used to indicate this.

<<Unicode file buffer state>>=
#ifdef UNI_USE_ICONV
uint8_t did_ic_eof;
#endif
@

<<Return a character using iconv>>=
while(1) {
  if(uf->olen) {
    --uf->olen;
    uint32_t cp = uf->obuf[uf->optr++];
    if(cp > 0x10FFFF || (cp >= 0xD800 && cp < 0xE000))
      return -3;
    return cp;
  }
  /* if not at end of input, refill buffer */
  if(!uf->ralen || uf->ralen == UNIF_BUF_LEN) {
    /* if more than UNIF_BUF_LEN needed for a pass of iconv, this is screwed */
    if(uf->ralen && !uf->raptr) {
      errno = ENOSPC;
      return -2;
    }
    memmove(uf->readahead, uf->readahead + uf->raptr, uf->ralen - uf->raptr);
    uf->ralen += fread(uf->readahead + uf->ralen, 1, UNIF_BUF_LEN - uf->ralen, uf->f);
    if(!uf->ralen) /* len was 0, and 0 additional bytes read: EOF @start */
      return -1; /* assume iconv wouldn't return anything either */
    uf->ralen -= uf->raptr;
    uf->raptr = 0;
  }
  char *obuf = (char *)uf->obuf;
  size_t olen = UNIF_BUF_LEN;
  if(uf->ralen == uf->raptr) {
    if(uf->did_ic_eof)
      return -1;
    uf->did_ic_eof = 1;
    if(!uf->ralen)
      uf->ralen = uf->raptr = 1; /* avoid reading again */
    if(iconv(uf->ic, NULL, NULL, &obuf, &olen) < 0)
      return -2;
  } else {
    char *ibuf = uf->readahead + uf->raptr;
    size_t ilen = uf->ralen - uf->raptr;
    if(iconv(uf->ic, &ibuf, &ilen, &obuf, &olen) < 0 &&
       ibuf == uf->readahead + uf->raptr) {
      uf->raptr++;
      return -2;
    }
    uf->raptr = uf->ralen - ilen;
  }
  uf->optr = 0;
  uf->olen = UNIF_BUF_LEN - olen;
}
@

\section{Internal Storage}

The previous section assumed that internal storage is in 32-bit words,
or raw UTF-32.  However, this is not the best storage method for all
applications.  The other two encoding techniques have their advantages
as well.

UTF-32 is easy to index and compute the length.  However, it always
wastes at least 11 bits of storage per character.  UTF-16 and UTF-8
require a scan of the entire string to find an index or compute the
length.  However, they only waste 11 bits in the worst case, and more
commonly (especially in western scripts) waste much less.  In order to
compensate for the length problem, two separate lengths could be
stored for every string: the number of words stored, and the actual
string length.  In order to compensate for the index problem, if the
index is required frequently, conversion to UTF-32 is recommended. Use
of a side array storing indices would take just as much space, and
offer no real advantage other than slightly less cost in computing
each entry; that cost would likely be repaid every time the character
at a particular index is actually retrieved.  It would be possible to
save space in a side array by storing only a single bit per word
flagging all but the last word of each multi-word sequence, thus
reducing the scan of the entire string down to a scan of the bit
array, but again, just converting to UTF-32 is probably best:

<<Scan continuation array>>=
/* pseudo code */
int find_offset(s, n, flags)
{
  int m = 0, c = 0;
  while(1) {
    int b = count_bits(flags, m .. n - 1);
    if(!b)
      return n;
    m = n + 1;
    n += b;
  }
}

int offset_of(s, n, flags)
{
  return n - count_bits(flags, 0 .. n - 1);
}
@

Nonetheless, for raw storage, functions are provided to compute the
offset of the first word of a character, and the character offset of a
given word offset.  No checks are made to see if the string is valid
is intended for internally-generated strings.  The UTF-32 versions are
pointless, except when creating generic code that constructs function
names based on size.

<<Unicode I/O Exports>>=
unsigned int uni_utf32_offset_of(const uint32_t *buf, unsigned int n);
#define uni_utf32_offset_of(b, n) (n)
unsigned int uni_utf32_index_of(const uint32_t *buf, unsigned int n);
#define uni_utf32_index_of(b, n) (n)
unsigned int uni_utf16_offset_of(const uint16_t *buf, unsigned int n);
unsigned int uni_utf16_index_of(const uint16_t *buf, unsigned int n);
unsigned int uni_utf8_offset_of(const uint8_t *buf, unsigned int n);
unsigned int uni_utf8_index_of(const uint8_t *buf, unsigned int n);
@

<<Unicode I/O functions>>=
unsigned int uni_utf16_offset_of(const uint16_t *buf, unsigned int n)
{
  unsigned int i, j;
  for(i = j = 0; i < n; i++, j++)
    if(buf[j] >= 0xd800 && buf[j] < 0xdc00)
      j++;
  return j;
}

unsigned int uni_utf16_index_of(const uint16_t *buf, unsigned int n)
{
  unsigned int i, j;
  for(i = j = 0; j < n; i++, j++)
    if(buf[j] >= 0xd800 && buf[j] < 0xdc00)
      j++;
  return i;
}

unsigned int uni_utf8_offset_of(const uint8_t *buf, unsigned int n)
{
  unsigned int i, j;
  for(i = j = 0; i < n; i++, j++)
    if(buf[j] >= 0x80)
      j += !(buf[j] & 0x20) ? 1 : !(buf[j] & 0x10) ? 2 : 3;
  return j;
}

unsigned int uni_utf8_index_of(const uint8_t *buf, unsigned int n)
{
  unsigned int i, j;
  for(i = j = 0; j < n; i++, j++)
    if(buf[j] >= 0x80)
      j += !(buf[j] & 0x20) ? 1 : !(buf[j] & 0x10) ? 2 : 3;
  return i;
}
@

A modified version of the above index functions can be used to compute
the length of a zero-terminated string.  Only the loop end condition
needs to change.  Unlike the above routines, a 32-bit version is
useful as well.

<<Unicode I/O Exports>>=
unsigned int uni_utf32_strlen(const uint32_t *buf);
unsigned int uni_utf16_strlen(const uint16_t *buf);
unsigned int uni_utf8_strlen(const uint8_t *buf);
@

<<Unicode I/O functions>>=
unsigned int uni_utf32_strlen(const uint32_t *buf)
{
  unsigned int i;
  for(i = 0; buf[i]; i++);
  return i;
}

unsigned int uni_utf16_strlen(const uint16_t *buf)
{
  unsigned int i, j;
  for(i = j = 0; buf[j]; i++, j++)
    if(buf[j] >= 0xd800 && buf[j] < 0xdc00)
      j++;
  return i;
}

unsigned int uni_utf8_strlen(const uint8_t *buf)
{
  unsigned int i, j;
  for(i = j = 0; buf[j]; i++, j++)
    if(buf[j] >= 0x80)
      j += !(buf[j] & 0x20) ? 1 : !(buf[j] & 0x10) ? 2 : 3;
  return i;
}
@

To skip around in a string, simply avoid all continuation characters.
Macros are provided to make this as simple and efficient as possible. 
The UTF-16 macros are not much less efficient, if at all, if called
either at the start or the end of a multi-word character.  However,
the utf-8 macro for the possibility of a middle of a word is more
complex, and I prefer things to be orthogonal, so the next/prev macros
assume the argument points to the start of a character, and an extra
macro finds the start of a character.  Each takes a buffer pointer,
and returns the number to add to the buffer pointer to get to the
chosen destination.  For consistency, UTF-32 functions are provided as
well.

<<Unicode I/O Exports>>=
int uni_utf32_startc(const uint32_t *buf);
#define uni_utf32_startc(buf) 0
int uni_utf32_nextc(const uint32_t *buf);
#define uni_utf32_nextc(buf) 1
int uni_utf32_prevc(const uint32_t *buf);
#define uni_utf32_prevc(buf) -1
int uni_utf16_startc(const uint16_t *buf);
#define uni_utf16_startc(buf) (*(buf) >= 0xdc00 && *(buf) < 0xe000 ? -1 : 0)
int uni_utf16_nextc(const uint16_t *buf);
#define uni_utf16_nextc(buf) (*(buf) < 0xd800 || *(buf) >= 0xdc00 ? 1 : 2)
int uni_utf16_prevc(const uint16_t *buf);
#define uni_utf16_prevc(buf) ((buf)[-1] < 0xd800 || (buf)[-1] >= 0xe000 ? -1 : -2)
int uni_utf8_startc(const uint8_t *buf);
#define uni_utf8_startc(buf) (*(buf) < 0x80 || (*(buf) & 0x40) ? 0 : \
                               ((buf)[-1] & 0x40) ? -1 : \
			       ((buf)[-2] & 0x40) ? -2 : -3)
int uni_utf8_nextc(const uint8_t *buf);
#define uni_utf8_nextc(buf) (*(buf) < 0x80 ? 1 : !(*(buf) & 0x20) ? 2 : \
                              !(*(buf) & 0x10) ? 3 : 4)
int uni_utf8_prevc(const uint8_t *buf);
#define uni_utf8_prevc(buf) (uni_utf8_startc((buf) - 1) - 1)
@

There is some performance benefit to decoding UTF-16 and UTF-8 without
error checking.  To emphasize that these are only for internally
generated, guaranteed valid strings, the functions to do this have
``valid'' in their name.  In addition, since errors are never
returned, the return value is unsigned.

<<Unicode I/O Exports>>=
uint32_t uni_valid_utf16_decode(const uint16_t *s, unsigned int *nread, int bige);
uint32_t uni_valid_utf8_decode(const uint8_t *s, unsigned int *nread);
@

<<Unicode I/O functions>>=
uint32_t uni_valid_utf16_decode(const uint16_t *s, unsigned int *nread, int bige)
{
  uint16_t w = bige ? be16toh(*s) : le16toh(*s);
  if(w < 0xd800 || w >= 0xe000) {
    if(nread)
      *nread = 1;
    return w;
  }
  if(nread)
    *nread = 2;
  uint16_t w2 = bige ? be16toh(s[1]) : le16toh(s[1]);
  return (w2 & 0x3ff) + ((uint32_t)(w & 0x3ff) << 10) + 0x10000;
}

uint32_t uni_valid_utf8_decode(const uint8_t *s, unsigned int *nread)
{
  if(*s < 0x7f) {
    if(nread)
      *nread = 1;
    return *s;
  }
  uint32_t c = *s++, ec;
  if((c & 0xf8) == 0xf0) {
    c &= 0x07;
    ec = *s++;
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if(nread)
      *nread = 4;
    c = (c << 6) + (ec & 0x3f);
    return c;
  } else if((c & 0xf0) == 0xe0) {
    c &= 0x0f;
    ec = *s++;
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if(nread)
      *nread = 3;
    c = (c << 6) + (ec & 0x3f);
    return c;
  } else /* if((c & 0xe0) == 0xc0) */ {
    c &= 0x1f;
    ec = *s++;
    if(nread)
      *nread = 2;
    c = (c << 6) + (ec & 0x3f);
    return c;
  }
}
@

Actually, since internal UTF-32 has the underlying architecture's
natural endianness, the UTF-16 internal routines should use this as
well.  These have ``int'' in the name to distinguish themselves.  For
consistency, UTF-8 and UTF-32 versions of these functions are
provided, as well.  Since all but the UTF-8 routines are simple, they
are all in-lined.  The UTF-16 versions do this by being a static
function.

<<Unicode I/O Exports>>=
int uni_int_utf32_encode(uint32_t *buf, uint32_t cp);
#define uni_int_utf32_encode(buf, cp) ((*(buf) = cp), 1)
/* make sure at least 2 words avail in buf */
/* returns # of words written */
#ifdef __GNUC__
__attribute__((unused))
#endif
static int uni_int_utf16_encode(uint16_t *buf, uint32_t cp)
{
  if(cp < 0x10000) {
    *buf = cp;
    return 1;
  }
  cp -= 0x10000;
  *buf++ = 0xd800 + (cp >> 10);
  *buf = 0xdc00 + (cp & 0x3ff);
  return 2;
}
int uni_int_utf16_encode(uint16_t *buf, uint32_t cp);
int uni_int_utf8_encode(uint8_t *buf, uint32_t cp);
#define uni_int_utf8_encode uni_utf8_encode
@

<<C Prototypes>>=
int uni_int_utf16_encode(uint16_t *buf, uint32_t cp);
@

<<Unicode I/O Exports>>=
#ifdef __GNUC__
__attribute__((unused))
#endif
static uint32_t uni_int_utf16_decode(const uint16_t *s, unsigned int *nread)
{
  uint16_t w = *s;
  if(w < 0xd800 || w >= 0xe000) {
    if(nread)
      *nread = 1;
    return w;
  }
  if(nread)
    *nread = 2;
  return (s[1] & 0x3ff) + ((uint32_t)(w & 0x3ff) << 10) + 0x10000;
}
/* gcc warns (-Waddress) if nr is constant */
/* #define uni_int_utf32_decode(buf, nr) ((nr) ? (*(int *)(nr) = 1), *(buf) : *(buf)) */
#ifdef __GNUC__
__attribute__((unused))
#endif
static uint32_t uni_int_utf32_decode(const uint32_t *s, unsigned int *nread)
{
  if(nread)
    *nread = 1;
  return *s;
}
uint32_t uni_int_utf8_decode(const uint8_t *s, unsigned int *nread);
#define uni_int_utf8_decode uni_valid_utf8_decode
@

<<C Prototypes>>=
uint32_t uni_int_utf32_decode(const uint32_t *s, unsigned int *nread);
uint16_t uni_int_utf16_decode(const uint16_t *s, unsigned int *nread);
@

All functions developed from this point forward which take only one
character for input always take UTF-32; since they would need to do
the translation anyway, it is no burden to require the calling of one
of the above two functions to decode the other formats.  Functions
which take more than one character for input should also take
(guaranteed) valid UTF-16 and UTF-8.  Similarly, functions which
return just one code point always return UTF-32, but those which
return more than one can return UTF-16 or UTF-8 as well.  Returning
UTF-32 for single characters eliminates the need for output buffer
management.

There are at least three different ways to return the potentially
multi-character results: return a pointer to a constant string, fill
in a provided return buffer of fixed size (up to its length), or fill
in a provided buffer of variable size (expanding using [[realloc]] if
necessary).  Since constant strings are only one format, they can
really only be used internally.  The choice between the other two
depends largely on how much effort is required to compute the results
repeatedly:  the function must be called twice if the return buffer is
too small.  Of course if memory management makes the use of
[[realloc]] inappropriate, forcing the third method is not a good
idea, either.  So, for consistency, a hybrid of the latter two methods
is used.  Each output buffer is passed in using three parameters: a
pointer to a pointer to the buffer, an integer offset to where the
results should be placed (appended), and a pointer to the current
buffer length.  If the offset is less than zero, the buffer is never
expanded.  As a special shortcut, a fixed-length buffer of size zero
can be indicated with a buffer length pointer of [[NULL]], and the
zero-length buffer need not be a valid pointer, either.  Rather than
use my [[resize]] macro for this, the function sets the output buffer
to [[NULL]] on memory allocation errors.  The number of words which
would have been returned, had the buffer been large enough, is
returned. The following helpers are provided to implement this policy.

Constant strings are always the input; currently only UTF-32 and
UTF-16 input is supported.  UTF-32 support is there because it is
easy, and UTF-16 support is there because it is not much harder, and
takes less space on average.

<<Buffer return parameters for UTF-(@sz)>>=
uint<<@sz>>_t **buf, int off, unsigned int *buf_len
@

<<[[uni_returnN_buf]](@sz) proto>>=
uint32_t uni_return<<@csz>>_buf<<@sz>>(const uint<<@csz>>_t *str, uint32_t len,
                                <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<[[uni_return]](@csz)[[_bufN]] protos>>=
<<[[uni_returnN_buf]][[32]] proto>>;
<<[[uni_returnN_buf]][[16]] proto>>;
<<[[uni_returnN_buf]][[8]] proto>>;
@

<<Unicode I/O Exports>>=
<<[[uni_return]][[32]][[_bufN]] protos>>
<<[[uni_return]][[16]][[_bufN]] protos>>
<<[[uni_return]][[8]][[_bufN]] protos>>
@

<<Unicode I/O functions>>=
<<[[uni_return]][[32]][[_bufN]]>>
<<[[uni_return]][[16]][[_bufN]]>>
<<[[uni_return]][[8]][[_bufN]]>>
@

<<[[uni_return]](@csz)[[_bufN]]>>=
<<[[uni_returnN_buf]][[32]]>>
<<[[uni_returnN_buf]][[16]]>>
<<[[uni_returnN_buf]][[8]]>>
@

<<[[uni_returnN_buf]](@sz)>>=
<<[[uni_returnN_buf]][[<<@sz>>]] proto>>
{
  unsigned int i, rlen;
  
  /* not very efficient, I guess: always calculate output len first */
  <<Calculate result length [[rlen]] for [[str]] of length [[len]]>>
  /* now, store into output buffer */
  if(off < 0) {
    <<Store result into constant-length [[*buf]] of length [[*buf_len]]>>
    return rlen;
  }
  <<Store result into resizable [[*buf]] of length [[*buf_len]] at [[off]]>>
  return rlen;
}
@

<<Calculate result length [[rlen]] for [[str]] of length [[len]]>>=
if(<<@csz>> == <<@sz>>)
  rlen = len;
else
  for(i = rlen = 0; i < len; ) {
    unsigned int clen;
#ifdef __GNUC__ /* shut up gcc warning for UTF-32 */
    __attribute__((unused))
#endif
    uint32_t c = uni_int_utf<<@csz>>_decode(str + i, &clen);
    i += clen;
    rlen += uni_utf<<@sz>>_enclen(c);
  }
@

<<Store result into constant-length [[*buf]] of length [[*buf_len]]>>=
/* store in constant-length buffer of length *buf_len */
len = buf_len ? *buf_len : 0;
uint<<@sz>>_t *bptr;
if(len > rlen)
  len = rlen;
if(!len)
  return rlen;
bptr = *buf;
while(len-- > 0) {
  unsigned int clen;
  uint32_t c = uni_int_utf<<@csz>>_decode(str, &clen);
  str += clen;
  if(len >= 3) { /* room for at least 4 */
    int csz = uni_int_utf<<@sz>>_encode(bptr, c);
    bptr += csz;
    len -= csz - 1;
  } else {
    uint<<@sz>>_t cbuf[4];
    int csz = uni_int_utf<<@sz>>_encode(cbuf, c);
    if(csz > clen + 1)
      csz = clen + 1;
    cpybuf(bptr, cbuf, csz);
    bptr += csz;
    len -= csz - 1;
  }
}
@

<<Store result into resizable [[*buf]] of length [[*buf_len]] at [[off]]>>=
/* allocate space if needed */
if(!*buf)
  *buf_len = 0; /* shouldn't be necessary */
else if(!*buf_len) { /* should never happen */
  free(*buf); /* will probably corrupt memory */
  *buf = NULL;
}
if(rlen + off > *buf_len) {
  if(!*buf_len) {
    unsigned int asize = off + rlen;
    if(asize < 5)
      asize = 5;
    *buf = malloc(asize * sizeof(**buf));
    if(!*buf)
      return rlen;
    *buf_len = asize;
  } else {
    while(*buf_len < rlen + off)
      *buf_len *= 2;
    uint<<@sz>>_t *nbuf = realloc(*buf, *buf_len * **buf);
    if(!nbuf) {
      free(*buf);
      *buf = NULL;
      return rlen;
    }
    *buf = nbuf;
  }
}
/* store result; sufficient space guaranteed */
uint<<@sz>>_t *bptr = *buf + off;
if(<<@sz>> == <<@csz>>)
  cpybuf(bptr, str, rlen);
else {
  len = rlen;
  while(len-- > 0) {
    unsigned int clen;
    uint32_t c = uni_int_utf<<@csz>>_decode(str, &clen);
    str += clen;
    int csz = uni_int_utf<<@sz>>_encode(bptr, c);
    bptr += csz;
    len -= csz - 1;
  }
}
@

\chapter{Properties}

This library's primary purpose is to preparse the Unicode information
and query it efficiently.  The Unicode Character Database defines
numerous properties for characters.  Since there are many properties,
and any particular application may only need a few of them, an attempt
is made to keep each property in a separate object file.  Static
linking will only pull in the required properties, and there is little
penalty for shared libraries which include everything.  Some code is
shared by all properties, though; this is placed in a common file.

<<Library [[uni]] headers>>=
#include "uni_prop.h"
@

<<uni_prop.h>>=
<<Common C Warning>>
#ifndef UNI_PROP_H
#define UNI_PROP_H

#include "uni_io.h"
<<Unicode property exports>>
#endif /* UNI_PROP_H */
@

\lstset{language=make}
<<makefile.rules>>=
uni_prop.h: uni_io.h
@

<<Library [[uni]] Members>>=
uni_prop.o
@

\lstset{language=C}
<<uni_prop.c>>=
<<Common C Header>>
#include "uni_prop.h"
// static_proto

<<Unicode property functions>>
@

Some Unicode property exports are generated, and some are hand-coded.
Some of the hand-coded exports may be useful for generation, so they
are in a separate code chunk, which can be included in the generator
without depending on the generated output. 

<<Unicode property exports>>=
<<Unicode property exports for generator>>
@

<<Unicode property functions>>=
<<Unicode property functions for generator>>
@

These properties come in a variety of formats, but for the purpose of
this library, they may be generally classified according to the type
of value:  Boolean, String, Enumerated, or Numeric.  Each class of
values has a particular set of desirable operations related to a
property of that class.

\section{Boolean Properties}

Boolean properties are either true or false.  They define a subset of
characters: those for which the value is true.  As with any set, the
desirable operations are:

% Begin-doc op-table
%% l2h substitution ominus &ominus;
% tidy doesn't understand &ominus
%% l2h substitution ominus &#8854;
% substitution replaces # with space, so use 0-arg macro instead
% l2h macro ominus 0 &##8854;
% l2h substitution oplus &oplus;
% l2h substitution wedge &and;
% l2h substitution vee &or;
% l2h substitution neg &not;
% l2h macro overline 1 <span style="text-decoration:overline">#1</span>
% l2h substitution emptyset &empty;
% l2h substitution alpha &alpha;
% l2h substitution in &isin;
% l2h substitution notin &notin;
% l2h substitution cap &cap;
% l2h substitution cup &cup;
% End-doc op-table

\begin{itemize}
\item Finding out if a character \emph{is} or is not an element of the
set.
\item Querying the \emph{members} of a set.
\item Applying set \emph{operators} on a pair of sets.  There are
16 possible set operations between two sets (each potential member is
either in or not in each of the 2 sets = $2^2$ membership
combinations; member is either in or not in result = $4^2$ total
combinations).  See table \ref{tab:setop} for a complete list of
operations.

% Begin-doc op-table
\begin{table}
\begin{quote}
\begin{tabular}{lllllcrll}
$x\in A$&false&false&true&true&&~&\\
$x\in B$&false&true&false&true&&&\\
\emph{op}&\multicolumn{4}{c}{$x\in A\textrm{ \emph{op} }B$}&\emph{when}&\multicolumn{2}{l}{\emph{Op \#}}&\emph{Op Name}\\
$\emptyset$&false&false&false&false&never&0&&NIL\\
$\alpha$&true&true&true&true&always&1&&ALL\\
$A$&false&false&true&true&$x\in A$&2&&A\\
$\overline A$&true&true&false&false&$x\notin A$&3&&INV\_A NOT\_A\\
$B$&false&true&false&true&$x\in B$&4&&B\\
$\overline B$&true&false&true&false&$x\notin B$&5&&INV\_B NOT\_B\\
$A\ominus B$&false&true&true&false&$(x\in A)\oplus (x\in B)$&6&&SYM\_DIFF XOR\\
$\overline{A\ominus B}$&true&false&false&true&$(x\in A)\overline\oplus (x\in B)$&7&&INV\_SYM\_DIFF XNOR\\
$A\cap B$&false&false&false&true&$(x\in A)\wedge (x\in B)$&8&&INTER AND\\
$\overline{A\cap B}$&true&true&true&false&$(x\in A)\overline\wedge (x\in B)$&9&&INV\_INTER NAND\\
$A-B$&false&false&true&false&$(x\in A)\wedge (x\notin B)$&10&&A\_MINUS\_B\\
$\overline{A-B}$&true&true&false&true&$(x\notin A)\vee (x\in B)$&11&&INV\_A\_MINUS\_B\\
$B-A$&false&true&false&false&$(x\notin A)\wedge (x\in B)$&12&&B\_MINUS\_A\\
$\overline{B-A}$&true&false&true&true&$(x\in A)\vee (x\notin B)$&13&&INV\_B\_MINUS\_A\\
$A\cup B$&false&true&true&true&$(x\in A)\vee (x\in B)$&14&&UNION OR\\
$\overline{A\cup B}$&true&false&false&false&$(x\in A)\overline\vee (x\in B)$&15&&INV\_UNION NOR\\
\end{tabular}

{\small Operation numbers are assigned using the formula $o_0 \oplus
(x \in A)\wedge o_1 \oplus (x \in B)\wedge o_2 \oplus (x \in A\cap
B)\wedge o_3$, where $o_n$ is true if bit $n$ is one in the operation
number.  Operation names are of the enumeration type [[uni_sop_t]],
and are prefixed with [[UNI_SOP_]].  Where two operation names are
given, either will work.}
\end{quote}
% End-doc op-table
\caption{\label{tab:setop}Set Operations}
\end{table}
\end{itemize}

\subsection{Storage Methods}

All of the above require that every member be stored.  The \emph{is}
test can be stored in a single bit.  However, there are over a million
valid character values, so the total storage for a simple bit array
would still be over 128 kilobytes, even if the array is limited to the
valid code point range.  Some properties only apply to a limited range
of characters, so these might use a little less space.  Even so, with
numerous properties, this adds up quickly, and may even slow things
down by keeping everything out of the cache.  The set
\emph{operations} are easy to perform, but require manipulating up to
384 kilobytes of data.

<<Unicode property exports for generator>>=
/* for bit arrays */
/* max bit number for Unicode */
#define UNI_MAX_CP 0x10ffff
/* find array element index from bit number */
#define UNI_BSET_ELT(a, i) ((i)/(sizeof(*(a))*8))
/* find array element from bit number; can take address of this */
#define UNI_BSET_AELT(a, i) (a)[UNI_BSET_ELT(a, i)]
/* find array element's bit number from full bit number */
#define UNI_BSET_BIT(a, i) ((i)%(sizeof(*(a))*8))
/* find full bit number given array element index and element bit number */
#define UNI_BSET_ENTRY(a, e, b) ((e)*(sizeof(*(a))*8) + b)
/* convert full bit number into single-bit mask for element */
#define UNI_BSET_MASK(a, i) (1ULL << UNI_BSET_BIT(a, i))
/* true if full bit number i is set */
#define UNI_BSET_IS_SET(a, i) ((UNI_BSET_AELT(a, i) & UNI_BSET_MASK(a, i)) != 0)
/* set bit number i */
#define UNI_BSET_SET(a, i) UNI_BSET_AELT(a, i) |= UNI_BSET_MASK(a, i)
/* clear bit number i */
#define UNI_BSET_CLEAR(a, i) UNI_BSET_AELT(a, i) &= ~UNI_BSET_MASK(a, i)
@

<<C Prototypes>>=
uint_ei_type UNI_BSET_ELT(uint_e_type *a, uint_i_type i);
lval uint_e_type UNI_BSET_AELT(uint_e_type *a, uint_i_type i);
uint_b_type UNI_BSET_BIT(uint_e_type *a, uint_i_type i);
uint_i_type UNI_BSET_ENTRY(uint_e_type *a, uint_ei_type e, uint_b_type b);
uint_e_type UNI_BSET_MASK(uint_e_type *a, uint_i_type i);
int UNI_BSET_IS_SET(uint_e_type *a, uint_i_type i);
void UNI_BSET_SET(uint_e_type *a, uint_i_type i);
void UNI_BSET_CLEAR(uint_e_type *a, uint_i_type i);
@

<<[[uni_setop_t]]>>=
typedef enum {
  UNI_SOP_NIL, UNI_SOP_ALL, UNI_SOP_A, UNI_SOP_INV_A,
  UNI_SOP_B, UNI_SOP_INV_B, UNI_SOP_SYM_DIFF, UNI_SOP_INV_SYM_DIFF,
  UNI_SOP_INTER, UNI_SOP_INV_INTER, UNI_SOP_A_MINUS_B, UNI_SOP_INV_A_MINUS_B,
  UNI_SOP_B_MINUS_A, UNI_SOP_INV_B_MINUS_A, UNI_SOP_UNION, UNI_SOP_INV_UNION
} uni_set_op_t;
@

<<Unicode property exports for generator>>=
/* set operation names */
<<[[uni_setop_t]]>>
/* logical operation names */
#define UNI_SOP_NOT_A UNI_SOP_INV_A
#define UNI_SOP_NOT_B UNI_SOP_INV_B
#define UNI_SOP_XOR UNI_SOP_SYM_DIFF
#define UNI_SOP_XNOR UNI_SOP_INV_SYM_DIFF
#define UNI_SOP_AND UNI_SOP_INTER
#define UNI_SOP_NAND UNI_SOP_INV_INTER
#define UNI_SOP_OR UNI_SOP_UNION
#define UNI_SOP_NOR UNI_SOP_INV_UNION

#define UNI_BIT_SET_OP_BIT_(op, n) (~((((op) >> n) & 1) - 1))
#define UNI_BIT_SET_OP(A, op, B) \
  (((A) & UNI_BIT_SET_OP_BIT_(op, 1)) ^ ((B) & UNI_BIT_SET_OP_BIT_(op, 2)) ^ \
   ((A) & (B) & UNI_BIT_SET_OP_BIT_(op, 3)) ^ UNI_BIT_SET_OP_BIT_(op, 0))
@

<<C Prototypes>>=
uint_t UNI_BIT_SET_OP(uint_t A, uni_set_op_t op, uint_t B);
@

<<Known Data Types>>=
uni_set_op_t,%
@

The set \emph{operations} must be performed on arrays of a particular
type.  Since C does not really support generics, and using the
preprocessor would be impractical, the NoWeb parameterized macro
facility is used.  Previous versions of this document wrote the
function definition to an include file, and used the preprocessor to
redefine types and names for four separate [[#include]]s.  That is
probably the only way to get debugable code using the preprocessor.

<<Unicode property exports for generator>>=
#include <stdint.h>
@

<<Unicode property exports>>=
<<[[uni_setop_bit]] proto for [[64]]-bit integers>>; /* uint64_t is not standard, but common */
<<[[uni_setop_bit]] proto for [[32]]-bit integers>>;
<<[[uni_setop_bit]] proto for [[16]]-bit integers>>;
<<[[uni_setop_bit]] proto for [[8]]-bit integers>>;
@

<<[[uni_setop_bit]] proto for (@sz)-bit integers>>=
void uni_setop_bit<<@sz>>(uint<<@sz>>_t *a, uint32_t a_low, uint32_t a_len,
                        uni_set_op_t op,
		        uint<<@sz>>_t *b, uint32_t b_low, uint32_t b_len,
		        uint<<@sz>>_t **res, uint32_t *res_low, uint32_t *res_len,
		        uint32_t *res_max)
@

<<Unicode property functions>>=
<<[[uni_setop_bit]] for [[64]]-bit integers>>
<<[[uni_setop_bit]] for [[32]]-bit integers>>
<<[[uni_setop_bit]] for [[16]]-bit integers>>
<<[[uni_setop_bit]] for [[8]]-bit integers>>
@

<<[[uni_setop_bit]] for (@sz)-bit integers>>=
<<[[uni_setop_bit]] proto for [[<<@sz>>]]-bit integers>>
{
  <<Perform set operation on bit strings>>
}
@

First, there are few degenerate cases which do not involve [[a]]
and/or [[b]].  While the general routine will return the same result,
the logic for the degenerate cases is much simpler, so a value can be
returned immediately.  The $\overline A$ and $\overline B$ operations
are degenerate cases by this definition, but they require a bit of
finesse that is provided by the main routine, anyway.  So, instead of
duplicating that code here, the unused input is simply zeroed to
improve its performance.

<<Perform set operation on bit strings>>=
if(op == UNI_SOP_INV_A)
  b_len = 0;
else if(op == UNI_SOP_INV_B)
  a_len = 0;
@

If both inputs are empty, the operation is determined by applying the
operator to two zeroes.  This will always match bit zero of the
operation number, and will be equivalent to applying $\emptyset$ or
$\alpha$.  If just one is empty, then that reduces the operations to
four: the above two, the non-empty set, or the inverse of the
non-empty set.  The operation number is adjusted to reflect this.

<<Perform set operation on bit strings>>=
int zz_op = /* UNI_BIT_SET_OP(0, op, 0) */ op & UNI_SOP_ALL;
if(!a_len && !b_len)
  op &= UNI_SOP_ALL;
else if(!a_len)
  op &= UNI_SOP_B | UNI_SOP_ALL;
else if(!b_len)
  op &= UNI_SOP_A | UNI_SOP_ALL;
@

The $\emptyset$ operation returns an empty result.  The $\alpha$
operation returns a result with all bits set.  The $A$ and $B$
operations just return copies of the requested input.

<<Perform set operation on bit strings>>=
if(op == UNI_SOP_NIL) {
  *res_len = 0;
  *res_low = 0;
  if(*res_max) {
    *res_max = 0;
    free(*res);
  }
  return;
}
if(op == UNI_SOP_ALL) {
  *res_len = UNI_MAX_CP / <<@sz>> + 1;
  *res_low = 0;
  if(!*res_max)
    inisize(*res, (*res_max = *res_len));
  else if(*res_max < *res_len)
    resize(*res, (*res_max = *res_len));
  memset(*res, ~0, *res_len * <<@sz>> / 8);
  return;
}
if(op == UNI_SOP_A) {
  *res_len = a_len;
  *res_low = a_low;
  if(!*res_max)
    inisize(*res, (*res_max = a_len));
  else if(*res_max < a_len)
    resize(*res, (*res_max = a_len));
  if(*res != a)
    cpybuf(*res, a, *res_len);
  return;
}
if(op == UNI_SOP_B) {
  *res_len = b_len;
  *res_low = b_low;
  if(!*res_max)
    inisize(*res, (*res_max = b_len));
  else if(*res_max < b_len)
    resize(*res, (*res_max = b_len));
  if(*res != b)
    cpybuf(*res, b, *res_len);
  return;
}
@

The bit strings do not necessarily start at the same position.  For
convenience, [[a]] will be reassigned to be the set with the earlier
start position, or only start position if one of the inputs is empty.
If a swap occurs, the operation number must also be adjusted by
swapping the $A$ and $B$ bits.  In addition, if one input is empty, it
is moved past the end of the first so it is not involved in
calculations until the end.

<<Perform set operation on bit strings>>=
if(!a_len || b_low < a_low) {
  uint<<@sz>>_t *t = a;
  a = b;
  b = t;
  a_len ^= b_len;
  b_len ^= a_len;
  a_len ^= b_len;
  a_low ^= b_low;
  b_low ^= a_low;
  a_low ^= b_low;
  op = (op & ~(UNI_SOP_A|UNI_SOP_B)) | ((op & UNI_SOP_A) << 1) |
                                       ((op & UNI_SOP_B) >> 1);
}
if(!b_len)
  b_low = a_low + a_len * <<@sz>>;
@

The bit strings do not necessarily start at zero.  If neither does,
then the region before the bit strings is all zeroes, meaning that the
result will be all zeroes or all ones depending on [[zz_op]].  If it
is all zeroes, the start of the result can be moved up.  In fact, the
start of the result only needs to be set once ones start rolling in,
so a flag is kept to determine if a one has been encountered yet.

<<Perform set operation on bit strings>>=
uint32_t i;
int did_start = 0;

if(a_low > 0 && zz_op) {
  uint32_t need = a_low / <<@sz>>;
  did_start = 1;
  *res_low = 0;
  if(!*res_max)
    inisize(*res, (*res_max = need + 4));
  else if(*res_max < need) {
    while(*res_max < need)
      *res_max *= 2;
    resize(*res, *res_max);
  }
  if(need > 0)
    memset(*res, ~0, need * sizeof(**res));
}
@

If the inputs do not start at the same place, there is an initial
region where only [[a]] has values other than zero.  Rather than
complicate the case where [[b]] is involved by checking for this case,
a separate loop is performed for this region.  If the result start was
set to zero above, the result may be misaligned with [[a]].  If so,
the bits from the previous operation are shifted into place.  Since
the above set all ones, the first previous value is also all ones.
While it is setting values, it also does not actually increase the
result size or set the value if it is zero.  Instead, it keeps track
of the last position that was set to zero after a non-zero set, so
that the result can be truncated with no excess zeroes.  Naturally,
when a new non-zero value comes along, the zeroes need to be filled in
after all.  The first non-zero value also sets the start position, if
it was not already set above.

<<Perform set operation on bit strings>>=
uint<<@sz>>_t cur_a = 0, prev_a, r;
uint32_t last_zero = 0;
int misalign_a = did_start ? (a_low - *res_low) % <<@sz>> : 0;
uint32_t end = a_low + a_len * <<@sz>>;
if(end < b_low)
  end = b_low;
for(; (i = a_low) < end - <<@sz>> + 1; a_low += <<@sz>>, a_len--, a++) {
  <<Obtain next aligned bits from source [[a]]>>
  r = UNI_BIT_SET_OP(prev_a, op, 0);
  <<Set res if non-zero or mark if zero>>
}
@

<<Obtain next aligned bits from source (@which)>>=
prev_<<@which>> = cur_<<@which>>;
cur_<<@which>> = *<<@which>>;
if(misalign_<<@which>>) {
  prev_<<@which>> |= cur_<<@which>> @<< misalign_<<@which>>;
  cur_<<@which>> >>= <<@sz>> - misalign_<<@which>>;
} else
  prev_<<@which>> = cur_<<@which>>;
@

<<Set res if non-zero or mark if zero>>=
if(!r) {
  if(!last_zero && did_start)
    last_zero = i - *res_low;
} else {
  if(!did_start) {
    did_start = 1;
    *res_low = i;
  }
  if(!*res_max)
    inisize(*res, (*res_max = 4));
  else if(*res_max <= (i - *res_low) / <<@sz>>) {
    while(*res_max <= (i - *res_low) / <<@sz>>)
      *res_max *= 2;
    resize(*res, *res_max);
  }
  if(last_zero) {
    memset(&UNI_BSET_AELT(*res, last_zero), 0, (i - *res_low - last_zero) / 8);
    last_zero = 0;
  }
  UNI_BSET_AELT(*res, i - *res_low) = r;
}
@

Now, either [[a]] is pointing past the end of the lower input, or it
is pointing at a word which contains the position [[b_low]].  In the
former case, there may be a(nother) region where both [[a]] and [[b]]
are not present.  If so, the region can be set to [[zz_op]] as before,
except for the first word if there was a misalignment.  In either
case, if there was a misalignment, [[cur_a]] contains some of the bits
to be applied to the next word.  Only full words which do not contain
[[b_low]] are processed here.

<<Perform set operation on bit strings>>=
if(misalign_a && i < b_low - <<@sz>> - 1) {
  r = UNI_BIT_SET_OP(cur_a, op, 0);
  <<Set res if non-zero or mark if zero>>
  i += <<@sz>>;
  cur_a = 0; /* a_len must be 0 */
}
if(!a_len) {
  misalign_a = 0; /* cur_a is aligned, and remaining 0s do not need align */
  if(did_start)
    i -= (i - *res_low) % <<@sz>>; /* align i with result instead of a */
}
if(i < b_low - <<@sz>> - 1) {
  uint32_t skip = (b_low - i) / <<@sz>>;
  skip *= <<@sz>>;
  if(!zz_op) {
    if(!last_zero && did_start)
      last_zero = i - *res_low;
    if(!did_start)
      i = b_low; /* if b causes a start, *res_low should be b_low */
    else
      i += skip;
  } else {
    <<Set gap to all ones>>
  }
}
@

<<Set gap to all ones>>=
if(!did_start) {
  did_start = 1;
  *res_low = i;
}
uint32_t tot = i - *res_low + skip;
if(!*res_max)
  inisize(*res, (*res_max = tot > <<@sz>> ? tot / <<@sz>> : 4));
else if(*res_max <= tot / <<@sz>>) {
  while(*res_max <= tot / <<@sz>>)
    *res_max *= 2;
  resize(*res, *res_max);
}
if(last_zero) {
  memset(&UNI_BSET_AELT(*res, last_zero), 0, (i - *res_low - last_zero) / 8);
  last_zero = 0;
}
memset(&UNI_BSET_AELT(*res, i - *res_low), ~0, skip / 8);
i += skip;
@

Now the position is at the word containing [[b_low]], if [[b]] is
non-empty.  If [[a]] has not yet ended, or ended in the last word,
bits remain in [[cur_a]] for this word.  If it ended earlier than
that, [[cur_a]] is zero, so either way [[cur_a]] needs to be applied. 
Both [[b]] and [[a]] need to be aligned to the result before use.  If
the result has not yet started, it will start aligned with [[a]] if
[[a]] has not yet ended (i.e., [[i]] is an integral [[<<@sz>>]]
multiple following [[a_low]]); otherwise, due to the above code, it
will start aligned with [[b]] (i.e., [[i]] was set to [[b_low]]).
First any full words in common with both [[a]] and [[b]] are
processed; then whichever one has words left is processed.  Note that
it is not possible for [[a]] to have data left to process if [[b]] is
absent.

<<Perform set operation on bit strings>>=
if(b_len) {
  uint32_t misalign_b = did_start ? (b_low - *res_low) % <<@sz>> : 
                                    (b_low - i) % <<@sz>>;
  uint<<@sz>>_t cur_b = 0, prev_b;
  for(; a_len && b_len; b++, b_len--, b_low += <<@sz>>, i += <<@sz>>,
                        a++, a_len--, a_low += <<@sz>>) {
    <<Obtain next aligned bits from source [[b]]>>
    <<Obtain next aligned bits from source [[a]]>>
    r = UNI_BIT_SET_OP(prev_a, op, prev_b);
    <<Set res if non-zero or mark if zero>>
  }
  for(; b_len; b++, b_len--, b_low += <<@sz>>, i += <<@sz>>) {
    <<Obtain next aligned bits from source [[b]]>>
    r = UNI_BIT_SET_OP(cur_a, op, prev_b);
    <<Set res if non-zero or mark if zero>>
    cur_a = 0;
  }
  for(; a_len; a++, a_len--, a_low += <<@sz>>, i += <<@sz>>) {
    <<Obtain next aligned bits from source [[a]]>>
    r = UNI_BIT_SET_OP(prev_a, op, cur_b);
    <<Set res if non-zero or mark if zero>>
    cur_b = 0;
  }
  if(cur_a || cur_b) {
    r = UNI_BIT_SET_OP(cur_a, op, cur_b);
    <<Set res if non-zero or mark if zero>>
    i += <<@sz>>;
  }
}
@

Finally, if the upper set did not end at the last valid code point,
the upper region needs to be set the same way as the initial region.

<<Perform set operation on bit strings>>=
if(zz_op) {
  if(did_start)
    i -= (i - *res_low) % <<@sz>>; /* align i with result */
  if(i <= UNI_MAX_CP) {
    uint32_t skip = (UNI_MAX_CP - i) / <<@sz>> + 1;
    skip *= <<@sz>>;
    <<Set gap to all ones>>
  }
}
@

The only thing remaining is to update the length of the result.

<<Perform set operation on bit strings>>=
if(did_start)
  *res_len = ((last_zero ? last_zero : i) - *res_low) / <<@sz>>;
else
  *res_len = 0;
@

A plain bit array also makes it difficult to perform the
\emph{members} operation: every single potential member needs to be
tested.  Other storage strategies are needed.

<<Unicode property exports>>=
/* List members enumeration not directly supported */
@

<<Unicode property functions>>=
#if 0
  <<List bit array members>>
#endif
@

<<List bit array members>>=
int i; elt_t e;
for(i = 0; i < tab_size; i++) {
  e = tab[i];
  while(e) {
    /* strings.h usually selects optimal built-in implementation of ffs() */
    int b = ffs(e); /* or ffsl, or ffsll if glibc */
    add_ent(UNI_BSET_ENTRY(tab, i, b - 1));
    e &= ~(1 << (b - 1));
  }
}
@

One of the simplest strategies is to provide a sorted table of
members.  Membership is tested by binary search, so the speed of
lookup is proportional to $\log_2(N)$, where $N$ is the number of set
members.  Querying the \emph{members} of the set is trival: the table
is already the list.  Applying set \emph{operations} only requires
looking at the members already present, with the exception of the
odd-numbered ones:  inversion may be more expensive than with bit
arrays.  This does not save much space for properties which apply to
a large number of characters; in fact, it might even take more space.

<<Unicode property exports for generator>>=
int uni_cmp_cp(const void *a, const void *b); /* for qsort/bsearch */
@

<<Unicode property exports>>=
#define uni_is_cp(c, tab, tab_len) \
  bsearch(&c, tab, tab_len, sizeof(tab[0]), uni_cmp_cp)

/* members are already in list */
/* set operations not directly supported */
@

<<C Prototypes>>=
int uni_is_cp(elt_t c, elt_t *tab, size_t tab_len);
@

<<Unicode property functions for generator>>=
int uni_cmp_cp(const void *a, const void *b)
{
  return *(int32_t *)a - *(int32_t *)b;
}
@

<<Unicode property functions>>=
#if 0
  <<Perform set ops on cp array>>
#endif
@

<<Perform set ops on cp array>>=
uint32_t i = 0;
uint32_t tab[max], tab_len = 0;
int invert = /* UNI_BIT_SET_OP(0, op, 0) */ op & 1;
uint32_t aptr, bptr;
for(aptr = bptr = 0; aptr < a_len || bptr < b_len; ) {
  int res;
  uint32_t cp;
  if(bptr == b_len || (aptr < a_len && a[aptr] < b[bptr])) {
    res = UNI_BIT_SET_OP(~0, op, 0);
    cp = a[aptr++];
  } else if(aptr == a_len || (bptr < b_len && a[aptr] > b[bptr])) {
    res = UNI_BIT_SET_OP(0, op, ~0);
    cp = b[bptr++];
  } else {
    res = UNI_BIT_SET_OP(~0, op, ~0);
    cp = a[aptr++];
    bptr++;
  }
  if(invert)
    while(i < cp)
      tab[tab_len++] = i++;
  if(res)
    tab[tab_len++] = cp;
}
if(invert)
  while(i < max)
    tab[tab_len++] = i++;
@

A possible improvement on that strategy is to store ranges instead of
individual code points.  This relies on the fact that most properties
apply to large consecutive ranges; if this is not true, then this is
actually worse than storing individual code points.  Whenever the
number of ranges is less than half of the number of individual code
points, this table will be smaller, requiring less time to search, and
less time to perform set \emph{operations}.  In particular, negation
is usually much faster with a range table than with an individual code
point table.  Not only can absent members be skipped, but present
members within a range can be skipped as well.

<<[[uni_chrrng_t]]>>=
typedef struct {
    uint32_t low, high;
} uni_chrrng_t;
@

<<Unicode property exports for generator>>=
<<[[uni_chrrng_t]]>>
int uni_cmprng(const void *a, const void *b); /* for qsort/bsearch */
int uni_is_cp_chrrng(uint32_t cp, const uni_chrrng_t *tab, uint32_t tab_len);
@

<<Known Data Types>>=
uni_chrrng_t,%
@

<<Unicode property functions for generator>>=
int uni_cmprng(const void *a, const void *b)
{
    const uni_chrrng_t *_a = a, *_b = b;

    if(_a->high < _b->low)
        return -1;
    else if(_b->high < _a->low)
        return 1;
    else
        return 0;
}

int uni_is_cp_chrrng(uint32_t cp, const uni_chrrng_t *tab, uint32_t tab_len)
{
#if 0
  uni_chrrng_t cr = {cp, cp};
  return bsearch(&cr, tab, tab_len, sizeof(uni_chrrng_t), uni_cmprng) ? 1 : 0;
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > tab[j].high)
      l = j + 1;
    else
      return 1;
  }
  return 0;
#endif
}
@

<<Unicode property exports for generator>>=
/* members are already in list, albeit in compact form */
void uni_chrrng_setop(const uni_chrrng_t *a, uint32_t a_len, uni_set_op_t op,
                      const uni_chrrng_t *b, uint32_t b_len,
		      uni_chrrng_t **rtab, uint32_t *r_len);
@

While it would be possible to invert in place (and even use the same
array as the input), if there is enough room, it's easier to just
generate a new array.  At most one range is added (all the ranges
below the current ranges plus one above the top-most range), and at
most one range is removed (replaced by ranges below if top-most is at
top, except if one is already at the bottom).

<<Unicode property functions for generator>>=
static void uni_chrrng_invert(const uni_chrrng_t *a, uint32_t a_len,
		              uni_chrrng_t **rtab, uint32_t *r_len)
{
  uni_chrrng_t *tab;
  uint32_t tab_len = 0;
  inisize(tab, a_len + 1);
  uint32_t i;
  if(a_len && a[0].low) {
    tab[0].low = 0;
    tab[0].high = a[0].low - 1;
    tab_len++;
  }
  for(i = 0; i < a_len - 1; i++, tab_len++) {
    tab[tab_len].low = a[i].high + 1;
    tab[tab_len].high = a[i + 1].low - 1;
  }
  if(a_len && a[a_len - 1].high < UNI_MAX_CP) {
    tab[tab_len].low = a[a_len - 1].high + 1;
    tab[tab_len].high = UNI_MAX_CP;
    tab_len++;
  }
  *rtab = tab;
  *r_len = tab_len;
}
@

For the general operation, as with ordinary bit sets, a few degenerate
cases are handled immediately.  Again, rather than supporting a
previously allocated result structure, a new one is always allocated.
Unlike inversion, it is very likely that this is necessary in order to
avoid overwriting either one of the inputs when they are misaligned.

The generic routine operates by finding points where either input
changes state.  Then, the entire range where the the states where the
same get the result from a single operation call whose result is
almost staticaly calculated using constant inputs.

<<Unicode property functions for generator>>=
void uni_chrrng_setop(const uni_chrrng_t *a, uint32_t a_len, uni_set_op_t op,
                      const uni_chrrng_t *b, uint32_t b_len,
		      uni_chrrng_t **rtab, uint32_t *r_len)
{
  uni_chrrng_t *tab;
  uint32_t tab_len = 0, max_tab = 8;
  <<Shortcut all ops that don't involve both range table sets>>
  inisize(tab, max_tab);
  int invert = /* UNI_BIT_SET_OP(0, op, 0); */ op & 1;
  uint32_t aptr = 0, bptr = 0;
  uint32_t alow, ahigh, blow, bhigh;
  <<Set low and high for [[a]]>>
  <<Set low and high for [[b]]>>
  int32_t lasthigh = -1, curlow = -1, curhigh = -1;
  while(alow <= UNI_MAX_CP || blow <= UNI_MAX_CP) {
    uint32_t nextlow, nexthigh, res;
    <<Find next range where either [[a]] or [[b]] is different>>
    /* into nextlow-nexthigh and compute bit op as well (into res) */
    /* Invert the skipped range (where a=b=0) if necessary */
    if(invert && nextlow > lasthigh + 1) {
      if(curlow < 0)
        curlow = lasthigh + 1;
      curhigh = nextlow - 1;
    }
    /* store res into result */
    if(res) {
      /* extend or create current range */
      if(curlow < 0)
        curlow = nextlow;
      curhigh = nexthigh;
    } else if(curlow >= 0) {
      /* store & finish current range */
      if(tab_len == max_tab)
        resize(tab, max_tab *= 2);
      tab[tab_len].low = curlow;
      tab[tab_len++].high = curhigh;
      curlow = -1;
    }
    lasthigh = nexthigh;
  }
  /* Invert the final range (where a=b=0) if necessary */
  if(invert && lasthigh < UNI_MAX_CP) {
    if(curlow < 0)
      curlow = lasthigh + 1;
    curhigh = UNI_MAX_CP;
  }
  /* store & finish the current range */
  if(curlow >= 0) {	
    if(tab_len == max_tab)
      resize(tab, max_tab *= 2);
    tab[tab_len].low = curlow;
    tab[tab_len++].high = curhigh;
  }
  *rtab = tab;
  *r_len = tab_len;
}
@

<<Set low and high for (@x)>>=
<<@x>>low = <<@x>>ptr < <<@x>>_len ? <<@x>>[0].low : UNI_MAX_CP + 1;
<<@x>>high = <<@x>>ptr < <<@x>>_len ? <<@x>>[0].high : UNI_MAX_CP + 1;
@

<<Find next range where either [[a]] or [[b]] is different>>=
if(alow < blow) {
  nextlow = alow;
  res = UNI_BIT_SET_OP(~0, op, 0);
  if(ahigh < blow) {
    nexthigh = ahigh;
    aptr++;
    <<Set low and high for [[a]]>>
  } else {
    nexthigh = blow - 1;
    alow = blow;
  }
} else if(alow > blow) {
  nextlow = blow;
  res = UNI_BIT_SET_OP(0, op, ~0);
  if(bhigh < alow) {
    nexthigh = bhigh;
    bptr++;
    <<Set low and high for [[b]]>>
  } else {
    nexthigh = alow - 1;
    blow = alow;
  }
} else { /* alow == blow */
  nextlow = alow;
  res = UNI_BIT_SET_OP(~0, op, ~0);
  nexthigh = ahigh;
  if(ahigh <= bhigh) {
    aptr++;
    <<Set low and high for [[a]]>>
  } else
    alow = bhigh + 1;
  if(bhigh <= nexthigh) {
    nexthigh = bhigh;
    bptr++;
    <<Set low and high for [[b]]>>
  } else
    blow = nexthigh + 1;
}
@

Any operations involving just one set are done immediately.  Although
it would be possible to shortcut operations against empty (or full)
sets as well, the slower generic routines are used instead of trying
to test for those cases.

<<Shortcut all ops that don't involve both range table sets>>=
if(op == UNI_SOP_NIL) {
  inisize(tab, 1); /* so free always works */
  *rtab = tab;
  *r_len = 0;
  return;
} else if(op == UNI_SOP_ALL) {
  inisize(tab, 1);
  tab[0].low = 0;
  tab[0].high = UNI_MAX_CP;
  *rtab = tab;
  *r_len = 1;
  return;
} else if(op == UNI_SOP_A) {
  inisize(tab, a_len);
  cpybuf(tab, a, a_len * sizeof(*a));
  *rtab = tab;
  *r_len = a_len;
  return;
} else if(op == UNI_SOP_INV_A) {
  uni_chrrng_invert(a, a_len, rtab, r_len);
  return;
} else if(op == UNI_SOP_B) {
  inisize(tab, b_len);
  cpybuf(tab, b, b_len);
  *rtab = tab;
  *r_len = b_len;
  return;
} else if(op == UNI_SOP_INV_B) {
  uni_chrrng_invert(b, b_len, rtab, r_len);
  return;
}
@

Another possible simple improvement is to store fixed ranges, for
example 32 characters at at time, with a bit array for that range.
Again, two words are required per entry (start and bit mask), so this
only saves storage space and time if each block has on average two or
more bits set.  This loses the range advantage of being able to skip
large contiguous groups of present members, but also partly loses the
range disadvantage of requiring contiguous members for efficiency.  It
also partly gains the disadvantage of requiring scanning of bit arrays
to produce the \emph{members} list.  No sample implementation is
provided here.

A more complex possible improvement is to split the original bit array
into a multi-level table, with duplicate subtables shared.  The bit
array is partitioned into chunks; these are stored at the lowest
level.  Higher levels are just tables of pointers to lower levels.
This increased indirection is less efficient than the raw bit array,
but having two pointers to the same subtable can significantly reduce
storage requirements.  Note that another name for this structure is an
optimized prefix tree (aka optimized trie) on the code point bit string.

The multi-level table approach requires fewer lookups than the code
point arrays (just as many table lookups as there are levels, vs. a
binary search against a long range list that might take, say, 10 or
more table probes), but even when using space-saving measures will
likely still produce larger tables than even unoptimized range tables
(range tables could be optimized for space by using only 24 bits per
code point, for example.) To share duplicate subtables, additional
management of a list of unique lower-level tables at each level, along
with their users, is required. Making a change of any kind is a
non-trivial operation.  This additional storage and computational
expense makes these more suited to static one-time generation.  On the
other hand, their straight lookup performance benefit over ranged
tables is three-fold or more for many Unicode tables, without a need
to do any special optimizations.

There are probably very efficient algorithms to generate efficient
multi-level tables, but I am not aware of them.  The optimal number of
levels and level size is difficult to find, but a pretty good number
can be found by taking each level in turn, and finding the size which
produces the minimum total length taking into account all uniques and
the length of the array at the next level.  Of course the size of the
next level is determined by redundancy elimination as well, so this
requires a recursive search with backtracking.  In addition, having
too many levels eliminates some of the performance benefit, so the
search depth cannot be infinite.%
\footnote{Then again, a specific structure could be imposed,
eliminating the need for a search, but not always selecting the
optimal size.  For example, choosing a 3-level X/16/8 table might be
good enough for most cases.  However, in order to gain the space
savings, the routine would still need to perform a lot of block
comparisons to eliminate redundancy.  If it's going to take a lot of
time and memory anyway, why not go all the way?}
The initial bit array may be a subset of the desired values, in which
case an initial range can (actually, must) be specified as well.  When
a value lies outside of that range, it can be given a default;
currently only all zeroes and all ones are supported. The structure
may also store multiple bit arrays at once, in which case it may be
necessary to keep more than one byte together contiguously at the
lowest level; this may be specified using [[minwidth]].  By default,
[[minwidth]] can never be lower than 4, since 32-bit words are used.

<<Unicode property exports for generator>>=
void uni_bits_to_multi(const uint8_t *bits, uint32_t len, uint32_t low,
                       uint32_t high, uint8_t def, uint32_t minwidth,
		       <<Multi-level table type>> **ml, uint32_t *ml_len);
/* return: 0 == all 0, ~0 == all 1, 1 == *ret is ptr to data */
/* *ret is also set to NULL for 0 and ~0 returns */
int uni_multi_tab_lookup(const <<Multi-level table type>> *dat, uint32_t val,
                         const uint8_t **ret, uint8_t def);
@

<<Unicode property functions for generator>>=
#define UNI_MAX_MULTILEV 3
void uni_bits_to_multi(const uint8_t *bits, uint32_t len, uint32_t low,
                       uint32_t high, uint8_t def, uint32_t minwidth,
		       <<Multi-level table type>> **ml, uint32_t *ml_len)
{
  <<Split [[bits]] into multi-level table [[*ml]]>>
}
@

<<Unicode property functions>>=
int uni_multi_tab_lookup(const <<Multi-level table type>>*dat, uint32_t val,
                         const uint8_t **ret, uint8_t def)
{
  <<Find multi-level table entry [[val]]>>
}
@

Recursion is actually accomplished using a backtracking stack.

<<Split [[bits]] into multi-level table [[*ml]]>>=
uint32_t i;
<<Saved multi-level table information>>
int curlev = 0;
/* The block size of the block currently being worked on */
uint32_t curblk[UNI_MAX_MULTILEV];

curblk[0] = 0;
while(1) {
  <<Unsaved multi-level table information>>

  <<Compute next [[curblk[curlev]]]>>
  if(!curblk[curlev]) {
    if(!curlev--)
      break;
    <<Free [[curlev]]>>
    continue;
  }
  <<Compute level [[curlev]]>>
  <<Create next level bit array>>
  ++curlev;
  curblk[curlev] = 0;
  <<Save or free [[curlev]]>>
  if(curlev == UNI_MAX_MULTILEV - 1) {
    curlev--;
    <<Free [[curlev]]>>
  }
}
@

The minimum block size is twice the pointer size at the next level.
Any smaller, and the next level could just as well be a copy.
Actually, any smaller than the architecture's memory alignment may
cause performance issues.  For now, this is set to a minimum of 4. 
The pointer size at the next level is 1, 2, or 4 depending on the
number of blocks at this level.  The maximum block size is one half
the total size; if the total size is not binary, the data is padded
with zeroes.

<<Saved multi-level table information>>=
/* raw data array for current set of levels */
uint8_t *lev[UNI_MAX_MULTILEV] = { (uint8_t *)bits };
uint32_t curlen[UNI_MAX_MULTILEV] = { len };
@

<<Unsaved multi-level table information>>=
uint32_t blks;
@

<<Compute next [[curblk[curlev]]]>>=
if(!curblk[curlev]) {
  if(curlen[curlev] <= 256 * 2)
#if 0 /* too small, really */
    curblk[curlev] = 2;
#else
    curblk[curlev] = 4;
#endif
  else if(curlen[curlev] <= 256 * 256 * 4)
    curblk[curlev] = 4;
  else
    curblk[curlev] = 8;
  if(!curlev && curblk[curlev] < minwidth)
    curblk[curlev] = minwidth;
} else {
  curblk[curlev] *= 2;
  if(curblk[curlev] >= curlen[curlev])
    curblk[curlev] = 0;
}
if(curblk[curlev])
  blks = (curlen[curlev] + curblk[curlev] - 1) / curblk[curlev];
@

For each pass, a new pointer array is created.  While this will
eventually be reduced to the pointer size, 32-bit pointers are used
during the search.  In addition, a side array is created to store only
the unique block indices.  This alone prevents having to scan the
entire return array for block matches, speeding things up orders of
magnitude with sparse data.  Additionally, this array is kept sorted,
allowing binary searching to further reduce the number of comparisons
needed.  Using a hash table for this would require extra storage, and
may speed things up further.  In any case, each block is simply added
to the pointer array, checking first for duplicates.  As another
special compensation for sparse (or dense) arrays, all-zero entries
are always encoded as the pointer zero, and all-one entries are always
encoded as the maximum pointer value (all ones).  Not only does this
save a tiny bit of space, but scanning through dense or sparse
sections of the array can be much faster.  Care must be taken during
the comparisons to not access data past the end of the actual bit
array, instead treating them as zeroes.

<<Unsaved multi-level table information>>=
uint32_t j;
uint32_t *ptr, *ublocks, nublocks = 0;
uint8_t *data = lev[curlev];
uint32_t blklen, shortlen;
@

<<Compute level [[curlev]]>>=
blklen = shortlen = curblk[curlev];
ptr = malloc(blks * sizeof(*ptr));
ublocks = malloc(blks * sizeof(*ublocks));
for(i = 0; i < blks; i++) {
  int h, l;
  /* first, check for 0 or 1 */
  int is0 = 1, is1 = 1;
  if(i == blks - 1) {
    shortlen = curlen[curlev] % blklen;
    if(!shortlen)
      shortlen = blklen;
  }
  for(l = 0; l < shortlen && (is0 || is1); l++) {
    if(data[blklen * i + l])
      is0 = 0;
    if(data[blklen * i + l] != (uint8_t)~0)
      is1 = 0;
  }
  if(is0 && (!def || shortlen == blklen)) {
    ptr[i] = 0;
    continue;
  }
  if(is1 && (def || shortlen == blklen)) {
    ptr[i] = ~0;
    continue;
  }
  h = nublocks - 1;
  l = 0;
  while(l <= h) {
    j = (l + h) / 2;
    /* comparing in reverse order to make shortlen cmp easier */
    int c = memcmp(data + blklen * ublocks[j], data + blklen * i, shortlen);
    if(!c) {
      uint32_t k;
      if(shortlen == blklen)
        break;
      /* make c > 0 if there is any non-0 element in ublock */
      for(k = shortlen; k < blklen && !c; k++)
        c = data[blklen * ublocks[j] + k];
      if(!c)
        break;
    }
    if(c > 0)
      h = j - 1;
    else
      l = j + 1;
  }
  if(l > h) {
    if(++h == nublocks)
      ublocks[nublocks++] = i;
    else {
      movebuf(ublocks + h + 1, ublocks + h, nublocks - h);
      ++nublocks;
      ublocks[h] = i;
    }
    j = h;
  }
  ptr[i] = ublocks[j] + 1;
}
@

Now that the search is complete, the pointers can be compressed to the
minimum word size required.  The data will eventually be shifted down,
but it is not necessary (or safe) yet.  However, some preparation must
be done here, so that the pointers will eventually point to the
shifted blocks.  The actual pointers need to be adjusted to use the
offset of that pointer in the unique blocks array.  This could not
have been done above, because the array was having members inserted in
the middle, invalidating all of the indices.  To look up the index
more quickly, the block array is stored by block number first (it was
sorted by block contents, instead).

<<Saved multi-level table information>>=
/* unique block pointers */
uint32_t *curublk[UNI_MAX_MULTILEV] = {NULL};
uint32_t curnublk[UNI_MAX_MULTILEV];
@

<<Create next level bit array>>=
curublk[curlev] = ublocks;
curnublk[curlev] = nublocks;
qsort(ublocks, nublocks, sizeof(uint32_t), uni_cmp_cp);
/* for comparison with bsearch, adjust to actual stored value */
for(i = 0; i < nublocks; i++)
  ++ublocks[i];
for(i = 0; i < blks; i++) {
  if(!ptr[i] || ptr[i] == (uint32_t)~0)
    continue;
  uint32_t *p = bsearch(ptr + i, ublocks, nublocks, sizeof(uint32_t), 
                        uni_cmp_cp);
  ptr[i] = p - ublocks + 1;
}
lev[curlev + 1] = (uint8_t *)ptr;
/* note: 0 and ~0 are reserved values, so space for 2 must be reserved */
if(nublocks <= 254) {
  uint8_t *p = (uint8_t *)ptr;
  for(i = 0; i < blks; i++)
    p[i] = ptr[i];
  curlen[curlev + 1] = blks;
} else if(nublocks <= 65534) {
  uint16_t *p = (uint16_t *)ptr;
  for(i = 0; i < blks; i++)
    p[i] = ptr[i];
  curlen[curlev + 1] = blks * 2;
} else
  curlen[curlev + 1] = blks * 4;
@

To retain the minimum-sized level, the current set of minima is
stored.  If the current level is either smaller or the same size, but
with fewer levels, it is saved.  Otherwise, it and all of its children
are freed.

<<Saved multi-level table information>>=
uint32_t cursize[UNI_MAX_MULTILEV];
uint8_t *minlev[UNI_MAX_MULTILEV] = { (uint8_t *)bits };
uint32_t *minublk[UNI_MAX_MULTILEV] = { NULL };
uint32_t minlen[UNI_MAX_MULTILEV] = { len }, minnublk[UNI_MAX_MULTILEV];
uint32_t minblk[UNI_MAX_MULTILEV] = { 0 }, minsize = len;
@

<<Create next level bit array>>=
cursize[curlev] = nublocks * curblk[curlev];
cursize[curlev + 1] = curlen[curlev + 1];
@

<<Save or free [[curlev]]>>=
uint32_t save_size = 0;
for(i = 0; curblk[i]; i++)
  save_size += cursize[i];
save_size += cursize[i];
int save_lev = save_size < minsize;
if(save_size == minsize) {
  for(j = 0; minblk[j]; j++);
  save_lev = i < j;
}
if(save_lev) {
  for(i = 0; i < UNI_MAX_MULTILEV; i++) {
    if(minlev[i] != lev[i] && minlev[i])
      free(minlev[i]);
    if(minublk[i] != curublk[i] && minublk[i])
      free(minublk[i]);
    minblk[i] = curblk[i];
    minlev[i] = lev[i];
    minlen[i] = curlen[i];
    minublk[i] = curublk[i];
    minnublk[i] = curnublk[i];
  }
  minsize = save_size;
}
@

<<Free [[curlev]]>>=
if(curublk[curlev] != minublk[curlev])
  free(curublk[curlev]);
curublk[curlev] = NULL;
if(lev[curlev + 1] != minlev[curlev + 1])
  free(lev[curlev + 1]);
lev[curlev + 1] = NULL;
@

Now, the data can be combined into a single buffer, which is parsed
for every lookup.  This buffer is an opaque buffer of 32-bit integers,
with each data chunk aligned to a 32-bit boundary.  That way, 16-bit
and 32-bit values can be read directly from the buffer without
alignment issues.  The buffer is read from top to bottom, traversing
pointer tables until the data is reached.


<<Multi-level table type>>=
uint32_t
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
#define lev_psz(l) \
  (!(l) ? 1 : minnublk[l - 1] <= 254 ? 1 : minnublk[l - 1] <= 65534 ? 2 : 4)
for(curlev = 0; curlev < UNI_MAX_MULTILEV - 1 && minlev[curlev + 1]; curlev++);
*ml_len = 0<<Multi-level table buffer length>>;
inisize(*ml, *ml_len);
uint32_t *mp = *ml;
@

To determine the size of the buffer, we need to know what the buffer
will contain.  I will approach this by developing the lookup function
at the same time.  This function returns either a pointer into the
lowest-level data arrays, or an integer indicating that this is a zero
or one entry.

<<Find multi-level table entry [[val]]>>=
*ret = NULL;
@

First, we need to filter out the main range.  The default, 0 or 1, is
encoded as the first bit of the low end, after shifting that up.

<<Find multi-level table entry [[val]]>>=
if(val < (*dat >> 1) || val > dat[1])
  return def ? def : *dat & 1 ? ~0 : 0;
val -= *dat >> 1;
dat += 2;
@

<<Multi-level table buffer length>>=
+2
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
*mp++ = (low << 1) + (def & 1);
*mp++ = high;
@

After looking up the value at a particular level, the level needs to
be skipped.  The size of the table can be stored as a compressed
pointer beyond the end of the next level's data.  This is not possible
for the first level, though, so the length needs to be stored
separately.  There is no point in storing a compression length as
well, so this is stored as a 32-bit integer.

<<Find multi-level table entry [[val]]>>=
uint32_t skip = *dat++;
@

<<Multi-level table buffer length>>=
+1
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
*mp++ = minlen[curlev] / lev_psz(curlev);
@

Following this is a list of levels.  The first word of the last level
(i.e., the data level) is zero to terminate the list.

<<Find multi-level table entry [[val]]>>=
<<Prepare to scan multi-level table pointers>>
while(*dat) {
  <<Scan multi-level table pointers>>
}
@

<<Multi-level table buffer length>>=
+1;
for(i = 1; i <= curlev; i++)
  *ml_len += 0<<Multi-level table buffer pointer [[i]] length>>;
*ml_len += 0<<Multi-level table buffer data length>>
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
while(curlev > 0) {
  <<Dump multi-level table pointers>>
  --curlev;
}
*mp++ = 0;
@

Next, we need information regarding this level of the array.  First,
each array corresponds to a particular set of bits; all bits below
that are ignored.  The bit number below which the rest may be ignored
is stored next.  For the lowest level of the array, this is always
zero, so this corresponds with the loop terminator.  Rather than store
the number of bits in an entire 32-bit word, it is stored in a byte.
The other three bytes are available for other information that is only
needed for a pointer level.

<<Scan multi-level table pointers>>=
uint32_t desc = *dat++;
uint8_t shift = desc;
uint32_t idx = val >> shift;

val -= idx << shift;
@

<<Multi-level table buffer pointer [[i]] length>>=
+1
@

<<Dump multi-level table pointers>>=
uint32_t desc;
<<Compute pointer level descriptor>>
*mp++ = desc;
@

<<Compute pointer level descriptor>>=
for(i = 0, desc = 0; i < curlev; i++)
  desc += lg2(minblk[i] / lev_psz(i));
@

Now that the index into the subarray is known, to actually look
something up in the table requires the size of the table entries.
This is stored next.

<<Scan multi-level table pointers>>=
desc >>= 8;
uint8_t psz = desc;
@

<<Compute pointer level descriptor>>=
uint8_t psz = lev_psz(curlev);
desc |= psz << 8;
@

It also needs the offset of the subtable.  Since that is what we are
loooking up, it can be obtained from the previous pass.  For the first
pass, the offset is always zero.

<<Prepare to scan multi-level table pointers>>=
uint32_t toff = 0;
@

Now the lookup can be performed.  First, the offset beyond the table
is read, and then the pointer itself.  Then, the data pointer is
advanced past the end of the entire level.

<<Scan multi-level table pointers>>=
switch(psz) {
  case 1: {
    const uint8_t *p = (const uint8_t *)dat;
    toff = p[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint8_t)~0)
      return ~0;
    dat += skip / 4 + 1; /* aka (skip + 1 + 3) / 4 */
    skip = *p;
    break;
  }
  case 2: {
    const uint16_t *p = (const uint16_t *)dat;
    toff = p[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint16_t)~0)
      return ~0;
    dat += skip / 2 + 1; /* aka (skip + 1 + 1) / 2 */
    skip = *p;
    break;
  }
  case 4:
    toff = dat[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint32_t)~0)
      return ~0;
    toff = *dat;
    dat += skip + 1;
    break;
}
@

<<Multi-level table buffer pointer [[i]] length>>=
+((i != curlev ? minnublk[i] * minblk[i] : minlen[i]) + lev_psz(i) + 3) / 4
@

<<Dump multi-level table pointers>>=
uint8_t *mpp = (uint8_t *)mp;
switch(psz) {
 case 1:
   *mpp++ = minnublk[curlev - 1];
   break;
 case 2:
   *(uint16_t *)mpp = minnublk[curlev - 1];
   mpp += 2;
   break;
 case 4:
   *(uint32_t *)mpp = minnublk[curlev - 1];
   mpp += 4;
   break;
}
<<Copy table data out>>
free(minlev[curlev]);
@

<<Copy table data out>>=
if(minblk[curlev]) {
  /* copy all but last blindly */
  for(i = 0; i < minnublk[curlev] - 1; i++) {
    cpybuf(mpp, minlev[curlev] + minblk[curlev] * (minublk[curlev][i] - 1),
           minblk[curlev]);
    mpp += minblk[curlev];
  }
  /* the last may be short */
  uint32_t shortlen;
  if(minublk[curlev][i] == (minlen[curlev] + minblk[curlev] - 1) / minblk[curlev]) {
    shortlen = minlen[curlev] % minblk[curlev];
    if(!shortlen)
      shortlen = minblk[curlev];
  } else
    shortlen = minblk[curlev];
  cpybuf(mpp, minlev[curlev] + minblk[curlev] * (minublk[curlev][i] - 1),
         shortlen);
  /* silence valgrind, even though random garbage is perfectly safe */
  if(shortlen < minblk[curlev])
    memset(mpp + shortlen, 0, minblk[curlev] - shortlen);
  mpp += minblk[curlev];
  free(minublk[curlev]);
} else {
  cpybuf(mpp, minlev[curlev], minlen[curlev]);
  mpp += minlen[curlev];
}
/* align mpp */
if((mpp - (uint8_t *)mp) & 3) {
  /* silence valgrind here as well */
  memset(mpp, 0, 4 - ((mpp - (uint8_t *)mp) & 3));
  mpp += 4 - ((mpp - (uint8_t *)mp) & 3);
}
mp = (uint32_t *)mpp;
@

The table offset just acquired is a block number (actually, plus one
to allow for zero).  To convert it to a word offset, it needs to be
multiplied by the next block size, in pointer size units.  This is
stored next in [[desc]].  Since there are two bytes left, it is stored
as a 16-bit integer.  If another byte is ever needed, it can be
obtained by changing this to a shift value (it is always a power of
2).

<<Scan multi-level table pointers>>=
desc >>= 8;
toff = (toff - 1) * desc;
skip *= desc;
@

<<Compute pointer level descriptor>>=
desc |= (minblk[curlev - 1] / lev_psz(curlev - 1)) << 16;
@

The final lookup simply finds the byte offset of the remaining value
in the selected subtable.  Note that only bytes may be addressed;
storing more than one byte requires shifting the value left first
before finding the first byte of the value.  Similarly, storing only
one bit requires that the value be shifted right first for addressing,
and then using the shifted out bits to create a bit mask.

<<Find multi-level table entry [[val]]>>=
*ret = (const uint8_t *)dat + 4 + toff + val;
return 1;
@

<<Multi-level table buffer data length>>=
+((curlev ? minnublk[0] * minblk[0] : len) + 3) / 4
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
uint8_t *mpp = (uint8_t *)mp;
<<Copy table data out>>
if(mp - *ml != *ml_len) {
  fprintf(stderr, "len mismatch: %d %d\n", (int)(mp - *ml), (int)*ml_len);
  exit(1);
}
@

Generally, even with the complexity I added by compressing pointers,
the table lookups are faster than equivalent range table lookups.  As
long as duplicates are removed, the space taken up is comparable to
ordinary range tables, and sometimes even better.  With the
suppression of zero and one blocks, it is faster to find the
\emph{members} list than with plain bit tables, but is still not very
fast.  On the other hand, making modifications, or creating new tables
from scratch, is very expensive. Even if we stick with a table's
current structure, there needs to be side storage that deals with the
shared arrays.  If a change is made to an array at any level, then all
pointers at the next level need to be checked: if they point to the
same place, the block needs to be duplicated first, and after updating
the block (duplicate or not), it needs to be compared against all
existing blocks at the same level for re-merging.  The last step is
not strictly necessary for run-time operations, but there is no way
with the current structure to detect shared blocks.  An additional bit
for each pointer could be used to indicate sharing, which would never
be turned off even if the last owner disappears.  There are many
possibilities, and I do not wish to dwell on it any more.  For now,
the multi-level tables are expected to be read-only and only provide
the \emph{in} function.

However, converting a multi-level table to a sorted list of code point
ranges adequately satisfies the [[list]] function, and is relatively
easy to implement.  First, since the entire table is to be scanned
sequentally, the structure of the table is read.  Next, a loop simply
advances counters at each level of the table to get to the data, and
parses the data (slowly) using the generic bit tester.  The only
shortcuts here are when reading all zeroes or all ones, which bypass
the lowest level scan.  The function might get a little faster if it
didn't re-read every level's pointer for every low-level block, but
doing it this way is simpler and probably not a big performance hit.
A bigger speed gain could probably be had by optimizing the bit tester
loop.

<<Unicode property exports>>=
void uni_multi_bit_to_range(const uint32_t *mt, uni_chrrng_t **ret,
                            uint32_t *rlen);
@

<<Unicode property functions>>=
void uni_multi_bit_to_range(const uint32_t *mt, uni_chrrng_t **ret,
                            uint32_t *rlen)
{
  <<Multi-level structure storage>>
  
  <<Gather multi-level structure>>
  <<Convert multi-level structure to range list>>
}
@

<<Multi-level structure storage>>=
const uint32_t *dat[UNI_MAX_MULTILEV + 1];
uint32_t sh[UNI_MAX_MULTILEV + 1], psz[UNI_MAX_MULTILEV + 1],
         bsz[UNI_MAX_MULTILEV + 1];
uint32_t low, high;
int inv;
uint32_t nl;
@

<<Gather multi-level structure>>=
inv = mt[0] & 1;
low = (mt[0] >> 1) * 8;
high = mt[1] * 8 + 7;
uint32_t desc;
uint32_t nskip = mt[2], skip;
uint32_t l;
mt += 3;
bsz[0] = nskip; /* top level is just one block */
for(l = 0; (desc = *mt++); l++) {
  dat[l] = mt;
  psz[l] = (desc >> 8) & 0xff;
  sh[l] = desc & 0xff;
  switch(psz[l]) {
    case 1:
      skip = nskip / 4 + 1;
      nskip = *(uint8_t *)mt;
      break;
    case 2:
      skip = nskip / 2 + 1;
      nskip = *(uint16_t *)mt;
      break;
    default:
      skip = nskip + 1;
      nskip = *mt;
  }
  nskip *= (bsz[l + 1] = desc >> 16);
  mt += skip;
}
dat[l] = mt;
nl = l;
@

<<Convert multi-level structure to range list>>=
uint32_t pno[UNI_MAX_MULTILEV];
uint32_t ret_max;
int32_t rno, in_r;
ret_max = 8;
inisize(*ret, ret_max);
rno = in_r = 0;
if(inv && low > 0) {
  (*ret)[0].low = 0;
  in_r = 1;
}
for(l = 0; l < nl; l++)
  pno[l] = 0;
while(1) {
  uint32_t ptr = 1;
  for(l = 0; l < nl; l++) {
    switch(psz[l]) {
      case 1:
        ptr = ((uint8_t *)dat[l] + (ptr - 1) * bsz[l])[pno[l] + 1];
	if(ptr == 0xff)
	  ptr = ~0;
	break;
      case 2:
        ptr = ((uint16_t *)dat[l] + (ptr - 1) * bsz[l])[pno[l] + 1];
	if(ptr == 0xffff)
	  ptr = ~0;
	break;
      default:
        ptr = (dat[l] + (ptr - 1) * bsz[l])[pno[l] + 1];
    }
    if(!ptr || ptr == (uint32_t)~0)
      break;
  }
  if(!ptr) {
    if(in_r) {
      (*ret)[rno++].high = low - 1;
      in_r = 0;
    }
    low += 8 << sh[l];
  } else if(ptr == (uint32_t)~0) {
    if(!in_r) {
      if(rno == ret_max)
        resize(*ret, ret_max *= 2);
      (*ret)[rno].low = low;
      in_r = 1;
    }
    low += 8 << sh[l];
  } else {
    uint32_t i;
    uint8_t *rdat = (uint8_t *)dat[nl] + (ptr - 1) * bsz[nl];
    for(i = 0; i < bsz[nl] * 8 && low <= high; i++, low++)
      if(!UNI_BSET_IS_SET(rdat, i) != !in_r) {
	if(!in_r) {
	  if(rno == ret_max)
	    resize(*ret, ret_max *= 2);
	  (*ret)[rno].low = low;
	  in_r = 1;
	} else {
	  (*ret)[rno++].high = low - 1;
	  in_r = 0;
	}
      }
  }
  if(low > high) {
    low = high + 1;
    break;
  }
  if(l == nl)
    l--;
  while(++pno[l] == bsz[l])
    l--;
  for(l++; l < nl; l++)
    pno[l] = 0;
}
if(in_r) {
  if(!inv)
    (*ret)[rno++].high = low - 1;
  else
    (*ret)[rno++].high = UNI_MAX_CP; /* or ~0? */
} else if(inv && high < UNI_MAX_CP) { /* or ~0? */
  if(rno == ret_max)
    resize(*ret, ret_max *= 2);
  (*ret)[rno].low = low;
  (*ret)[rno++].high = UNI_MAX_CP;  /* or ~0? */
}
*rlen = rno;
@

The inverse operation is not so simple, but is worth implementing for
use with the UCD parser.  The parser can simply read in a range table,
and convert it to a multi-level table.  Since the multi-level table
generation algorithm partitions the bit array, the range table
converter actually constructs the bit array first, and calls the
function already developed above to do the conversion.

This is fairly inelegant and inefficient, but it is not meant to be
called at run-time very often.  Tables are meant to be pre-generated.

<<Unicode property exports for generator>>=
uint32_t *uni_rng_to_multi_bit(const uni_chrrng_t *tab, uint32_t tab_len,
                               uint32_t *ml_len);
@

<<Unicode property functions for generator>>=
uint32_t *uni_rng_to_multi_bit(const uni_chrrng_t *tab, uint32_t tab_len,
                               uint32_t *ml_len)
{
  uint32_t low, high, len, i;
  uint32_t *ml;
  uint8_t *bits, def;

  /* degenerate cases:  all 0, all 1 */
  if(!tab_len) {
    uni_bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  if(tab_len == 1 && !tab[0].low && tab[0].high == UNI_MAX_CP) {
    uni_bits_to_multi(NULL, 0, 1, 0, 1, 0, &ml, ml_len);
    return ml;
  }
  /* not all 0 or all 1 */
  low = tab[0].low;
  high = tab[tab_len - 1].high;
  /* if starts & ends with 1, make it an inverse array */
  /* should also do this if low == 0 && tab[0].high > UNI_MAX_CP - high */
  /* or vice-versa, but it complicates things a bit */
  def = !low && high == UNI_MAX_CP;
  if(def) {
    low = tab[0].high + 1;
    high = tab[tab_len - 1].low - 1;
  }
  /* align with byte boundary */
  low &= ~7;
  len = ((high - low + 1 + 7) / 8);
  inisize(bits, len);
  /* Optimize(maybe): only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  if(def)
    /* fill in ones if low not aligned with byte boundary */
    bits[0] = (1 << (tab[0].high + 1) % 8) - 1;
  for(i = def; i < tab_len - def; i++) {
    uint32_t l = tab[i].low - low, h = tab[i].high - low + 1;
    uint8_t hm = (1 << h % 8) - 1;
    uint8_t lm = ~((1 << l % 8) - 1);
    if(h / 8 == l / 8) {
      UNI_BSET_AELT(bits, l) |= lm & hm;
      continue;
    }
    if(lm != (uint8_t)~0) {
      UNI_BSET_AELT(bits, l) |= lm;
      l += 8 - (l & 7);
    }
    if(l < (h & ~7))
      memset(&UNI_BSET_AELT(bits, l), 0xff, h / 8 - l / 8);
    if(hm)
      UNI_BSET_AELT(bits, h) |= hm;
  }
  if(def && (tab[tab_len - 1].low - low) % 8)
    /* fill in ones if high + 1 not aligned with byte boundary */
    bits[(tab[tab_len - 1].low - low) / 8] |=
                         ~((1 << (tab[tab_len - 1].low - low) % 8) - 1);
  uni_bits_to_multi(bits, len, low / 8, high / 8, def, 0, &ml, ml_len);
  free(bits);
  return ml;
}
@

As mentioned earlier, I am not aware of any fast, efficient algorithms
for generating the multi-level table structure.  This makes general
bit operations difficult at best.  The general algorithm for unaligned
bit sets above would seem trivial in comparison to the tricks needed
to get unaligned multi-level tables together.  Right now, the only
supported method for set operations is to convert to a range table,
perform the operation, and convert back.  For less frequent
operations, simply performing the set operation on the results of
multiple lookups will do the trick.  This is extremely inefficient,
but at least it does not require creating a huge unmaintanable mess of
code for something that should rarely be done.

Another possibility would be to use more complex data structures.  I
doubt I could come up with a more space-efficient storage method than
the sorted range table, but some are much better at operations like
lookup, scanning, or modification.  An appropriately designed hash
table could be optimized off-line to provide even faster lookups, at
the cost of fragility due to the hash algorithm probably depending on
a particular Unicode release.  Prefix trees (tries, if you prefer)
have good theoretical performance, and in fact the multi-level table
is just a prefix tree of sorts.  It and many other faster tree
approaches achieve their performance by assuming a constant time
parent-to-child traversal with a large set of children, which once
again equates to large storage penalties.  I have reduced the penalty
somewhat in my own implementation by using smaller pointers, at the
expense of slowing the lookup down a bit.  For now, the multi-level
table, sorted range table, and sorted code point table are the only
supported data structures.

\subsection{Testing}

In order to actually compare performance and size, a separate test
program is provided.

<<C Test Support Executables>>=
tsttab \
@

<<makefile.rules>>=
tsttab.o: uni_prop.h
@

<<tsttab.c>>=
<<Common C Header>>

#include "uni_prop.h"
// static_proto

#include <sys/resource.h>
static struct rusage ru;

static void tstart(void)
{
  getrusage(RUSAGE_SELF, &ru);
}

static unsigned long tend(void)
{
  struct rusage ru2;
  getrusage(RUSAGE_SELF, &ru2);
  return (ru2.ru_utime.tv_sec - ru.ru_utime.tv_sec) * 1000000 +
            (ru2.ru_utime.tv_usec - ru.ru_utime.tv_usec);
}

<<Functions to help test generated tables>>

int main(void)
{
  <<Test generated tables>>
  return 0;
}
@

To test a boolean, a function is provided to run the entire test, and
a call to that function is done in the mainline.  The function is
passed the range table, the multi-level table, and the name of the
property.  These can all be generated from the property name with a
preprocessor macro.

<<Functions to help test generated tables>>=
volatile int32_t tres;

#define bool(x) doit_bool(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab)
static void print_mtab_info(const uint32_t *, uint32_t);
static void doit_bool(const char *name, const uni_chrrng_t *rng, uint32_t nent,
                      const uint32_t *mtab)
{
    uint32_t i;
    
    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      int rv = uni_is_cp_chrrng(i, rng, nent);
      int mv = <<range table result for [[i]]>>;
      if(!rv != !mv) {
        fprintf(stderr, "mismatch %s@%d %d %d\n", name, i, rv, mv);
	exit(1);
      }
    }
    /* check performance */
    int j;
    unsigned long tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = uni_is_cp_chrrng(i, rng, nent);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = <<range table result for [[i]]>>;
    tt = tend();
    printf("  r%ld t%ld %.2fx\n", tr, tt, (double)tr / (double)tt);
    /* check conversion */
    uni_chrrng_t *rng2;
    uint32_t nent2;
    uni_multi_bit_to_range(mtab, &rng2, &nent2);
    if(nent2 != nent || memcmp(rng, rng2, nent * sizeof(*rng))) {
      for(j = 0; j < nent && j < nent2; j++)
        if(memcmp(&rng[j], &rng2[j], sizeof(*rng)))
	  break;
      fprintf(stderr, "misconvert %d/%d @%d\n", (int)nent, (int)nent2, j);
      exit(1);
    }
}
@

<<Functions to help test generated tables>>=
static void print_mtab_info(const uint32_t *mt, uint32_t rsize)
{
  uint32_t i, l;
  uint32_t skip, desc, nskip, psz;

  fputs("  mtab: ", stdout);
  if(mt[0] & 1)
    fputs("*inv* ", stdout);
  nskip = mt[2];
  i = 3;
  printf(" %d", nskip);
  for(l = 0; (desc = mt[i]); l++) {
    printf("/%d", desc >> 16);
    psz = (desc >> 8) & 0xff;
    switch(psz) {
      case 1:
	skip = nskip / 4 + 1;
        nskip = *(uint8_t *)&mt[++i];
	break;
      case 2:
	skip = nskip / 2 + 1;
        nskip = *(uint16_t *)&mt[++i];
	break;
      default:
	skip = nskip + 1;
        nskip = mt[++i];
    }
    nskip *= desc >> 16;
    i += skip;
  }
  skip = (nskip + 3) / 4;
  i += skip;
  printf(" (%d bytes, %d lookups max) (%.2fx rng)\n", i * 4, l + 1,
         (double)(i * 4) / rsize);
}
@

\subsection{Parsing the UCD}

A parser program is used to generate these tables as static,
compilable C code from the UCD tables.  One of the advantages of the
range table method is that creating range tables is trival, and could
be done using a simple shell script.  However, during the stages of
this project where that was in use, the generation time began to
dominate compilation time, so this was converted to C.  Since it is in
C anyway, it may as well generate multi-level tables directly, as
well.  In fact, the logic outside of the scripts was becoming
cumbersome as well, so all of that is now incorporated into the C
program.

This program has a special build rule so it can be used to generate
other C code without depending on that C code itself (as with
[[cproto.h]]).  The code itself can't use the standard header macro,
either, since it needs to limit its include files.  One thing it does
need, though, is the ability to read lines of arbitrary length.  This
is provided in my support library, which is referenced rather than
built here, again to avoid building [[cproto.h]].  Since
[[uni_prop.h]] and [[uni_prop.c]] depend on the output of this
program, but this program also needs to use some of the functions
defined therein, the functions and definitions are reinserted directly
into [[parse-ucd.c]] without any dependencies on the generated data.

\lstset{language=make}
<<C Build Executables>>=
parse-ucd \
@

\lstset{language=C}
<<parse-ucd.c>>=
<<Common C Warning>>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <ctype.h>
#include <unistd.h>
#include <stdint.h>
<<Additional parse-ucd includes>>
<<Unicode property exports for generator>>

<<Unicode property functions for generator>>

<<UCD parser local definitions>>

<<UCD parser local functions>>

int main(void)
{
  <<UCD parser variables>>

  <<Parse character data files>>
  <<Post-process property data>>
  <<Dump character information as C code>>
  <<Clean up after parsing UCD files>>
  return 0;
}
@

\lstset{language=make}
<<makefile.vars>>=
PARSE_UCD_DEPS := parse-ucd.c $(SUPT_LIB_LOC)/libsupt.a <<Additional parse-ucd C files>>

PARSER_CFLAGS :=
PARSER_LDFLAGS :=
@

<<makefile.config>>=
# The location of my support library (tjm-ext.nw)
SUPT_LIB_LOC := ../build
@

<<makefile.rules>>=
parse-ucd: $(PARSE_UCD_DEPS)
	$(CC) $(CFLAGS) $(EXTRA_CFLAGS) $(PARSER_CFLAGS) \
	   -o $@ parse-ucd.c -L$(SUPT_LIB_LOC) -lsupt $(LDFLAGS) $(PARSER_LDFLAGS)
@

<<Additional parse-ucd C files>>=
mfgets.h \
@

\lstset{language=C}
<<Additional parse-ucd includes>>=
#include "mfgets.h"
@

All files will be parsed by the same program.  The only necessary
parameters are the locations of the UCD files, which can just be
compiled in based on the configuration parameters.

\lstset{language=make}
<<makefile.vars>>=
PARSER_CFLAGS += -DUCD_LOC=\"$(UCD_LOC)\" -DUNIHAN_LOC=\"$(UNIHAN_LOC)\"
@

Rather than construct file names using the provided paths, the program
just changes to the location directory before reading files, and all
files are read relative to the current directory.  In order to be able
to change back to the starting directory, once again my utility
library has a portable [[getcwd]] that does not use fixed buffer sizes.

\lstset{language=C}
<<UCD parser variables>>=
<<Parser common variables>>
char *cwd = getcwd_full();
@

<<Parser common variables>>=
FILE *f;
char *lbuf = NULL;
unsigned int lbuflen, llen;
#define open_f(fn) do { \
  if(!(f = fopen(fn, "r"))) { \
    perror(fn); \
    exit(1); \
  } \
} while(0)
@

<<Parse character data files>>=
<<Initialize UCD files>>
<<Parse UCD files>>
<<Initialize Unihan files>>
<<Parse Unihan files>>
@

<<Initialize UCD files>>=
chdir(UCD_LOC);
@

<<Initialize Unihan files>>=
chdir(cwd); /* in case unihan_loc is relative */
chdir(UNIHAN_LOC);
@

<<Dump character information as C code>>=
chdir(cwd);
free(cwd);
@

UCD text files consist of semicolon-separated fields, with the code
point(s) in the first field.  Comments are introduced with the number
sign.  Blank lines and purely comment lines have no fields. 
Whitespace is stripped from the beginning and ending of each field.
Comments are stripped off, but kept on the side, since one particular
file stores relevant information in a line comment.  Rather than
process each field as it is encountered, like I would normally do,
each line is split into an array of fields first.  This makes the
parsing routines more readable and obvious as well.

<<Process a line of [[UnicodeData.txt]]>>=
split_line(lbuf);
if(num_fields < 15) { /* should never happen! */
  perror("UnicodeData.txt");
  exit(1);
}
@

<<UCD parser local definitions>>=
static char **fields;
static int num_fields, max_fields = 0;
static char *line_comment;
@

<<Additional parse-ucd C files>>=
mallocdef.h \
@

<<Additional parse-ucd includes>>=
#include "mallocdef.h"
@

<<UCD parser local functions>>=
static void split_line(char *buf)
{
  char fc = 0;

  if(!max_fields)
    inisize(fields, (max_fields = 16));
  num_fields = 0;
  line_comment = NULL;
  while(isspace(*buf)) buf++;
  if(!*buf || *buf == '#')
    return;
  while(1) {
    while(isspace(*buf))
      buf++;
    if(fc == '#')
      line_comment = buf;
    char *f = buf, *nf;
    for(nf = buf; *nf && (fc == '#' || (*nf != ';' && *nf != '#')); nf++);
    fc = *nf;
    buf = nf + 1;
    while(nf > f && isspace(nf[-1])) --nf;
    *nf = 0;
    if(line_comment)
      return;
    if(num_fields == max_fields)
      resize(fields, (max_fields *= 2));
    fields[num_fields++] = f;
    if(!fc)
      return;
  }
}
@

Since named properties are being parsed, and named variables will be
created, it is useful to have a database of all of the property names
and their aliases before starting.  For this reason, the first file to
process is the file which contains those names:  [[PropertyAliases.txt]].
This file is read in and stored in a local table, sorted by name.  The
sorting is done after the fact to reduce complexity.

<<[[uni_alias_t]]>>=
typedef struct {
  const char *short_name, *long_name, *alt_name, *alt_name2;
} uni_alias_t;
@

<<Unicode property exports for generator>>=
<<[[uni_alias_t]]>>
@

<<Known Data Types>>=
uni_alias_t,%
@

<<UCD parser local definitions>>=
static uni_alias_t *prop_aliases;
static int num_prop_aliases = 0, max_prop_aliases = 0;
@

<<Initialize UCD files>>=
open_f("PropertyAliases.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  split_line(lbuf);
  if(num_fields < 2)
    continue;
  if(!max_prop_aliases)
    inisize(prop_aliases, (max_prop_aliases = 128));
  if(max_prop_aliases == num_prop_aliases)
    resize(prop_aliases, (max_prop_aliases *= 2));
  prop_aliases[num_prop_aliases].short_name = strdup(fields[0]);
  prop_aliases[num_prop_aliases].long_name = strdup(fields[1]);
  /* there are no comments on alias lines, so field count is correct */
  prop_aliases[num_prop_aliases].alt_name =
                 num_fields > 2 ? strdup(fields[2]) : NULL;
  prop_aliases[num_prop_aliases].alt_name2 =
                 num_fields > 3 ? strdup(fields[3]) : NULL;
  if(num_fields > 4) {
    perror("PropertyAliases.txt");
    exit(1);
  }
  num_prop_aliases++;
}
fclose(f);
@

There are actually two fields to sort by: long name and short name.
The main table will be sorted by long name, and a side table with
pointers will be used to sort the table by short name as well.  That
way, lookups can use binary searching on either field.

<<UCD parser local functions>>=
static int cmp_longname(const void *a, const void *b)
{
  return strcmp(((uni_alias_t *)a)->long_name, ((uni_alias_t *)b)->long_name);
}
@

<<Initialize UCD files>>=
qsort(prop_aliases, num_prop_aliases, sizeof(uni_alias_t), cmp_longname);
@

<<UCD parser local definitions>>=
uni_alias_t **prop_aliases_short;
@

<<UCD parser local functions>>=
static int cmp_shortname(const void *a, const void *b)
{
  return strcmp((*(uni_alias_t **)a)->short_name,
                (*(uni_alias_t **)b)->short_name);
}
@

<<Initialize UCD files>>=
inisize(prop_aliases_short, num_prop_aliases);
for(i = 0; i < num_prop_aliases; i++)
  prop_aliases_short[i] = &prop_aliases[i];
qsort(prop_aliases_short, num_prop_aliases, sizeof(*prop_aliases_short),
      cmp_shortname);
@


Properties are added to a master property list, containing the name
and the parsed contents.  This is simply tacked on next to the
like-named property in the property name table, if applicable.  Some
properties are artificial, though, so they are appended to the end.  A
separate count is provided for this purpose.

<<UCD parser local definitions>>=
<<[[prop_t]] prerequisites>>
typedef struct {
  const char *name;
  <<Property parsed contents>>
} prop_t;
static prop_t *parsed_props;
static uint32_t nparsed, maxparsed = 0;
@

<<Known Data Types>>=
prop_t,%
@

<<UCD parser local functions>>=
static int add_prop(const char *name)
{
  uni_alias_t *pn, n;
  uint32_t i;
  static char **added_names = NULL;
  static int *added_names_p = NULL;
  static int num_added_names = 0, max_added_names = 0;

  if(!maxparsed) {
    inisize(parsed_props, (maxparsed = num_prop_aliases));
    nparsed = num_prop_aliases;
    clearbuf(parsed_props, nparsed);
  }
  n.long_name = name;
  pn = bsearch(&n, prop_aliases, num_prop_aliases, sizeof(*prop_aliases),
               cmp_longname);
  if(!pn) {
    uni_alias_t **ppn;
    pn = &n;
    n.short_name = name;
    ppn = bsearch(&pn, prop_aliases_short, num_prop_aliases,
                  sizeof(*prop_aliases_short), cmp_shortname);
    if(ppn)
      pn = *ppn;
    else
      pn = NULL;
  }
  if(pn)
    i = (int)(pn - prop_aliases);
  else {
    int h, l, m, c;
    for(l = 0, h = num_added_names - 1; l <= h; ) {
      m = (h + l) / 2;
      c = strcmp(name, added_names[m]);
      if(c < 0)
        h = m - 1;
      else if(c > 0)
        l = m + 1;
      else
        return added_names_p[m];
    }
    if(maxparsed == nparsed) {
      resize(parsed_props, (maxparsed *= 2));
      <<Resize prop-associated arrays>>
    }
    i = nparsed;
    clearbuf(&parsed_props[nparsed], 1);
    <<Clear prop-associated array[i]>>
    nparsed++;
    if(num_added_names == max_added_names) {
      if(!max_added_names) {
        inisize(added_names, (max_added_names = 10));
	inisize(added_names_p, max_added_names);
      } else {
        resize(added_names, (max_added_names *= 2));
	resize(added_names_p, max_added_names);
      }
    }
    if(l < num_added_names) {
      movebuf(added_names + l + 1, added_names + l, num_added_names - l);
      movebuf(added_names_p + l + 1, added_names_p + l, num_added_names - l);
    }
    added_names[l] = strdup(name);
    added_names_p[l] = i;
    num_added_names++;
  }
  if(!parsed_props[i].name)
    parsed_props[i].name = strdup(name);
  return i;
}
@

The first file to parse for actual data is the main database file,
[[UnicodeData.txt]].

<<Parse UCD files>>=
open_f("UnicodeData.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Process a line of [[UnicodeData.txt]]>>
}
fclose(f);
@

The first field is a code point.  While it appears that each line only
deals with one code point, there are actually ranges as well.  These
are specified by two consecutive entries with names ending in \texttt{,
First>} and \texttt{, Last>}, respectively.

<<Parser common variables>>=
uint32_t low, high;
char *s;
@

<<Process a line of [[UnicodeData.txt]]>>=
low = high = strtol(fields[0], NULL, 16);
if(fields[1][0] == '<' && (s = strchr(fields[1], ',')) &&
   !strcasecmp(s, ", First>")) {
  if(!mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    perror("UnicodeData.txt");
    exit(1);
  }
  split_line(lbuf);
  if(num_fields < 15 ||
     fields[1][0] != '<' || !(s = strchr(fields[1], ',')) ||
     strcasecmp(s, ", Last>")) { /* should also never happen! */
    perror("UnicodeData.txt");
    exit(1);
  }
  high = strtol(fields[0], NULL, 16);
}
@

The parsed contents for boolean properties are the range table and the
associated multi-level table.  In order to build the range table in
place, the storage size of the range table is kept as well.

<<Property parsed contents>>=
uni_chrrng_t *rng;
uint32_t len, max_len;
uint32_t *mt;
@

<<Parser common variables>>=
uint32_t i, j;
@

<<UCD parser local functions>>=
static void add_bool_rng(prop_t *p, uint32_t low, uint32_t high)
{
  if(!p->max_len)
    inisize(p->rng, (p->max_len = 8));
  if(p->len && p->rng[p->len - 1].high == low - 1)
    p->rng[p->len - 1].high = high;
  else {
    if(p->len == p->max_len)
      resize(p->rng, (p->max_len *= 2));
    p->rng[p->len].low = low;
    p->rng[p->len].high = high;
    ++p->len;
  }
}
@

<<UCD parser local definitions>>=
#define decl_bool(n) int prop_##n = -1
#define add_bool(n) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  add_bool_rng(&parsed_props[prop_##n], low, high); \
} while(0)
@

The only directly derived boolean properties in [[UnicodeData.txt]] are
ASSIGNED, which is true if the character is present, and
Bidi\_Mirrored, which is true if field 10 is Y%
\footnote{This property, like many boolean properties, has property
value aliases that indicate Yes, T, and True as possible truth values
as well.  For now, though, all files are parsed assuming Y, since that
is always the case in the current UCD.}%
.

<<UCD parser variables>>=
decl_bool(ASSIGNED);
decl_bool(Bidi_Mirrored);
@

<<Process a line of [[UnicodeData.txt]]>>=
add_bool(ASSIGNED);
if(fields[9][0] == 'Y')
  add_bool(Bidi_Mirrored);
@

The [[PropList.txt]] file has more boolean properties, though.  In
fact, all entries in this file specify boolean properties.

<<Parse UCD files>>=
open_f("PropList.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Process a line of [[PropList.txt]]>>
}
fclose(f);
@

Like most of the rest of the UCD files, this file specifies ranges
using double-dot-separated code points.  Also, there are numerous
blank lines and comments, which need to be skipped.

<<Process a line of [[PropList.txt]]>>=
<<Parse dotted cp>>
@

<<Parse dotted cp>>=
split_line(lbuf);
if(!num_fields || !isxdigit(*lbuf))
  continue;
low = high = strtol(lbuf, &s, 16);
if(s && *s == '.' && s[1] == '.')
  high = strtol(s + 2, &s, 16);
if(*s) { /* should never happen, but if it does: ignore */
  fprintf(stderr, "bad col 1: %s\n", lbuf);
  continue;
}
@

Each boolean property is true if field two is the name of that
property.  Since the [[add_prop]] routine finds the correct entry based
on the name (assuming the name exists), it can be called every time,
and the correct entry will be updated.  Hyphen is a deprecated
property, so it is explicitly excluded.  It would be nice if there
were some place where a list of deprecated propreties could be read.

<<Process a line of [[PropList.txt]]>>=
if(!strcmp(fields[1], "Hyphen"))
  continue;
<<Just add field two as binary property>>
@

<<Just add field two as binary property>>=
add_bool_rng(&parsed_props[add_prop(fields[1])], low, high);
@

Speaking of [[DerivedCoreProperties.txt]], this file has entirely
boolean properties as well.  The Grapheme\_Link property is
deprecated, so it is explicitly excluded.  It would be nice if there
were a file listing all deprecated properties to exclude.

<<Parse UCD files>>=
open_f("DerivedCoreProperties.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  if(!strcmp(fields[1], "Grapheme_Link"))
    continue;
  <<Just add field two as binary property>>
}
fclose(f);
@

The [[CompositionExclusions.txt]] file is simply a commmented list of
code points, one per line, each of which has the
Composition\_Exclusion property.  Since comments are processed as
fields, and all lines have trailing comments, the number of fields is
sufficient to use the same parsing technique as for the other files.

<<UCD parser variables>>=
decl_bool(Composition_Exclusion);
@

<<Parse UCD files>>=
open_f("CompositionExclusions.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_bool(Composition_Exclusion);
}
fclose(f);
@

Unfortunately, this file does not have all entries in order. Instead,
it lists multiple groups, and each group is in order.  This requires
post-processing.  In fact, since this might happen with other files as
well, this processing may as well be done for all tables.

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng)
    fixup_rng(&parsed_props[i]);
@

<<UCD parser local functions>>=
static void fixup_rng(prop_t *p)
{
  uint32_t i;
  qsort(p->rng, p->len, sizeof(uni_chrrng_t), uni_cmprng);
  /* starting at top means only optimized entries are memmove'd */
  for(i = p->len - 1; i > 0; i--) {
    uint32_t j = i;
    while(i > 0 && p->rng[i - 1].high == p->rng[i].low - 1)
      i--;
    if(i == j)
      continue;
    p->rng[i].high = p->rng[j].high;
    if(j < p->len - 1)
        movebuf(p->rng + i + 1, p->rng + j + 1, p->len - (j + 1));
    p->len -= j - i;
    if(!i)
      break;
  }
}
@

[[DerivedNormalizationProps.txt]] contains the only remaining required
boolean UCD properties.  Unlike the others, there are some non-boolean
properties in this file as well.  Luckily, some boolean values can be
detected by simply counting the fields.  The Expands\_On\_* properties
are deprecated.  Technically, some of the quick check fields could be
booleans as well, but they will be taken care of as enumerations.

<<Parse UCD files>>=
open_f("DerivedNormalizationProps.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  <<Process a line of [[DerivedNormalizationProps.txt]]>>
}
fclose(f);
@

<<Process a line of [[DerivedNormalizationProps.txt]]>>=
if(num_fields == 2) {
  if(!strncmp(fields[1], "Expands_On_", 11))
    continue;
  <<Just add field two as binary property>>
  continue;
}
@

There are no boolean fields exported from Unihan.  The only field
which might be considered boolean (kIICore) is not required by
anything I use.

\lstset{language=txt}
<<FIXME>>=
Unihan_IRGSources.txt: kIICore
@

Now, to finish up post-processing, the multi-level table is generated.

\lstset{language=C}
<<Post-process property data>>=
uint32_t ml_len;
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng)
    parsed_props[i].mt = uni_rng_to_multi_bit(parsed_props[i].rng,
                                              parsed_props[i].len,
                                              &ml_len);
@

\subsection{Generating the Static Data}

The boolean properties can now be printed, using the canonical short
name.  Declarations are all printed to the same file:
[[uni_prop.gen.h]].  However, each boolean property is printed to a
different file, so that static linking will only pull in the desired
properties and representations.  They are then linked into a separate
library from the support routines.  The names of the files are
extracted from the generated header file, and as such, [[parse-ucd]]
is a prerequisite to generating the full makefile.  This complicates
things, and makes builds much slower.  In the future, this may be
changed to use a recursive make, so that only generating the library
is dependent on the generated code.

\lstset{language=make}
<<makefile.rules>>=
uni_prop.gen.h: parse-ucd
	./parse-ucd
@

\lstset{language=C}
<<Library [[uni]] headers>>=
#include "uni_prop.h"
@

<<Unicode property exports>>=
#include "uni_prop.gen.h"
@

<<makefile.rules>>=
uni_prop.h: uni_prop.gen.h
@

<<UCD parser local definitions>>=
#define open_wf(f, fn) \
  FILE *f; \
  if(!(f = fopen(fn, "w"))) { \
    perror(fn); \
    exit(1); \
  }
@

<<Dump character information as C code>>=
open_wf(gen_h, "uni_prop.gen.h");
@

<<Clean up after parsing UCD files>>=
fclose(gen_h);
@

\lstset{language=make}
<<makefile.vars>>=
MAKEFILES+=makefile.unidat
@

<<makefile.rules>>=
makefile.unidat: uni_prop.gen.h
	printf 'RNGDAT_NAMES := ' >$@
	fgrep '[]' $^ | <<Filter generated file names>>
	       sed -e 's/\[.*//;s/.* //' | tr \\n ' ' >>$@
-include makefile.unidat
$(RNGDAT_NAMES:%=%.gen.c): uni_prop.gen.h
	@touch "$@"
$(RNGDAT_NAMES:%=%.gen.o): uni_prop.h
@

<<Library [[uni]] Members>>=
$(RNGDAT_NAMES:%=%.gen.o)
@

<<C Headers>>=
uni_prop.gen.h \
@

Converting these to lower-case would be more consistent, but once the
name is known, casing shouldn't really matter.  In addition, a simple
query function (actually, preprocessor macro wrapper) is printed for
the multi-level table lookup functions.  Finally, a list of boolean
properties needs to be converted into calls to [[bool()]] for the test
program.  This is generated directly into a fixed-named C file,
[[tsttab.tests.gen.c]].

<<Plain Built Files>>=
tsttab.tests.gen.c \
@

\lstset{language=C}
<<Test generated tables>>=
#include "tsttab.tests.gen.c"
@

<<Dump character information as C code>>=
open_wf(tstf, "tsttab.tests.gen.c");
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    char nbuf[64];
    sprintf(nbuf, "uni_%s_rng.gen.c", name);
    open_wf(of, nbuf);
    fprintf(of, "#include \"uni_prop.h\"\n\n"
                "const uni_chrrng_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].len; j++)
      fprintf(of, "\t{ 0x%04X, 0x%04X }%s\n", parsed_props[i].rng[j].low,
                                              parsed_props[i].rng[j].high,
					      j < parsed_props[i].len - 1 ? "," : "");
    fputs("};\n", of);
    fclose(of);
    fprintf(gen_h, "extern const uni_chrrng_t uni_%s_rng[];\n"
		   "#define uni_%s_rng_len %d /* %d lookups max */\n"
                   "#define is_uni_%s(x) is_uni_x(x, uni_%s_mtab)\n",
		   name, name, parsed_props[i].len,
		   lg2(parsed_props[i].len + 1), name, name);
    print_mtab(name, parsed_props[i].mt, gen_h);
    fprintf(tstf, "bool(%s);\n", name);
  }
}
@

<<Clean up after parsing UCD files>>=
fclose(tstf);
@

<<Unicode property exports>>=
int is_uni_x(uint32_t cp, const uint32_t *tab);
@

<<Unicode property functions>>=
int is_uni_x(uint32_t cp, const uint32_t *tab)
{
  const uint8_t *mr;
  uint8_t mv = uni_multi_tab_lookup(tab, cp / 8, &mr, 0);
  if(mr)
    mv = *mr;
  return mv & UNI_BSET_MASK(mr, cp) ? 1 : 0;
}
@

<<range table result for [[i]]>>=
is_uni_x(i, mtab)
@

\lstset{language=sh}
<<Clean temporary files>>=
rm -f uni_*.gen.[co]
@

When printing the multi-level table, comments are added to indicate
the struture.

\lstset{language=C}
<<UCD parser local functions>>=
static void print_mtab(const char *name, const uint32_t *mt, FILE *gen_h)
{
  char nbuf[64];
  sprintf(nbuf, "uni_%s_mtab.gen.c", name);
  open_wf(mf, nbuf);
  uint32_t i, j, l;
  uint32_t skip, desc, nskip, psz;
  fprintf(mf, "#include <stdint.h>\n\n"
              "const uint32_t uni_%s_mtab[] = {\n", name);
  fprintf(mf, "\t/* low/inv, high, l0len */\n"
              "\t(0x%X << 1) + %d, 0x%X, %d,\n",
	      mt[0] >> 1, mt[0] & 1, mt[1], (nskip = mt[2]));
  i = 3;
  fprintf(gen_h, "extern const uint32_t uni_%s_mtab[]; /* %d", name, nskip);
  for(l = 0; (desc = mt[i]); l++) {
    fprintf(mf, "\t/* level %d (pointers) */\n"
                "\t0x%X, /* shift=%d psz=%d nextblk=%d */\n",
                l, desc, desc & 0xff, (psz = (desc >> 8) & 0xff), desc >> 16);
    fprintf(gen_h, "/%d", desc >> 16);
    switch(psz) {
      case 1:
	skip = nskip / 4 + 1;
        nskip = *(uint8_t *)&mt[++i];
	break;
      case 2:
	skip = nskip / 2 + 1;
        nskip = *(uint16_t *)&mt[++i];
	break;
      default:
	skip = nskip + 1;
        nskip = mt[++i];
    }
    fprintf(mf, "\t/* raw pointers; pointer[0] = l%dlen = %d */\n\t", l + 1, nskip);
    nskip *= desc >> 16;
    for(j = 0; j < skip - 1; j++, i++)
      fprintf(mf, "0x%X,%s", mt[i], !((j + 1) % 8) ? "\n\t" : " ");
    fprintf(mf, "0x%X,\n", mt[i++]);
  }
  skip = (nskip + 3) / 4;
  fprintf(mf, "\t/* level %d (raw data) */\n"
              "\t0%s", l, skip ? ",\n\t" : "\n");
  for(j = 0, i++; j < skip; j++, i++)
    fprintf(mf, "0x%X%s", mt[i], j == skip - 1 ? "\n" :
                                                 !((j + 1) % 8) ? ",\n\t" :
						                  ", ");
  fputs("};\n", mf);
  fclose(mf);
  fprintf(gen_h, " (%d-level) */\n"
                 "#define uni_%s_mtab_len %d\n", l + 1, name, i);
}
@

\subsection{Additional Properties}

A few specific character classes that are useful for some applications
do not warrant a full set of tables.  Instead, they are implemented as
macros that compare directly to the limited set they contain.  These
are:  characters which act as line terminators, characters which
may initiate backslash-escapes, and UTF-16 surrogates.

<<Unicode property exports for generator>>=
int is_uni_nl(uint32_t cp);
#define is_uni_nl(x) ((x) == '\n' || (x) == '\r' || (x) == '\v' || (x) == '\f' || \
                  (x) == 0x0085 || (x) == 0x2028 || (x) == 0x2029)
/* just use == '\\' for nfkc_cf */
int is_uni_bs(uint32_t cp);
#define is_uni_bs(x) ((x) == '\\' || (x) == 0xFE68 || (x) == 0xFF3C)
int is_uni_surrogate(uint32_t cp);
#define is_uni_surrogate(x) ((x) >= 0xD800 && (x) < 0xE000)
@

\section{Enumerated Properties}

Enumerated properties have a limited number of values, which have
names of their own.  These names may have aliases, as well.  Of course
all properties have a limited number of values, so it is a matter of
interpretation whether it qualifies as an enumeration.  The general
rule followed here is that this only includes properties with an
explicit list of aliases.  That includes one psuedo-property: the list
of properties itself.  An enumerated property can be split into sets,
each of which consists of the characters having one specific
enumerated value.  This means that each enumerated property also
defines a number of boolean properties, which are covered above.
Additional useful operations include:

\begin{itemize}
\item Find out the value \emph{of} the property for a character.  This
should be a numeric value indicating the enumeration literal index.
\item Given a value, find out its \emph{name} and primary \emph{alias}.
\item Given a name or alias, find out its \emph{value}.  This should
support Unicode sloppy matching:  case-insensitive, with dashes and
underscores removed (except as noted).
\item Obtain a \emph{list} of all possible values and aliases, for more
general searching.
\end{itemize}

\subsection{Storage Methods}

Again, for the \emph{of} operation, the multi-level table seems
appropriate.  Rather than storing the actual string value, the
enumeration is simply converted into an integer from zero to the
number of elements, and that integer is stored.  For all enumerations
but the character name, this just one byte.  For the character name, a
different approach is required.  Since every code point has a
different name, there will never be any sharing of lower-level tables.
The only possible sharing would be in the algorithmically generated
code points.  Since this is a significant special case, it gets its
own section, later on.

To obtain a subset for boolean operations, the range list is provided
as well.  It is augmented by a data field.  No enumeration actually
requires 32 bits, so adding another separate field is wasteful.
Instead, an 8-bit bit field is added.  This does slow things down
slightly, but for performance, the multi-level table is better, anyway.

<<[[uni_chrrng_dat8_t]]>>=
typedef struct {
    uint32_t low, high:24, dat:8;
} uni_chrrng_dat8_t;
@

<<Unicode property exports for generator>>=
#if 1
<<[[uni_chrrng_dat8_t]]>>
#else /* 32-bit dat during testing */
typedef struct {
    uint32_t low, high, dat;
} uni_chrrng_dat8_t;
#endif
int uni_cmprng_dat8(const void *a, const void *b); /* for bsearch/qsort */
uint32_t uni_chrrng_dat8(uint32_t cp, const uni_chrrng_dat8_t *tab,
                         uint32_t tab_len, uint32_t def);
@

<<Known Data Types>>=
uni_chrrng_dat8_t,%
@

<<Unicode property functions for generator>>=
int uni_cmprng_dat8(const void *a, const void *b)
{
    const uni_chrrng_dat8_t *_a = a, *_b = b;

    if(_a->high < _b->low)
        return -1;
    else if(_b->high < _a->low)
        return 1;
    else
        return 0;
}

uint32_t uni_chrrng_dat8(uint32_t cp, const uni_chrrng_dat8_t *tab,
                         uint32_t tab_len, uint32_t def)
{
#if 0
  uni_chrrng_dat8_t cr = {cp, cp}, *tab_el;
  tab_el = bsearch(&cr, tab, tab_len, sizeof(uni_chrrng_dat8_t), uni_cmprng_dat8);
  return tab_el ? tab_el->dat : def;
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > tab[j].high)
      l = j + 1;
    else
      return tab[j].dat;
  }
  return def;
#endif
}
@

While no enumeration requires more than 8 bits, there may be other
data types which require more.  For these, larger types are needed.
For 16 bits, the [[low]] field can be split as well.  For 32 bits, if
they are unique, it is unlikely that long ranges (if any ranges at
all) exist.  For this reason, a low+length approach is used instead;
this splits the [[low]] field to make room for a [[length]] field
(which is the length of the range minus one).

<<[[uni_chrrng_dat16_t]]>>=
typedef struct {
    uint32_t low:24, datl:8, high:24, dath:8;
} uni_chrrng_dat16_t;
@

<<[[uni_chrrng_dat32_t]]>>=
typedef struct {
    uint32_t low:24, len:8, dat;
} uni_chrrng_dat32_t;
@

<<Known Data Types>>=
uni_chrrng_dat16_t,uni_chrrng_dat32_t,%
@

<<Unicode property exports for generator>>=
<<[[uni_chrrng_dat16_t]]>>
<<[[uni_chrrng_dat32_t]]>>
int uni_cmprng_dat16(const void *a, const void *b); /* for bsearch/qsort */
uint16_t uni_chrrng_dat16(uint32_t cp, const uni_chrrng_dat16_t *tab,
                          uint32_t tab_len);
int uni_cmprng_dat32(const void *a, const void *b); /* for bsearch/qsort */
uint32_t uni_chrrng_dat32(uint32_t cp, const uni_chrrng_dat32_t *tab,
                          uint32_t tab_len);
@

<<Unicode property functions for generator>>=
int uni_cmprng_dat16(const void *a, const void *b)
{
    const uni_chrrng_dat16_t *_a = a, *_b = b;

    if(_a->high < _b->low)
        return -1;
    else if(_b->high < _a->low)
        return 1;
    else
        return 0;
}

int uni_cmprng_dat32(const void *a, const void *b)
{
    const uni_chrrng_dat32_t *_a = a, *_b = b;

    if(_a->low + _a->len < _b->low)
        return -1;
    else if(_b->low + _b->len < _a->low)
        return 1;
    else
        return 0;
}

uint16_t uni_chrrng_dat16(uint32_t cp, const uni_chrrng_dat16_t *tab,
                          uint32_t tab_len)
{
#if 0
  uni_chrrng_dat16_t cr = {cp, cp}, *tab_el;
  tab_el = bsearch(&cr, tab, tab_len, sizeof(uni_chrrng_dat16_t), uni_cmprng_dat16);
  return tab_el ? (tab_el->dath << 8) + tab_el->datl : 0;
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > tab[j].high)
      l = j + 1;
    else
      return (tab[j].dath << 8) + tab[j].datl;
  }
  return 0;
#endif
}

uint32_t uni_chrrng_dat32(uint32_t cp, const uni_chrrng_dat32_t *tab,
                          uint32_t tab_len)
{
#if 0
  uni_chrrng_dat32_t cr = {cp, cp}, *tab_el;
  tab_el = bsearch(&cr, tab, tab_len, sizeof(uni_chrrng_dat32_t), uni_cmprng_dat32);
  return tab_el ? tab_el->dat : 0;
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > tab[j].low + tab[j].len)
      l = j + 1;
    else
      return tab[j].dat;
  }
  return 0;
#endif
}
@

For the \emph{name} and \emph{alias} operations, a simple lookup table
can be provided, indexed on the enumeration value (again, except for
the name property).  The table fulfulls the \emph{list} requirement as
well.

For the \emph{value} operation, either a sorted table of names or some
other structure can be provided.  The advantage of the sorted table of
names is that it takes no effort to create, search, and provides the
\emph{list} operation (specifially for searching) as well.  A
statically generated hash function might be faster, though.  Since
applications that need to look up a lot of \emph{value}s are probably
rare, only the simple sorted table is provided, and the library user
is expected to convert that to a hash table, prefix tree, or some
other structure as needed.

\subsection{Name Tables}

The first thing to read in is the list of aliases.  These are keyed on
property short name.  Since there is already a list of property names,
the array of aliases may as well correspond.  As the list of aliases
is read, the comment may be stored as an alias as well.  The group
names for the gc property are stored in this file, and the comment for
the definition line for these names indicates the fundamental values
that are combined into this group.

<<UCD parser local definitions>>=
uni_alias_t **val_aliases;
int *num_val_aliases, *max_val_aliases;
@

<<Initialize UCD files>>=
inisize(val_aliases, num_prop_aliases);
clearbuf(val_aliases, num_prop_aliases);
inisize(num_val_aliases, num_prop_aliases);
clearbuf(num_val_aliases, num_prop_aliases);
inisize(max_val_aliases, num_prop_aliases);
clearbuf(max_val_aliases, num_prop_aliases);
@

<<Resize prop-associated arrays>>=
resize(val_aliases, maxparsed);
resize(num_val_aliases, maxparsed);
resize(max_val_aliases, maxparsed);
@

<<Clear prop-associated array[i]>>=
val_aliases[i] = NULL;
num_val_aliases[i] = max_val_aliases[i] = 0;
@

Then, the alias file can be read and stored in the array.  While it
would be more efficient to only look up the destination when things
change, it's easier to just go ahead and put it in place.

<<Initialize UCD files>>=
open_f("PropertyValueAliases.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  split_line(lbuf);
  if(num_fields < 3)
    continue;
  uni_alias_t me, *mep = &me;
  me.short_name = fields[0];
  uni_alias_t **ret = bsearch(&mep, prop_aliases_short, num_prop_aliases,
                              sizeof(*prop_aliases_short), cmp_shortname);
  if(!ret) {
    perror("PropertyValueAliases.txt");
    exit(1);
  }
  int idx = (int)(*ret - prop_aliases);
  if(!max_val_aliases[idx])
    inisize(val_aliases[idx], (max_val_aliases[idx] = 16));
  if(max_val_aliases[idx] == num_val_aliases[idx])
    resize(val_aliases[idx], (max_val_aliases[idx] *= 2));
  val_aliases[idx][num_val_aliases[idx]].short_name = strdup(fields[1]);
  val_aliases[idx][num_val_aliases[idx]].long_name = strdup(fields[2]);
  /* there are no comments on alias lines, so field count is correct */
  val_aliases[idx][num_val_aliases[idx]].alt_name =
                 num_fields > 3 ? strdup(fields[3]) : NULL;
  val_aliases[idx][num_val_aliases[idx]].alt_name2 =
                 num_fields > 4 ? strdup(fields[4]) : NULL;
  if(num_fields > 5) {
    perror("PropertyValueAliases.txt");
    exit(1);
  }
  if(line_comment && !strcmp(fields[0], "gc"))
    val_aliases[idx][num_val_aliases[idx]].alt_name2 = strdup(line_comment);
  num_val_aliases[idx]++;
}
fclose(f);
@

Since we're about to scan files with the enumerations in text form,
the next logical step is to build a search mechanism.  In this case,
just a simple sorted array, using the standard loose matching.  This
is necessary because some fields do not match any of the literal
values we have gathered so far.  This table will be dumped as well,
for the \emph{value} function.

<<[[uni_valueof_t]]>>=
typedef struct {
  const char *name;
  int val;
} uni_valueof_t;
@

<<Unicode property exports for generator>>=
<<[[uni_valueof_t]]>>
@

<<Known Data Types>>=
uni_valueof_t,%
@

<<UCD parser local definitions>>=
uni_valueof_t **enum_vals;
uint32_t *enum_vals_len;
@

<<Unicode property exports for generator>>=
int uni_cmp_valueof(const void *a, const void *b);
@

<<Unicode property functions for generator>>=
int uni_cmp_valueof(const void *a, const void *b)
{
  return strcmp(((uni_valueof_t *)a)->name, ((uni_valueof_t *)b)->name);
}
@

<<Initialize UCD files>>=
inisize(enum_vals, num_prop_aliases);
clearbuf(enum_vals, num_prop_aliases);
inisize(enum_vals_len, num_prop_aliases);
clearbuf(enum_vals_len, num_prop_aliases);
for(i = 0; i < num_prop_aliases; i++) {
  if(!val_aliases[i])
    continue;
  /* ignore boolean properties */
  if(num_val_aliases[i] == 2 && !strcmp(val_aliases[i][0].short_name, "N") &&
     !strcmp(val_aliases[i][1].short_name, "Y"))
    continue;
  uni_valueof_t *valueof;
  int num_valueof = 0;
  inisize(valueof, num_val_aliases[i] * 4);
  for(j = 0; j < num_val_aliases[i]; j++) {
    const uni_alias_t *va = &val_aliases[i][j];
    valueof[num_valueof].name = strdup(va->short_name);
    valueof[num_valueof++].val = j;
    valueof[num_valueof].name = strdup(va->long_name);
    valueof[num_valueof++].val = j;
    if(va->alt_name) {
      valueof[num_valueof].name = strdup(va->alt_name);
      valueof[num_valueof++].val = j;
    }
    /* need to skip comments stored in alt_name2; all contain | */
    if(va->alt_name2 && !strchr(va->alt_name2, '|')) {
      valueof[num_valueof].name = strdup(va->alt_name2);
      valueof[num_valueof++].val = j;
    }
  }
  for(j = 0; j < num_valueof; j++) {
    char *n = (char *)valueof[j].name, *d = n;
    for(; *n; n++) {
      if(isupper(*n))
        *d++ = tolower(*n);
      else if(*n != '_' && *n != '-' && !isspace(*n))
        *d++ = *n;
    }
    *d = 0;
  }
  qsort(valueof, num_valueof, sizeof(*valueof), uni_cmp_valueof);
  /* remove duplicates now that they are always contiguous */
  for(j = 1; j < num_valueof; j++)
    if(!strcmp(valueof[j].name, valueof[j - 1].name)) {
      free((char *)valueof[j].name);
      if(valueof[j].val != valueof[j - 1].val) {
        perror(valueof[j - 1].name);
	exit(1);
      }
      movebuf(valueof + j, valueof + j + 1, num_valueof - (j + 1));
      --j;
      --num_valueof;
    }
  enum_vals[i] = valueof;
  enum_vals_len[i] = num_valueof;
}
@

<<Resize prop-associated arrays>>=
resize(enum_vals, maxparsed);
resize(enum_vals_len, maxparsed);
@

<<Clear prop-associated array[i]>>=
enum_vals[i] = NULL;
enum_vals_len[i] = 0;
@

For each value alias, an enumeration needs to be printed to the header
file.  Both the short and long name should be supported, but there is
little value in supporting the secondary aliases.  This excludes
binary values, which have pointless yes/no aliases, and, if possible,
deprecated values.  Special care must be taken with the version
aliases, as the numeric values have decimal points.  The CCC aliases
are numeric as well, but in this case, the short name should be
ignored and used as the value directly instead.

In order to support the \emph{name} and \emph{value} functions, tables
need to be dumped as well.  First, a table similar to the locally
stored table is printed; this does the \emph{name} operation.  Next,
value table built just previously is dumped for the \emph{value}
operation.  Unlike the enumerations, no values may be skipped when
generating the string tables.  The string tables are stored in the
same file, because both files use the same set of string constants,
and most compilers share storage for like string constants.

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  const char *pn = i < num_prop_aliases ? prop_aliases[i].short_name :
                                          parsed_props[i].name;
  if(!enum_vals[i])
    continue;
  /* ignore boolean properties */
  if(parsed_props[i].rng)
    continue;
  <<Ignore unimplemented enums>>
  <<Generate C code for enumeration constants>>
}
@

<<Generate C code for enumeration constants>>=
fprintf(gen_h, "extern const uni_alias_t uni_%s_nameof[];\n", pn);
char nbuf[64];
sprintf(nbuf, "uni_%s_nameof.gen.c", pn);
open_wf(nf, nbuf);
fprintf(nf, "#include \"uni_prop.h\"\n\n"
	    "const uni_alias_t uni_%s_nameof[] = {\n", pn);
/* generating enum on gen_h */
fputs("typedef enum {\n", gen_h);
int last_nameof = -1;
for(j = 0; j < num_val_aliases[i]; j++) {
  const uni_alias_t *va = &val_aliases[i][j];
  /* print nameof table entry */
  char *e_nameof;
  int cur_nameof = strtol(va->short_name, &e_nameof, 10);
  if(!*e_nameof && strcmp(va->short_name, va->long_name))
    while(++last_nameof != cur_nameof)
      fputs("\t{ NULL },\n", nf);
  fprintf(nf, "\t{ \"%s\"", va->short_name);
  if(strcmp(va->long_name, va->short_name))
    fprintf(nf, ", \"%s\"", va->long_name);
  if(va->alt_name && strcmp(va->alt_name, va->long_name))
    fprintf(nf, ", \"%s\"", va->alt_name);
  /* again, | is to skip comments */
  if(va->alt_name2 && !strchr(va->alt_name2, '|') &&
     strcmp(va->alt_name, va->alt_name2))
    fprintf(nf, ", \"%s\"", va->alt_name2);
  fputs(" }", nf);
  if(j < num_val_aliases[i] - 1)
    putc(',', nf);
  putc('\n', nf);
  /* filter out bad enum values */
  /* skip version short name */
  if(strchr(va->short_name, '.')) {
    fprintf(gen_h, "\tUNI_%s_%s,\n", pn, va->long_name);
    continue;
  }
  /* use numeric values directly instead of making into enum */
  if(!isdigit(va->short_name[0]) || !strcmp(va->long_name, va->short_name)) {
    const char *np;
    /* slow, but necessary with e.g. ID_Restrict_Type */
    fprintf(gen_h, "\tUNI_%s_", pn);
    for(np = va->short_name; *np; np++) {
      putc(isalnum(*np) ? *np : '_', gen_h);
      /* + and - may both appear, so make + different */
      if(*np == '+')
        putc('_', gen_h);
    }
    putc(',', gen_h);
  }
  if(strcmp(va->long_name, va->short_name)) {
    fprintf(gen_h, "%cUNI_%s_%s = ", isdigit(va->short_name[0]) ? '\t' : ' ',
                    pn, va->long_name);
    if(!isdigit(va->short_name[0]))
      fprintf(gen_h, "UNI_%s_%s,", pn, va->short_name);
    else
      fprintf(gen_h, "%s,", va->short_name);
  }
  /* filter out alt names with dashes */
  if(va->alt_name && !strchr(va->alt_name, '-') &&
     strcmp(va->alt_name, va->long_name))
    fprintf(gen_h, " UNI_%s_%s = UNI_%s_%s,", pn, va->alt_name, pn, va->long_name);
  /* again, | is to skip comments */
  if(va->alt_name2 && !strchr(va->alt_name2, '|') && !strchr(va->alt_name2, '-') &&
     strcmp(va->alt_name, va->long_name) && strcmp(va->alt_name, va->alt_name2))
    fprintf(gen_h, " UNI_%s_%s = UNI_%s_%s,", pn, va->alt_name2, pn, va->long_name);
  fputc('\n', gen_h);
}
fputs("};\n", nf);
fprintf(gen_h, "\tUNI_NUM_%s\n} uni_%s_t;\n", pn, pn);
fprintf(gen_h, "extern const uni_valueof_t uni_%s_valueof[];\n", pn);
fprintf(nf, "const uni_valueof_t uni_%s_valueof[] = {\n", pn);
for(j = 0; j < enum_vals_len[i]; j++) {
  const char *name = val_aliases[i][enum_vals[i][j].val].short_name;
  if(isdigit(*name))
    name = val_aliases[i][enum_vals[i][j].val].long_name;
  fprintf(nf, "\t{ \"%s\", UNI_%s_", enum_vals[i][j].name, pn);
  /* slow, but necessary with e.g. ID_Restrict_Type */
  /* also, the CLDR ones have /, +, space, etc. */
  for(; *name; name++) {
    putc(isalnum(*name) ? *name : '_', nf);
    /* + and - may both appear, so make + different */
    if(*name == '+')
      putc('_', nf);
  }
  fprintf(nf, " }%s\n", j < enum_vals_len[i] - 1 ? "," : "");
}
fputs("};\n", nf);
fclose(nf);
fprintf(gen_h, "#define uni_%s_valueof_len %d /* %d lookups max */\n",
               pn, enum_vals_len[i], lg2(enum_vals_len[i] + 1));
@

Since the two arrays are stored in the same file, the logic to
generate file names needs to skip the valueof array.

<<Filter generated file names>>=
fgrep -v _valueof | \
@

Another useful array to print is for approximate name matching.  In
this case, approximate is defined by UAX44-LM3: case-insensitive,
optionally without any prefix of is, and without any spaces,
underscores, and dashes.  A function to perform most of this
transformation in-place is provided as well.  If additional
transformations are required, such as compatibility decomposition, they
must be done before calling this function.  All enumerations are
ASCII-only, so no decomposition is needed, and they are simply stored
lower-case with dashes, spaces, and underscores removed.  Initial
``is'' is left in; that should only be removed from user input.  To
compare using this function, strip, find a match, and if none is found,
search again with initial ``is'' removed if present.  A sample search
function is provided.

<<Unicode property exports for generator>>=
void uni_enum_name_strip(char *n);
@

<<Unicode property functions for generator>>=
void uni_enum_name_strip(char *n)
{
  char *s, *d;
  for(s = d = n; *s; s++)
    if(*s != '-' && *s != '_' && *s != ' ')
      *d++ = tolower(*s);
  *d = 0;
}
@

<<Unicode property exports>>=
/* note: this is not the fastest way to do this */
uint8_t uni_x_valueof_approx(const char *n, const uni_valueof_t *tab, int len,
                             uint8_t def);
@

<<Unicode property functions>>=
uint8_t uni_x_valueof_approx(const char *n, const uni_valueof_t *tab, int len,
                             uint8_t def)
{
  /* yuck - strdup on every call */
  char *s = strdup(n);
  uni_valueof_t v, *f;
  uni_enum_name_strip(s);
  v.name = s;
  /* more yuck - bsearch is 1/2 as fast as hand-coded binary search */
  f = bsearch(&v, tab, len, sizeof(*tab), uni_cmp_valueof);
  if(!f && !strncmp(s, "is", 2) && s[2]) {
    v.name = s + 2;
    /* more yuck - bsearch is 1/2 as fast as hand-coded binary search */
    f = bsearch(&v, tab, len, sizeof(*tab), uni_cmp_valueof);
  }
  free(s);
  return f ? f->val : def;
}
@

<<Generate C code for enumeration constants>>=
sprintf(nbuf, "uni_%s_valueof_approx.gen.c", pn);
open_wf(anf, nbuf);
fprintf(gen_h, "extern const uni_valueof_t uni_%s_valueof_approx[];\n", pn);
fprintf(anf, "const uni_valueof_t uni_%s_valueof_approx[] = {\n", pn);
uni_valueof_t *approx_array;
inisize(approx_array, enum_vals_len[i]);
for(j = 0; j < enum_vals_len[i]; j++) {
  approx_array[j].name = strdup(enum_vals[i][j].name);
  approx_array[j].val = enum_vals[i][j].val;
  uni_enum_name_strip((char *)approx_array[j].name);
}
qsort(approx_array, enum_vals_len[i], sizeof(*approx_array), uni_cmp_valueof);
for(j = 0; j < enum_vals_len[i]; j++) {
  const char *name = val_aliases[i][approx_array[j].val].short_name;
  if(isdigit(*name))
    name = val_aliases[i][approx_array[j].val].long_name;
  fprintf(anf, "\t{ \"%s\", UNI_%s_", approx_array[j].name, pn);
  /* slow, but necessary with e.g. ID_Restrict_Type */
  /* also, the CLDR ones have /, +, space, etc. */
  for(; *name; name++) {
    putc(isalnum(*name) ? *name : '_', anf);
    /* + and - may both appear, so make + different */
    if(*name == '+')
      putc('_', anf);
  }
  fprintf(anf, " }%s\n", j < enum_vals_len[i] - 1 ? "," : "");
  free((char *)approx_array[j].name);
}
fputs("};\n", anf);
fclose(anf);
free(approx_array);
fprintf(gen_h, "#define uni_%s_lookup(n) "
                   "uni_x_valueof_approx(n, %s_valueof_approx, %d, ~0)\n",
	       pn, pn, enum_vals_len[i]);
@

There is still one nagging issue:  the gc aliases include some that
are actually combinations of the others.  Since each value can only
hold one value, this is not going to work.  To support this, an extra
table is generated which translates a value into a 64-bit mask that
includes all base values.  For the base values themselves, only their
own bit will be set.  The pseudo values are recognized by a comment in
their last field, which is stored in one of the aliases.  This is the
reason the vertical bar was filtered out above:  all of the aliases
have vertical bars in their last field.

<<Generate C code for enumeration constants>>=
if(!strcmp(pn, "gc")) {
  fputs("extern const uint64_t uni_gc_trans[];\n", gen_h);
  open_wf(tf, "uni_gc_trans.gen.c");
  fputs("#include \"uni_prop.h\"\n\n"
	"const uint64_t uni_gc_trans[] = {\n", tf);
  for(j = 0; j < num_val_aliases[i]; j++) {
    const char *desc;
    fprintf(tf, "\t/* %2s */ ", val_aliases[i][j].short_name);
    desc = val_aliases[i][j].alt_name2;
    if(desc && !strchr(desc, '|'))
      desc = NULL;
    if(!desc)
      fprintf(tf, "1ULL << UNI_gc_%s", val_aliases[i][j].short_name);
    else {
      const char *desce;
      while(1) {
        desce = strchr(desc, ' ');
	if(!desce)
	  desce = desc + strlen(desc);
	fprintf(tf, "(1ULL << UNI_gc_%.*s)%s", (int)(desce - desc), desc,
	                                     *desce ? " | " : "");
	if(!*desce)
	  break;
	desc = desce + 3; /* past ' | ' */
      }
    }
    if(j < num_val_aliases[i] - 1)
      putc(',', tf);
    putc('\n', tf);
  }
  fputs("};\n", tf);
  fclose(tf);
}
@

One other set of aliases has already been read, and is treated in some
contexts almost like enumeration properties: the property names
themselves.  While it defeats the purpose of placing properties in
separate files for statically linking only needed properties, a master
table of all properties and their exported symbol values will make
other projects' data file generation much easier.

<<[[uni_propdesc_t]]>>=
typedef struct {
  uni_alias_t name;
  const uint32_t *mtab;
  uint32_t mtab_len;
  const void *tab;
  uint32_t tab_len;
  uni_prop_type_t type;
  <<Additional property structure members>>
} uni_propdesc_t;
@

<<Unicode property exports for generator>>=
typedef enum {
  UNI_PROP_TYPE_NONE,
  <<Additional property type names>>
  UNI_PROP_TYPE_BOOL, UNI_PROP_TYPE_ENUM
} uni_prop_type_t;
<<[[uni_propdesc_t]]>>
@

<<Known Data Types>>=
uni_propdesc_t,uni_prop_type_t,%
@

<<UCD parser local definitions>>=
uni_valueof_t *prop_ptrs;
@

<<Dump character information as C code>>=
fprintf(gen_h, "extern const uni_propdesc_t uni_propdesc[];\n");
open_wf(pnf, "uni_propdesc.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_propdesc_t uni_propdesc[] = {\n", pnf);
uint32_t num_prop_ptrs = 0, max_prop_ptrs;
inisize(prop_ptrs, max_prop_ptrs = 20);
int index = 0;
for(i = 0; i < nparsed; i++) {
  uni_prop_type_t t = UNI_PROP_TYPE_NONE;
  const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                            parsed_props[i].name;
  if(!parsed_props[i].mt)
    continue;
  if(parsed_props[i].rng)
    t = UNI_PROP_TYPE_BOOL;
  <<Set prop type for export>>
  if(t == UNI_PROP_TYPE_NONE)
    continue;
  if(index)
    fputs(",\n", pnf);
  if(num_prop_ptrs >= max_prop_ptrs - 4)
    resize(prop_ptrs, max_prop_ptrs *= 2);
  fprintf(pnf, "\t{{\"%s\"", name);
  prop_ptrs[num_prop_ptrs].name = strdup(name);
  prop_ptrs[num_prop_ptrs++].val = index;
  if(i < num_prop_aliases) {
    fprintf(pnf, ", \"%s\"", prop_aliases[i].long_name);
    if(strcmp(prop_aliases[i].long_name, name)) {
      prop_ptrs[num_prop_ptrs].name = strdup(prop_aliases[i].long_name);
      prop_ptrs[num_prop_ptrs++].val = index;
    }
    if(prop_aliases[i].alt_name && strcmp(prop_aliases[i].alt_name, name) &&
       strcmp(prop_aliases[i].long_name, prop_aliases[i].alt_name)) {
      fprintf(pnf, ", \"%s\"", prop_aliases[i].alt_name);
      prop_ptrs[num_prop_ptrs].name = strdup(prop_aliases[i].alt_name);
      prop_ptrs[num_prop_ptrs++].val = index;
    }
    if(prop_aliases[i].alt_name2 && strcmp(prop_aliases[i].alt_name, name) &&
       strcmp(prop_aliases[i].long_name, prop_aliases[i].alt_name) &&
       strcmp(prop_aliases[i].alt_name, prop_aliases[i].alt_name2)) {
      fprintf(pnf, ", \"%s\"", prop_aliases[i].alt_name2);
      prop_ptrs[num_prop_ptrs].name = strdup(prop_aliases[i].alt_name);
      prop_ptrs[num_prop_ptrs++].val = index;
    }
  }
  fprintf(pnf, "},\n"
               "\t\tuni_%s_mtab, uni_%s_mtab_len, uni_%s_%s, uni_%s_%s_len,\n",
               name, name, name, <<Plain table name>> "rng", name,
	       <<Plain table name>> "rng");
  fprintf(pnf, "\t\t%d", (int)t);
  <<Print additional property structure members>>
  fputc('}', pnf);
  index++;
}
fprintf(gen_h, "#define uni_propdesc_len %d\n"
               "extern const uni_valueof_t uni_propdesc_valueof[];\n",
               index);
fputs("\n};\n\n"
      "const uni_valueof_t uni_propdesc_valueof[] = {\n", pnf);
qsort(prop_ptrs, num_prop_ptrs, sizeof(*prop_ptrs), uni_cmp_valueof);
for(i = num_prop_ptrs - 1; i > 0; i--) {
  if(!strcmp(prop_ptrs[i -1].name, prop_ptrs[i].name)) {
    if(prop_ptrs[i - 1].val != prop_ptrs[i].val) {
      fprintf(stderr, "Prop name conflict %s\n", prop_ptrs[i].name);
      exit(1);
    }
    free((char *)prop_ptrs[i].name);
    num_prop_ptrs--;
    movebuf(prop_ptrs + i, prop_ptrs + i + 1, num_prop_ptrs - i);
  }
}
for(i = 0; i < num_prop_ptrs; i++)
  fprintf(pnf, "\t{\"%s\", %d}%s\n", prop_ptrs[i].name, prop_ptrs[i].val,
                                   i < num_prop_ptrs - 1 ? "," : "");
fputs("};\n", pnf);
fclose(pnf);
fprintf(gen_h, "#define uni_propdesc_valueof_len %d /* %d lookups max */\n"
               "extern const uni_valueof_t uni_propdesc_valueof_approx[];\n",
	       num_prop_ptrs, lg2(num_prop_ptrs + 1));
open_wf(paf, "uni_prop_valueof_approx.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_valueof_t uni_propdesc_valueof_approx[] = {\n", paf);
for(i = 0; i < num_prop_ptrs; i++)
  uni_enum_name_strip((char *)prop_ptrs[i].name);
qsort(prop_ptrs, num_prop_ptrs, sizeof(*prop_ptrs), uni_cmp_valueof);
for(i = num_prop_ptrs - 1; i > 0; i--) {
  if(!strcmp(prop_ptrs[i -1].name, prop_ptrs[i].name)) {
    if(prop_ptrs[i - 1].val != prop_ptrs[i].val) {
      fprintf(stderr, "Prop name loose conflict %s\n", prop_ptrs[i].name);
      exit(1);
    }
    num_prop_ptrs--;
    free((char *)prop_ptrs[i].name);
    movebuf(prop_ptrs + i, prop_ptrs + i + 1, num_prop_ptrs - i);
  }
}
for(i = 0; i < num_prop_ptrs; i++)
  fprintf(paf, "\t{\"%s\", %d}%s\n", prop_ptrs[i].name, prop_ptrs[i].val,
                                   i < num_prop_ptrs - 1 ? "," : "");
fputs("};\n", paf);
fclose(paf);
fprintf(gen_h, "#define uni_propdesc_valueof_approx_len %d /* %d lookups max */\n",
	       num_prop_ptrs, lg2(num_prop_ptrs + 1));
@

Now that the aliases have been taken care of, it's time to read in the
enumeration properties themselves.  As with the boolean types, a range
table will be built first, and then converted to a multi-level table
when finished.

<<Property parsed contents>>=
uni_chrrng_dat8_t *rng_dat8;
uint8_t def;
@

<<UCD parser local functions>>=
static void add_enum_rng(prop_t *p, uint32_t low, uint32_t high, uint8_t val)
{
  if(p->def == val)
    return;
  if(!p->max_len)
    inisize(p->rng_dat8, (p->max_len = 8));
  if(p->len && p->rng_dat8[p->len - 1].high == low - 1 &&
     p->rng_dat8[p->len - 1].dat == val)
    p->rng_dat8[p->len - 1].high = high;
  else {
    if(p->len == p->max_len)
      resize(p->rng_dat8, (p->max_len *= 2));
    p->rng_dat8[p->len].low = low;
    p->rng_dat8[p->len].high = high;
    p->rng_dat8[p->len].dat = val;
    ++p->len;
  }
}
@

<<Set prop type for export>>=
if(parsed_props[i].rng_dat8)
  t = UNI_PROP_TYPE_ENUM;
@

<<Additional property structure members>>=
uint8_t def;
const uni_alias_t *nameof;
const uni_valueof_t *valueof, *valueof_approx;
uint32_t nameof_len, valueof_len;
@

<<Print additional property structure members>>=
fprintf(pnf, ", %d", (int)parsed_props[i].def);
if(t == UNI_PROP_TYPE_ENUM && enum_vals[i]) {
  fprintf(pnf, ", uni_%s_nameof, uni_%s_valueof, uni_%s_valueof_approx,\n"
               "\t\tUNI_NUM_%s, uni_%s_valueof_len",
               name, name, name, name, name);
} else
  fputs(", NULL, NULL, NULL, 0, 0", pnf);
@

<<UCD parser local functions>>=
static uint32_t enum_val(int pno, const char *v)
{
  char *s = strdup(v), *t, *d;
  uni_valueof_t me, *vp;
  me.name = s;

  d = t = s;
  for(; *t; t++) {
    if(isupper(*t))
      *d++ = tolower(*t);
    else if(*t != '_' && *t != '-' && !isspace(*t))
      *d++ = *t;
  }
  *d = 0;
  vp = bsearch(&me, enum_vals[pno], enum_vals_len[pno], sizeof(me),
               uni_cmp_valueof);
  /* permit excess prefix of "is" */
  if(!vp && s[0] == 'i' && s[1] == 's' && s[2]) {
    me.name = s + 2;
    vp = bsearch(&me, enum_vals[pno], enum_vals_len[pno], sizeof(me),
                 uni_cmp_valueof);
  }
  free(s);
  if(!vp) {
    perror(v);
    exit(1);
  }
  return vp->val;
}
@

<<UCD parser local definitions>>=
#define decl_enum(n, d) \
  int prop_##n = -1; \
  const char *def_##n = d
#define add_enum(n, v) do { \
  if(prop_##n < 0) { \
    prop_##n = add_prop(#n); \
    parsed_props[prop_##n].def = def_##n ? enum_val(prop_##n, def_##n) : 0; \
  } \
  if(*v) \
    add_enum_rng(&parsed_props[prop_##n], low, high, enum_val(prop_##n, v)); \
} while(0)
#define add_int(n, val) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(isdigit(*val)) \
    add_enum_rng(&parsed_props[prop_##n], low, high, strtol(val, NULL, 0)); \
  else \
    add_enum(n, val); \
} while(0)
@

\subsection{Testing}

To test the table implementations, a different, but nearly identical,
set of routines is used compared to the ones one for booleans.

<<Functions to help test generated tables>>=
#define dat8(x, d) doit_dat8(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab, d)

static void doit_dat8(const char *name, const uni_chrrng_dat8_t *rng, uint32_t nent,
                      const uint32_t *mtab, uint8_t def)
{
    uint32_t i;

    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      uint8_t rv = uni_chrrng_dat8(i, rng, nent, def);
      uint8_t mv = <<range table data for [[i]]>>;
      if(rv != mv) {
        fprintf(stderr, "mismatch %s@%d %d %d\n", name, i, (int)rv, (int)mv);
	exit(1);
      }
    }
    /* check performance */
    int j;
    unsigned long tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = uni_chrrng_dat8(i, rng, nent, def);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = <<range table data for [[i]]>>;
    tt = tend();
    printf("  r%ld t%ld %.2fx\n", tr, tt, (double)tr / (double)tt);
}
@

\subsection{Parsing the UCD}

First, [[UnicodeData.txt]] has a few fields.  Field 3 is gc.  Field 4
is ccc.  Field 5 provides bc, but its default value is complicated and
more easily obtained from [[extracted/DerivedBidiClass.txt]]%
\footnote{\label{fn:extracted}Technically, extracted files are
informative only, and are not guaranteed to be generated correctly.
However, I will trust that they are.  Generating the correct data from
scratch is trivial, but an extra complication to the code I'd rather
avoid.}%
. Field 6 provides
dt, albeit indirectly. Fields 7 through 9 provide nt, even more
indirectly%
\footnote{[[extracted/DerivedNumericType.txt]] might be a better
source for nt, as it includes Unihan data.}%
.

<<Initialize UCD files>>=
decl_enum(gc, "Cn");
decl_enum(ccc, 0);
decl_enum(bc, "L");
decl_enum(dt, "None");
decl_enum(nt, "None");
@

<<Process a line of [[UnicodeData.txt]]>>=
add_enum(gc, fields[2]);
add_int(ccc, fields[3]);
/* add_enum(bc, fields[4]); */
if(fields[5][0] == '<') {
  char *eval = fields[5] + 1;
  while(*eval && *eval != '>')
    eval++;
  if(!*eval) {
    perror("dt");
    exit(1);
  }
  *eval = 0;
  add_enum(dt, fields[5] + 1);
  *eval = '>';
} else if(fields[5][0])
  add_enum(dt, "can");
if(fields[6][0])
  add_enum(nt, "decimal");
else if(fields[7][0])
  add_enum(nt, "digit");
else if(fields[8][0])
  add_enum(nt, "numeric");
@

<<Parse UCD files>>=
open_f("extracted/DerivedBidiClass.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(bc, fields[1]);
}
fclose(f);
@

The other file that has mixed field types is
[[DerivedNormalizationProps.txt]].  The Boolean parser simply looked
at the number of fields, and added if the number of fields was too low
to be anything but a boolean.  This parser can't really do the same,
without first loading the expected default values.  There are only
four properties, so it's safer to just do them by hand.  In fact, two
of them can only have two values: Yes and No.  This makes them boolean
in my view, so that they will become.  There is one other caveat:  the
default value for those two is Yes, and the listed value is always No,
so they need to be inverted when finished.

<<Initialize UCD files>>=
decl_enum(NFC_QC, "Y");
decl_bool(NFD_QC);
decl_enum(NFKC_QC, "Y");
decl_bool(NFKD_QC);
@

<<Process a line of [[DerivedNormalizationProps.txt]]>>=
if(num_fields == 3) {
  if(!strcmp(fields[1], "NFC_QC"))
    add_enum(NFC_QC, fields[2]);
  else if(!strcmp(fields[1], "NFD_QC"))
    add_bool(NFD_QC);
  else if(!strcmp(fields[1], "NFKC_QC"))
    add_enum(NFKC_QC, fields[2]);
  else if(!strcmp(fields[1], "NFKD_QC"))
    add_bool(NFKD_QC);
}
@

<<Parse UCD files>>=
uni_chrrng_t *new;
uint32_t new_len;
uni_chrrng_setop(parsed_props[prop_NFD_QC].rng,
                 parsed_props[prop_NFD_QC].len, UNI_SOP_INV_A,
		 NULL, 0, &new, &new_len);
free(parsed_props[prop_NFD_QC].rng);
parsed_props[prop_NFD_QC].rng = new;
parsed_props[prop_NFD_QC].len = new_len;
uni_chrrng_setop(parsed_props[prop_NFKD_QC].rng,
                 parsed_props[prop_NFKD_QC].len, UNI_SOP_INV_A,
		 NULL, 0, &new, &new_len);
free(parsed_props[prop_NFKD_QC].rng);
parsed_props[prop_NFKD_QC].rng = new;
parsed_props[prop_NFKD_QC].len = new_len;
@

Then there are a number of files that describe just one enumerated
property.  Many of these follow here.

<<Initialize UCD files>>=
decl_enum(sc, "Zzzz");
decl_enum(blk, "No_Block");
decl_enum(hst, "NA");
decl_enum(lb, "XX");
decl_enum(GCB, "XX");
decl_enum(SB, "XX");
decl_enum(WB, "XX");
decl_enum(ea, "N");
decl_enum(age, "unassigned");
@

<<Parse UCD files>>=
open_f("Scripts.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  /* need to manually convert UNI_SC_Hrkt to UNI_SC_Hira+UNI_SC_Kana */
  add_enum(sc, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("Blocks.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(blk, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("HangulSyllableType.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(hst, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("LineBreak.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(lb, fields[1]);
}
fclose(f);
@

\lstset{language=txt}
<<FIXME>>=
test with auxiliary/LineBreakTest.txt
support CLDR override
@

\lstset{language=C}
<<Parse UCD files>>=
open_f("auxiliary/GraphemeBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
   add_enum(GCB, fields[1]);
}
fclose(f);
@

\lstset{language=txt}
<<FIXME>>=
test with auxiliary/GraphemeBreakTest.txt
support CLDR override
@

\lstset{language=C}
<<Parse UCD files>>=
open_f("auxiliary/SentenceBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(SB, fields[1]);
}
fclose(f);
@

\lstset{language=txt}
<<FIXME>>=
test with auxiliary/SentenceBreakTest.txt
support CLDR override
@

\lstset{language=C}
<<Parse UCD files>>=
open_f("auxiliary/WordBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(WB, fields[1]);
}
fclose(f);
@

\lstset{language=txt}
<<FIXME>>=
test with auxiliary/WordBreakText.txt
support CLDR override
@

\lstset{language=C}
<<Parse UCD files>>=
open_f("EastAsianWidth.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(ea, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("DerivedAge.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(age, fields[1]);
}
fclose(f);
@

And there is one file that has two properties per line:
[[ArabicShaping.txt]]%
\footnote{Actually, [[ArabicShaping.txt]] has three values per line,
but field 2 (descriptive name) is informative and will not be read in
as a property unless I find a use.}%
.  However, like the bc property, the jt
property's default value is not a single value.  For that reason, it
is read from the derived file instead%
\footnote{see footnote \ref{fn:extracted} on page \pageref{fn:extracted}.}%
.

<<Initialize UCD files>>=
decl_enum(jt, "U");
decl_enum(jg, "No_Joining_Group");
@

<<Parse UCD files>>=
open_f("ArabicShaping.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  /* add_enum(jt, fields[2]); */
  add_enum(jg, fields[3]);
}
fclose(f);
open_f("extracted/DerivedJoiningType.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(jt, fields[1]);
}
fclose(f);
@

\subsection{Generating the Static Data}

Once finished, the tables can be dumped.  Just as with boolean
properties, only range tables have been built, so it's time to
generate the multi-level table, assuming that the range data is
correct.  Since the way that the automatic enumerations are generated
does not guarantee that zero is the default value, and zero is stored
more efficiently than any other value, all values are incremented by
one and the default value is aliased to zero as well.  The default
value itself is never stored in the table; it is supplied in the
lookup function as well.  The actual multi-level table data is just
the adjusted enumeration value, one byte per entry.

<<Unicode property exports for generator>>=
uint32_t *uni_rng_dat8_to_multi(const uni_chrrng_dat8_t *tab, uint32_t tab_len,
                                uint32_t *ml_len, uint8_t def);
@

<<Unicode property functions for generator>>=
uint32_t *uni_rng_dat8_to_multi(const uni_chrrng_dat8_t *tab, uint32_t tab_len,
                                uint32_t *ml_len, uint8_t def)
{
  uint32_t low, high, len, i;
  uint32_t *ml;
  uint8_t *bits;

  /* degenerate case:  always out-of-range */
  if(!tab_len) {
    uni_bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  for(i = tab_len; i > 0; i--)
    if(tab[i - 1].dat != def)
      break;
  high = tab[i - 1].high;
  for(i = 0; i < tab_len; i++)
    if(tab[i].dat != def)
      break;
  low = tab[i].low;
  len = high - low + 1;
  inisize(bits, len);
  /* Optimize(maybe): only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  for(; i < tab_len; i++)
    if(tab[i].dat != def)
      memset(bits + tab[i].low - low, tab[i].dat + 1, tab[i].high - tab[i].low + 1);
  uni_bits_to_multi(bits, len, low, high, 0, 0, &ml, ml_len);
  free(bits);
  return ml;
}
@

<<UCD parser local functions>>=
static void fixup_rng_dat8(prop_t *p)
{
  uint32_t i;
  qsort(p->rng_dat8, p->len, sizeof(uni_chrrng_dat8_t), uni_cmprng_dat8);
  /* starting at top means only optimized entries are memmove'd */
  for(i = p->len - 1; i > 0; i--) {
    uint32_t j = i;
    while(i > 0 && p->rng_dat8[i - 1].high == p->rng_dat8[i].low - 1 &&
          p->rng_dat8[i - 1].dat == p->rng_dat8[i].dat)
      i--;
    if(i == j)
      continue;
    p->rng_dat8[i].high = p->rng_dat8[j].high;
    if(j < p->len - 1)
        movebuf(p->rng_dat8 + i + 1, p->rng_dat8 + j + 1, p->len - (j + 1));
    p->len -= j - i;
    if(!i)
      break;
  }
}
@

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng_dat8) {
    fixup_rng_dat8(&parsed_props[i]);
    parsed_props[i].mt = uni_rng_dat8_to_multi(parsed_props[i].rng_dat8,
                                               parsed_props[i].len,
                                               &ml_len, parsed_props[i].def);
}
@

There are no 16-bit and 32-bit values to store yet, but here is a good
a place as any to write the multi-tab conversion functions.  Since
these are not for enumerations, no special handling is provided for
the default value:  it is always assumed to be zero.  Therefore the
multi-level table data is just the raw native-endian 16-bit or 32-bit
data.

<<Unicode property exports for generator>>=
uint32_t *uni_rng_dat16_to_multi(const uni_chrrng_dat16_t *tab, uint32_t tab_len,
                                 uint32_t *ml_len);
@

<<Unicode property functions for generator>>=
uint32_t *uni_rng_dat16_to_multi(const uni_chrrng_dat16_t *tab, uint32_t tab_len,
                                 uint32_t *ml_len)
{
  uint32_t low, high, len, i, j;
  uint32_t *ml;
  uint16_t *bits;

  /* degenerate case:  always out-of-range */
  if(!tab_len) {
    uni_bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  for(i = tab_len; i > 0; i--)
    if(tab[i - 1].datl || tab[i - 1].dath)
      break;
  high = tab[i - 1].high;
  for(i = 0; i < tab_len; i++)
    if(tab[i].datl || tab[i].dath)
      break;
  low = tab[i].low;
  len = high - low + 1;
  inisize(bits, len);
  /* Optimize(maybe): only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  for(; i < tab_len; i++)
    if(tab[i].datl || tab[i].dath)
      for(j = tab[i].low; j <= tab[i].high; j++)
        bits[j - low] = (tab[i].dath << 8) + tab[i].datl;
  uni_bits_to_multi((uint8_t *)bits, len * 2, low * 2, high * 2, 0, 0, &ml, ml_len);
  free(bits);
  return ml;
}
@

<<Unicode property exports for generator>>=
uint32_t *uni_rng_dat32_to_multi(const uni_chrrng_dat32_t *tab, uint32_t tab_len,
                                 uint32_t *ml_len);
@

<<Unicode property functions for generator>>=
uint32_t *uni_rng_dat32_to_multi(const uni_chrrng_dat32_t *tab, uint32_t tab_len,
                                 uint32_t *ml_len)
{
  uint32_t low, high, len, i, j;
  uint32_t *ml;
  uint32_t *bits;

  /* degenerate case:  always out-of-range */
  if(!tab_len) {
    uni_bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  for(i = tab_len; i > 0; i--)
    if(tab[i - 1].dat)
      break;
  high = tab[i - 1].low + tab[i - 1].len;
  for(i = 0; i < tab_len; i++)
    if(tab[i].dat)
      break;
  low = tab[i].low;
  len = high - low + 1;
  inisize(bits, len);
  /* Optimize(maybe): only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  for(; i < tab_len; i++)
    if(tab[i].dat)
      for(j = 0; j <= tab[i].len; j++)
        bits[tab[i].low - low + j] = tab[i].dat;
  uni_bits_to_multi((uint8_t *)bits, len * 4, low * 4, high * 4, 0, 0, &ml, ml_len);
  free(bits);
  return ml;
}
@

In addition to the property tables, a simple query function is
printed.  This calls a generic search function using the multi-level
table.  The test cases are printed as well, and added to the boolean
tests.

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng_dat8) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    char nbuf[64];
    sprintf(nbuf, "uni_%s_rng.gen.c", name);
    open_wf(of, nbuf);
    fprintf(of, "#include \"uni_prop.h\"\n\n"
		"const uni_chrrng_dat8_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].len; j++) {
      uint32_t val = parsed_props[i].rng_dat8[j].dat;
      uni_alias_t *aliases = val_aliases[i];
      fprintf(of, "\t{ 0x%04X, 0x%04X, ",
                  parsed_props[i].rng_dat8[j].low,
	          parsed_props[i].rng_dat8[j].high);
      if(!aliases || (isdigit(aliases[0].short_name[0]) &&
                      !strchr(aliases[0].short_name, '.')))
        fprintf(of, "%d", (int)val);
      else {
        const uni_alias_t *alias = &aliases[val];
	const char *val_name = alias->short_name;
        if(isdigit(val_name[0]))
	  val_name = alias->long_name;
	if(strchr(val_name, '-')) {
	  const char *np;
	  /* slow, but necessary with e.g. ID_Restrict_Type */
	  fprintf(of, "UNI_%s_", name);
	  for(np = val_name; *np; np++)
	    putc(*np == '-' ? '_' : *np, of);
	} else
	  fprintf(of, "UNI_%s_%s", name, val_name);
      }
      fprintf(of, " }%s\n", j < parsed_props[i].len - 1 ? "," : "");
    }
    fputs("};\n", of);
    fclose(of);
    fprintf(gen_h, "extern const uni_chrrng_dat8_t uni_%s_rng[];\n"
		   "#define uni_%s_rng_len %d /* %d lookups max */\n"
		   "#define uni_%s_of(x) uni_x_of(x, uni_%s_mtab, %d)\n",
		   name, name, parsed_props[i].len,
		   lg2(parsed_props[i].len + 1), name, name,
		   parsed_props[i].def);
    print_mtab(name, parsed_props[i].mt, gen_h);
    fprintf(tstf, "dat8(%s, %d);\n", name, parsed_props[i].def);
  }
}
@

<<Unicode property exports>>=
int uni_x_of(uint32_t cp, const uint32_t *tab, uint8_t def);
@

<<Unicode property functions>>=
int uni_x_of(uint32_t cp, const uint32_t *tab, uint8_t def)
{
  const uint8_t *mr;
  uint8_t mv = uni_multi_tab_lookup(tab, cp, &mr, 0);
  if(mr)
    mv = *mr;
  if(!mv)
    return def;
  else
    return mv - 1;
}
@

<<range table data for [[i]]>>=
uni_x_of(i, mtab, def)
@

\lstset{language=txt}
<<FIXME>>=
IndicSyllabicCategory.txt: field 2 == InSC (default = Other)
IndicMatraCategory.txt: field 2 == InMC (default = NA)
@

\section{Numeric Properties}

Numeric properties are a subset of enumerated properties.  The
property's primary values are integers or two dot-separated integers
(i.e., floating point values).  They may have non-numeric values and
aliases as well.  The main difference is that the sloppy matching
should also support matching the plain integer against any like-valued
integer (e.g., 01 matches 1) and the dotted value against any
like-valued floating point number (e.g., 01.10 matches 1.1).  In
addition, the age property is meant to match equal to or less than the
search value.  For example, 3.0 matches 1.1 as well.

\subsection{Storage Methods}

Numeric values are stored as rational numbers (fractions).  Since the
current standard has no numbers requiring denominators over 127, the
denominator is stored in a single byte.  The numerator needs no more
than 19 bits, so it is stored in the remaining 3 bytes.  The generic
32-bit data type developed in the last section can be used to store
these, by casting the 32-bit value back and forth between the native
fraction structure ([[uni_frac_t]]).

\lstset{language=C}
<<[[uni_frac_t]]>>=
typedef struct {
    int32_t num:24;
    uint32_t denom:8;
} uni_frac_t;
@

<<Unicode property exports for generator>>=
<<[[uni_frac_t]]>>
void uni_chrrng_val(uint32_t cp, const uni_chrrng_dat32_t *tab,
                    uint32_t tab_len, int32_t *num, uint8_t *denom);
@

<<Known Data Types>>=
uni_frac_t,%
@

<<Unicode property functions for generator>>=
void uni_chrrng_val(uint32_t cp, const uni_chrrng_dat32_t *tab,
                    uint32_t tab_len, int32_t *num, uint8_t *denom)
{
  uint32_t res = uni_chrrng_dat32(cp, tab, tab_len);
  uni_frac_t *f = (uni_frac_t *)&res;
  if(!f->num)
    f->denom = 1;
  *num = f->num;
  *denom = f->denom;
}
@

<<Property parsed contents>>=
uni_chrrng_dat32_t *rng_dat32;
int rng_num; /* flag: rng_dat32's data is actually uni_frac_t */
@

<<UCD parser local functions>>=
static void add_dat32_rng(prop_t *p, uint32_t low, uint32_t high,
                          uint32_t dat)
{
  if(!p->max_len)
    inisize(p->rng_dat32, (p->max_len = 8));
  if(p->len) {
    uni_chrrng_dat32_t *last = &p->rng_dat32[p->len -1];
    if(last->len < 255 && last->low + last->len == low - 1 &&
       last->dat == dat) {
      if(high < last->low + 256) {
        last->len = high - last->low;
	return;
      }
      last->len = 255;
      low = last->low + 256;
    }
  }
  while(1) {
    if(p->len == p->max_len)
      resize(p->rng_dat32, (p->max_len *= 2));
    p->rng_dat32[p->len].dat = dat;
    p->rng_dat32[p->len].low = low;
    ++p->len;
    if(high < low + 256) {
      p->rng_dat32[p->len - 1].len = high - low;
      return;
    }
    p->rng_dat32[p->len - 1].len = 255;
    low += 256;
  }
}

static void add_num_rng(prop_t *p, uint32_t low, uint32_t high,
                        int32_t num, uint8_t denom)
{
  uni_frac_t f = {num, denom};
  uint32_t *v = (uint32_t *)&f;
  if(!num) /* store 0 as 0/0 for consistency */
    f.denom = 0;
  p->rng_num = 1;
  add_dat32_rng(p, low, high, *v);
}
@

<<UCD parser local definitions>>=
#define decl_num(n) int prop_##n = -1
#define add_num(n, num, denom) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  add_num_rng(&parsed_props[prop_##n], low, high, num, denom); \
} while(0)
@

\subsection{Testing}

To test the table implementations, a different, but nearly identical,
set of routines is used compared to the ones one for booleans.

<<Functions to help test generated tables>>=
#define num(x) doit_num(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab)

static void doit_num(const char *name, const uni_chrrng_dat32_t *rng, uint32_t nent,
                     const uint32_t *mtab)
{
    uint32_t i;

    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      int32_t rn, mn;
      uint8_t rd, md;
      uni_chrrng_val(i, rng, nent, &rn, &rd);
      <<range table val for [[i]]>>;
      if(rn != mn || rd != md) {
        fprintf(stderr, "mismatch %s@%d %d/%d %d/%d\n", name, i,
	                (int)rn, (int)rd, (int)mn, (int)md);
	exit(1);
      }
    }
    /* check performance */
    int j;
    unsigned long tr, tt;
    tstart();
    int32_t mn;
    uint8_t md;
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        uni_chrrng_val(i, rng, nent, &mn, &md);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        <<range table val for [[i]]>>;
    tt = tend();
    printf("  r%ld t%ld %.2fx\n", tr, tt, (double)tr / (double)tt);
}
@

\subsection{Parsing the UCD}

Numeric properties from [[UnicodeData.txt]] are ccc (field 4) and nv
(field 9)%
\footnote{[[extracted/DerivedNumericValues.txt]] might be a better
source for nv, as it includes Unihan data.}%
.  Since ccc has aliases, it is already added as an
enumeration.  The nv field is already in normal rational form.

<<Initialize UCD files>>=
decl_num(nv);
@

<<Process a line of [[UnicodeData.txt]]>>=
if(*fields[8]) {
  int32_t num = strtol(fields[8], &s, 10);
  uint8_t denom = *s ? strtol(s + 1, NULL, 10) : 1;
  add_num(nv, num, denom);
}
@

An optional numeric property comes from
[[Unihan_RadicalStrokeCounts.txt]]: cjkRSUnicode%
\footnote{Listed in UniHan as Informative, but still needed for regex.}%
.  It is, like all unihan files, a tab-separated file, with the single
code point in U+ notation in field 1.  Note that the actual optional
property is CJK\_Radical, which I interpret as meaning the same as
cjkRSUnicode. However, I will not add a non-normative alias
(especially considering that the library currently only supports four
names, and there are already four names for this property).

Technically, this file also includes a number of other stroke count
properties (kRSAdobe\_Japan1\_6, kRSJapanese, kRSKangXi, kRSKanWa,
kRSKorean)%
\footnote{Listed in UniHan as Provisional}%
.  These are not required for regular expressions, so I will
not be provided as a property until I find a use for them.  Most of
them would be trivial to add, though, since they are the same format
as kRSUnicode.  However, kRSAdobe\_Japan1\_6 has additional
information that would turn it into a string property.

There is another, related file ([[CJKRadicals.txt]]) which provides
additional interpretation for the radical numbers returned by this
property.  Since this file's values have no proprety label, I will not
provide them as a property for now.

<<UCD parser local functions>>=
static void split_line_tab(char *buf)
{
  if(!max_fields)
    inisize(fields, (max_fields = 16));
  num_fields = 0;
  while(*buf != '\t' && isspace(*buf)) buf++;
  if(!*buf || *buf == '#')
    return;
  while(1) {
    while(isspace(*buf) && *buf != '\t')
      buf++;
    char *f = buf, *nf, fc;
    for(nf = buf; *nf && *nf != '\t'; nf++);
    fc = *nf;
    buf = nf + 1;
    while(nf > f && isspace(nf[-1])) --nf;
    *nf = 0;
    if(num_fields == max_fields)
      resize(fields, (max_fields *= 2));
    fields[num_fields++] = f;
    if(!fc)
      return;
  }
}
@

<<Initialize Unihan files>>=
decl_num(cjkRSUnicode);
@

<<Parse Unihan files>>=
open_f("Unihan_RadicalStrokeCounts.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse Unihan cp>>
  if(!strcmp(fields[1], "kRSUnicode")) {
    /* due to complexity of this field, the num/denom is interpreted */
    /* differently: */
    /* denom is number after .; if after !., then it is negated */
    /* num is 3 byte fields; lowest field is left of . or !. */
    /* if there are two values, then the 2nd value is in the upper bytes */
    int32_t num;
    uint8_t denom;
    num = strtol(fields[2], &s, 10);
    if(*s == '!')
      denom = -strtol(s + 2, &s, 10);
    else
      denom = strtol(s + 1, &s, 10);
    if(*s) {
      num |= strtol(s, &s, 10) << 8;
      if(*s == '!')
        num |= -strtol(s + 2, &s, 10) << 16;
      else
        num |= strtol(s + 1, &s, 10) << 16;
    }
    add_num(cjkRSUnicode, num, denom);
  }
}
fclose(f);
@

<<Parse Unihan cp>>=
split_line_tab(lbuf);
if(num_fields < 2)
  continue;
low = high = strtol(lbuf + 2, &s, 16);
if(!low || *s) { /* should never happen, but if it does: ignore */
  fprintf(stderr, "bad col 1: %s\n", lbuf);
  continue;
}
@

<<Unicode property exports>>=
/* extract l.r value; l!.r is encoded as l.-r */
#define uni_kRSUnicode_val1(n, d, l, r) do { \
  l = (uint8_t)(n); \
  r = (int8_t)d; \
} while(0)
#define uni_kRSUnicode_val2(n, d, l, r) do { \
  l = (uint8_t)(n >> 8); \
  r = (int32_t)n >> 16; \
} while(0)
@

<<C Prototypes>>=
void uni_kRSUnicode_val1(int32_t num, uint8_t denom, uint8_t &l,
                         int8_t &r);
void uni_kRSUnicode_val2(int32_t num, uint8_t denom, uint8_t &l,
                         int8_t &r);
@

\lstset{language=txt}
<<FIXME>>=
CJKRadicals.txt:
  field 1 = radical # (index); may include ' (maps to !?)
  field 2 = CJK Radical character
  field 3 = CJK Unified Ideograph character
 2 & 3 are always 1 16-bit char, so could store as 32-bit value per index
@

The only other numeric property currently supported is age, which
comes from [[DerivedAge.txt]].  Since it has aliases, it is already
added as an enumeration.

\subsection{Generating the Static Data}

Once finished, the tables can be dumped.  Just as with other
properties, only range tables have been built, so it's time to
generate the multi-level table, assuming that the range data is correct.

\lstset{language=C}
<<UCD parser local functions>>=
static void fixup_rng_dat32(prop_t *p)
{
  uint32_t i;
  qsort(p->rng_dat32, p->len, sizeof(uni_chrrng_dat32_t), uni_cmprng_dat32);
  /* starting at top means only optimized entries are memmove'd */
  for(i = p->len - 1; i > 0; i--) {
    uint32_t j = i, len = p->rng_dat32[i].len, k;
    while(i > 0 &&
          p->rng_dat32[i - 1].low + p->rng_dat32[i - 1].len == p->rng_dat32[i].low - 1 &&
          p->rng_dat32[i - 1].dat == p->rng_dat32[i].dat) {
      i--;
      len += p->rng_dat32[i].len;
    }
    if(i == j)
      continue;
    k = i;
    while(len >= 256) {
      p->rng_dat32[k++].len = 255;
      p->rng_dat32[k].low = p->rng_dat32[k - 1].low + 256;
    }
    p->rng_dat32[k].len = len;
    if(k != j) {
      if(j < p->len - 1)
          movebuf(p->rng_dat32 + k + 1, p->rng_dat32 + j + 1, p->len - (j + 1));
      p->len -= j - k;
    }
    if(!i)
      break;
  }
}
@

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng_dat32) {
    fixup_rng_dat32(&parsed_props[i]);
    parsed_props[i].mt = uni_rng_dat32_to_multi(parsed_props[i].rng_dat32,
                                                parsed_props[i].len,
                                                &ml_len);
}
@

In addition to the property tables, a simple query function is
printed.  This calls a generic search function using the multi-level
table.

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng_dat32) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    char nbuf[64];
    sprintf(nbuf, "uni_%s_rng.gen.c", name);
    open_wf(of, nbuf);
    fprintf(of, "#include \"uni_prop.h\"\n\n"
                "const uni_chrrng_dat32_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].len; j++)
      fprintf(of, "\t{ 0x%04X, %d, 0x%08x }%s\n",
                  parsed_props[i].rng_dat32[j].low,
	          parsed_props[i].rng_dat32[j].len,
	          parsed_props[i].rng_dat32[j].dat,
		  j < parsed_props[i].len - 1 ? "," : "");
    fputs("};\n", of);
    fclose(of);
    fprintf(gen_h, "extern const uni_chrrng_dat32_t uni_%s_rng[];\n"
		   "#define uni_%s_rng_len %d /* %d lookups max */\n",
		   name, name, parsed_props[i].len,
		   lg2(parsed_props[i].len + 1));
    <<Print special 32-bit range table stuff>>
    print_mtab(name, parsed_props[i].mt, gen_h);
  }
}
@

<<Print special 32-bit range table stuff>>=
if(parsed_props[i].rng_num) {
  fprintf(gen_h, "#define uni_%s_of(x, n, d) uni_x_val(x, uni_%s_mtab, n, d)\n",
	         name, name);
  fprintf(tstf, "num(%s);\n", name);
}
@

<<Unicode property exports>>=
uint32_t uni_x_dat32(uint32_t cp, const uint32_t *tab);
void uni_x_val(uint32_t cp, const uint32_t *tab, int32_t *num, uint8_t *denom);
@

<<Unicode property functions>>=
uint32_t uni_x_dat32(uint32_t cp, const uint32_t *tab)
{
  const uint8_t *mr;
  int r = uni_multi_tab_lookup(tab, cp * 4, &mr, 0);
  return mr ? *(uint32_t *)mr : r ? ~0 : 0;
}

void uni_x_val(uint32_t cp, const uint32_t *tab, int32_t *num, uint8_t *denom)
{
  const uint8_t *mr;
  uni_multi_tab_lookup(tab, cp * 4, &mr, 0);
  if(!mr)
    *num = 0;
  else {
    const uni_frac_t *v = (const uni_frac_t *)mr;
    *num = v->num;
    *denom = v->denom;
  }
  if(!*num)
    *denom = 1;
}
@

<<range table val for [[i]]>>=
uni_x_val(i, mtab, &mn, &md);
@

<<Additional property type names>>=
UNI_PROP_TYPE_NUM,
@

<<Set prop type for export>>=
if(parsed_props[i].rng_num)
  t = UNI_PROP_TYPE_NUM;
@

\lstset{language=txt}
<<FIXME>>=
numeric (UAX44-LM1):
  compare as floating-point number w/ limited precision
  e.g. 01.00 == 1, 0.3333 = 1/3
@

<<FIXME>>=
Unihan_NumericValues.txt: kAccountingNumeric, kOtherNumeric, kPrimaryNumeric
  (may need inclusion in nt/nv)
  (note: listed in unihan as "informative")
  (note: no character has more than one of these)
Unihan_DictionaryLikeData.txt:  kFourCornerCode, kFrequency, kGradeLevel,
                                kHKGlyph, kTotalStrokes
				kCHaiT: maybe; maybe needs alt. rep.
				kFenn, kPhonetic: could put last bit in denom
Unihan_DictionaryIndices.txt: lots.  may need alt. rep to fit values
Unihan_OtherMappings.txt: lots (may need string for some)
@

\section{String Properties}

String properties are those which have a value that cannot be
represented using the previously described methods.  Since the UCD is
text, this is generally either an ASCII or Unicode string.  Useful
operations include:

\begin{itemize}
\item Find out the string-valued value \emph{of} the property for a
character.  For variable-length strings, this should support two-part
retrieval: first obtain the length, and then obtain the string.  If
there is a maximum length, it should be listed so that the sizing step
can be avoided.
\end{itemize}

\subsection{Storage Methods}

The storage methods used for boolean properties can be used for
strings as well, with some adjustments.  First of all, there are very
few consecutive characters with the same value (other than the default
value, which is an empty string), so using a range list is wasteful.
Instead, a simple code point list with value is used.  One way to
store the string would be as a pointer to a zero-terminated string (no
strings use zero as a component value).  However, this requires
computing string length at every access, so a better way would be to
store a pointer and a length.  For 64-bit systems, this would require
at least 8 bytes for the pointer, and probably another 8 bytes to
align the structure, so 4 bytes for the code point and 4 bytes for the
length.  However, a more efficient storage method would be to use a
32-bit pointer into a single string containing all possible values,
reducing storage by 4 bytes.  Additional savings can be had if the
combined string size can be kept below 64K, in which case 16-bit
integers can represent the offset and length.  The length could be
stored in the string as well, eliminating the need for a separate
length, and generally reducing the space required for that to a byte.
Since fewer than 24 bits are used for the code point, the length could
be stored in the code point's lower byte as well; masking would be
required to test equality, though.  On the other hand, using UTF-16 to
encode strings will, on average, consume significantly less space than
UTF-32, but will not give space for an extra length byte.  Another
space-saving measure would be to store similar properties together.
For example, the main string properties are either case
folding-related or normalization-related, and some have the others as
their default values.  These could be combined when they have many
elements in common.  For now, though, plain tables are built using
32-bit pointers and lengths.  The merging into a single string table
can be done during post-processing. However, to assist with this,
multiple tables sharing the same string table can use the [[flags]]
field to distinguish themselves.

\lstset{language=C}
<<[[prop_t]] prerequisites>>=
typedef struct {
  uint32_t cp, off, flags: 8, len: 24;
} raw_cp_str_t;
@

<<Known Data Types>>=
raw_cp_str_t,%
@

<<Property parsed contents>>=
raw_cp_str_t *str_arr;
uint32_t *strs;
uint32_t strs_len, max_strs;
@

<<UCD parser local functions>>=
static void add_str_rng(prop_t *p, uint32_t low, uint32_t high, const uint32_t *val,
                        uint32_t len)
{
  uint32_t off = p->strs_len;
  if(!p->max_len) {
    inisize(p->str_arr, (p->max_len = 8));
    inisize(p->strs, (p->max_strs = 32));
  }
  while(off + len > p->max_strs)
    resize(p->strs, (p->max_strs *= 2));
  cpybuf(p->strs + off, val, len);
  p->strs_len += len;
  for(; low <= high; low++) {
    if(p->len == p->max_len)
      resize(p->str_arr, (p->max_len *= 2));
    p->str_arr[p->len].cp = low;
    p->str_arr[p->len].off = off;
    p->str_arr[p->len].len = len;
    p->str_arr[p->len].flags = 0;
    ++p->len;
  }
}
@

<<UCD parser local definitions>>=
#define decl_str(n) \
  int prop_##n = -1
#define add_str(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[64]; /* max known value == 30, so 64 should be enough */ \
    uint32_t len; \
    for(s = v, len = 0; *s; len++) \
      str[len] = strtol(s, &s, 16); \
    add_str_rng(&parsed_props[prop_##n], low, high, str, len); \
  } \
} while(0)
@

\subsection{Parsing the UCD -- Decomposition}

The first string field from [[UnicodeData.txt]] is the decomposition
mapping (dm) field.  For compatibility decompositions, the initial
part of the field is the decomposition type in angle brackets, which
has already been encoded as an enumeration (dt), so it is skipped,
along with the space which always follows it%
\footnote{The [[NormalizationCorrections.txt]] file provides previous
versions of this field, but since I see no use for it, and no property
name for it, it will not be provided.}%
.

<<Initialize UCD files>>=
decl_str(dm);
@

<<Process a line of [[UnicodeData.txt]]>>=
s = fields[5];
if(*s == '<')
  s = strchr(s, '>') + 2;
add_str(dm, s);
@

While the default values for dt and dm are normally blank, one range
actually has defined canonical values: the Hangul
Syllables\footnote{Note that I am not Korean, so I may refer to things
incorrectly or inappropriately.  Everything I know about Korean is
what I read about in the Unicode standard (and maybe a little watching
of subtitled Korean TV dramas).}, from AC00 through D7A3.  These are
only described in the standard, and can be generated with a simple
algorithm.  An entry in this range consists of three parts, called L,
V, and T.  There are normal, named code points for each L, V, and T
outside of this range, and every LV and LVT combination is contained
within this range.  There are defined decomposition mappings from LVT
to LV and T, and from LV to L and V. Rather than fill the dm string
table with easily generated values, the value is represented as an
empty result.  A function is then provided to convert that empty
string into the correct decomposition mapping.  The canonical
dm entry for LVT is LV T, so a flag is used to select the full L V T
decomposition directly.

<<Process a line of [[UnicodeData.txt]]>>=
if(low == 0xAC00) {
  /* hangul syllables */
  add_enum_rng(&parsed_props[prop_dt], low, high, enum_val(prop_dt, "can"));
  add_str_rng(&parsed_props[prop_dm], low, high, NULL, 0);
}
@

<<Unicode property exports>>=
/* requires 3-char buf; returns actual len */
int uni_hangul_syllable_decomp(uint32_t cp, uint32_t *res, int full);
@

<<Unicode property functions>>=
int uni_hangul_syllable_decomp(uint32_t cp, uint32_t *res, int full)
{
  int L, V, T;
  if(cp < 0xAC00 || cp > 0xD7A3)
    return -1;
  cp -= 0xac00;
  T = cp % 28;
  V = (cp / 28) % 21;
  L = cp / (28 * 21);
  if(!T) { /* LV -> L V */
    res[0] = 0x1100 + L;
    res[1] = 0x1161 + V;
    return 2;
  }
  if(!full) { /* LVT -> LV T */
    res[0] = 0xAC00 + 28 * (L * 21 + V);
    res[1] = 0x11A7 + T;
    return 2;
  }
  /* LVT -> L V T */
  res[0] = 0x1100 + L;
  res[1] = 0x1161 + V;
  res[2] = 0x11A7 + T;
  return 3;
}
@

The decomposition mappings are more useful if they are fully
decomposed.  Each character is given both a canonical and
compatibility full decomposition.  There is no real problem with
storing two entries with the same key in a sorted code point table, so
both are stored in the same table, with different flags.  Bit zero
indicates compatibility decomposition.

While I think the raw dm value is worthless, the Unicode standard
requires that it be provided for regular expressions.  It is stored in
the same string table as well, marked with bit one set if different
from the full decomposition.  Like the full decomposition, bit zero is
used to indicate compatibility decompositions, in order to avoid an
extra dt table lookup.

<<Initialize UCD files>>=
decl_str(dm_full);
@

<<Parse UCD files>>=
prop_dm_full = add_prop("dm_full");
prop_t *dmf_prop = &parsed_props[prop_dm_full];
prop_t *dm_prop = &parsed_props[prop_dm];
uint8_t can_val = enum_val(prop_dt, "can");
uint8_t none_val = enum_val(prop_dt, "None");
prop_t *dt_prop = &parsed_props[prop_dt];
for(i = 0; i < dm_prop->len; i++) {
  uint32_t cp = dm_prop->str_arr[i].cp;
  uint32_t str[64], len = dm_prop->str_arr[i].len;
  int is_can = uni_chrrng_dat8(cp, dt_prop->rng_dat8, dt_prop->len,
                               none_val) == can_val;
  int do_dec = !is_can;
  int did_repl = 0;
  raw_cp_str_t *unex;

  /* add unexpanded decomp */
  add_str_rng(dmf_prop, cp, cp, NULL, 0);
  if(!dmf_prop->strs_len) { /* deferred until first alloc */
    free(dmf_prop->strs);
    dmf_prop->strs = dm_prop->strs;
    dmf_prop->max_strs = dm_prop->max_strs;
    dmf_prop->strs_len = dm_prop->strs_len;
    dm_prop->strs = NULL;
    dm_prop->max_strs = dm_prop->strs_len = 0;
  }
  unex = &dmf_prop->str_arr[dmf_prop->len - 1];
  unex->off = dm_prop->str_arr[i].off;
  unex->len = len;
  cpybuf(str, dmf_prop->strs + unex->off, len);
  if(is_can)
    for(j = 0; j < len; j++) {
      uint8_t dt = uni_chrrng_dat8(str[j], dt_prop->rng_dat8, dt_prop->len,
                                   none_val);
      if(dt == can_val) {
        raw_cp_str_t *dec = bsearch(str + j, dm_prop->str_arr, dm_prop->len,
				    sizeof(raw_cp_str_t), uni_cmp_cp);
        if(dec->len > 1)
          movebuf(str + j + dec->len, str + j + 1, len - j - 1);
        cpybuf(str + j, dec->off + dmf_prop->strs, dec->len);
        len += dec->len - 1;
	did_repl = 1;
        j--; /* recheck 1st char of replacement */
      } else if(dt != none_val)
        do_dec = 1;
  }
  if(did_repl) {
    unex->flags = 2;
    add_str_rng(dmf_prop, cp, cp, str, len);
  }
  if(do_dec) {
    for(j = 0; j < len; j++) {
      uint8_t dt = uni_chrrng_dat8(str[j], dt_prop->rng_dat8, dt_prop->len,
                                   none_val);
      if(dt != none_val) {
        raw_cp_str_t *dec = bsearch(str + j, dm_prop->str_arr, dm_prop->len,
				    sizeof(raw_cp_str_t), uni_cmp_cp);
        if(dec->len > 1)
          movebuf(str + j + dec->len, str + j + 1, len - j - 1);
        cpybuf(str + j, dec->off + dmf_prop->strs, dec->len);
	did_repl = 1;
        len += dec->len - 1;
        j--; /* recheck 1st char of replacement */
      }
    }
    if(did_repl) {
      if(!is_can)
        unex->flags = 3;
      add_str_rng(dmf_prop, cp, cp, str, len);
      dmf_prop->str_arr[dmf_prop->len - 1].flags = 1;
    } else if(!is_can)
      unex->flags = 1;
  }
}
@

\subsection{Generating the Static Data}

This string table can now be printed.  Before doing so, the strings
are sorted, and any strings which are either the same or overlap from
the start share space.  This does not save as much as true common
substring elimination would, but it does save a little.  Additional
space could be saved by storing 24-bit (or even 21-bit) words, at the
expense of retrieval inefficiency.  Alternatively, the words could be
stored in UTF-16 format, which is somewhat natural and nearly always
takes up less space than UTF-32.  The same could be said of UTF-8, but
UTF-8 is not always smaller than UTF-16, and requires more complicated
encoding and decoding.  A proprety-wide parameter selects the format:
UTF-16 or UTF-32.

If 8-bit data is really wanted, it should probably be ASCII.  In that
case, the 32 bits per character are excessive, and should instead be
filled 4 characters per word.  This format is supported as well;
strings are assumed to be zero-terminated, or exactly four times the
length of the 32-bit string if no zeroes are encountered first.  The
terminating zero must always be in the last word.  Note that merging
strings works identically for 16-bit and 32-bit strings, but 8-bit
strings need a second pass to detect end-of-string overlaps.

<<Property parsed contents>>=
int strs_char_size; /* 8, 16, 32, or 0 (16) */
@

<<UCD parser local functions>>=
/* qsort has no user data, so this is a static */
const uint32_t *sort_strs;
static int cmp_strs(const void *a, const void *b)
{
   const raw_cp_str_t *A = a, *B = b;
   uint32_t len = A->len;
   int c;

   if(len > B->len)
     len = B->len;
   c = memcmp(sort_strs + A->off, sort_strs + B->off, len * 4);
   if(c)
     return c;
   return (int32_t)A->len - (int32_t)B->len;
}
@

<<UCD parser local functions>>=
static void merge_strs(prop_t *p)
{
  uint32_t i;

  sort_strs = p->strs;
  qsort(p->str_arr, p->len, sizeof(*p->str_arr), cmp_strs);
  for(i = p->len - 1; i; i--)
    if(p->str_arr[i - 1].len <= p->str_arr[i].len &&
       !memcmp(p->strs + p->str_arr[i - 1].off,
               p->strs + p->str_arr[i].off,
	       p->str_arr[i - 1].len * 4)) {
      p->str_arr[i - 1].off = p->str_arr[i].off;
  }
}
@

<<UCD parser local functions>>=
static void dump_strs(prop_t *p, FILE *gen_h)
{
  int i;
  unsigned int off, saved = 0;
  char nbuf[64];
  int char_size = p->strs_char_size;

  if(!char_size)
    char_size = 16;
  sprintf(nbuf, "uni_%s_strs.gen.c", p->name);
  open_wf(of, nbuf);
  fprintf(of, "#include <stdint.h>\n\n"
              "const uint%d_t uni_%s_strs[] = {\n\t", char_size, p->name);
  const char *prefix = "";
  merge_strs(p);
  for(i = p->len - 1, off = 0; i >= 0; i--) {
    int j, len = p->str_arr[i].len;
    unsigned int soff = p->str_arr[i].off;
    if(!len) {
      p->str_arr[i].off = 1;
      continue;
    }
    /* adjust 8-bit len */
    if(char_size == 8) {
      const char *s = (const char *)(p->strs + soff + len);
      len *= 4;
      while(!*--s)
        len--;
      p->str_arr[i].len = len;
    }

    p->str_arr[i].off = off;
    if(char_size > 8)
      for(j = 0; j < len; j++) {
        uint32_t c = p->strs[soff + j];
        if(char_size == 16 && c > 0xFFFF) {
          c -= 0x10000;
          fprintf(of, "%s0x%X,", prefix, 0xD800 + (c >> 10));
	  prefix = !((off + j + 1) % 8) ? "\n\t" : " ";
	  c = 0xDC00 + (c & 0x3ff);
	  p->str_arr[i].len++;
	  off++;
        }
        fprintf(of, "%s0x%04X", prefix, c);
	prefix = !((off + j + 1) % 8) ? ",\n\t" : ", ";
      }
    else {
      const char *s = (const char *)(p->strs + soff);
      for(j = 0; j < len; j++, s++) {
        if(*s == '\\' || *s == '\'')
	  fprintf(of, "%s'\\%c'", prefix, *s);
        else if(isprint(*s))
	  fprintf(of, "%s '%c'", prefix, (int)*s);
	else
	  fprintf(of, "%s0x%02X", prefix, (int)(uint8_t)*s);
	prefix = !((off + j + 1) % 8) ? ",\n\t" : ", ";
      }
    }
    off += len;
    /* skip predecessor if prefix of string already in table */
    /* but adjust length if necessary */
    while(i > 0 && p->str_arr[i - 1].off == soff) {
      if(char_size == 16) {
        for(j = len = 0; j < p->str_arr[i - 1].len; j++)
	  len += uni_utf16_enclen(p->strs[soff + j]);
	p->str_arr[i - 1].len = len;
	saved += len;
      } else if(char_size == 8) {
        len = p->str_arr[i - 1].len * 4;
	if(len > p->str_arr[i].len)
	  len = p->str_arr[i].len;
	p->str_arr[i - 1].len = len;
	saved += len;
      } else
        saved += p->str_arr[i - 1].len;
      p->str_arr[i - 1].off = p->str_arr[i].off;
      --i;
    }
    if(char_size == 8) {
      while(i > 0 && p->str_arr[i - 1].len &&
                     p->str_arr[i - 1].len * 4 <= p->str_arr[i].len) {
        len = p->str_arr[i - 1].len * 4;
        const char *s = (const char *)(p->strs + soff),
                   *t = (const char *)(p->strs + p->str_arr[i - 1].off),
		   *e = t + len;
        while(!*--e)
	  len--;
        if(!memcmp(s, t, len)) {
	  for(j = --i; j > 0; j--)
	    if(p->str_arr[j - 1].off != p->str_arr[i].off)
	      break;
	  for(; i >= j; i--) {
	    int plen = p->str_arr[i].len * 4;
	    if(plen > len)
	      plen = len;
	    p->str_arr[i].len = plen;
            p->str_arr[i].off = p->str_arr[i + 1].off;
	    saved += plen;
	  }
	  i++; /* last loop goes under by one */
        } else
	  break;
      }
      if(!i)
        break;
    }
  }
  fputs("\n};\n", of);
  fclose(of);
  fprintf(gen_h, "extern const uint%d_t uni_%s_strs[];\n"
                 "#define uni_%s_strs_len %d /* (%d bytes) (%d words saved) */\n",
		 char_size, p->name, p->name, off,
		 off * char_size / 8, saved);
}
@

<<Dump character information as C code>>=
/* ensure that each property has like-named string table */
s = (char *)dmf_prop->name;
dmf_prop->name = "dm";
dump_strs(dmf_prop, gen_h);
dmf_prop->name = s;
fputs("#define uni_canon_decomp_strs uni_dm_strs\n"
      "#define uni_canon_decomp_strs_len uni_dm_strs_len\n"
      "#define uni_compat_decomp_strs uni_dm_strs\n"
      "#define uni_compat_decomp_strs_len uni_dm_strs_len\n", gen_h);
@

Now that the string offsets and lengths are correct, the plain table
is ready to be converted into a multi-level table, and then both can
be printed.  The plain table could simply be the same as the raw
table, using the flags to distinguish entries.  The multi-level table,
though, expects only one value per index, and each value must be the
same size.  Rather than figure out a way to squeeze all three values
into an efficient representation, three separate tables are generated.
While it would definitely be possible to encode the offset and length
into 24 bits, the current multi-level table implementation only
supports sizes which are powers of two.  Encoding into 16 bits might
be possible, but is likely too complex to be useful.  Other string
propreties may also benefit from more breathing room.  Instead, 32
bits are used: 16 for the offset and 16 for the length.  The raw dm
property also needs to distinquish between canonical and compatibility
results.  There is a binary property to help, but that requires an
extra lookup.  Instead, a flag is ored into the length field. The
compatibility decomposition table uses entries from the canonical
table where no compatibility decomposition exists.  Rather than make
the plain tables behave completely differently than the multi-level
tables (i.e., be a single merged table), three separate plain tables
will be generated as well.

<<UCD parser local functions>>=
/* sort by flag first, then cp */
static int cmp_cp_flg(const void *a, const void *b)
{
   const raw_cp_str_t *A = a, *B = b;
   int32_t c;

   c = (int32_t)A->cp - (int32_t)B->cp;
   if(c)
     return c;
   return (int32_t)A->flags - (int32_t)B->flags;
}
@

<<[[uni_str_arr_t]]>>=
typedef struct {
  uint32_t cp;
  uint16_t off, len;
} uni_str_arr_t;
@

<<[[uni_str_ptr_t]]>>=
typedef struct {
  uint16_t off, len;
} uni_str_ptr_t;
@

<<Unicode property exports for generator>>=
<<[[uni_str_arr_t]]>>
<<[[uni_str_ptr_t]]>>
@

<<Known Data Types>>=
uni_str_arr_t,uni_str_ptr_t,%
@

<<Dump character information as C code>>=
qsort(dmf_prop->str_arr, dmf_prop->len, sizeof(*dmf_prop->str_arr), cmp_cp_flg);
uni_str_arr_t *dec;
inisize(dec, dmf_prop->len);
/* the easy table: canon full mapping is all w/ flags == 0 */
open_wf(cf, "uni_canon_decomp_arr.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_arr_t uni_canon_decomp_arr[] = {\n\t", cf);
uint32_t curent;
for(i = 0; dmf_prop->str_arr[i].flags; i++);
for(curent = 0; i < dmf_prop->len; i++, curent++) {
  dec[curent].cp = dmf_prop->str_arr[i].cp;
  dec[curent].off = dmf_prop->str_arr[i].off;
  dec[curent].len = dmf_prop->str_arr[i].len;
  while(i < dmf_prop->len - 1 && dmf_prop->str_arr[i + 1].flags)
    ++i;
  fprintf(cf, "{ 0x%04X, %d, %d }%s", (int)dec[curent].cp, (int)dec[curent].off,
                                      (int)dec[curent].len,
				      i < dmf_prop->len - 1 ? ",\n\t" : "\n};\n");
}
fclose(cf);
fprintf(gen_h, "extern const uni_str_arr_t uni_canon_decomp_arr[];\n"
	       "#define uni_canon_decomp_arr_len %d /* %d lookups max */\n",
	       curent, lg2(curent + 1));
@

<<Clean up after parsing UCD files>>=
free(dec);
@

<<[[uni_cp_val_t]]>>=
typedef struct {
  uint32_t cp;
  uint32_t val;
} uni_cp_val_t;
@

<<Unicode property exports for generator>>=
<<[[uni_cp_val_t]]>>
uint32_t *uni_cp_val_to_multi(const uni_cp_val_t *tab, uint32_t tab_len,
                              uint32_t *ml_len);
@

<<Known Data Types>>=
uni_cp_val_t,%
@

<<Unicode property functions for generator>>=
uint32_t *uni_cp_val_to_multi(const uni_cp_val_t *tab, uint32_t tab_len,
                              uint32_t *ml_len)
{
  uint32_t low, high, len, i;
  uint32_t *ml;
  uint32_t *bits;

  /* degenerate case:  always out-of-range */
  if(!tab_len) {
    uni_bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  low = tab[0].cp;
  high = tab[tab_len - 1].cp;
  len = high - low + 1;
  inisize(bits, len);
  /* Optimize(maybe): only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  for(i = 0; i < tab_len; i++)
    bits[tab[i].cp - low] = tab[i].val;
  uni_bits_to_multi((uint8_t *)bits, len * 4, low * 4, high * 4, 0, 4, &ml, ml_len);
  free(bits);
  return ml;
}
@

<<Dump character information as C code>>=
uint32_t *tmt = uni_cp_val_to_multi((uni_cp_val_t *)dec, curent, &ml_len);
print_mtab("canon_decomp", tmt, gen_h);
free(tmt);
@

<<Dump character information as C code>>=
/* the next harder table: flags == 1 -> full compat decomp */
/* needs to duplicate full canon decomp when empty */
open_wf(kf, "uni_compat_decomp_arr.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_arr_t uni_compat_decomp_arr[] = {\n\t", kf);
for(i = curent = 0; i < dmf_prop->len; i++, curent++) {
  raw_cp_str_t *cur = &dmf_prop->str_arr[i];
  /* could be canon followed by compat, or compat followed by dm */
  if(i < dmf_prop->len - 1 && cur[1].cp == cur->cp &&
     !(cur->flags & 1) && (cur[1].flags & 1)) {
    ++i;
    ++cur;
  }
  dec[curent].cp = cur->cp;
  dec[curent].off = cur->off;
  dec[curent].len = cur->len;
  if(cur->flags & 1)
    dec[curent].len |= 0x8000;
  /* could be canon/compat followed by dm */
  if(i < dmf_prop->len - 1 && cur->cp == cur[1].cp)
    ++i;
  fprintf(kf, "{ 0x%04X, %d, %d%s }%s",
              (int)cur->cp, (int)cur->off, (int)cur->len,
	      (cur->flags & 1) ? " | 0x8000" : "",
	      i < dmf_prop->len - 1 ? ",\n\t" : "\n};\n");
}
fclose(kf);
fprintf(gen_h, "extern const uni_str_arr_t uni_compat_decomp_arr[];\n"
	       "#define uni_compat_decomp_arr_len %d /* %d lookups max */\n",
	       curent, lg2(curent + 1));
tmt = uni_cp_val_to_multi((uni_cp_val_t *)dec, curent, &ml_len);
print_mtab("compat_decomp", tmt, gen_h);
free(tmt);
@

<<Dump character information as C code>>=
/* the hardest table: 2/3 -> dm */
/* needs to duplicate full canon decomp when empty */
/* needs to duplicate full compat decomp when empty & full canon empty*/
open_wf(dm, "uni_dm_arr.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_arr_t uni_dm_arr[] = {\n\t", dm);
for(i = curent = 0; i < dmf_prop->len; i++) {
  raw_cp_str_t *cur = &dmf_prop->str_arr[i];

  if(i < dmf_prop->len - 1 && cur[1].cp == cur->cp) {
    /* possibilities: canon,compat,dm canon,dm canon,compat compat,dm */
    if(cur[1].flags & 2) {
      ++i; /* compat,dm or canon,dm: skip full */
      cur++;
    } else if(i < dmf_prop->len - 2 && cur->cp == cur[2].cp) {
      i += 2; /* canon,compat,dm: skip both full */
      cur += 2;
    } else /* canon,compat; skip compat when done */
      i++;
  }
  dec[curent].cp = cur->cp;
  dec[curent].off = cur->off;
  dec[curent].len = cur->len;
  if(cur->flags & 1)
    dec[curent].len |= 0x8000;
  fprintf(dm, "{ 0x%04X, %d, %d%s }%s", (int)cur->cp, (int)cur->off,
                                        (int)cur->len,
				        (cur->flags & 1) ? " | 0x8000" : "",
				        i < dmf_prop->len - 1 ? ",\n\t" : "\n};\n");
  curent++;
}
fclose(dm);
fprintf(gen_h, "extern const uni_str_arr_t uni_dm_arr[];\n"
	       "#define uni_dm_arr_len %d /* %d lookups max */\n",
	       curent, lg2(curent + 1));
tmt = uni_cp_val_to_multi((uni_cp_val_t *)dec, curent, &ml_len);
print_mtab("dm", tmt, gen_h);
free(tmt);
@

The generic all-property table needs to have a pointer to the string
table for each property.

<<Property parsed contents>>=
const char *strs_name;
@

<<Parse UCD files>>=
parsed_props[add_prop("canon_decomp")].strs_name = "dm";
parsed_props[add_prop("compat_decomp")].strs_name = "dm";
parsed_props[add_prop("dm")].strs_name = "dm";
parsed_props[add_prop("canon_decomp")].mt = (uint32_t *)~0;
parsed_props[add_prop("compat_decomp")].mt = (uint32_t *)~0;
parsed_props[add_prop("dm")].mt = (uint32_t *)~0;
@

<<Additional property type names>>=
UNI_PROP_TYPE_STR,
@

<<Set prop type for export>>=
if(parsed_props[i].strs_name || parsed_props[i].strs)
  t = UNI_PROP_TYPE_STR;
@

<<Additional property structure members>>=
const uint8_t *strs8;
const uint16_t *strs16;
const uint32_t *strs32;
uint32_t strs_len;
@

<<Print additional property structure members>>=
const char *strs_name = parsed_props[i].strs_name;
if(parsed_props[i].strs || strs_name) {
  if(!strs_name)
    strs_name = name;
  int char_size = parsed_props[i].strs_char_size;
  if(!char_size)
    char_size = 16;
  fprintf(pnf, "%s%s, uni_%s_strs%s%s, uni_%s_strs_len",
               char_size > 8 ? ", NULL" : "", char_size > 16 ? ", NULL" : "",
               strs_name,
               char_size < 32 ? ", NULL" : "", char_size < 16 ? ", NULL" : "",
	       strs_name);
} else
  fputs(", NULL, NULL, NULL, 0", pnf);
@

<<Plain table name>>=
t == UNI_PROP_TYPE_STR ? "arr" :
@

The lookup functions return offset and length.  An additional lookup
function fills in a string given the offset and length.  This is so
that the exact name of the special value for Hangul Syllables is
encoded in these functions, rather than everywhere lookups are used.

<<Unicode property exports>>=
/* 2-step lookup: uni_find_xxx followed by uni_get_decomp */
/* off is uint16_t; len and compat are uint8_t; compat is optional */
void uni_find_canon_decomp(uint32_t cp, int16_t *off, uint8_t *len);
#define uni_find_canon_decomp(cp, off, len) \
  uni_x_dec(cp, uni_canon_decomp_mtab, off, len, NULL, 1)
void uni_find_compat_decomp(uint32_t cp, int16_t *off, uint8_t *len,
                            uint8_t *compat);
#define uni_find_compat_decomp(cp, off, len, compat) \
  uni_x_dec(cp, uni_compat_decomp_mtab, off, len, compat, 1)
void uni_find_dm(uint32_t cp, int16_t *off, uint8_t *len, uint8_t *compat);
#define uni_find_dm(cp, off, len, compat) \
  uni_x_dec(cp, uni_dm_mtab, off, len, compat, 0)
void uni_get_decomp(uint32_t cp, uint32_t *buf, int16_t *off, uint8_t *len);
#define uni_get_decomp(cp, buf, off, len) do { \
  if((off) == -1) \
    uni_hangul_syllable_decomp(cp, buf, (len) == 3); \
  else \
    cpybuf(buf, uni_dm_strs + (off), len); \
} while(0)
@

<<Unicode property exports>>=
void uni_x_dec(uint32_t cp, const uint32_t *tab, int16_t *off, uint8_t *len,
               uint8_t *compat, int h);
@

<<Unicode property functions>>=
void uni_x_dec(uint32_t cp, const uint32_t *tab, int16_t *off, uint8_t *len,
               uint8_t *compat, int h)
{
  const uint8_t *mr;
  uni_multi_tab_lookup(tab, cp * 4, &mr, 0);
  if(!mr) {
    *off = 0;
    *len = 0;
    if(compat)
      *compat = 0;
    return;
  } else {
    uni_str_ptr_t *v = (void *)mr;
    if(h >= 0 && !v->len && v->off) {
      *off = -1;
      *len = h && (cp - 0xAC00) % 28 ? 3 : 2;
      if(compat)
        *compat = 0;
    } else {
      *off = v->off;
      *len = v->len & 0x7fff;
      if(compat)
        *compat = v->len >> 15;
    }
  }
}
@

\subsection{Testing}

To test the table implementations, a different, but nearly identical,
set of routines is used compared to the ones one for booleans.

<<Functions to help test generated tables>>=
#define str(x) doit_str(#x, uni_##x##_arr, uni_##x##_arr_len, uni_##x##_mtab)

static void doit_str(const char *name, const uni_str_arr_t *rng, uint32_t nent,
                     const uint32_t *mtab)
{
    uint32_t i;

    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    const uni_str_ptr_t *ms, *rs;
    static const uni_str_ptr_t z = {0};
    uni_str_arr_t *r;
    for(i = 0; i < 0x110000; i++) {
      r = bsearch(&i, rng, nent, sizeof(*rng), uni_cmp_cp);
      rs = r ? (uni_str_ptr_t *)&r->off : &z;
      ms = uni_x_str_of(i, mtab);
      if(memcmp(ms, rs, sizeof(*ms))) {
        fprintf(stderr, "mismatch %s@%d %d/%d/%d %d/%d/%d\n", name, i,
	                (int)rs->off, (int)rs->len & 0x7fff, (int)rs->len >> 15,
			(int)ms->off, (int)ms->len & 0x7fff, (int)ms->len >> 15);
	exit(1);
      }
    }
    /* check performance */
    int j;
    unsigned long tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        r = bsearch(&i, rng, nent, sizeof(*rng), uni_cmp_cp);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        uni_x_str_of(i, mtab);
    tt = tend();
    printf("  r%ld t%ld %.2fx\n", tr, tt, (double)tr / (double)tt);
}
@

<<Unicode property exports>>=
const uni_str_ptr_t *uni_x_str_of(uint32_t cp, const uint32_t *tab);
@

<<Unicode property functions>>=
const uni_str_ptr_t *uni_x_str_of(uint32_t cp, const uint32_t *tab)
{
  const uni_str_ptr_t *ret;
  static const uni_str_ptr_t z = {0};
  uni_multi_tab_lookup(tab, cp * 4, (const uint8_t **)&ret, 0);
  return ret ? ret : &z;
}
@

<<Dump character information as C code>>=
fputs("str(canon_decomp);\n"
      "str(compat_decomp);\n"
      "str(dm);\n", tstf);
@

\subsection{Parsing the UCD -- DUCET Decompositions}

An alternate decomposition table is that provided by
\texttt{decomps.txt}.  This file is informative: no official algorithm
uses this, but it can be handy for examining the DUCET.  Some DUCET
mappings automatically decompose the character they are mapping.
These include canonical compositions, compatibility compositions, and
``other'' compositions defined by this text file.  The format of the
file is similar to \texttt{UnicodeData.txt}: semicolon-separated
fileds, the first of which is a single code point.  The second field
is similar to the dt portion of the decomposition mapping field, but
is separate from the dm-equivalent field.  The third field is the
dm-equivalent field, but without dt information.  A special
decomposition type, \texttt{sort}, is used to indicate mappings which
are only in this file.  Rather than create a new enumeration (or add
\texttt{sort} to the dt enumeration), this is encoded as compat.  The
way to distinguish real compat mappings from \texttt{sort} mappings is
to look it up in the regular dt table.

While the \texttt{sort} value is documented, decomps also adds
\texttt{circlekata} and \texttt{smallnarrow}, which are tranlsated to
\texttt{circle} and \texttt{narrow}, repsectively, for now.  No great
effort will be put into solving this problem; I used this table when
analyzing the DUCET, and see little other value for it.

<<Initialize UCD files>>=
decl_str(DUCET_dm);
decl_enum(DUCET_dt, "None");
@

<<Parse UCD files>>=
prop_DUCET_dt = add_prop("DUCET_dt");
parsed_props[prop_DUCET_dt].def = parsed_props[prop_dt].def;
/* copy enum decls from dt for lookup */
enum_vals[prop_DUCET_dt] = enum_vals[prop_dt];
enum_vals_len[prop_DUCET_dt] = enum_vals_len[prop_dt];
open_f("decomps.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  char *dt_val;
  split_line(lbuf);
  if(num_fields != 3) /* e.g. comment lines */
   continue;
  low = high = strtol(fields[0], NULL, 16);
  dt_val = fields[1];
  if(*dt_val) {
    if(*dt_val++ != '<') {
      perror("decomps");
      exit(1);
    }
    for(s = dt_val; *s && *s != '>'; s++);
    if(*s != '>' || s[1]) {
      perror("decomps");
      exit(1);
    }
    *s = 0;
    if(!strcmp(dt_val, "sort"))
      dt_val = (char *)"compat";
    else if(!strcmp(dt_val, "circlekata"))
      dt_val[6] = 0;
    else if(!strcmp(dt_val, "smallnarrow"))
      dt_val += 5;
  }
  if(*dt_val)
    add_enum(DUCET_dt, dt_val);
  else
    add_enum(DUCET_dt, "can");
  add_str(DUCET_dm, fields[2]);
}
/* remove enum info so new enum not created */
enum_vals[prop_DUCET_dt] = NULL;
enum_vals_len[prop_DUCET_dt] = 0;
@

Unfortunately, some of the dm strings have multiple entries for one
index.  Luckily, the dt values do not have the same problem.  To fix
the dm strings, the result is sorted, and any duplicate indices are
removed by merging the two entries, separated by a zero.  Nothing ever
maps to zero, so there should be no problem distinguishing the
entries.  No attempt is made to sort the entries; there are at most
two anyway, so searching is no great effort.

<<Parse UCD files>>=
prop_t *prop_ddm = &parsed_props[prop_DUCET_dm];
qsort(prop_ddm->str_arr, prop_ddm->len, sizeof(raw_cp_str_t), uni_cmp_cp);
for(i = prop_ddm->len - 1; i > 0; i--) {
  for(j = i; j > 0; j--)
    if(prop_ddm->str_arr[j - 1].cp != prop_ddm->str_arr[i].cp)
      break;
  if(j == i)
    continue;
  low = j;
  int len = 0;
  for(j = low; j <= i; j++)
    len += prop_ddm->str_arr[j].len + 1;
  while(prop_ddm->max_strs < prop_ddm->strs_len + len)
    resize(prop_ddm->strs, prop_ddm->max_strs *= 2);
  for(j = low, len = 0; j <= i; j++) {
    movebuf(prop_ddm->strs + prop_ddm->strs_len + len,
            prop_ddm->strs + prop_ddm->str_arr[j].off,
	    prop_ddm->str_arr[j].len);
    len += prop_ddm->str_arr[j].len + 1;
    prop_ddm->strs[prop_ddm->strs_len + len - 1] = 0;
  }
  len--;  /* strip final 0 */
  prop_ddm->str_arr[low].off = prop_ddm->strs_len;
  prop_ddm->str_arr[low].len = len;
  prop_ddm->strs_len += len;
  movebuf(prop_ddm->str_arr + low + 1, prop_ddm->str_arr + i + 1,
          prop_ddm->len - i - 1);
  prop_ddm->len -= i - low;
  if(!(i = low))
    break;
}
@

To dump the tables, the generic routines can be relied upon.  However,
there is not yet a generic routine to dump string tables.  In order to
avoid stomping on the work done for the decomposition tables, the
generic routine for strings needs to be called manually.

<<Dump character information as C code>>=
dump_str_tabs(prop_ddm, gen_h, tstf);
@

<<UCD parser local functions>>=
static void dump_str_tabs(prop_t *prop, FILE *gen_h, FILE *tstf)
{
  uint32_t i;
  char nbuf[64];
  uni_str_arr_t *short_str;
  inisize(short_str, prop->len);
  dump_strs(prop, gen_h);
  /* dump_strs "un-sorts" str_arr */
  qsort(prop->str_arr, prop->len, sizeof(*prop->str_arr), uni_cmp_cp);
  sprintf(nbuf, "uni_%s_arr.gen.c", prop->name);
  open_wf(rt, nbuf);
  fprintf(rt, "#include \"uni_prop.h\"\n\n"
              "const uni_str_arr_t uni_%s_arr[] = {\n\t", prop->name);
  for(i = 0; i < prop->len; i++) {
    short_str[i].cp = prop->str_arr[i].cp;
    short_str[i].off = prop->str_arr[i].off;
    short_str[i].len = prop->str_arr[i].len;
    fprintf(rt, "{ 0x%04X, %d, %d}%s", (int)short_str[i].cp, (int)short_str[i].off,
                                       (int)short_str[i].len,
				       i < prop->len - 1 ? ",\n\t" : "\n};\n");
  }
  fclose(rt);
  fprintf(gen_h, "extern const uni_str_arr_t uni_%s_arr[];\n"
	         "#define uni_%s_arr_len %d /* %d lookups max */\n",
	         prop->name, prop->name, i, lg2(i + 1));
  uint32_t ml_len, *tmt = uni_cp_val_to_multi((uni_cp_val_t *)short_str, i, &ml_len);
  print_mtab(prop->name, tmt, gen_h);
  free(tmt);
  free(short_str);
  /* for direct lookup */
  fprintf(gen_h, "#define uni_%s_of(cp) uni_x_str_of(cp, uni_%s_mtab)\n",
                 prop->name, prop->name);
  fprintf(tstf, "str(%s);\n", prop->name);
}
@

\subsection{Parsing the UCD -- Composition}

As mentioned earlier, composition takes place on pairs.  The canonical
composition of A and B is C if the plain dm value for C is A B, and dt
for C is canonical, and C does not have the Comp\_Ex property.  Given
these conditions, B is removed and A is replaced by C.  After
replacement, C becomes A for a further pass at composition.  It is not
possible to make a ``fully composed'' table that, like the full
decomposition table, iterates thorugh all possible compositions. So,
the composition table is basically a table returning C (or nothing)
given A and B.  

There are a number of possible strategies for storing these values for
efficient retrieval. For sorted arrays, the pair of A and B could
simply be used as the key. When both A and B are known, this is the
most effective form of the sorted array.  However, real normalization
takes place by knowing A, and then scanning all potential candidates
for B.  For the sorted array, this is still not too terrible: the
first and last occurrence of A can be found relatively quickly, and
that limited subset can be searched for B.  The search for the last
and first occurence can be removed by instead having A return a table
(generally an address and a table length) indexed on B.  The B-indexed
table then returns C.  This arrangement can be used regardless of the
actual A-indexed table structure.

Rather than creating a new structure to store these B-indexed tables,
they are stored as if they were strings themselves: A is used to look
up a string, which is actually a code point/value table to look up C
given B.  As with the dm table, the Hangul Syllables are given special
treatment. The A-indexed entries for all LV and L characters point to
zero-length tables, indicating that the [[uni_hangul_syllable_comp]]
function should be used to compose instead.

After converting the string to UTF-16, the binary search no longer
works properly, because the size of each entry is not always exactly
two words.  In order to still support binary searching, all entries
are forced to become 32 bits by padding any non-32-bit entries with
trailing zeroes.  It is easy to detect this on lookup: if the second
word of the string is either a surrogate or a zero, 32-bit lookups are
required.

<<Initialize UCD files>>=
decl_str(cm);
@

<<Parse UCD files>>=
prop_cm = add_prop("canon_comp");
prop_t *fce_prop = &parsed_props[add_prop("Comp_Ex")];
prop_t *cm_prop = &parsed_props[prop_cm];
for(i = 0; i < dm_prop->len; i++) {
  uint32_t str[2];
  uint32_t cp = dm_prop->str_arr[i].cp; /* cp == C */
  if(dm_prop->str_arr[i].len != 2 ||
     uni_chrrng_dat8(cp, dt_prop->rng_dat8, dt_prop->len, none_val) !=
         can_val ||
     uni_is_cp_chrrng(cp, fce_prop->rng, fce_prop->len))
    continue; /* skip if not canonical composition */
  str[0] = dmf_prop->strs[dm_prop->str_arr[i].off + 1]; /* B */
  str[1] = cp;
  cp = dmf_prop->strs[dm_prop->str_arr[i].off]; /* A */
  /* just add; tables are merged later */
  add_str_rng(cm_prop, cp, cp, str, 2);
}
/* add entries for Hangul */
add_str_rng(cm_prop, 0x1100, 0x1112, NULL, 0); /* L */
for(i = 0xAC00; i <= 0xD7A3; i += 28) /* LV */
  add_str_rng(cm_prop, i, i, NULL, 0);
/* merge B->C tables, and add padding zeroes if needed */
qsort(cm_prop->str_arr, cm_prop->len, sizeof(*cm_prop->str_arr), uni_cmp_cp);
for(i = cm_prop->len - 1; i > 0; i--) {
  int off, k, is_32 = 0;;
  j = i;
  while(j > 0 && cm_prop->str_arr[i].cp == cm_prop->str_arr[j - 1].cp)
    j--;
  uint32_t jb = cm_prop->strs[cm_prop->str_arr[j].off];
  uint32_t jc = cm_prop->strs[cm_prop->str_arr[j].off + 1];
  if(j == i && ((jb < 0x10000 && jc < 0x10000) ||
                (jb > 0xFFFF  && jc > 0xFFFF)))
    continue; /* note: in UCD 6.2, this path always taken if j == i */
  /* leave room for 2* (B+C) and possible padding zeroes (B+C again) */
  while(cm_prop->strs_len + 4 * (i - j + 1) > cm_prop->max_strs)
    resize(cm_prop->strs, (cm_prop->max_strs *= 2));
  uint32_t *dest = cm_prop->strs + cm_prop->strs_len;
  for(k = j, off = 0; k <= i; k++) {
    if((dest[off++] = cm_prop->strs[cm_prop->str_arr[k].off]) > 0xFFFF)
      is_32 = 1;
    if((dest[off++] = cm_prop->strs[cm_prop->str_arr[k].off + 1]) > 0xFFFF)
      is_32 = 1;
  }
  /* sort B->C table by B */
  qsort(dest, off / 2, 2 * sizeof(*cm_prop->strs), uni_cmp_cp);
  /* insert zeroes in string if 32-bit needed */
  /* NOTE: UCD 6.2 never touches this code (all are 1-entry, b & c 32-bit) */
  if(is_32) {
    for(k = off - 1; k >= 0; k--) {
      if(cm_prop->strs[cm_prop->strs_len + k] < 0x10000) {
        movebuf(dest + k + 2, dest + k + 1, off - k - 1);
	dest[k + 1] = 0;
	off++;
      }
    }
  }
  /* finish entry */
  cm_prop->str_arr[j].off = cm_prop->strs_len;
  cm_prop->str_arr[j].len = off;
  cm_prop->strs_len += off;
  /* remove excess entries */
  movebuf(cm_prop->str_arr + j + 1, cm_prop->str_arr + i + 1,
          cm_prop->len - i - 1);
  cm_prop->len -= i - j;
  if(!(i = j))
    break;
}
@

<<Unicode property exports>>=
/* returns 0 if invalid */
uint32_t uni_hangul_syllable_comp(uint32_t a, uint32_t b);
@

<<Unicode property functions>>=
uint32_t uni_hangul_syllable_comp(uint32_t a, uint32_t b)
{
  if(a >= 0x1100 && a <= 0x1112) /* L */
    if(b >= 0x1161 && b <= 0x1175) /* V */
      return 0xAC00 + 28 * ((a - 0x1100) * 21 + (b - 0x1161)); /* LV */
  if(b > 0x11A7 && b <= 0x11C2) /* T */
    if(a >= 0xAC00 && a <= 0xD7A3 && !((a - 0xAC00) % 28)) /* LV */
      return a + (b - 0x11A7); /* LVT */
  return 0;
}
@

There is probably little value in converting the B-indexed lookup
tables to multi-level tables, so the string table can be printed as is
now. The [[dump_strs]] function is the most convenient method to do
so. The fucntion will pointlessly try to compress the table, but
that's better than writing a dumper function just for this table.

<<Dump character information as C code>>=
int max_len = 0;
for(i = 0; i < cm_prop->len; i++)
  if(cm_prop->str_arr[i].len > max_len)
    max_len = cm_prop->str_arr[i].len;
max_len /= 2;
fprintf(gen_h, "/* %d max B->C len (%d lookups) */\n", max_len, lg2(max_len + 1));
dump_strs(cm_prop, gen_h);
@

The A-indexed plain table can now be sorted, converted to a
multi-level table, and printed.  The [[dec]] table built above is
reused as temporary storage to convert to the smaller format for the
multi-level table. It is guaranteed to be large enough, because every
composition entry corresponds to at least one decomposition entry.

<<Dump character information as C code>>=
qsort(cm_prop->str_arr, cm_prop->len, sizeof(*cm_prop->str_arr), uni_cmp_cp);
open_wf(cm, "uni_canon_comp_arr.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_arr_t uni_canon_comp_arr[] = {\n\t", cm);
for(i = 0; i < cm_prop->len; i++) {
  raw_cp_str_t *cur = &cm_prop->str_arr[i];
  dec[i].cp = cur->cp;
  dec[i].off = cur->off;
  dec[i].len = cur->len;
  fprintf(cm, "{ 0x%04X, %d, %d }%s", (int)cur->cp, (int)cur->off, (int)cur->len,
				      i < cm_prop->len - 1 ? ",\n\t" : "\n};\n");
}
fclose(cm);
fprintf(gen_h, "extern const uni_str_arr_t uni_canon_comp_arr[];\n"
	       "#define uni_canon_comp_arr_len %d /* %d lookups max */\n",
	       i, lg2(i + 1));
cm_prop->mt = uni_cp_val_to_multi((uni_cp_val_t *)dec, cm_prop->len, &ml_len);
print_mtab("canon_comp", cm_prop->mt, gen_h);
fputs("str(canon_comp);\n", tstf);
@

<<Parse UCD files>>=
cm_prop->mt = (uint32_t *)~0; /* force addition to prop table */
@

The same lookup function as was used for decomposition lookups mostly
works for the A-indexed level as well.  For the B-indexed level, a
binary search is performed.  This is hand-coded for speed and to
easily adjust for 32-bit entries.

<<Unicode property exports>>=
/* to compose A+B->C: */
/* look up B->C table for A (cp) */
void uni_find_canon_comp(uint32_t cp, int16_t *off, uint8_t *len);
#define uni_find_canon_comp(cp, off, len) \
  uni_x_dec(cp, uni_canon_comp_mtab, off, len, NULL, -1)
/* then look up C given B->C table info */
uint32_t uni_canon_comp(uint32_t cpa, uint32_t cpb, uint16_t off, uint8_t len);
#define uni_canon_comp(cpa, cpb, off, len) \
  ((len) > 0 ? uni_lookup_compent(cpb, uni_canon_comp_strs + (off), len) : \
               uni_hangul_syllable_comp(cpa, cpb))
@

<<Unicode property exports for generator>>=
uint32_t uni_lookup_compent(uint32_t cp, const uint16_t *tab, uint32_t len);
@

<<Unicode property functions for generator>>=
uint32_t uni_lookup_compent(uint32_t cp, const uint16_t *tab, uint32_t len)
{
  if(tab[1] && !is_uni_surrogate(tab[1])) {
    int32_t l = 0, h = len / 2 - 1;
    while(h >= l) {
      uint32_t m = (l + h) / 2;
      int32_t c = (int32_t)tab[m * 2] - (int32_t)cp;
      if(!c)
        return tab[m * 2 + 1];
      else if(c < 0)
        l = m + 1;
      else
        h = m - 1;
    }
  } else {
    int32_t l = 0, h = len / 4 - 1;
    while(h >= l) {
      uint32_t m = (l + h) / 2;
      uint32_t mc = uni_int_utf16_decode(tab + m * 4, NULL);
      int32_t c = (int32_t)mc - (int32_t)cp;
      if(!c) {
        mc = uni_int_utf16_decode(tab + m * 4 + 2, NULL);
	return mc;
      } else if(c < 0)
        l = m + 1;
      else
        h = m - 1;
    }
  }
  return 0;
}
@

\subsection{Parsing the UCD -- Case Conversion}

The next strings are the various case conversion tables.
[[UnicodeData.txt]] contains three of them: the simple case mappings
(lower, upper, and title case).  These are always just one character
long, so they are added as a simple value, instead.

It might make sense to make a sort of combined case conversion table,
but it's easier to just dump the three as separate tables.  The only
savings is to suppress redundant entries:  any which match the
character (which should never occur, but it doesn't hurt to check just
in case) or any title case entries which match the upper case entries
(which does occur).

<<Initialize UCD files>>=
decl_num(slc);
decl_num(suc);
decl_num(stc);
@

<<Process a line of [[UnicodeData.txt]]>>=
{
  const char *slcs = fields[13], *sucs = fields[12], *stcs = fields[14];
  uint32_t slc = strtol(slcs, NULL, 16);
  uint32_t suc = strtol(sucs, NULL, 16);
  uint32_t stc = strtol(stcs, NULL, 16);

  if(*slcs && slc != low)
    add_num(slc, slc, 1);
  if(*sucs && suc != low)
    add_num(suc, suc, 1);
  if(*stcs && stc != low && (!*sucs || stc != suc))
    add_num(stc, stc, 1);
}
@

[[SpecialCasing.txt]] lists the exceptions to the above case mappings.
The above values are always either zero or one character (technically
always one character, since the default value is the unmodified code
point); the ones from this file may be more than one character, or may
require more than one character as input, or may only apply under
certain conditions.  The mappings with conditions override the simple
mappings above when the condition is in effect.  It would be nice to
only require one lookup to do case conversion, but combining the above
tables with this one is not that simple.  Instead, this table needs to
always be consulted first, followed by the above tables.

The file is short enough that it can be encoded in a relatively
inefficient format, which simply directly encodes the contents of the
file.  Three tables are generated, corresponding to the desired case
conversion.  If the conversion is already in one of the above tables,
it is simply skipped.  Otherwise, it is encoded as a condition word,
followed by the result string.  Since the condition flags can be
encoded in fewer than 16 bits, the lower 5 bits also encode the length of
the conversion.  That way, multiple conversions with different
conditions can simply be appended to the string.  Unfortunately,
multiple entries with the same key are not always consecutive, so this
merging is done after they have all been gathered.

The string table generator converts 32-bit values to UTF-16, but
assumes that they are valid (i.e., not surrogates themselves).  This
assumption works to our advantage here, in that the flag/length field
is just a plain 16-bit value, and will not be mangled by the
conversion.  However, the length field needs to be adjusted for any
values that encode into two 16-bit values.

<<Special casing conditions>>=
/* all known conditions from SpecialCasing.txt, version 6.2 */
/* future revisions may need more flags/parsing */
#define UNI_SC_FL_LT                (1<<5)  /* lt locale */
#define UNI_SC_FL_AZ                (1<<6)  /* az locale */
#define UNI_SC_FL_TR                (1<<7)  /* tr locale */
#define UNI_SC_FL_LOCALE            (0x07 << 5)   /* any locale flags */

#define UNI_SC_FL_NOT               (1<<8)  /* applies to subsequent conditions */
#define UNI_SC_FL_AFTER_I           (1<<9)  /* capital I in same grapheme cl. */
#define UNI_SC_FL_AFTER_SOFT_DOTTED (1<<10) /* lower-case i in same gc */
#define UNI_SC_FL_BEFORE_DOT        (1<<11) /* combining dot above in same gc */
#define UNI_SC_FL_MORE_ABOVE        (1<<12) /* grave, acute, etc. in same gc */
#define UNI_SC_FL_FINAL_SIGMA       (1<<13) /* no following letters in same word */
#define UNI_SC_FL_CONTEXT           (0x1f << 9) /* any condition flags */

#define UNI_SC_FL_LEN_MASK          (0x1f)
@

<<Unicode property exports for generator>>=
<<Special casing conditions>>
@

<<Initialize UCD files>>=
decl_str(lc);
decl_str(uc);
decl_str(tc);
@

<<Parse UCD files>>=
open_f("SpecialCasing.txt");
#define add_sc(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  uint32_t str[64]; /* max known value == 30, so 64 should be enough */ \
  uint32_t len, len16; \
  for(s = v, len = len16 = 0; *s; len++, len16++) { \
    str[len + 1] = strtol(s, &s, 16); \
    if(str[len + 1] > 0xFFFF) \
      len16++; \
  } \
  str[0] = flg | len16; \
  if(len == 1 && low == high) { \
    int32_t num; \
    uint8_t den; \
    prop_t *sc = &parsed_props[prop_s##n]; \
    uni_chrrng_val(low, sc->rng_dat32, sc->len, &num, &den); \
    if(den && num == str[1]) \
      break; \
  } \
  add_str_rng(&parsed_props[prop_##n], low, high, str, len + 1); \
} while(0)
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  uint16_t flg = 0;
  if(num_fields > 4) {
    s = fields[4];
    /* slow parse, but at least accurate */
    if(!strncasecmp(s, "lt", 2)) {
      flg |= UNI_SC_FL_LT;
      s += 2;
      if(*s)
        ++s;
    } else if(!strncasecmp(s, "az", 2)) {
      flg |= UNI_SC_FL_AZ;
      s += 2;
      if(*s)
        ++s;
    } else if(!strncasecmp(s, "tr", 2)) {
      flg |= UNI_SC_FL_TR;
      s += 2;
      if(*s)
        ++s;
    }
    if(!strncasecmp(s, "Not_", 4)) {
      flg |= UNI_SC_FL_NOT;
      s += 4;
    }
    if(!strcasecmp(s, "After_I"))
      flg |= UNI_SC_FL_AFTER_I;
    else if(!strcasecmp(s, "After_Soft_Dotted"))
      flg |= UNI_SC_FL_AFTER_SOFT_DOTTED;
    else if(!strcasecmp(s, "Before_Dot"))
      flg |= UNI_SC_FL_BEFORE_DOT;
    else if(!strcasecmp(s, "More_Above"))
      flg |= UNI_SC_FL_MORE_ABOVE;
    else if(!strcasecmp(s, "Final_Sigma"))
      flg |= UNI_SC_FL_FINAL_SIGMA;
    else if(*s) {
      fprintf(stderr, "Unknown case condition: %s\n", s);
      exit(1);
    }
  }
  add_sc(lc, fields[1]);
  add_sc(uc, fields[3]);
  /* note that this condition may be too simple */
  /* it does not account for excess ignorable whitespace */
  /* it does not check if it ends up changing stc */
  /* if(strcmp(fields[3], fields[2]) */
    add_sc(tc, fields[2]);
}
fclose(f);
@

<<Parse UCD files>>=
<<Postprocess simple casing for [[lc]]>>
<<Postprocess simple casing for [[uc]]>>
<<Postprocess simple casing for [[tc]]>>
@

<<Postprocess simple casing for (@c)>>=
{
  prop_t *prop = &parsed_props[prop_<<@c>>];
  qsort(prop->str_arr, prop->len, sizeof(*prop->str_arr), uni_cmp_cp);
  /* pass 1: merge */
  for(i = 1; i < prop->len; i++) {
    raw_cp_str_t *prev = &prop->str_arr[i - 1], *cur = &prop->str_arr[i];
    if(cur->cp == prev->cp) {
      uint32_t newstr[cur->len + prev->len];
      cpybuf(newstr, prop->strs + prev->off, prev->len);
      cpybuf(newstr + prev->len, prop->strs + cur->off, cur->len);
      prev->cp = 0;
      add_str_rng(prop, cur->cp, cur->cp, newstr, cur->len + prev->len);
      cur->off = prop->str_arr[prop->len - 1].off;
      cur->len += prev->len;
      prop->len--;
    }
  }
  /* pass 2: remove merged */
  /* doing it one at a time is inefficient, but simple and good enough */
  for(i = 0; i < prop->len; i++)
    if(!prop->str_arr[i].cp) {
      movebuf(&prop->str_arr[i], &prop->str_arr[i + 1], prop->len - i);
      prop->len--;
      i--;
    }
}
@

<<Additional parse-ucd C files>>=
btricks.h \
@

<<Additional parse-ucd includes>>=
#include "btricks.h"
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_lc], gen_h, tstf);
dump_str_tabs(&parsed_props[prop_uc], gen_h, tstf);
dump_str_tabs(&parsed_props[prop_tc], gen_h, tstf);
@

<<Parse UCD files>>=
/* force addition to prop table */
parsed_props[prop_lc].mt = (uint32_t *)~0;
parsed_props[prop_uc].mt = (uint32_t *)~0;
parsed_props[prop_tc].mt = (uint32_t *)~0;
@

While the properties were named after the final result, these tables
only provide a small part of it.  To get the final result, helper
functions are provided, along with macros to use those functions.  A
generic overall helper function returns a pointer to the raw results,
while the more specific functions convert the results into one of the
three supported output formats.

<<Unicode property exports>>=
/* return is len; buf filled w/ up to buf_len chars */
/* simple, special are mtabs for simple/special casing */
/* strs is strings for special */
/* cond is bit flags of condtion, or: */
/* cond == ~0 -> return -2 if condition required */
/* return: -1 means no change */
/*         -2 means condition required */
/*         0+ == length of potential return */
<<[[uni_case_convert]][[32]] proto>>;
<<[[uni_case_convert]][[16]] proto>>;
<<[[uni_case_convert]][[8]] proto>>;
@

<<[[uni_case_convert]](@sz) proto>>=
int uni_case_convert<<@sz>>(uint32_t cp, uint32_t cond,
                       <<Buffer return parameters for UTF-[[<<@sz>>]]>>,
                       const uint32_t *simple, const uint32_t *special,
		       const uint16_t *strs)
@

<<Unicode property functions>>=
/* return code is similar to above, except when ret == NULL, positive */
/*  return is return code point */
/* rather than filling a buffer, ret is the return string */
static int32_t uni_case_convert(uint32_t cp, uint32_t cond,
                                const uint16_t **ret, const uint32_t *simple,
				const uint32_t *special, const uint16_t *strs)
{
  const uint8_t *mr;
  uni_multi_tab_lookup(special, cp * 4, &mr, 0);
  if(!mr) {
    uni_multi_tab_lookup(simple, cp * 4, &mr, 0);
    if(!mr)
      return -1;
    ret = NULL;
    return *(uint32_t *)mr >> 8;
  }
  uni_str_ptr_t *v = (void *)mr;
  const uint16_t *str = strs + v->off;
  if(!(*str & ~UNI_SC_FL_LEN_MASK) && v->len == (*str & UNI_SC_FL_LEN_MASK) + 1) {
    /* no conditions */
    *ret = str + 1;
    return v->len - 1;
  }
  if(cond == (uint32_t)~0)
    return -2;
  uint32_t rem = v->len;
  const uint16_t *ncond = NULL;
  while(rem > 0) {
    if(!(*str & UNI_SC_FL_LOCALE) || (*str & UNI_SC_FL_LOCALE & cond)) {
      if(!(*str & UNI_SC_FL_CONTEXT)) {
        /* locale condition is stronger than no condition at all */
        if(!ncond || (*str & UNI_SC_FL_LOCALE))
	  ncond = str;
      } else if(!(*str & cond & UNI_SC_FL_CONTEXT) == !!(*str & UNI_SC_FL_NOT)) {
        /* condition true: return result (assumes no other cond could be true) */
	ncond = str;
	break;
      }
    }
    rem -= (*str & UNI_SC_FL_LEN_MASK) + 1;
    str += (*str & UNI_SC_FL_LEN_MASK) + 1;
  }
  if(!ncond) {
    uni_multi_tab_lookup(simple, cp * 4, &mr, 0);
    if(!mr)
      return -1;
    *ret = NULL;
    return *(uint32_t *)mr >> 8;
  }
  *ret = ncond + 1;
  return *ncond & UNI_SC_FL_LEN_MASK;
}
@

<<Unicode property functions>>=
<<[[uni_case_convert]][[32]]>>
<<[[uni_case_convert]][[16]]>>
<<[[uni_case_convert]][[8]]>>
@

<<[[uni_case_convert]](@sz)>>=
<<[[uni_case_convert]][[<<@sz>>]] proto>>
{
  const uint16_t *rbuf;
  int32_t ret = uni_case_convert(cp, cond, &rbuf, simple, special, strs);
  if(ret < 0)
    return ret;
  if(!rbuf)
    return uni_return32_buf<<@sz>>((uint32_t *)&ret, 1, buf, off, buf_len);
  return uni_return16_buf<<@sz>>(rbuf, ret, buf, off, buf_len);
}
@

<<[[uni_case_convert]] support for (@type)>>=
<<[[uni_case_convert]][[32]] support>>
<<[[uni_case_convert]][[16]] support>>
<<[[uni_case_convert]][[8]] support>>
@

<<[[uni_case_convert]](@sz) support>>=
int uni_<<@type>><<@sz>>(uint32_t cp, uint32_t cond, <<Buffer return parameters for UTF-[[<<@sz>>]]>>);
#define uni_<<@type>><<@sz>>(cp, cond, buf, off, buf_len) \
  uni_case_convert<<@sz>>(cp, cond, buf, off, buf_len, \
                     uni_s<<@type>>_mtab, uni_<<@type>>_mtab, uni_<<@type>>_strs)

@

<<Unicode property exports>>=
/* note for all below: if((res = uni_xx(cp, ...)) == -1) { res = 1; *buf = cp } */
<<[[uni_case_convert]] support for [[lc]]>>
<<[[uni_case_convert]] support for [[uc]]>>
/* don't forget: if((res = uni_tc(cp, ...)) == -1) res = uni_uc(cp, ...) */
<<[[uni_case_convert]] support for [[tc]]>>
@

For more case transformation fun, there is [[CaseFolding.txt]].  It
provides four different case folding (lower-case) transformations:
common, simple, full, and Turkic special cases.  The common and simple
cases are guaranteed to always have length one.  There are few enough
full and Turkic cases that it makes sense to just encode them using
the same method as the full case conversion tables above.  Note that
the official cf property ignores the Turkic cases.

<<Initialize UCD files>>=
decl_num(scf);
decl_str(cf);
@

<<Parse UCD files>>=
open_f("CaseFolding.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  uint32_t flg;
  uint32_t scf = strtol(fields[2], NULL, 16);
  switch(fields[1][0]) {
    case 'C': /* default; value == cp */
      add_num(scf, scf, 1);
      break;
    case 'S': /* scf == C+S */
      add_num(scf, scf, 1);
      break;
    case 'F': /* cf == C+F */
      flg = 0;
      add_sc(cf, fields[2]);
      break;
    case 'T': /* tr, az lang only; not used by anything, technically */
      flg = UNI_SC_FL_AZ | UNI_SC_FL_TR;
      add_sc(cf, fields[2]);
      add_str(cf, fields[2]);
      break;
  }
}
fclose(f);
<<Postprocess simple casing for [[cf]]>>
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_cf], gen_h, tstf);
@

<<Parse UCD files>>=
parsed_props[prop_cf].mt = (uint32_t *)~0; /* force addition to prop table */
@

Technically, any entry which is in the S class but not in the F class
should have no translation.  However, I believe that there is no such
case.  Like with the case conversions, a set of macros is provided for
cf property lookup.  Since there is no condition input, a special macro
is provided for Turkic locales as well.

<<[[uni_case_convert]] cf support for (@type) using condition (@cond)>>=
<<[[uni_case_convert]][[32]] cf support>>
<<[[uni_case_convert]][[16]] cf support>>
<<[[uni_case_convert]][[8]] cf support>>
@

<<[[uni_case_convert]](@sz) cf support>>=
int uni_<<@type>><<@sz>>(uint32_t cp, <<Buffer return parameters for UTF-[[<<@sz>>]]>>);
#define uni_<<@type>><<@sz>>(cp, buf, off, buf_len) \
  uni_case_convert<<@sz>>(cp, <<@cond>>, buf, off, buf_len, \
                     uni_scf_mtab, uni_cf_mtab, uni_cf_strs)

@

<<Unicode property exports>>=
<<[[uni_case_convert]] cf support for [[cf]] using condition [[0]]>>
/* in turkic locales */
<<[[uni_case_convert]] cf support for [[tcf]] using condition [[UNI_SC_FL_TR]]>>
@

[[DerivedNormalizationProps.txt]] contains the final case folding
method%
\footnote{The FC\_NFKC property is related to case folding, but is
deprecated and therefore not provided as a property.}%
.  The main difference between it and the other methods is that
it applies multiple transformations simultaneously.  In addition to
case folding, it also removes default ignorable code points and
performs compatibility decomposition on the results repeatedly until
the result is stable.

<<Initialize UCD files>>=
decl_str(NFKC_CF);
@

<<Process a line of [[DerivedNormalizationProps.txt]]>>=
if(num_fields == 3 && !strcmp(fields[1], "NFKC_CF"))
  add_str(NFKC_CF, fields[2]);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_NFKC_CF], gen_h, tstf);
@

<<Parse UCD files>>=
parsed_props[prop_NFKC_CF].mt = (uint32_t *)~0; /* force addition to prop table */
@

It might also quite useful for the other case folding techniques to
include normalization.  This saves a pass in the transformation step.
There are a number of combinations:  NFC\_CF, NFC\_SCF,
NFKC\_SCF.  Whether or not any of those combinations should include
removal of default ignorable code points is difficult to tell.

\lstset{language=txt}
<<FIXME>>=
 combine case folding and normalization here
   create string tabs NFC_CF, NFC_SCF, NFKC_SCF(?)
@

\subsection{Additional Support for Normalization}

The raw property lookup functions are not meant to be used directly. 
Instead, here are some more high-level functions.  They are stored in
their own object file, even though the actual amount of code is tiny.
That way, the tables they reference will not be pulled in for all
properties.

<<Library [[uni]] Members>>=
uni_norm.o
@

\lstset{language=C}
<<Library [[uni]] headers>>=
#include "uni_norm.h"
@

<<uni_norm.h>>=
<<Common C Warning>>
#ifndef UNI_NORM_H
#define UNI_NORM_H

#include "uni_prop.h"
<<Unicode normalization support exports>>
#endif /* UNI_NORM_H */
@

<<uni_norm.c>>=
<<Common C Header>>
#include "uni_all.h"

<<Unicode normalization support local definitions>>

<<Unicode normalization support functions>>
@

The first step in any normalization is to decompose.  The only
difference in the three function groups is what table they use, so
only one group of functions exists; each group just calls the helper
with the appropriate tables.  This is one case where Ada or C++
generics would be very handy; each group of functions is virtually
triplicated with minor changes due to character type.  NoWeb makes
things additionally complicated by disallowing chunk references inside
preprocessor macro definitions; otherwise, the definition of each
group could have been made using a single preprocessor call.

There are a number of ways a function which modifies a Unicode string
might operate.  For example, it might use separate parameters to
specify input and output strings, or it might always allocate space
for its return value from memory, or it might use the input buffer as
an output buffer as well.  The original versions of these functions
only supported the latter.  However, they assumed that enough room was
available for the output, and moved the tail end of the string around
every time a character's transformed length was not one.  In order to
support checking lengths and resizable output buffers, the input
buffer is provided separately.  The only case where the input buffer
and output buffer are safe to be the same is when the input
buffer length is one character (which may result in [[ilen]] greater
than one in UTF-16 or UTF-8).  No check will be made to ensure that
they do not overlap, though.  To convert an entire string, the user
should duplicate the input string, first.  Convenience functions are
provided to do just that.

<<Unicode normalization support exports>>=
/* wrappers */

/* decomp functions can operate on any number of chars, including 1 */
/* decomp functions return final length */
<<[[uni_X_dec]] protos for [[NFD_dec]]>>
<<[[uni_X_dec]] protos for [[NFKD_dec]]>>
/* NFKC_Casefold assumes NFD has already been run */
<<[[uni_X_dec]] protos for [[NFKC_Casefold]]>>
@

<<[[uni_X_dec]] protos for (@type)>>=
<<[[uni_X_dec]][[32]] proto>>;
<<[[uni_X_dect]][[32]] proto>>;
<<[[uni_X_dec]][[16]] proto>>;
<<[[uni_X_dect]][[16]] proto>>;
<<[[uni_X_dec]][[8]] proto>>;
<<[[uni_X_dect]][[8]] proto>>;
@

<<[[uni_X_dec]](@sz) proto>>=
int uni_<<@type>><<@sz>>(const uint<<@sz>>_t *ibuf, unsigned int ilen,
                 <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<[[uni_X_dect]](@sz) proto>>=
int uni_<<@type>>t<<@sz>>(<<Buffer return parameters for UTF-[[<<@sz>>]]>>, unsigned int ilen)
@

<<Unicode normalization support functions>>=
<<[[do_any_dec]][[32]]>>
<<[[do_any_dec]][[16]]>>
<<[[do_any_dec]][[8]]>>
@

<<[[do_any_dec]](@sz)>>=
static int do_any_dec<<@sz>>(const uint<<@sz>>_t *ibuf, unsigned int ilen,
                          <<Buffer return parameters for UTF-[[<<@sz>>]]>>,
                          const uint32_t *tab, const uint16_t *str)
{
  int i;
  int rlen = 0;
  unsigned int blen = off > 0 ? ~0 : buf_len ? *buf_len : 0, clen = 0;
  uint<<@sz>>_t *bptr = off < 0 && buf ? *buf : NULL;
  for(i = 0; i < ilen; i += clen) {
    int16_t doff;
    uint8_t len;
    uint8_t compat;
    const uint32_t *rptr32 = NULL;
    const uint16_t *rptr16 = NULL;
    uint32_t hbuf[3], cp = uni_int_utf<<@sz>>_decode(ibuf + i, &clen);
    uni_x_dec(cp, tab, &doff, &len, &compat, 1);
    if(!len && !doff) {
      len = 1;
      rptr32 = &cp;
    } else if(doff < 0) {
      uni_hangul_syllable_decomp(cp, hbuf, 1);
      rptr32 = hbuf;
    } else
      rptr16 = str + doff;
    len = rptr16 ? uni_return16_buf<<@sz>>(rptr16, len, off < 0 ? &bptr : buf,
	                                   off, off < 0 ? &blen : buf_len)
                 : uni_return32_buf<<@sz>>(rptr32, len, off < 0 ? &bptr : buf,
	                                   off, off < 0 ? &blen : buf_len);
    rlen += len;
    blen -= blen > len ? len : blen;
    if(off >= 0)
      off += len;
    else
      bptr += len;
  }
  return rlen;
}
@

<<Unicode normalization support functions>>=
<<[[do_any_dect]][[32]]>>
<<[[do_any_dect]][[16]]>>
<<[[do_any_dect]][[8]]>>
@

<<[[do_any_dect]](@sz)>>=
static int do_any_dect<<@sz>>(<<Buffer return parameters for UTF-[[<<@sz>>]]>>,
                           unsigned int ilen,
                           const uint32_t *tab, const uint16_t *str)
{
  uint<<@sz>>_t *ibuf;
  int ret;
  if(!ilen)
    return 0;
  if(off < 0 && !buf_len) /* support rval as fixed len */
    buf_len = &ilen;
  /* technically an error if ilen > *buf_len, but no check */
  inisize(ibuf, ilen);
  cpybuf(ibuf, off > 0 ? *buf + off : *buf, ilen);
  ret = do_any_dec<<@sz>>(ibuf, ilen, buf, off, buf_len, tab, str);
  free(ibuf);
  return ret;
}
@

<<[[uni_dec_any]] for (@type) using (@tab) and (@str)>>=
<<[[uni_dec_any]][[32]] pair>>
<<[[uni_dec_any]][[16]] pair>>
<<[[uni_dec_any]][[8]] pair>>
@

<<[[uni_dec_any]](@sz) pair>>=
<<[[uni_X_dec]][[<<@sz>>]] proto>>
{
  return do_any_dec<<@sz>>(ibuf, ilen, buf, off, buf_len, <<@tab>>, <<@str>>);
}

<<[[uni_X_dect]][[<<@sz>>]] proto>>
{
  return do_any_dect<<@sz>>(buf, off, buf_len, ilen, <<@tab>>, <<@str>>);
}
@

<<Unicode normalization support functions>>=
<<[[uni_dec_any]] for [[NFD_dec]] using [[uni_canon_decomp_mtab]] and [[uni_dm_strs]]>>
<<[[uni_dec_any]] for [[NFKD_dec]] using [[uni_compat_decomp_mtab]] and [[uni_dm_strs]]>>
<<[[uni_dec_any]] for [[NFKC_Casefold]] using [[uni_NFKC_CF_mtab]] and [[uni_NFKC_CF_strs]]>>
@

Canonical ordering is the second step in any normalization.  This
requires that any consecutive characters with non-zero canonical
combining class be ordered by their canonial combining class.  The
procedure is described as a bubble sort, so the sort must be stable.
These two requirements indicate the need for a side array for sorting,
containing the original position of each character along with its ccc
value.  The code point itself is stored as well, so that no effort is
needed to write out the sorted array without clobbering the old string.
If the last character passed in has a non-zero ccc, there may be more,
so a continuation is requested.  Any characters which have already
been processed, and could not possibly be reordered, are returned as
output.

Since this is just a reordering of characters, there is no need to
provide resizable output or separate input and output buffers.
Instead, only an in-place sorter is provided.

<<Unicode normalization support exports>>=
/* canon order function may need more chars than passed in */
/* set last to non-0 if no continuations possible */
/* returns less than ilen if more chars needed */
/* can skip # of chars equal to return value */
<<[[canon_order]][[32]] proto>>;
<<[[canon_order]][[16]] proto>>;
<<[[canon_order]][[8]] proto>>;
@

<<[[canon_order]](@sz) proto>>=
int uni_Canon_Order<<@sz>>(uint<<@sz>>_t *buf, unsigned int ilen, int last)
@

<<Unicode normalization support functions>>=
<<[[canon_order]][[32]]>>
<<[[canon_order]][[16]]>>
<<[[canon_order]][[8]]>>
@

<<Unicode normalization support local definitions>>=
/* for stable sort */
struct ccs {
  uint32_t cp /* :24 */, ccc /* :8 */;
  unsigned int opos;
};

static int cmpcc(const void *a, const void *b)
{
  const struct ccs *p1 = (const struct ccs *)a;
  const struct ccs *p2 = (const struct ccs *)b;
  if(p1->ccc != p2->ccc)
    return p1->ccc - p2->ccc;
  return p1->opos - p2->opos;
}
@

<<[[canon_order]](@sz)>>=
<<[[canon_order]][[<<@sz>>]] proto>>
{
  int i;
  for(i = 0; i < ilen; i += uni_utf<<@sz>>_nextc(buf + i)) {
    uint32_t cp = uni_int_utf<<@sz>>_decode(buf + i, NULL);
    int cc = uni_ccc_of(cp);
    if(cc) {
      int ni = i + uni_utf<<@sz>>_nextc(buf + i);
      int j, l;
      for(l = 1, j = ni; j < ilen; j += uni_utf<<@sz>>_nextc(buf + j), l++) {
        uint32_t cp2 = uni_int_utf<<@sz>>_decode(buf + j, NULL);
	int cc2 = uni_ccc_of(cp2);
	if(!cc2)
	  break;
      }
      if(j == ilen && !last)
        return i;
      struct ccs ccbuf[l];
      ccbuf[0].ccc = cc;
      ccbuf[0].cp = cp;
      ccbuf[0].opos = i;
      int k;
      for(l = 1, k = ni; k < j; k += uni_utf<<@sz>>_nextc(buf + k), l++) {
        uint32_t cp2 = uni_int_utf<<@sz>>_decode(buf + k, NULL);
	ccbuf[l].ccc = uni_ccc_of(cp2);
	ccbuf[l].cp = cp2;
	ccbuf[l].opos = k;
      }
      qsort(ccbuf, l, sizeof(*ccbuf), cmpcc);
      for(k = 0, j = i; k < l; k++, j += uni_utf<<@sz>>_nextc(buf + j))
        if(ccbuf[k].opos != j)
	  uni_int_utf<<@sz>>_encode(buf + j, ccbuf[k].cp);
    }
  }
  return ilen;
}
@

The last step in any composition normalization is canonical
composition.  Two characers may be combined if the first character has
a canonical combining class of zero and all intervening characters
have a non-zero canonical combining class less than the canonical
combining class of the second character.  This requires a continuation
flag, just like canonical ordering.  However, since this is once again
a function which can transform its input, more thought is required.
Unlike decomposition, it is impossible for a string to expand in
length.  This means that in-place conversion is possible.  In fact,
just like canonical ordering, only in-place transformation is supported.

<<Unicode normalization support exports>>=
/* canon comp function may need more chars than passed in */
/* set nok to non-NULL if continuation possible */
/* returns nok < ilen if more chars needed */
/* can skip nok chars */
/* return value is updated length */
<<[[uni_NFC_comp]][[32]] proto>>;
<<[[uni_NFC_comp]][[16]] proto>>;
<<[[uni_NFC_comp]][[8]] proto>>;
@

<<[[uni_NFC_comp]](@sz) proto>>=
int uni_NFC_comp<<@sz>>(uint<<@sz>>_t *buf, unsigned int ilen, unsigned int *nok)
@

<<Unicode normalization support functions>>=
<<[[uni_NFC_comp]][[32]]>>
<<[[uni_NFC_comp]][[16]]>>
<<[[uni_NFC_comp]][[8]]>>
@

<<[[uni_NFC_comp]](@sz)>>=
<<[[uni_NFC_comp]][[<<@sz>>]] proto>>
{
  int i, last0 = -1;
  for(i = 0; i < ilen; i += uni_utf<<@sz>>_nextc(buf + i)) {
    uint32_t cp = uni_int_utf<<@sz>>_decode(buf + i, NULL);
    int ccc = uni_ccc_of(cp);
    /* first char must be ccc=0 */
    if(ccc)
      continue;
    int16_t off;
    uint8_t len;
    uni_find_canon_comp(cp, &off, &len);
    if(!len && !off)
      continue; /* no compositions for this char */
    last0 = i;
    int lastccc = 0;
    int j;
    for(j = i + uni_utf<<@sz>>_nextc(buf + i); j < ilen;
        j += uni_utf<<@sz>>_nextc(buf + j)) {
      uint32_t cp2 = uni_int_utf<<@sz>>_decode(buf + j, NULL);
      ccc = uni_ccc_of(cp2);
      /* second char must have no intervening equal ccc */
      /* this assumes canonical ordering has been done */
      if(ccc && lastccc && ccc == lastccc)
        continue;
      /* or greater ccc; this only happens if back down to 0 */
      if(!ccc && lastccc) {
        i = j - 1;
	break;
      }
      lastccc = ccc;
      int ccp = uni_canon_comp(cp, cp2, off, len);
      if(ccp > 0) {
        /* composition replaces char 1 and deletes char 2 */
	int clen = uni_utf<<@sz>>_enclen(ccp);
	int len1 = uni_utf<<@sz>>_nextc(buf + i), len2 = uni_utf<<@sz>>_nextc(buf + j);
	/* make room for ccp, if necessary */
	if(len1 != clen) {
	  /* WARNING: this may cause buffer overrun if */
	  /*  clen > len1 + len2 */
	  movebuf(buf + i + clen, buf + i + len1, j - i - len1);
	  len2 += len1 - clen;
	}
	uni_int_utf<<@sz>>_encode(buf + i, ccp);
	if(len2 > 0 && ilen - j - len2)
	  movebuf(buf + j, buf + j + len2, ilen - j - len2);
        ilen -= len2;
	/* start over with new start char */
	cp = ccp;
	uni_find_canon_comp(cp, &off, &len);
	if(!len && !off)
	  break; /* no compositions for this char */
        j = i;
	lastccc = 0;
	continue;
      }
      /* if no match and hit a ccc=0, try next 1st char */
      if(!ccc) {
        i = j + uni_utf<<@sz>>_prevc(buf + j);
	break;
      }
    }
  }
  if(nok) {
    /* need more if we got a potential start char */
    /* but don't need to look at anything before that char */
    if(last0 < 0)
      *nok = ilen;
    else {
      int16_t off;
      uint8_t len;
      uni_find_canon_comp(uni_int_utf<<@sz>>_decode(buf + last0, NULL), &off, &len);
      *nok = len || off ? last0 : ilen;
    }
  }
  return ilen;
}
@

The above routine may cause buffer overruns if the composition causes
the string to grow.  This happens when the composition result's word
length exceeds the sum of the composed characters' word lengths.  This
can never happen with 32-bit words, because all lengths are one.  This
can never happen with 16-bit words, because the maximum composed
character length is two, and the minimum lengths of the components are
one.  However, with 8-bit words, the maximum result length is 4, and
the minimum component lengths are one.  This leaves plenty of room for
growth.  To ensure this never happens, a check is made while reading
the data.  The only data which does not need to be checked is Hangul
syllable composition: all three are always of length 3.  For the
versions against which this library was built, the check has
succeeded.  If it ever fails, the composition code will need to be
rewritten to support resizable buffers.

<<Parse UCD files>>=
for(i = 0; i < cm_prop->len; i++) {
  const uint32_t *bctab = cm_prop->strs + cm_prop->str_arr[i].off;
  int len1 = uni_utf8_enclen(cm_prop->str_arr[i].cp);
  for(j = 0; j < cm_prop->str_arr[i].len; j += 2) {
    int len2 = uni_utf8_enclen(bctab[j]);
    int clen = uni_utf8_enclen(bctab[j + 1]);
    if(clen > len1 + len2) {
      fprintf(stderr, "Compose UTF-8 overflow: %04X(%d) + %04X(%d) < %04X(%d)\n",
                      (int)cm_prop->str_arr[i].cp, len1, (int)bctab[j], len2,
		      (int)bctab[j + 1], clen);
      exit(1);
    }
  }
}
@

<<Additional parse-ucd includes>>=
#include "uni_io.h" /* uni_utf8_enclen() */
@

<<Additional parse-ucd C files>>=
uni_io.h \
@

To demonstrate proper normalization, here are some functions which do the
full procedure, either in-place or using a separate input buffer.

<<Unicode normalization support exports>>=
<<[[full_dec]] proto for [[NFD]]>>
<<[[full_dec]] proto for [[NFKD]]>>
@

<<[[full_dec]] proto for (@type)>>=
<<[[full_dec]][[32]] proto>>
<<[[full_dec]][[16]] proto>>
<<[[full_dec]][[8]] proto>>
@

<<[[full_dec]](@sz) proto>>=
<<[[uni_dec]] proto>>;
<<[[uni_dect]] proto>>;
@

<<[[uni_dec]] proto>>=
int uni_<<@type>><<@sz>>(uint<<@sz>>_t *ibuf, unsigned int ilen,
                 <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<[[uni_dect]] proto>>=
int uni_<<@type>>t<<@sz>>(<<Buffer return parameters for UTF-[[<<@sz>>]]>>,
                    unsigned int ilen)
@

<<Unicode normalization support functions>>=
<<[[full_dec]] for [[NFKD]]>>
<<[[full_dec]] for [[NFD]]>>
@

<<[[full_dec]] for (@type)>>=
<<[[full_dec]][[32]]>>
<<[[full_dec]][[16]]>>
<<[[full_dec]][[8]]>>
@

<<[[full_dec]](@sz)>>=
<<[[uni_dec]] proto>>
{
  if(!ilen)
    return 0;
  if(!buf_len || !*buf_len)
    return -1; /* length query not supported */
  int ret = uni_<<@type>>_dec<<@sz>>(ibuf, ilen, buf, off, buf_len);
  if(!*buf || (off < 0 && ret > *buf_len))
    return -1; /* not enough space */
  return uni_Canon_Order<<@sz>>(*buf, ret, 1);
}
<<[[uni_dect]] proto>>
{
  uint<<@sz>>_t *ibuf;
  if(!ilen)
    return 0;
  if(!buf_len)
    return -1; /* length query not supported */
  inisize(ibuf, ilen);
  cpybuf(ibuf, off > 0 ? *buf + off : *buf, ilen);
  int ret = uni_<<@type>>_dec<<@sz>>(ibuf, ilen, buf, off, buf_len);
  free(ibuf);
  if(!*buf || (off < 0 && ret > *buf_len))
    return -1; /* not enough space */
  return uni_Canon_Order<<@sz>>(*buf, ret, 1);
}
@

<<Unicode normalization support exports>>=
<<[[full_dec]] proto for [[NFC]]>>
<<[[full_dec]] proto for [[NFKC]]>>
@

<<Unicode normalization support functions>>=
<<[[full_comp]] for [[NFKC]] using [[NFKD]]>>
<<[[full_comp]] for [[NFC]] using [[NFD]]>>
@

<<[[full_comp]] for (@type) using (@dec)>>=
<<[[full_comp]][[32]]>>
<<[[full_comp]][[16]]>>
<<[[full_comp]][[8]]>>
@

<<[[full_comp]](@sz)>>=
<<[[uni_dec]] proto>>
{
  int ret = uni_<<@dec>><<@sz>>(ibuf, ilen, buf, off, buf_len);
  if(ret <= 0)
    return ret;
  return uni_NFC_comp<<@sz>>(*buf, ret, NULL);
}
<<[[uni_dect]] proto>>
{
  int ret = uni_<<@dec>>t<<@sz>>(buf, off, buf_len, ilen);
  if(ret <= 0)
    return ret;
  return uni_NFC_comp<<@sz>>(*buf, ret, NULL);
}
@

Note that there is no in-place [[NFKC_Casefold]] function, so both
versions allocate a buffer for the case folding phase.  The only
difference is whether or not a buffer is allocated for the
decomposition phase.  This should probably be fixed to be less
expensive.

<<Unicode normalization support exports>>=
<<[[full_dec]] proto for [[NFKC_CF]]>>
@

<<Unicode normalization support functions>>=
<<[[NFKC_CF]] normalizer using UTF-[[32]]>>
<<[[NFKC_CF]] normalizer using UTF-[[16]]>>
<<[[NFKC_CF]] normalizer using UTF-[[8]]>>
@

<<(@type) normalizer using UTF-(@sz)>>=
<<[[uni_dec]] proto>>
{
  int ret = uni_NFD<<@sz>>(ibuf, ilen, buf, off, buf_len);
  if(ret <= 0)
    return ret;
  return uni_NFKC_Casefoldt<<@sz>>(buf, off, buf_len, ret);
}
<<[[uni_dect]] proto>>
{
  int ret = uni_NFDt<<@sz>>(buf, off, buf_len, ilen);
  if(ret <= 0)
    return ret;
  return uni_NFKC_Casefoldt<<@sz>>(buf, off, buf_len, ret);
}
@

\subsection{Generic Normalized Unicode Input}

To make things even easier, we can make a function which returns a
normalized character from an input stream.  Any normalization type we
have code for, we support.

<<Unicode normalization support exports>>=
typedef enum {
  UNI_NORM_NONE, UNI_NORM_NFD, UNI_NORM_NFKD, UNI_NORM_NFC, UNI_NORM_NFKC,
  UNI_NORM_NFKC_CF
} uni_normtype_t;
@

<<Known Data Types>>=
uni_normtype_t,%
@

<<Unicode normalization support exports>>=
#include "uni_io.h"

int32_t uni_fgetc_norm(uni_file_t *uf, uni_normtype_t nt);
@

<<Unicode normalization support functions>>=
int32_t uni_fgetc_norm(uni_file_t *uf, uni_normtype_t nt)
{
  <<Get normalized character>>
}
@

A simple function which returns a normalized character requires that
the current decomposition and composition state be kept.  It would be
ideal to extend the [[uni_file_t]] structure with the extra information
rather than allocating yet another wrapper, but for now I would prefer
[[uni_file_t]] to stay isolated to unnormalized I/O.

In order to make this transparent, the extra support structure is
automatically created and placed on a linked list.  It is assumed that
the file will not be closed and reopened without an intervening EOF.
It is also assumed that the function will not be called any more after
it returns EOF.

<<Unicode normalization support local definitions>>=
typedef struct unifile_norm {
  struct unifile_norm *next;
  uni_file_t *uf;
  <<Normalized input buffer state>>
} unifile_norm_t;
static unifile_norm_t *norm_files = NULL;
@

<<Known Data Types>>=
unifile_norm_t,%
@

<<Get normalized character>>=
unifile_norm_t *ufn = norm_files;

while(ufn && ufn->uf != uf)
  ufn = ufn->next;
if(!ufn) {
  inisize(ufn, 1);
  clearbuf(ufn, 1);
  ufn->next = norm_files;
  norm_files = ufn;
}

<<Return normalized character if not EOF>>

unifile_norm_t **bufp;
for(bufp = &norm_files; *bufp != ufn; bufp = &(*bufp)->next);
*bufp = ufn->next;
free(ufn);

return -1;
@

For support, we'll need to store at least the maximal decomposition of
a character.

<<Normalized input buffer state>>=
uint32_t *buf;
unsigned int buf_len;
@

We'll also need to know how many characters are in the buffer, and how
many of those are good to go.  Since more than one may be good to go,
a pointer to the next character to go is kept as well.  Since the last
good-to-go character may be the last in the file, a flag is added to
detect this.

<<Normalized input buffer state>>=
int len, oklen, okptr, eof;
@

To return a single character, we'll need to normalize until at least
one character could not possibly be modified any more (i.e., [[oklen]]
is non-zero).

<<Return normalized character if not EOF>>=
unsigned int oklen = ufn->oklen;
while(!oklen && !ufn->eof) {
  /* read a char into end of buf, if needed & possible */
  <<Read and normalize a character>>
}
if(oklen) {
  /* now we have oklen chars in buf that are ready to return */
  int c = ufn->buf[ufn->okptr++];
  if(ufn->okptr == oklen) {
    ufn->okptr = ufn->oklen = 0;
    if(ufn->len > oklen)
      movebuf(ufn->buf, ufn->buf + oklen, ufn->len - oklen);
    ufn->len -= oklen;
  } else
   ufn->oklen = oklen;
  return c;
} else if(ufn->eof)
  return -1;
@

To add more characters, the decomposition of the next read character
is appended to the buffer.  The total buffer length after
decomposition is stored in [[bp]].  The Unicode standard states that
case folding requires an additional NFD step beforehand, but reading
of the data suggests that instead, it needs canonical ordering and
composition afterwards, just like the NFC and NFKC forms.  This makes
case folding just another decomposition method.

Switching forms mid-stream is not really supportable.  Any characters
in the buffer have already been processed, at least using the
decomposition method which was in effect before the switch.  The only
compromise is to switch as soon as possible, performing no additional
work on the buffered characters.  A custom routine should be used if
mid-stream switching is desired.

<<Read and normalize a character>>=
int32_t c;
int bp = ufn->len;
if(bp && nt == UNI_NORM_NONE) {
  oklen = 1;
  break;
}
c = uni_fgetc(uf);
if(c == -1)
  ufn->eof = 1;
if(c < -1)
  c = 0xFFFD; /* recommended REPLACEMENT CHARACTER */
if(nt == UNI_NORM_NONE)
  return c; /* don't bother allocating buf */
if(c != -1)
  switch(nt) {
    case UNI_NORM_NONE: /* here to remove warning */
    case UNI_NORM_NFD:
    case UNI_NORM_NFC:
      bp += uni_NFD_dec32((uint32_t *)&c, 1, &ufn->buf, bp, &ufn->buf_len);
      break;
    case UNI_NORM_NFKD:
    case UNI_NORM_NFKC:
      bp += uni_NFKD_dec32((uint32_t *)&c, 1, &ufn->buf, bp, &ufn->buf_len);
      break;
    case UNI_NORM_NFKC_CF:
      bp += uni_NFKC_Casefold32((uint32_t *)&c, 1, &ufn->buf, bp, &ufn->buf_len);
  }
@

Then, the entire set of characters already read in can be ordered.  The
number of ordered characters is saved in [[oblen]].  If no characters
can be ordered, try to read more right away.

<<Read and normalize a character>>=
int oblen = uni_Canon_Order32(ufn->buf, bp, ufn->eof);
if(!oblen) {
  ufn->len = bp;
  continue;
}
@

Then, all characters which have been ordered can be composed.  If
composition might need more characters, the next trip around the loop
will get them.

<<Read and normalize a character>>=
switch(nt) {
  case UNI_NORM_NONE: /* here to remove warning */
  case UNI_NORM_NFD:
  case UNI_NORM_NFKD:
    /* no composition */
    oklen = oblen;
    break;
  case UNI_NORM_NFC:
  case UNI_NORM_NFKC:
  case UNI_NORM_NFKC_CF: {
    int cblen = uni_NFC_comp32(ufn->buf, oblen, ufn->eof ? NULL : &oklen);
    if(ufn->eof)
      oklen = cblen;
    /* move unordered characters down if composition removed chars */
    if(cblen < oblen) {
      if(bp > oblen)
        movebuf(ufn->buf + cblen, ufn->buf + oblen, bp - oblen);
      bp -= oblen - cblen;
    }
    break;
  }
}
ufn->len = bp;
@

\subsection{Testing - Normalization}

To fully demonstrate that the normalization data is correct, the
official test suite is run.  This is provided by the UCD in
NormalizationTest.txt.  This time, rather than compiling the file's
location in, it is simply fed into standard input.

\lstset{language=make}
<<C Test Support Executables>>=
tstnorm \
@

<<Additional Tests>>=
./tstnorm <$(UCD_LOC)/NormalizationTest.txt
@

\lstset{language=C}
<<tstnorm.c>>=
<<Common C Header>>
#include "uni_all.h"

/* longest line == 587 chars */
char lbuf[1024];

/* longest string == 18 chars */
uint32_t ibuf[5][128], ilen[5];
uint32_t obuf[128];
uint32_t * /* const */ obptr = obuf;
/* const */ unsigned int oblen = 128;

int main(void)
{
  int ret = 0;
  <<Read and process NormalizationTest.txt>>
  return ret;
}
@

First, we need to track when we're in part 1, and mark every code
point encountered there as having been processed.  According to the
test, after exiting part 1, we can run all four standard
normalizations on any code point not encountered and get no effect.

In general, when entering a new section, print the section name.
Otherwise, print a dot after every 50 tests.

<<Read and process NormalizationTest.txt>>=
char *didnorm;
inisize(didnorm, 0x110000);
clearbuf(didnorm, 0x110000);
int inpart1 = 0;
int ntests = 0;
while(fgets(lbuf, sizeof(lbuf), stdin)) {
  if(*lbuf == '#')
    continue;
  if(*lbuf == '@') {
    if(ntests)
      putchar('\n');
    if(lbuf[5] == '1')
      inpart1 = 1;
    else if(inpart1) {
      inpart1 = 0;
      int i;
      for(i = 0; i < 0x110000; i++)
        if(!didnorm[i]) {
	  <<Test normalization does not affect [[i]]>>
	}
    }
    fputs(lbuf, stdout);
    continue;
  }
  if(inpart1) {
    int cp = strtol(lbuf, NULL, 16);
    didnorm[cp] = 1;
  }
  <<Run normalization test for [[lbuf]]>>
  if(!(++ntests % 50)) {
    putchar('.');
    fflush(stdout);
  }
}
putchar('\n');
@

First, let's parse a line into the input buffers.  Each line has five
semicolon-separated fields, terminated by a semicolon.  Each field has
space-separated hexadecimal numbers.

<<Run normalization test for [[lbuf]]>>=
int i;
char *s = lbuf;
for(i = 0; i < 5; i++) {
  int l = 0;
  while(1) {
    ibuf[i][l++] = strtol(s, &s, 16);
    while(isspace(*s))
      s++;
    if(*s == ';')
      break;
  }
  ilen[i] = l;
  if(*s == ';')
    s++;
  else {
    fprintf(stderr, "bad line %s\n", lbuf);
    exit(1);
  }
}
@

Then, run the conformance tests.

<<Run normalization (@type) on fields (@i0) through (@il) to produce field (@r)>>=
for(i = <<@i0>>; i < <<@il>>; i++) {
  cpybuf(obuf, ibuf[i], ilen[i]);
  int l = uni_<<@type>>t32(&obptr, -1, &oblen, ilen[i]);
  if(l != ilen[<<@r>>] || memcmp(obuf, ibuf[<<@r>>], l * sizeof(obuf[0]))) {
    ret = 1;
    fprintf(stderr, "Failed <<@type>> test %d/%d on %s\nGot: ",
                    <<@r>> + 1, i + 1, lbuf);
    int j;
    for(j = 0; j < l; j++)
      fprintf(stderr, " %04X", obuf[j]);
    fputs("\nExpected: ", stderr);
    for(j = 0; j < ilen[<<@r>>]; j++)
      fprintf(stderr, " %04X", ibuf[<<@r>>][j]);
    fputc('\n', stderr);
    continue;
  }
}
@

<<Run normalization test for [[lbuf]]>>=
<<Run normalization [[NFC]] on fields [[0]] through [[3]] to produce field [[1]]>>
<<Run normalization [[NFC]] on fields [[3]] through [[5]] to produce field [[3]]>>
@

<<Run normalization test for [[lbuf]]>>=
<<Run normalization [[NFD]] on fields [[0]] through [[3]] to produce field [[2]]>>
<<Run normalization [[NFD]] on fields [[3]] through [[5]] to produce field [[4]]>>
@

<<Run normalization test for [[lbuf]]>>=
<<Run normalization [[NFKC]] on fields [[0]] through [[5]] to produce field [[3]]>>
@

<<Run normalization test for [[lbuf]]>>=
<<Run normalization [[NFKD]] on fields [[0]] through [[5]] to produce field [[4]]>>
@

<<Test normalization does not affect [[i]]>>=
obuf[0] = i;
unsigned int l1 = 1;
if(uni_NFC32(obuf, 1, &obptr, -1, &l1) != 1 || obuf[0] != i ||
   uni_NFD32(obuf, 1, &obptr, -1, &l1) != 1 || obuf[0] != i ||
   uni_NFKC32(obuf, 1, &obptr, -1, &l1) != 1 || obuf[0] != i ||
   uni_NFKD32(obuf, 1, &obptr, -1, &l1) != 1 || obuf[0] != i) {
  fprintf(stderr, "Failed Part1 end check at %04x\n", i);
  ret = 1;
}
@

\subsection{Parsing the UCD -- Names}

An ``Other'' property which can be treated as a string for now is
the na property, the last property to be retrieved from
[[UnicodeData.txt]]%
\footnote{Fields 11 and 12 (isc and na1) are obsolete and informative,
so they will not be read in as a property unless I find a use for
them.  na1 was useful prior to 6.2, since it contained names for
control characters, but those are now officially aliases.}%
.  This includes the Name\_Alias property (from
[[NameAliases.txt]]%
\footnote{The third field of [[NameAliases.txt]] (Type) is
informative, and will not be read in as a property unless I find a use.}%
), as well.

Since these names are always pure ASCII, storing one character per
32-bit word is even more wasteful than above:  more than 25 bits are
always wasted on each character.  For this reason, the string table
will be dumped as 8-bit characters.  The prerequisite format is to
cast the 32-bit string pointer to an 8-bit string pointer, and pad
with zeroes to the next 32-bit boundary.  In fact, since they are
always upper-case letters, digits, spaces, and hyphens, they could be
represented as 6 bits per word, but that amount of compression could
just end up being too expensive to extract.

Some characters' names are generated by appending the hexadecimal code
point to a range name.  The Hangul syllable characters also have
specially generated names.  In both cases, UnicodeData.txt simply
indicates a range.  Since code point tables are not good at storing
ranges, these are stored in a separate property, narng.  In addition,
since the CJK COMPATIBILITY IDEOGRAPH characters (U+F900-U+FAD9,
U+2F800-U+2FA1D) are named as though they were numbered ranges, they
are stored in that property as well.  For now, the narng property is
stored as a plain string property, with each end point in the range
table.  Like-named ranges are never consecutive, so the range can be
determined by going down and/or up one.

<<UCD parser local definitions>>=
#define add_str8(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[64]; \
    uint32_t len = strlen(v); \
    str[len / 4] = 0; \
    memcpy(str, v, len); \
    len = (len + 3) / 4; \
    add_str_rng(&parsed_props[prop_##n], low, high, str, len); \
    parsed_props[prop_##n].strs_char_size = 8; \
  } \
} while(0)
@

<<Initialize UCD files>>=
decl_str(na);
decl_str(Name_Alias);
decl_str(narng);
@

<<Process a line of [[UnicodeData.txt]]>>=
if(fields[1][0] != '<' && (high < 0xF900 || low > 0xFAD9) &&
   (high < 0x2F800 || low > 0x2FA1D))
  add_str8(na, fields[1]);
else {
  uint32_t h = high;
  high = low;
  if(fields[1][0] == '<' && (s = strchr(fields[1], ','))) {
    *s = 0;
    /* standard claims all names are upper-case, but ranges have lower-case */
    for(s = fields[1]; *s; s++)
      if(islower(*s))
        *s = toupper(*s);
    add_str8(narng, fields[1] + 1);
    low = high = h;
    add_str8(narng, fields[1] + 1);
  } else if(low == 0xF900 || low == 0x2F800) {
    *strchr(fields[1], '-') = 0;
    add_str8(narng, fields[1]);
    high = low = low == 0xF900 ? 0xFAD9 : 0x2FA1D;
    add_str8(narng, fields[1]);
  } /* else either <control> or CJK COMPATIBILITY other than low */
}
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_na], gen_h, tstf);
dump_str_tabs(&parsed_props[prop_narng], gen_h, tstf);
@

In addition, the Hangul syllable names require the name of each part,
also known as the Jamo\_Short\_Name (JSN) property.  It is extracted
from [[Jamo.txt]].

<<Initialize UCD files>>=
decl_str(JSN);
@

<<Parse UCD files>>=
open_f("Jamo.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_str8(JSN, fields[1]);
}
fclose(f);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_JSN], gen_h, tstf);
@

Unfortunately, the Name\_Alias property may have multiple values per
code point.  To fix this, all aliases are combined into a single
string, prefixing each with its length.

<<Parse UCD files>>=
open_f("NameAliases.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  char nabuf[128];
  strcpy(nabuf + 1, fields[1]);
  nabuf[0] = strlen(fields[1]) - 1;
  add_str8(Name_Alias, nabuf);
}
fclose(f);
@

<<Post-process property data>>=
prop_t *na_alias = &parsed_props[prop_Name_Alias];
qsort(na_alias->str_arr, na_alias->len, sizeof(*na_alias->str_arr), uni_cmp_cp);
for(i = na_alias->len - 1; i > 0; i--) {
  unsigned int nlen = na_alias->str_arr[i].len;
  for(low = i; low > 0; low--) {
    if(na_alias->str_arr[low - 1].cp != na_alias->str_arr[i].cp)
      break;
    nlen += na_alias->str_arr[low - 1].len;
  }
  if(low == i)
    continue;
  while(na_alias->max_strs < na_alias->strs_len + nlen)
    resize(na_alias->strs, na_alias->max_strs *= 2);
  uint8_t *strs8 = (uint8_t *)(na_alias->strs + na_alias->strs_len);
  unsigned int strs8_len = 0;
  for(j = low; j <= i; j++) {
    uint8_t *p8 = (uint8_t *)(na_alias->strs + na_alias->str_arr[j].off);
    unsigned int len8 = *p8 + 2;
    memcpy(strs8 + strs8_len, p8, len8);
    strs8_len += len8;
  }
  while(strs8_len % 4)
    strs8[strs8_len++] = 0;
  movebuf(na_alias->str_arr + low + 1, na_alias->str_arr + i + 1,
          na_alias->len - (i + 1));
  na_alias->str_arr[low].off = na_alias->strs_len;
  na_alias->str_arr[low].len = strs8_len / 4;
  na_alias->strs_len += strs8_len / 4;
  na_alias->len -= i - low;
  if(!(i = low))
    break;
}
@

<<Dump character information as C code>>=
dump_str_tabs(na_alias, gen_h, tstf);
@

A related table which is not strictly a per-character property is the
Named Sequences table (from [[NamedSequences.txt]]%
\footnote{There is another file, [[NamedSequencesProv.txt]], which
contains provisional named sequence names.  These will not be
supported.  Insted, wait for them to become official.}%
).  This associates names with character sequences, rather than just
single characters. This is read in as well, and stored in the
pseudo-property naseq.  The names are indexed by the first character
of the sequence, and both the name and the additional index characters
are stored in the string value.

The index needs to be word-aligned in order to use the simple decoder.
If the index is at the end of the entry, the last entry might be
stripped (in 8-bit mode) if its last byte is zero.  If it is at the
beginning, finding its length may be hard.  Placing it one up in order
to insert a length byte wastes two bytes, so it is placed at the
beginning, but the length is placed at the end of the string.  The
name then occupies the middle.  Multiple concatenated entries are
word-aligned by padding with a zero.  In fact, the entire table needs
to be word-aligned.  One way to do that would be to use 16-bit mode,
but I prefer being able to sort of read the generated table, so it
uses 8-bit mode and always pads the name (rather than the whole
string) to force the full length to be word-aligned.

<<Initialize UCD files>>=
decl_str(naseq);
@

<<UCD parser local definitions>>=
#define add_strseq(n, seq, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[64]; \
    uint16_t *p16 = (uint16_t *)str; \
    low = strtol(seq, &s, 16); \
    uint32_t seqlen = 0; \
    while(*s) { \
      high = strtol(s + 1, &s, 16); \
      int clen = uni_int_utf16_encode(p16, high); \
      p16 += clen; \
      seqlen += clen; \
    } \
    uint8_t *p8 = (uint8_t *)p16; \
    strcpy((char *)p8, v); \
    int len = strlen((char *)p8); \
    p8 += len; \
    if(!(len & 1)) \
      *p8++ = 0; \
    *p8++ = (len - 1) | ((seqlen - 1) << 6); \
    /* current UCD has max strlen 57 and max seqlen 4 */ \
    if(seqlen > 4 || len > 64) { \
      fprintf(stderr, "Unparsable sequence file %s (%d/%d)\n", lbuf, len, seqlen); \
      exit(1); \
    } \
    len = p8 - (uint8_t *)str; \
    while(len % 4) { \
      *p8++ = 0; \
      len++; \
    } \
    add_str_rng(&parsed_props[prop_##n], low, low, str, len / 4); \
    parsed_props[prop_##n].strs_char_size = 8; \
  } \
} while(0)
@

<<Parse UCD files>>=
open_f("NamedSequences.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  split_line(lbuf);
  if(num_fields < 2 || !isxdigit(fields[1][0]))
    continue;
  add_strseq(naseq, fields[1], fields[0]);
}
@

<<Post-process property data>>=
prop_t *naseq = &parsed_props[prop_naseq];
qsort(naseq->str_arr, naseq->len, sizeof(*naseq->str_arr), uni_cmp_cp);
for(i = naseq->len - 1; i > 0; i--) {
  while(i > 0 && naseq->str_arr[i].cp == naseq->str_arr[i - 1].cp) {
    int jlen = naseq->str_arr[i - 1].len, ilen = naseq->str_arr[i].len;
    while(naseq->max_strs < naseq->strs_len + ilen + jlen)
      resize(naseq->strs, naseq->max_strs *= 2);
    uint16_t *newseq = (uint16_t *)(naseq->strs + naseq->strs_len);
    jlen *= 2;
    cpybuf(newseq, naseq->strs + naseq->str_arr[i - 1].off, jlen);
    if(!newseq[jlen - 1])
      jlen--;
    ilen *= 2;
    cpybuf(newseq + jlen, naseq->strs + naseq->str_arr[i].off, ilen);
    if(!newseq[jlen + ilen - 1])
      ilen--;
    if((ilen + jlen) % 2)
      newseq[ilen++ + jlen] = 0;
    naseq->str_arr[i - 1].len = (ilen + jlen) / 2;
    naseq->str_arr[i - 1].off = naseq->strs_len;
    naseq->strs_len += (ilen + jlen) / 2;
    movebuf(naseq->str_arr + i, naseq->str_arr + i + 1, naseq->len - (i + 1));
    naseq->len--;
    i--;
  }
}
@

<<Dump character information as C code>>=
dump_str_tabs(naseq, gen_h, tstf);
@

The string tables are much too large for 16-bit offsets (509,515 bytes
for the na property alone).  One possible fix would be to use 8-bit
lengths and 24-bit offsets, instead.  However, since a function needs
to be used to retrieve the name anyway due to the synthesized names, a
more complex format could be used.  One way to reduce the string table
is to remove common substrings.  However, common substring removal is
rather expensive, so instead words are split out.  Each word of more
than two characters is placed into its own array, and replaced by a
two-byte pointer into that array.  The first byte of the pointer has
its upper bit set to distinguish from other characters.

While the na property is the only one which really needs this
compression, the same will be applied to the naseq, narng, and
Name\_Alias properties.  In order to only have to manage pointers into
one string array during the word building phase, all names from the
other properties are temporarily added to na's strings.

<<UCD parser local functions>>=
#define NT_NAME  0
#define NT_SEQ   1
#define NT_ALIAS 2
static void collect_words(prop_t *words, prop_t *na, uint32_t *strs,
                          unsigned int off, int na_type)
{
  unsigned int i;

  if(off)
    cpybuf(strs + off, na->strs, na->strs_len);
  for(i = 0; i < na->len; i++) {
    <<For each word [[ws]] .. [[we]]>>
      if((we - ws) > 2) {
        if(words->max_len == words->len)
          resize(words->str_arr, words->max_len *= 2);
        words->str_arr[words->len].cp = 0; /* avoid valgrind warning */
        words->str_arr[words->len].off = ws - (uint8_t *)strs;
        words->str_arr[words->len].len = we - ws;
        words->len++;
      }
    <<End for each word [[ws]] .. [[we]]>>
  }
}
@

<<For each word [[ws]] .. [[we]]>>=
uint8_t *ws, *s, *n, *ne;
s = (uint8_t *)(strs + na->str_arr[i].off + off);
ne = (uint8_t *)(strs + na->str_arr[i].off + off + na->str_arr[i].len);
while(!ne[-1])
  ne--;
n = ne;
while(n != s) {
  uint8_t *e;
  if(na_type == NT_SEQ) {
    uint8_t wlen = *--n;
    if(!n[-1])
      n--;
    e = n;
    n -= (wlen & 0x3f) + 1;
    ws = n;
    n -=  2 * ((wlen >> 6) + 1);
  } else if(na_type == NT_ALIAS) {
    for(e = s; e < ne; e += *e + 2)
      if(n == e + *e + 2)
        break;
    n = e;
    ws = e + 1;
    e += *e + 2;
  } else {
    e = n;
    ws = n = s;
  }
  while(ws < e) {
    uint8_t *we;
    for(we = ws + 1; we < e; we++)
      if(*we == ' ' || *we == '-')
        break;
@

<<End for each word [[ws]] .. [[we]]>>=
    ws = we + 1;
    while(ws < e && (*ws == '-' || *ws == ' '))
      ws++;
    ws--; /* include leading char (space/-) in word */
  }
}
@

<<Post-process property data>>=
prop_t *na = &parsed_props[prop_na], *narng = &parsed_props[prop_narng];
prop_t words;
clearbuf(&words, 1);
/* collect all words as pointers into na_strs */
while(na->max_strs < na->strs_len + naseq->strs_len + na_alias->strs_len +
                     narng->strs_len)
  resize(na->strs, na->max_strs *= 2);
inisize(words.str_arr, words.max_len = 100);
collect_words(&words, na, na->strs, 0, NT_NAME);
collect_words(&words, naseq, na->strs, na->strs_len, NT_SEQ);
collect_words(&words, na_alias, na->strs, na->strs_len + naseq->strs_len, NT_ALIAS);
collect_words(&words, narng, na->strs,
              na->strs_len + naseq->strs_len + na_alias->strs_len, NT_NAME);
/* prepare for merge */
unsigned int wordno = 0;
inisize(words.strs, words.max_strs = words.len);
sort_strs = na->strs;
qsort(words.str_arr, words.len, sizeof(*words.str_arr), cmp_words);
@

<<UCD parser local functions>>=
static int cmp_words(const void *_a, const void *_b)
{
  const raw_cp_str_t *a = _a, *b = _b;
  const char *s = (const char *)sort_strs;
  int len = a->len < b->len ? a->len : b->len;
  int c = memcmp(s + a->off, s + b->off, len);
  if(c)
    return c;
  if(len < a->len)
    return 1;
  if(len < b->len)
    return -1;
  return 0;
}
@

<<Post-process property data>>=
/* fix words array */
uint8_t *na_strs = (uint8_t *)na->strs;
for(i = 0; i < words.len; i++) {
  /* merge duplicates */
  for(j = i + 1; j < words.len; j++)
    if(words.str_arr[j].len != words.str_arr[i].len ||
       memcmp(na_strs + words.str_arr[i].off, na_strs + words.str_arr[j].off,
              words.str_arr[i].len))
      break;
  /* copy unique word into words_strs and adjust pointers */
  int wlen = (words.str_arr[i].len + 3) / 4;
  while(words.strs_len + wlen > words.max_strs)
    resize(words.strs, words.max_strs *= 2);
  words.strs[words.strs_len + wlen - 1] = 0;
  memcpy(words.strs + words.strs_len, na_strs + words.str_arr[i].off,
         words.str_arr[i].len);
  /* set offset & word # for all occurrences */
  for(; i < j; i++) {
    words.str_arr[i].cp = wordno;
    words.str_arr[i].off = words.strs_len;
  }
  wordno++;
  words.strs_len += wlen;
  i--;
}
@

<<Post-process property data>>=
/* replace words in name strings by pointers */
set_words(&words, na, NT_NAME);
set_words(&words, naseq, NT_SEQ);
set_words(&words, na_alias, NT_ALIAS);
set_words(&words, narng, NT_NAME);
@

<<UCD parser local functions>>=
static void set_words(prop_t *words, prop_t *na, int na_type)
{
  unsigned int i;

  const uint32_t *strs = na->strs;
  int off = 0;
  for(i = 0; i < na->len; i++) {
    <<For each word [[ws]] .. [[we]]>>
      if((we - ws) > 2) {
        int l = 0, h = words->len - 1, m;
        while(l <= h) {
          m = (l + h) / 2;
	  int len = words->str_arr[m].len;
	  if(len > we - ws)
	    len = we - ws;
	  int cmp = memcmp(words->strs + words->str_arr[m].off, ws, len);
	  if(!cmp)
	    cmp = (int)words->str_arr[m].len - (int)(we - ws);
	  if(cmp < 0)
	    l = m + 1;
	  else if(cmp > 0)
	    h = m - 1;
	  else
	    break;
        }
        if(l <= h) {
	  if(na_type != NT_SEQ || !((ne - we) % 2))
            memmove(ws + 2, we, ne - we);
	  else {
	    /* need to keep things word-aligned */
	    memmove(ws + 2, we, e - we);
	    if(*e) {
	      ws[2 + (e - we)] = 0;
	      memmove(ws + 2 + (e - we) + 1, e, ne - e);
	      ne++;
	    } else {
	      memmove(ws + 2 + (e - we), e + 1, ne - (e + 1));
	      ne--;
	    }
	  }
	  ws[0] = (words->str_arr[m].cp >> 8) | 0x80;
	  ws[1] = words->str_arr[m].cp & 0xff;
	  e -= we - (ws + 2);
	  ne -= we - (ws + 2);
	  /* adjust name length byte */
	  if(na_type == NT_SEQ) {
	    if(*e)
	      *e -= we - (ws + 2);
	    else
	      e[1] -= we - (ws + 2);
	  } else if(na_type == NT_ALIAS)
	    *n -= we - (ws + 2);
	  we -= we - (ws + 2);
        }
      }
    <<End for each word [[ws]] .. [[we]]>>
    int slen = ne - (uint8_t *)(na->strs + na->str_arr[i].off);
    while(slen % 4) {
      *ne++ = 0;
      slen++;
    }
    na->str_arr[i].len = slen / 4;
  }
}
@

<<Dump character information as C code>>=
/* mark duplicates */
/* adjust word lens for dump_strtab */
for(i = 0; i < words.len; i++) {
  words.str_arr[i].len = (words.str_arr[i].len + 3) / 4;
  while(i < words.len - 1 && words.str_arr[i + 1].cp == words.str_arr[i].cp)
    words.str_arr[++i].len = 0;
}
/* compress & dump strings */
words.strs_char_size = 8;
words.name = strdup("na_words");
dump_strs(&words, gen_h);
/* sort by word # */
qsort(words.str_arr, words.len, sizeof(*words.str_arr), uni_cmp_cp);
/* dump table */
open_wf(naw, "uni_na_words_arr.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_ptr_t uni_na_words_arr[] = {\n", naw);
for(i = 0; i < words.len; i++) {
  if(!words.str_arr[i].len)
    continue;
  fprintf(naw, "\t{ %5u, %5u }", words.str_arr[i].off, words.str_arr[i].len);
  while(i < words.len - 1 && !words.str_arr[i + 1].len)
    i++;
  if(i < words.len - 1)
    putc(',', naw);
  putc('\n', naw);
}
fputs("};\n", naw);
fclose(naw);
/* dump header code */
fprintf(gen_h, "extern const uni_str_ptr_t uni_na_words_arr[];\n"
               "#define uni_na_words_arr_len %d\n", wordno);
@

This reduces the name table by well over half, to 179,282 bytes.  That
is still too long, but one easy way around it is to continue to retain
the 32-bit encoding.  The number of 32-bit words is only 54,286, which
adds a lot of padding (37,862 bytes), but allows 16-bit offsets.

<<Post-process property data>>=
na->strs_char_size = 32;
@

Since the format of the name table is not very simple, and there are
algorithmically generated names, a function is provided for name
lookup.  This function should also support looking up sequence names,
so the code point is passed in as a buffer pointer and length.  The
return format is similar to the UTF-8 static/dynamic return buffer
mechanism used by other string functions, plus a parameter to indicate
if passing more code points might result in a different name.  Since
the name table is rather large, this function is placed in a separate
object file.

<<Library [[uni]] Members>>=
cp_to_name.o
@

<<cp_to_name.c>>=
<<Common C Header>>
#include "uni_prop.h"

@

<<Unicode property exports>>=
/* |*seq_len| is # of chars matched; negative means possible longer match */
/* *alias is alias #: */
/*   if *alias is 0, returned *alias is max alias index */
/*   if *alias is valid non-0, returned *alias is *alias - 1 */
/*   if *alias is invalid non-0, returned *alias is -1 */
int uni_cp_to_name(const uint32_t *cp, unsigned int len, int *seq_len,
                   int *alias, <<Buffer return parameters for UTF-[[8]]>>);
@

<<cp_to_name.c>>=
<<[[uni_cp_to_name]] support>>
int uni_cp_to_name(const uint32_t *cp, unsigned int len, int *seq_len,
                   int *alias, <<Buffer return parameters for UTF-[[8]]>>)
{
  if(!len) {
    if(seq_len)
      *seq_len = 0;
    if(alias && *alias)
      *alias = -1;
    return 0;
  }
  if(alias && *alias < 0) {
    if(seq_len)
      *seq_len = 0;
    return 0;
  }
  <<Return name for [[*cp]]>>
}
@

The first thing to do is look up the code point in the [[naseq]] table.
If present, the longest possible match is taken, and if there is a
longer match beyond the passed-in length, [[*seq_len]] is negated as
well.  Since there may not be an exact match at all, this code may
just set [[*seq_len]] to $-1$ and fall through to the single-character
case.  Even if an exact match is found, every possible sequence is
checked, since there might be a longer sequence that matches.  Note
that there are no sequence aliases, so any attempt to get an alias is
denied.

<<Return name for [[*cp]]>>=
if(seq_len)
  *seq_len = 1;
const uni_str_ptr_t *lu = len > 1 || seq_len ? uni_naseq_of(*cp) : NULL;
if(lu && lu->len && (len == 1 || (alias && *alias))) {
  if(seq_len)
    *seq_len = -1;
} else if(lu && lu->len) {
  const uint8_t *found_na = NULL;
  unsigned int found_nalen = 0, found_seqlen = 0, seqlen, nalen;
  const uint8_t *ep = uni_naseq_strs + lu->off, *p = ep + lu->len;
  const uint16_t *seqptr;
  const uint32_t *cp2;
  while(p > ep) {
    nalen = *--p;
    seqlen = (nalen >> 6) + 1;
    nalen = (nalen & 0x3f) + 1;
    if(!p[-1])
      p--;
    p -= nalen + seqlen * 2;
    seqptr = (const uint16_t *)p;
    int sp = 0;
    for(cp2 = cp + 1; cp2 - cp < len; cp2++) {
      unsigned int clen;
      if(*cp2 != uni_int_utf16_decode(seqptr + sp, &clen))
        break;
      sp += clen;
      if(sp == seqlen) {
        if(seqlen > found_seqlen) {
          found_na = p + seqlen * 2;
	  found_nalen = nalen;
	  found_seqlen = seqlen;
	}
	break;
      }
    }
    /* if at end of string, mark as possibly longer */
    if(seq_len && sp < seqlen && cp2 - cp == len)
      *seq_len = -1;
  }
  if(found_na) {
    if(seq_len)
      *seq_len *= found_seqlen + 1; /* 1 or -1 */
    return expand_words(found_na, found_nalen, buf, off, buf_len);
  }
}
@

<<[[uni_cp_to_name]] support>>=
static int expand_words(const uint8_t *na_raw, unsigned int na_raw_len,
                        <<Buffer return parameters for UTF-[[8]]>>)
{
  unsigned int i;
  unsigned int na_len = na_raw_len;

  for(i = 0; i < na_raw_len; i++)
    if(na_raw[i] & 0x80) {
      unsigned int wno = (((unsigned int)na_raw[i] & 0x7f) << 8);
      if(i < na_raw_len - 1)
	wno += na_raw[++i];
      else
        na_len++;
      na_len += uni_na_words_arr[wno].len - 2;
    }
  /* shortcut for just computing length */
  if(off < 0 && (!buf_len || !*buf_len))
    return na_len;
  /* otherwise build name on stack & return the usual way */
  uint8_t na[na_len], *np = na;
  for(i = 0; i < na_raw_len; i++) {
    if(na_raw[i] & 0x80) {
      unsigned int wno = (((unsigned int)na_raw[i] & 0x7f) << 8);
      if(i < na_raw_len - 1)
	wno += na_raw[++i];
      memcpy(np, uni_na_words_strs + uni_na_words_arr[wno].off,
             uni_na_words_arr[wno].len);
      np += uni_na_words_arr[wno].len;
    } else
      *np++ = na_raw[i];
  }
  return uni_return8_buf8(na, na_len, buf, off, buf_len);
}
@

For the single code point case, the easiest thing to look up first is
the alias, if an alias index is provided.  The return value is either
that alias or an empty string to indicate an invalid alias index.  If
no alias was requested, the total alias count is returned in [[*alias]].

<<Return name for [[*cp]]>>=
if(alias) {
  int alias_no = *alias;
  const uni_str_ptr_t *ap = uni_Name_Alias_of(*cp);
  int alen = ap->len;
  if(!alen && alias_no) {
    if(seq_len)
      *seq_len = 0;
    *alias = -1;
    return 0;
  }
  if(alen) {
    const uint8_t *np = uni_Name_Alias_strs + ap->off;
    while(alen) {
      unsigned int nlen = *np + 1;
      if(nlen > alen - 1) /* in case last word# was cut off */
        nlen = alen - 1;
      np++;
      if(!--alias_no) {
        --*alias;
	if(seq_len)
	  *seq_len = 1;
	return expand_words(np, nlen, buf, off, buf_len);
      }
      alen -= nlen + 1;
      np += nlen;
    }
    if(alias_no > 0) {
      if(seq_len)
        *seq_len = 0;
      *alias = -1;
      return 0;
    }
    *alias = -alias_no;
  }
}
@

For the single code point non-alias case, the first step is once again
lookup in the name table.  A successful match can sort of be returned
immediately, but the word pointers need to be replaced with actual
words first.

<<Return name for [[*cp]]>>=
lu = uni_na_of(*cp);
if(lu->len) {
  const uint8_t *na_raw = (const uint8_t *)(uni_na_strs + lu->off);
  unsigned int na_raw_len = lu->len * 4;
  while(!(na_raw[na_raw_len - 1]))
    na_raw_len--;
  return expand_words(na_raw, na_raw_len, buf, off, buf_len);
}
@

Failure to look up a name may be because it is an invalid code point.
In that case, return an empty string.

<<Return name for [[*cp]]>>=
if(*cp > UNI_MAX_CP)
  return 0;
@

Otherwise, it must be a generated name.  All generated names consist
of the narng string for the code point's range followed by a generated
suffix.

<<Return name for [[*cp]]>>=
int l = 0, h = uni_narng_arr_len / 2 - 1, m;
while(l <= h) {
  m = (l + h) / 2;
  if(uni_narng_arr[m * 2].cp > *cp)
    h = m - 1;
  else if(uni_narng_arr[m * 2 + 1].cp < *cp)
    l = m + 1;
  else
    break;
}
if(l > h) /* code points not in UnicodeData.txt at all */
  return 0;
m *= 2;
unsigned int nalen = expand_words(uni_narng_strs + uni_narng_arr[m].off,
                                  uni_narng_arr[m].len, buf, off, buf_len);
unsigned int suflen;
<<Compute [[suflen]] for [[*cp]]'s suffix>>
/* shortcut for just computing length or if suffix won't fit */
if(off < 0 && (!buf_len || !*buf_len || *buf_len <= nalen))
  return nalen + suflen;
/* otherwise, build suffix from stack */
/* name was already built into output buffer by expand_words() */
char suffix[suflen + 1];
<<Build [[suffix]] from [[*cp]]>>
if(off < 0) {
  unsigned int blen = *buf_len - nalen;
  uint8_t *bptr = *buf + nalen;
  uni_return8_buf8((uint8_t *)suffix, suflen, &bptr, off, &blen);
} else if(off >= 0)
  uni_return8_buf8((uint8_t *)suffix, suflen, buf, off + nalen, buf_len);
return nalen + suflen;
@

The suffix for most ranges is just a dash, followed by the code point
in hex.  For Hangul syllables, though, the suffix is a space, followed
by the broken down compoents' names.

<<Compute [[suflen]] for [[*cp]]'s suffix>>=
<<Prepare to compute [[suflen]] for Hangul syllable>>
if(uni_narng_arr[m].cp == 0xAC00) {
  <<Compute [[suflen]] for Hangul syllable>>
} else
  suflen = 1 /* - */ + (lg2(*cp) + 3) / 4 /* XXXX */;
@

<<Build [[suffix]] from [[*cp]]>>=
if(uni_narng_arr[m].cp == 0xAC00) {
  <<Compute [[suffix]] for Hangul syllable>>
} else
  sprintf(suffix, "-%X", (int)*cp);
@

For Hangul syllables, the Jamo\_Short\_Name property contains the text
to append to the name.  First the syllable is broken down into L, V,
and T, and then the string corresponding to each is looked up.

<<Prepare to compute [[suflen]] for Hangul syllable>>=
const uni_str_ptr_t *ls = NULL, *vs = NULL, *ts = NULL; /* shut gcc up */
@

<<Compute [[suflen]] for Hangul syllable>>=
unsigned int jt = (*cp - 0xAC00) % 28;
unsigned int jv = (*cp - 0xAC00) / 28;
unsigned int jl = jv / 21;
jv %= 21;
ls = uni_JSN_of(0x1100 + jl);
vs = uni_JSN_of(0x1161 + jv);
ts = jt ? uni_JSN_of(0x11A7 + jt) : NULL;
suflen = 1 + ls->len + vs->len + (ts ? ts->len : 0);
@

The suffix itself is just a space, followed by the L, V, and T short
names.

<<Compute [[suffix]] for Hangul syllable>>=
suffix[0] = ' ';
memcpy(suffix + 1, uni_JSN_strs + ls->off, ls->len);
memcpy(suffix + 1 + ls->len, uni_JSN_strs + vs->off, vs->len);
if(ts)
  memcpy(suffix + 1 + ls->len + vs->len, uni_JSN_strs + ts->off, ts->len);
@

To test this, a simple program just generates every single name.  No
verification is done; this is meant to be verified manually.

<<C Test Support Executables>>=
tstcp_na \
@

<<tstcp_na.c>>=
<<Common C Header>>

#include "uni_prop.h"

int main(void)
{
  uint32_t cp;
  uint8_t *buf = NULL;
  unsigned int buf_len = 0, clen;
  int has_more, aliases;
  for(cp = 0; cp <= UNI_MAX_CP + 1; cp++) {
    clen = uni_cp_to_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
    while(!clen && aliases > 0)
      clen = uni_cp_to_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
    if(!clen)
      continue;
    printf("%04X %d %.*s\n", cp, has_more, clen, buf);
    while(aliases > 0) {
      clen = uni_cp_to_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
      printf("%04X %d &%.*s\n", cp, has_more, clen, buf);
    }
  }
  /* all sequences */
  int i;
  uint32_t seq[10];
  for(i = 0; i < uni_naseq_arr_len; i++) {
    const uint8_t *ep = uni_naseq_strs + uni_naseq_arr[i].off,
                  *p = ep + uni_naseq_arr[i].len;
    unsigned int seqlen, nalen;
    while(p > ep) {
      uint32_t *sp = seq;
      *sp = uni_naseq_arr[i].cp;
      printf("%04X", (int)*sp++);
      nalen = *--p;
      if(!p[-1])
        p--;
      seqlen = (nalen >> 6) + 1;
      nalen = (nalen & 0x3f) + 1;
      p -= nalen + seqlen * 2;
      const uint16_t *seqp = (const uint16_t *)p;
      while(seqlen > 0) {
        *sp = uni_int_utf16_decode(seqp, &clen);
	printf(":%04X", (int)*sp++);
	seqp += clen;
	seqlen -= clen;
      }
      clen = uni_cp_to_name(seq, (int)(sp - seq), &has_more, NULL, &buf, 0, &buf_len);
      printf(" %d %.*s\n", has_more, clen, buf);
    }
  }
  return 0;
}
@

The name tables are not really that useful as is:  few programs
display a Unicode code point's name.  However, there are many file
parsers which allow Unicode code points to be specified by name.  Thus
a reverse lookup table is required.  There is no perfect reverse
lookup method; it depends on the application.  It is expected that a
program using the name tables will dump a custom version of both
forward and reverse lookups as application-specific C code.

The obvious thing to provide is an explicit name lookup function, using
either a range list sorted by name instead of code point, or a hash
table, or something similar.  Matching loosely needs to be supported as
well, by stripping spaces and dashes for the lookup.

For the hash table, either some random hash function could be
provided, or a perfect hash function could be generated using
[[gperf]].%
\footnote{After I wrote this, a program called [[cmph]]
(\url{http://sourceforge.net/projects/cmph}) caught my attention. This
generates minimal perfect hash functions in linear time (sub second
time for the entire Unicode name set!).  Its main problems are that it
generates data files rather than code (requiring changes to the
library to support code generation instead; see
\url{http://sourceforge.net/tracker/?func=detail\&aid=3590339\&group_id=126608\&atid=706189}),
and that it does not perform the final comparison step, essentially
just returning the integer index of a possible hashed string (easily
correctable).}
My initial implementation used [[gperf]], but generating a perfect
hash for the full Unicode set took two hours on my machine, and an
additional hour to compile.

The first thing to do is generate a name table that is preprocessed
for lookup.  The lookup algorithm is always loose.  UAX44-LM2 states
that lookups should ignore case, whitespace, underscore, and medial
hyphens except in U+1180.  There are 15 names with non-medial hyphens,
but only two are actually ambiguous.  UTS18 (Regular Expressions),
section 2.5.1 ("Individually Named Characters") acknowledges this by
requiring hyphens only in U+1180, U+0F60, and U+0FB0.  While the
algorithm I use violates the original loose matching algorithm, I will
go ahead and only restrict those three.  The code could actually just
derive this list itself by looking for duplicates after removing all
hyphens and spaces.

Note that although the tables only need to strip spaces and hyphens,
the actual matching routine must strip any whitespace, hyphens, and
underscores from user input.  For the ambiguous entries, the locations
of hyphens is retained, and the ambiguity is resolved by checking for
one at the appropriate spot.

The lookup table contains all possible names:  the na property, the
Name\_Alias property, and the naseq property.  The narng property is
reencoded separately.  The returned value should be the code point,
but sequences require special handling.  In this case, each sequence
is assigned a number, and that number is added to [[UNI_CP_MAX]] to
obtain the code point.  A separate table maps this to an actual
sequence.

<<FIXME>>=
implement reverse lookup
  - generate sorted array of preprocessed names
  - app should really generate hash tables itself; the lib should just
    use bsearch (or should I support my patched cmph?)
@

---

Looking up Unicode names is much more difficult.  On a lookup failure,
we need to downcase the entire string and remove dashes and
underscores.  This requires allocating a buffer.  For now, this is
done on the stack.  To avoid stack overflow, no character names larger
than 128 characters are accepted (the actual maximum is around 83).

If the transformed name still can't be looked up, it needs to be
looked up as a range.

---

Then, that last word needs to be validated.  For all but the Hangul
syllables, the last word is a hexadecimal code point of at least four
characters with no superfluous leading zeroes.  There are no
multi-character sequences matching these names.  The table of ranges
returns the low end of the range, which is what we looked up, so if
they match, we're good to go.

---

Hangul is more difficult, requiring character-by-character parsing.
It might be possible to come up with a regular expression to do the
match, but that would not help with returning the correct code point.
Instead, lookup tables are used for all three parts.  The tables are
generated by splitting the JSN proprety by jt property, and providing
an easy-to-use lookup table.

\lstset{language=make}
<<makefile.vars>>=
UNIJAMO = $(UCD_LOC)/Jamo.txt
@

<<Character name data generation dependencies>>=
$(UNIJAMO) \
@

<<Generate character name data>>=
for y in CHO JUNG JONG; do \
  echo "static const struct abent $${y}_dec[] = {"; \
  fgrep $${y}SEONG $(UNIJAMO) | cut -d\# -f1 | fgrep \; | \
    dd conv=lcase 2>/dev/null | cut -d\; -f2 | cat -n | while read a b; do \
      echo " {\"$$b\", $$((a-1))},"; \
    done | sort | sed '$$s/,$$//'; \
  echo "};"; \
done >>$@
@

\lstset{language=C}
<<Character name generated structures>>=
struct abent {
    const char *name;
    uint32_t cp;
};
@

<<Character name lookup local definitions>>=
#define NUM_CHO (sizeof(CHO_dec)/sizeof(CHO_dec[0]))
#define NUM_JUNG (sizeof(JUNG_dec)/sizeof(JUNG_dec[0]))
#define NUM_JONG (sizeof(JONG_dec)/sizeof(JONG_dec[0]))
@

The L portion is always either one character or one doubled character
or blank.  The blank can be detected by the fact that V shares no
characters with L.  In order to search the table quickly, the first
character, and, if doubled, the second cahracter are joined into a
string and searched using plain binary search.

<<Look up code point of Unicode range element [[buf]]>>=
/* L is 0 char or 1 char or 1 doubled char */
if(*s == s[1]) {
  c = s[2];
  s[2] = 0;
} else {
  c = s[1];
  s[1] = 0;
}
/* look up in table */
/* could use bsearch() but seems silly */
int l, h, m, cmp = 0;
l = 0; h = NUM_CHO - 1;

while(l <= h) {
  m = (l + h) / 2;
  cmp = strcmp(s, CHO_dec[m].name);
  if(cmp < 0)
    h = m - 1;
  else if(cmp > 0)
    l = m + 1;
  else
    break;
}
/* if not found, assume L is blank */
if(cmp)
  m = 0;
int L = CHO_dec[m].cp;
s += strlen(s);
*s = c;
@

Binary search fails too easily due to possible T presence, and the V
table is short with short strings, so the search for V is linear.

<<Look up code point of Unicode range element [[buf]]>>=
/* find first match */
int ll = 0;
for(m = 0; m < NUM_JUNG; m++) {
  ll = strlen(JUNG_dec[m].name);
  if(!strncmp(s, JUNG_dec[m].name, ll))
    break;
}
/* there is no blank V, so not found == error */
if(m == NUM_JUNG)
  return -1;
/* find longest match */
int m2;
for(m2 = m + 1; m2 < NUM_JUNG; m2++) {
  int ll2 = strlen(JUNG_dec[m2].name);
  if(ll2 <= ll || memcmp(JUNG_dec[m2].name, JUNG_dec[m].name, ll))
    break;
  if(!strncmp(s, JUNG_dec[m2].name, ll2)) {
    ll = ll2;
    m = m2;
  }
}
int V = JUNG_dec[m].cp;
s += strlen(JUNG_dec[m].name);
@

Finally, the remaining characters are a full string again, so binary
search is the easiest.

<<Look up code point of Unicode range element [[buf]]>>=
/* last part can be binary searched again, if present */
int T = 0;
if(*s) {
  l = 0; h = NUM_JONG - 1;
  while(l <= h) {
    m = (l + h) / 2;
    cmp = strcmp(s, JONG_dec[m].name);
    if(cmp < 0)
      h = m - 1;
    else if(cmp > 0)
      l = m + 1;
    else
      break;
  }
  /* unrecognized suffix is error */
  if(cmp)
    return -1;
  T = JONG_dec[m].cp + 1;
}
*arr = NULL;
return 0xAC00 + T + (NUM_JONG + 1) * (V + NUM_JUNG * L);
@

---

The other transformation which was done for easy comparison is to
strip the dashes out.  To fix this, a table of dash locations is used.
This table stores the offsets into the name of up to three dashes; no
name was found with more than three.  The offset actually starts at one
so that the array of three can be zero-terminated.  This actually
requires a fourth byte as well.

% FIXME: use signed offset instead; sign == negative for dash, pos for
% space; also, remove cp and store offset into this table with name
% string, and expand d to 12 chars, and end loop at 12

<<Character name generated structures>>=
struct dashloc {
    int cp;
    unsigned char d[4];
};
@

<<Post-process regular Unicode character name>>=
const struct dashloc dl = {cp}, *dash;

/* FIXME: change to manual bsearch */
dash = bsearch(&dl, dashloc, sizeof(dashloc)/sizeof(*dashloc), sizeof(*dashloc),
               cmpdl);
if(dash) {
  const unsigned char *d = dash->d;
  while(*d && *d <= len)
    buf[*d++ - 1] = '-';
}
@

<<Character name lookup local definitions>>=
static int cmpdl(const void *a, const void *b)
{
    return ((struct dashloc *)a)->cp - ((struct dashloc *)b)->cp;
}
@

To generate the array, some shell code is used to find dashes and
count characters.

---

Once again, the ranges add more effort.  At least if it is not one of
the range code points, we can return immediately.  Otherwise, we may
as well copy out as much of the name as we already have.  There are no
dashes in any of these names, except just before the hexadecimal code
point on the ones which have this.

---

Speaking of which, those now require appending the hexadecimal code
point, and a dash.  Precomputing the length is pretty easy, since
there are either 4, 5, or 6 digits depending on the code point range.
That means we can finally return a value if no output buffer was
given.  Otherwise, we write the suffix, followed by a 0, and return
the computed length.

---

\lstset{language=txt}
<<FIXME>>=
Document lack of NamesList.txt (informative, harder to parse, mostly useless)
@

\subsection{Parsing the UCD -- DUCET}

The most complex optional string value is derived from the Default
Unicode Collation Element Table
(\url{http://unicode.org/Public/UCA/}).  It associates many strings
(collation elements, which are the index to the table) with explicit
sort keys (the data).  This includes several multi-character collation
elements (called contractions) and multi-entry keys (called
expansions).  Each sort key entry consists of a number for each
supported sort level, plus a flag.  Currently, the DUCET supports four
sort levels, although the standard states that more may be added in a
future version.  The flag indicates so-called variable entries.  The
flag does not actually need to be stored, since it is always true for
entries with a non-zero first level up to a certain top.  This top is
stored separately as a single integer.

Rather than make this reader as generic as possible, given the
definitions, it is tailored to the actual data present in the UCA
versions available at the time of this writing.  That means that
recoding may be necessary at a future date.  The first constraint is
that exactly four levels are supported.

While reading, each entry is indexed on the first character of the
collation element.  The raw string value is a four-word sequence for
every sort key entry, followed by the remainder of the collation
element index.  Since no multi-character index has more than four
characters, there is no problem distinguishing the character groups in
the value: the number of key entries is the length divided by four,
and the number of extra characters in the index is the remainder.
Since several multi-character collation elements start with the same
character (and in fact the well-formedness rules require this),
multiple entries with the same [[cp]] are created, but the raw string
table storage method does not care.

In addition to the DUCET itself, the data files include the same data
adjusted for use with the CLDR (\texttt{allkeys\_CLDR.txt} in
\texttt{CollationAuxiliary.zip}).  This file is in the same format. 
It should be unzipped under the same directory.  All CLDR data is based
off of this root locale, and it can be explicitly requested using the
locale extension \texttt{u-co-standard}.  The CLDR still requires the
normal DUCET; it can be selected manually using the locale extension
\texttt{u-co-ducet}.

\lstset{language=C}
<<Initialize UCD files>>=
decl_str(DUCET);
decl_str(DUCET_CLDR);
@

<<Parse UCD files>>=
<<Prepare for reading DUCET>>
static uint32_t ducet_vartop, cldr_vartop;
ducet_vartop = parse_ducet("allkeys.txt", (prop_DUCET = add_prop("DUCET")));
cldr_vartop = parse_ducet("CollationAuxiliary/allkeys_CLDR.txt",
                          (prop_DUCET_CLDR = add_prop("DUCET_CLDR")));
parsed_props[prop_DUCET].strs_char_size = 32;
parsed_props[prop_DUCET_CLDR].strs_char_size = 32;
@

<<UCD parser local functions>>=
<<DUCET parser globals>>
static uint32_t parse_ducet(const char *fname, int propn)
{
  <<Parser common variables>>
  prop_t *prop = &parsed_props[propn];
  uint32_t vartop = 0;
  open_f(fname);
  while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    uint32_t str[20*4]; /* current max: 18 * 4 */
    uint32_t len = 0;
    int is_var;

    split_line(lbuf);
    if(!num_fields || !isxdigit(*lbuf))
      continue;
    low = high = strtol(lbuf, &s, 16);
    for(s = fields[1]; s; s = strchr(s, '[')) {
      is_var = s[1] == '*';
      str[len++] = strtol(s + 2, &s, 16);
      if(is_var && str[len - 1] > vartop)
        vartop = str[len - 1];
      str[len++] = strtol(s + 1, &s, 16);
      str[len++] = strtol(s + 1, &s, 16);
      str[len++] = strtol(s + 1, &s, 16);
    }
    /* this assumes that fields[0] will never have more than 4 chars */
    /* that way, len % 4 == extra chars in fields[0]; len / 4 = # of keys */
    if((s = strchr(fields[0], ' '))) {
      do {
        str[len++] = strtol(s, &s, 16);
      } while(*s);
    }
    /* 6.2: 120007 words (6212 saved) */
    /* 6.2-CLDR: 120493 words (5724 saved) */
    add_str_rng(prop, low, high, str, len);
  }
  fclose(f);
  if(lbuf)
    free(lbuf);
  <<Post-process DUCET>>
  return vartop;
}
@

The raw data in the Unicode 6.2 DUCET comes to 126,219 words, of which
6,212 can be removed due to redundancy.  This is too much data for the
16-bit offset used by [[uni_str_arr_t]] type, so some adjustment needs
to be made.

A little space can be saved while storing this by making some strings
identical when they would normally not be, thereby increasing the
removals due to redundancy. Level 4 is usually equal to the code
point, and is never 1, so that case can be stored as 1 to indicate
that it is equal to the code point.  This saves an additional 6,972
words.

<<Post-process DUCET>>=
<<Prepare for DUCET post-processing>>
for(i = 0; i < prop->len; i++) {
  uint32_t len = prop->str_arr[i].len;
  uint32_t *str = prop->strs + prop->str_arr[i].off;
  uint32_t cp = prop->str_arr[i].cp;
  <<Reduce DUCET entry>>
}
@

<<Reduce DUCET entry>>=
for(j = 3; j < len; j += 4)
  if(str[j] == cp)
    str[j] = 1;
/* 6.2: 113035 words (13184 saved) */
/* 6.2-CLDR: 112169 words (14048 saved) */
@

One common expansion is for the sort key to be the concatenation of
the sort keys for each character in the canonical decomposition of the
collation element.  To save more space, this sort key string is
indicated by an empty sort key.  For sanity, the entire key is
checked, even though the current DUCET has no entries which fail the
complete check when just the last element matches the decomposition
characters.  This brings the total down to 117,631 words, with 11,800
removed due to redundancy.  In other words, it saves an additional
7,204 words.

This requires decomposition lookup during DUCET lookup.  However, the
UCA algorithm specifies NFD form for its input strings; in that case,
the decomposition will already be done, and no extra work is needed in
the lookup function.  However, a special case (FCD; see below) allows
for unnormalized data inputs, where this lookup would be necessary.

Note that the standard specifies a much more liberal decomposition
policy.  It uses compatibility decomposition instead of canonical
decomposition, and replaces level 3 with a translation of the dt
property value.  The amount of additional work at run-time isn't that
great, and it might reduce the table a lot, but it affects many
characterrs even after NFD decomposition, so for now, entries which
use this technique are not suppressed.

% tertiary weight table (note: can is impossible):
%  none     def 02
%  wide     def 03
%  compat   def 04
%  font     def 05
%  circle   def 06
%  can/none uc  08
%  wide     uc  09
%  compat   uc  0A
%  font     uc  0B
%  circle   uc  0C
%  small    sh  0D
%  none     nh  0E
%  small    sk  0F
%  narrow   snk 10
%  none     nk  11
%  narrow   nkh 12
%  circle   ck  13
%  super    def 14
%  sub      def 15
%  vertical def 16
%  initial  def 17
%  medial   def 18
%  final    def 19
%  isolated def 1A
%  noBreak  def 1B
%  square   def 1C
%  square/super/sub uc 1D
%  fraction def 1E
%  compat   --  1F
%
% uc == Uppercase, sh == small hira, nh == non-small hira,
% sk == small kata, nk == non-small kata, snk == small narrow kata,
% nkh == narrow katakana/hangul, ck == circled kata
% -- == def for last char in decomp; also used for disambiguation

% l4 weight:
%   l3-ignorable and Cc, Cf, or "variation selector":  0
%   otherwise:  cp (but what about contractions & expansions?)
% however, l4=0 weight rule is violated on 12 entries:
%  0600 0601 0602 0603 0604 0605 06DD 2061 2062 2063 2064 110BD
%   all of these are Cc or Cf, but have l4 == cp

<<Prepare for reading DUCET>>=
/* sort dmf for decomp lookup */
qsort(dmf_prop->str_arr, dmf_prop->len, sizeof(*dmf_prop->str_arr), cmp_cp_flg);
@

<<Prepare for DUCET post-processing>>=
prop_t *dmf_prop = &parsed_props[add_prop("dm_full")];
/* sort ducet for component lookup */
qsort(prop->str_arr, prop->len, sizeof(*prop->str_arr), uni_cmp_cp);
@

<<Reduce DUCET entry>>=
/* if it's decomposed first, then store that case as 0-len string */
if(str[3] && str[3] != 1 && !(len % 4) && len > 4) {
  raw_cp_str_t *dec = bsearch(&cp, dmf_prop->str_arr,
                              dmf_prop->len,
			      sizeof(raw_cp_str_t), uni_cmp_cp);
  if(dec) {
    while(dec->flags && dec > dmf_prop->str_arr && dec->cp == cp)
      dec--; /* select canonical full decomp only */
    if(dec->flags || dec->cp != cp)
      dec = NULL;
  }
  if(dec && dec->len * 4 == len) {
    for(j = 0; j < dec->len; j++) {
      raw_cp_str_t *cd;
      uint32_t ocp = str[3 + 4 * j];
      if(ocp != dmf_prop->strs[dec->off + j])
        break;
      cd = bsearch(&ocp, prop->str_arr,
	           prop->len, sizeof(raw_cp_str_t),
		   uni_cmp_cp);
      if(!cd)
        break;
      /* scan for single-element entry */
      /* some may have ocp as 1st char of multi-char entry */
      while(cd->len != 4 && cd > prop->str_arr && cd[-1].cp == ocp)
        cd--;
      while(cd->len != 4 && cd < prop->str_arr + prop->len - 1 &&
            cd[1].cp == ocp)
	cd++;
      if(cd->len != 4 ||
	 (prop->strs[cd->off + 3] != 1 && prop->strs[cd->off + 3] != ocp) ||
	 memcmp(prop->strs + cd->off, str + 4 * j, 12))
	break;
    }
    if(j == dec->len)
      prop->str_arr[i].len = 0;
  }
}
/* 6.2: 105831 words (11800 saved) */
/* 6.2-CLDR: 104937 words (12688 saved) */
@

Similarly, some multi-character collation elements are the canonical
decomposition of a single character, and the sort key is the sort key
for that character.  That case could also be encoded by a zero-length
key, but retaining the extra collation element characters as its
string.  However, not much is saved (about 415 words, although
surprisingly many more for the CLDR table), so this extra lookup
complication is not imposed.  Unlike the decomposition entries above,
the composition entries are legitimate multi-character collation
elements, even after NFD transformation.

While there is a pattern to most of the rest of the DUCET, it is
difficult to encode, and encoding would make lookup much more
expensive than it probably already is.  The only other features of the
numbers which can be easily taken advantage of are the ranges.  The
first three levels are never more than four digits, or 16 bits.  In
fact, they are quite a bit smaller.  Rather than relying on a single
revision's ranges, the ranges are extracted in the first pass.  This
is mainly to provide a sanity check on the only characteristic that is
likely to change rapidly.

<<Prepare for DUCET post-processing>>=
uint32_t max_0 = 0, max_1 = 0, max_2 = 0;
@

<<Reduce DUCET entry>>=
for(j = 0; j + 3 < len; j += 4) {
  if(str[j] > max_0)
    max_0 = str[j];
  if(str[j + 1] > max_1)
    max_1 = str[j + 1];
  if(str[j + 2] > max_2)
    max_2 = str[j + 2];
}
@

In practice, the first level can be stored in 16 bits, the second in 9
bits, and the third in 5 bits.  In other words, all three levels fit
easily in a single 32-bit word.  While the second and third could be
stored in more bits to allow for more wiggle room, storing minimally
leaves two extra bits for other purposes.  Note that in the current
DUCET, the first level can actually be restricted to 15 bits in case
an additional bit is ever needed.  The default value algorithm
requires the full 16 bits, but these are never stored in the table.
No allowance is made for larger required field sizes; recoding would
be needed in several places.

<<Post-process DUCET>>=
if(lg2(max_0) > 16 || lg2(max_1) > 9 || lg2(max_2) > 5) {
  fputs("FIXME: Can't reduce DUCET\n", stderr);
  exit(1);
}
@

The extra two bits can be used to encode certain common cases,
possibly eliminating the need to store a full 32-bit word for the
fourth level:

\begin{itemize}
\item 0: Level 4 is 0
\item 1: Level 4 is the index cp (encoded above as 1)
\item 2: The next word is level 4
\end{itemize}

When combining like this, each entry drops in length to 2 or fewer
words.  This means that if there is more than two characters in the
index string, it can no longer be detected by length alone.  Since
every character can be encoded in fewer than 30 bits, the extra index
characters are simply shifted left two, leaving the lower two bits for
special encoding as above:

\begin{itemize}
\item 3: This word is actually part of the collation element (index)
\end{itemize}

This also means that locating the extra index characters is more
difficult:  the entire string would need to be scanned rather than
just computing the length of the key and skipping it.  To prevent the
need for scanning, the extra index characters are moved to the front
of the string as well.

For consistency, the explicitly stored level 4 values are also shifted
left by two.  This way, the only characters in the string with the
lower two bits set are the index characters.

After collapsing the keys like this, over two thirds of the required
space is removed.  The result is 33,723 words, with 3,308 words
removed due to redundancy.  This saves an additional 75,416 words.

<<Post-process DUCET>>=
for(i = 0; i < prop->len; i++) {
  int k;
  uint32_t len = prop->str_arr[i].len;
  uint32_t extra[3], extra_len = len % 4;
  uint32_t *str = prop->strs + prop->str_arr[i].off;
  for(j = 0; j < extra_len; j++)
    extra[j] = str[len - extra_len + j];
  for(j = 0, k = extra_len; j + 3 < len; j += 4, k++) {
    str[k] = (str[j] << 16) + (str[j + 1] << 7) + (str[j + 2] << 2);
    if(str[j + 3] == 1)
      str[k] |= prop->str_arr[i].cp != 0; /* special case: 0 is 0 */
    else if(str[j + 3]) {
      str[k] |= 2;
      str[++k] = str[j + 3] << 2;
    }
  }
  prop->str_arr[i].len = k;
  for(k = 0; j < len; j++, k++)
    str[k] = (extra[k] << 2) | 3;
}
/* 6.2: 30639 words (3252 saved) */
/* 6.2-CLDR: 20489 words (3157 saved) */
@

The final problem to deal with is the possibility of multiple sort
keys with the same starting index character.  Because of this, the
string table cannot be converted to a multi-level table.  After the
last encoding, though, it is easy to simply concatenate all entries
into a single string.  Searching for entries after the first is not
too efficient, but at least it only needs to be done for relatively
few first index characters.  The encoding allows limited binary
searching, as well: the nearest index extension can be located by
checking the lower two bits for 3.

The raw DUCET data is actually of limited use.  Only one optional
poorly defined pseudo-property depends on it (``DUCET primary values''
--- even the property name is undefined).  The Unicode Collation
Algorithm is defined in terms of creating string sort keys from the
raw data as well, but in order to support modifications due to locale,
even that use is probably better served by a different representation.
Given the added complexity of run-time reordering of the DUCET, a
simpler, ``raw'' DUCET UCA implementation is provided to demonstrate
the use of the property.  However, merging the keys may make it
difficult to reorganize the table.  For this reason, the original data
is copied first.  Only the entry pointers are copied:  the strings
cannot be merged in-place anyway (since they might overlap), so
merging always appends new strings and leaves the old ones alone.

<<Post-process DUCET>>=
raw_cp_str_t *raw_ents;
uint32_t raw_ents_size;
inisize(raw_ents, raw_ents_size = prop->len);
cpybuf(raw_ents, prop->str_arr, raw_ents_size);
@

The only potential conflicts when merging are the zero-length canonical
decomposition entries.  The entry with no index extension is always
first, so detecting zero-length canonical decomposition entries is
easy: if the first character is an index extension or the end of the
string, then the key is blank.  A sanity check is made to ensure that
there is always a key for the zero-length extension case.

<<Post-process DUCET>>=
ducet_strs = prop->strs; /* for sort & abbrev */
for(i = prop->len - 1; i > 0; i--) {
  uint32_t cp = prop->str_arr[i].cp;
  for(j = i; j > 0 && prop->str_arr[j - 1].cp == cp; j--);
  if(i == j)
    continue;
  /* sort by index extension */
  low = j;
  qsort(prop->str_arr + low, i - low + 1, sizeof(*prop->str_arr), cmp_ducet_idx);
  if(prop->str_arr[low].len && (ducet_strs[prop->str_arr[low].off] & 3) == 3) {
    fprintf(stderr, "Error: no DUCET entry for %04X\n", (int)cp);
    exit(1);
  }
  uint32_t len = 0;
  for(j = low; j <= i; j++)
    len += prop->str_arr[j].len;
  while(prop->max_strs < prop->strs_len + len) {
    resize(prop->strs, (prop->max_strs *= 2));
    ducet_strs = prop->strs; /* for sort & abbrev */
  }
  cpybuf(ducet_strs + prop->strs_len,
         ducet_strs + prop->str_arr[low].off,
	 prop->str_arr[low].len);
  prop->str_arr[low].off = prop->strs_len;
  prop->strs_len += prop->str_arr[low].len;
  prop->str_arr[low].len = len;
  for(j = low + 1; j <= i; j++) {
    uint32_t *str = ducet_strs + prop->str_arr[j].off;
    len = prop->str_arr[j].len;
    cpybuf(ducet_strs + prop->strs_len, str, len);
    prop->strs_len += len;
  }
  movebuf(prop->str_arr + low + 1, prop->str_arr + i + 1,
          prop->len - i - 1);
  prop->len -= i - low;
  if(!(i = low))
    break;
}
/* 6.2: 30641 words (3250 saved) */
/* 6.2-CLDR: 29491 words (3155 saved) */
@

<<DUCET parser globals>>=
static uint32_t *ducet_strs;
static int cmp_ducet_idx(const void *a, const void *b)
{
  const raw_cp_str_t *_a = a, *_b = b;
  uint32_t *stra = ducet_strs + _a->off, *strb = ducet_strs + _b->off;
  uint32_t lena = _a->len, lenb = _b->len;

  /* assume all indices are unique, so equality will never be returned */
  while(lena > 0 && lenb > 0) {
    if((*stra & 3) != 3)
      return -1;
    if((*strb & 3) != 3)
      return 1;
    if(*stra != *strb)
      return *stra > *strb ? 1 : -1;
    lena--;
    lenb--;
    stra++;
    strb++;
  }
  return lena ? 1 : -1;
}
@

Unsurprisingly, combining entries reduces redundancy a bit (gaining 2
words).  Thus the original 126,219 words are reduced to 30,641 words.
This allows the use of 16-bit offsets, as supported by the
[[uni_str_arr_t]] type.  Note that this is just the string table; the
lookup tables add additional overhead (about 200kb for the code point
table and 100kb for the multi-level table). Finally, the raw DUCET is
ready to be dumped.  The collected default variable top is dumped to
the header as well.

<<Dump character information as C code>>=
fprintf(gen_h, "#define uni_DUCET_var_top %d\n", (int)ducet_vartop);
dump_str_tabs(&parsed_props[prop_DUCET], gen_h, tstf);
fprintf(gen_h, "#define uni_DUCET_CLDR_var_top %d\n", (int)cldr_vartop);
dump_str_tabs(&parsed_props[prop_DUCET_CLDR], gen_h, tstf);
@

<<Post-process DUCET>>=
prop->mt = (uint32_t *)~0; /* force addition to prop table */
@

In order to compensate for some of the compression above, a special
lookup function is provided.  It is intended for piece-wise lookup of
a full string's key sequence.  This is \emph{not} a key string as
defined by the UCA.  It is ordered by level first, and then key
element sequence, and includes zeroes at some levels.  The only thing
needed to convert this to a UCA key is to reorder the results by key
element sequence first, and then by level, and to remove zeroes.

In order to make it piece-wise, only one character is passed in at a
time, along with a running state.  Either a key element is returned,
or not.  Either the next character must be passed in next, or nothing.
It makes no sense for nothing to be returned, and no new character to
be required from the caller, so only three cases are consisdered:
returning a value, and requiring a new input next time ([[OK]]),
returning a value, and requiring no new input ([[AGAIN]]), and
returning no value, and requiring new input next time ([[NONE]]).  At
the end of input (flagged with a special character, [[END]]), the
final returned value is indicated by [[OK]] or [[NONE]].

Since this function directly references some large property tables, it
is stored in a separate object file.

<<Library [[uni]] Members>>=
ducet_lookup.o
@

<<ducet_lookup.c>>=
<<Common C Header>>
#include "uni_prop.h"
// static_proto

<<DUCET lookup functions>>
@

<<uni_DUCET_lookup parameters>>=
<<DUCET lookup format defs>>

/* function return value */
/* lev123 and lev4 are valid; give me next char */
/* unless char was END, in which case lev123 and lev4 are last return */
#define UNI_DUCET_LOOKUP_OK    0
/* lev123 and lev4 are valid, but don't give me a char next time */
/* in other words, next time c will be ignored */
#define UNI_DUCET_LOOKUP_AGAIN 1
/* lev123 and lev4 are invalid; give me next char */
/* unless char was END, in which case no more lev123 or lev4 */
#define UNI_DUCET_LOOKUP_NONE -1

/* special c parameter */
#define UNI_DUCET_LOOKUP_END 0xffffffff

/* pass in NULL first time to allocate */
/* will be freed by function when done */
typedef struct uni_ducet_lookup_state_t uni_ducet_lookup_state_t;
@

<<DUCET lookup format defs>>=
/* lev123 return value */
#define UNI_DUCET_LEV1_MASK  0xffff0000
#define UNI_DUCET_LEV1_SHIFT 16
#define UNI_DUCET_LEV2_MASK  0x0000ff80
#define UNI_DUCET_LEV2_SHIFT 7
#define UNI_DUCET_LEV3_MASK  0x0000007c
#define UNI_DUCET_LEV3_SHIFT 2
/* low 2 bits are always 0 */
@

<<Known Data Types>>=
uni_ducet_lookup_state_t,%
@

<<Unicode property exports>>=
<<uni_DUCET_lookup parameters>>
int uni_DUCET_lookup(uint32_t c, uint32_t *lev123, uint32_t *lev4,
                     uni_ducet_lookup_state_t **state);
@
                     
<<DUCET lookup functions>>=
<<Private DUCET lookup definitions>>
struct uni_ducet_lookup_state_t {
  <<DUCET lookup state members>>
};
<<Private DUCET lookup globals>>
int uni_DUCET_lookup(uint32_t c, uint32_t *lev123, uint32_t *lev4,
                     uni_ducet_lookup_state_t **state)
{
  if(!*state && c == UNI_DUCET_LOOKUP_END)
    return UNI_DUCET_LOOKUP_NONE;
  <<Initialize single DUCET element lookup>>
  <<Look up and return a single key element from DUCET>>
}
@

Rather than just look up raw values form the DUCET, some other
optional UCA-related processing may be done.  To select the processing
options, a separate structure and function are provided to set the
options.   The main reason for providing a separate function is to
ensure that options do not change mid-lookup.

<<[[uni_uca_opts_t]]>>=
typedef struct {
  <<Unicode UCA function options>>
} uni_uca_opts_t;
@

<<Known Data Types>>=
uni_uca_opts_t,%
@

<<Unicode property exports>>=
<<[[uni_uca_opts_t]]>>
/* set options for DUCET lookups; call before lookup() */
void uni_DUCET_lookup_opts(uni_ducet_lookup_state_t **state,
                           const uni_uca_opts_t *opts);
@

<<DUCET lookup functions>>=
void uni_DUCET_lookup_opts(uni_ducet_lookup_state_t **state,
                           const uni_uca_opts_t *opts)
{
  if(*state)
    return; /* error */
  inisize(*state, 1);
  clearbuf(*state, 1);
  uni_ducet_lookup_state_t *st = *state;
  if(opts) {
    <<Copy DUCET lookup options from [[opts]] to [[st]]>>
  }
  <<Set default DUCET lookup options>>
}
@

<<Initialize single DUCET element lookup>>=
/* if(!state) exit(1); */
uni_ducet_lookup_state_t *st = *state;
if(!st) {
  uni_DUCET_lookup_opts(state, NULL);
  st = *state;
}
@

First of all, there are two DUCET tables that could be used for
lookup.  One way to implement locales would be to load tables from
files, or to construct tables on the fly, so other tables may be used
as well.  The default table to use is the plain DUCET, but other
tables may be specified.  Rather than complicating the lookup function
with a parameter that should only be specified on the first call, a
separate function is provided to set the tables in the state
parameter.

<<Unicode UCA function options>>=
const uint32_t *tab, *strs;
@

<<DUCET lookup state members>>=
const uint32_t *tab, *strs;
@

<<Copy DUCET lookup options from [[opts]] to [[st]]>>=
st->tab = opts->tab;
st->strs = opts->strs;
@

<<Set default DUCET lookup options>>=
if(!st->tab) {
  st->tab = uni_DUCET_mtab;
  st->strs = uni_DUCET_strs;
}
@

One other easily implemented option is to select the comparison
strength, which is a level number above which other levels are zeroed
out.  This may reduce the number of returned elements as well, by
increasing the number of all-zero elements.  The first three levels
are most easily zeored using a mask, so that is stored in the state as
well.  In order to avoid constantly referring to a pointed-to value,
the return value is built in [[l123]] and [[l4]] rather than
[[*lev123]] and [[*lev4]].

<<Unicode UCA function options>>=
uint8_t max_level;
@

<<DUCET lookup state members>>=
uint32_t mask123;
uint8_t max_level;
@

<<Copy DUCET lookup options from [[opts]] to [[st]]>>=
st->max_level = opts->max_level;
@

<<Set default DUCET lookup options>>=
if(!st->max_level)
  st->max_level = 3;
if(st->max_level == 1)
  st->mask123 = UNI_DUCET_LEV1_MASK;
else if(st->max_level == 2)
  st->mask123 = UNI_DUCET_LEV1_MASK | UNI_DUCET_LEV2_MASK;
else
  st->mask123 = ~3;
@

<<Initialize single DUCET element lookup>>=
uint32_t l123, l4;
@

<<Mangle DUCET return value>>=
l123 &= st->mask123;
if(st->max_level < 4)
  l4 = 0;
@

Another option which immediately modifies results is the variable key
element processing.  Doing this requires the table's variable top and
a processing mode setting.  While the UCA apparently uses shifted mode
by default, this routine sets the default to minimal processing, which
is done with the non-ignorable mode.  The fact that non-ignorable is
the default mode for CLDR is irrelevant.

<<Unicode UCA function options>>=
uint8_t var_mode;
uint32_t var_top;
@

<<DUCET lookup state members>>=
uint8_t var_mode;
uint32_t var_top;
int in_var;
@

<<Copy DUCET lookup options from [[opts]] to [[st]]>>=
st->var_mode = opts->var_mode;
/* to allow direct <= comparison w/ lev123 */
if(opts->var_top)
  st->var_top = (opts->var_top << 16) | 0xffff;
@

<<Set default DUCET lookup options>>=
if(!st->var_top)
  /* to allow direct <= comparison w/ lev123 */
  st->var_top = (uni_DUCET_var_top << 16) | 0xffff;
@

<<Unicode property exports>>=
<<DUCET variable modes>>
@

<<DUCET variable modes>>=
#define UNI_DUCET_VAR_MODE_NON_IGNORABLE 0 /* the default; not UCA def */
#define UNI_DUCET_VAR_MODE_BLANKED       1
#define UNI_DUCET_VAR_MODE_SHIFTED       2 /* the UCA def; must be explicit */
#define UNI_DUCET_VAR_MODE_IGNORE_SP     3
#define UNI_DUCET_VAR_MODE_SHIFT_TRIMMED 4
@

<<Initialize single DUCET element lookup>>=
uint8_t var_mode = st->var_mode;
int shifted = var_mode >= UNI_DUCET_VAR_MODE_SHIFTED,
    blanked = var_mode == UNI_DUCET_VAR_MODE_BLANKED;
uint32_t var_top = st->var_top;
@

<<Mangle DUCET return value>>=
/* UCA variable mode modifies variable entry & anything following */
/* plus some other random ignorables */
if(shifted) {
  if(!l123)
    l4 = 0;
  else if(!(l123 & UNI_DUCET_LEV1_MASK)) {
    if(st->in_var)
      l4 = l123 = 0;
    else
      l4 = 0xffff;
  } else {
    st->in_var = 0;
    l4 = 0xffff;
  }
} else if(blanked && st->in_var) {
  if(!(l123 & UNI_DUCET_LEV1_MASK))
    l123 = l4 = 0;
  else
    st->in_var = 0;
}
/* due to modification of var_top above: */
/*    same as lev1 <= var_top && lev1 > 0 */
if(var_mode > 0 && l123 <= var_top && l123 > 0xffff) {
  st->in_var = 1;
  switch(var_mode) {
    /* UNI_DUCET_VAR_MODE_NON_IGNORABLE does nothing */
    case UNI_DUCET_VAR_MODE_BLANKED:
      l123 = l4 = 0;
      break;
    case UNI_DUCET_VAR_MODE_SHIFTED:
      l4 = l123 >> UNI_DUCET_LEV1_SHIFT;
      /* !!! Undocumented CLDR behavior (required to pass test): !!! */
      /* UCA says lev1 > 0 and lev1 <= var_top is variable */
      /*  but CLDR defines one entry with lev1 == 1, but it is not var */
      /*  yet in shifted mode, it sets lev4 == lev1 as above */
      /*  GRRRR.... */
      /*  at least UCA data files never use lev1 < 0x0200, so */
      /*  just testing for 1 is safe */
      /* who knows what IgnoreSP and Trimmed are supposed to work like */
      if(l4 > 1)
        l123 = 0;
      else
        st->in_var = 0;
      break;
    case UNI_DUCET_VAR_MODE_IGNORE_SP:
      if((uni_gc_trans[UNI_gc_P] & (1 << uni_gc_of(c))) ||
         is_uni_WSpace(c)) {
        l4 = l123 >> UNI_DUCET_LEV1_SHIFT;
	/* undocumented behavior copied from SHIFTED */
	if(l4 > 1)
	  l123 = 0;
	else
	  st->in_var = 0;
      } else
        st->in_var = 0;
      break;
    case UNI_DUCET_VAR_MODE_SHIFT_TRIMMED:
      l4 = l123 >> UNI_DUCET_LEV1_SHIFT;
      /* undocumented behavior copied from SHIFTED */
      if(l4 > 1)
        l123 = 0;
      else
        st->in_var = 0;
      break;
  }
}
@

Stripping trailing level 4 [[0xFFFF]] values, as required for
shift-trimmed mode, requires delaying any [[0xFFFF]] output until
either a non-[[0xFFFF]] output is found, or the final value is being
returned.  That could be best accomplished by the caller, by simply
accumulating the entire result and trimming the trailing [[0xFFFF]]
values.  However, it may be difficult to document that this mode, and
only this mode, is only partially implemented.  So, levels 1--3 of any
output whose level 4 is [[0xFFFF]] are saved in an expandable buffer.
Once a non-[[0xFFFF]] level 4 is encountered, all accumulated entries
need to be output before the key elements with a non-[[0xFFFF]] level
4, followed by that key element.  Once the end is encountered, all
accumulated entries need to be output, but with a level 4 of zero.
For the former case, levels 1--3 of the non-[[0xFFFF]] element can be
saved in place of the first saved element to be output, but level 4
needs to be saved separately.

Note that the caller can still prevent this accumulation by using
shifted mode, and trimming manually.

<<DUCET lookup state members>>=
uint32_t *l4_ffff;
unsigned int l4_ffff_len, l4_ffff_max, l4_ffff_ptr;
uint32_t non_ffff_l4;
@

<<Free DUCET lookup state members>>=
if(st->l4_ffff)
  free(st->l4_ffff);
@

<<Mangle DUCET return value>>=
if(var_mode == UNI_DUCET_VAR_MODE_SHIFT_TRIMMED) {
  if(l4 == 0xffff) {
    if(!st->l4_ffff_max)
      inisize(st->l4_ffff, st->l4_ffff_max = 10);
    if(st->l4_ffff_len == st->l4_ffff_max)
      resize(st->l4_ffff, st->l4_ffff_max *= 2);
    st->l4_ffff[st->l4_ffff_len++] = l123;
    <<Skip [[l4_ffff]] member>>
  } else if(st->l4_ffff_len) {
    uint32_t l123s = l123;
    l123 = st->l4_ffff[0];
    st->l4_ffff[0] = l123s;
    st->non_ffff_l4 = l4;
    l4 = 0xffff;
    st->l4_ffff_ptr = 1;
    <<Return [[l4_ffff]] members>>
  }
}
@

<<Check for [[l4_ffff]] members>>=
if(st->l4_ffff_ptr) {
  <<Maybe check for [[l4_ffff]] members at end>>
  if(st->l4_ffff_ptr < st->l4_ffff_len) {
    l123 = st->l4_ffff[st->l4_ffff_ptr++];
    l4 = 0xffff;
    <<Return [[l4_ffff]] members>>
  } else {
    l123 = st->l4_ffff[0];
    l4 = st->non_ffff_l4;
    st->l4_ffff_len = st->l4_ffff_ptr = 0;
    <<Return post-[[l4_ffff]] members>>
  }
}
@

<<Check for [[l4_ffff]] members at end>>=
while(st->l4_ffff_len) {
  l123 = st->l4_ffff[st->l4_ffff_ptr++];
  l4 = 0;
  if(st->l4_ffff_ptr == st->l4_ffff_len) {
    st->l4_ffff_ptr = st->l4_ffff_len = 0;
    <<Return last [[l4_ffff]] member>>
  } else if(l123) {
    <<Return [[l4_ffff]] members at end>>
  }
}
@

There is one other UCA parameter: reversed level 2.  That is, level 2
values are returned in reverse order.  To do this, the entire string
would have be be accumulated before generating the first output.  Since
some routines may be more efficient if results are returned one at a
time, this could be a problem.  Instead, it is the responsibility of
the caller to implement this option.

There are additional CLDR-specific parameters which could affect the
results of this function.  These may be implemented at a future date.
Adding shift mode processing does not affect complexity much, but
adding DUCET reordering, additional pseudo-levels, numeric
accumulation and parsing, and other CLDR options will make this
function very complex.

\lstset{language=txt}
<<FIXME>>=
  Unimplemented CLDR options:
    caseLevel=on/off: add level "2.5" "case level" (even if 2 not present)
      l2.5 == LOWEST ? 1 : upper_first ? (UPPER ? 2 : MIXED ? 3 : 4) :
                                         (UPPER ? 4 : MIXED ? 3 : 2)
         or 0 if sorting by l1 and l1 = 0
	 or 0 if sorting by l2+ and l2 = 0
      for DUCET_CLDR members:
        LOWEST if U+FFFE
        UPPER if l3 == 08-0C, 0E, 11-12, 1D
	UNCASED otherwise
      for locale customizations of single char could take old l3.  for
      multi-char locale customizations, canonically decompose & compute
      for each char:
        UNCASED if gc == Lm
	LOWER if Lowercase or Changes_When_Uppercased or "exception":
	   sc=Hira or sc=Kata and na="*SMALL*" and dm=""
	   U+2129
	UPPER if Uppercase or Changes_When_Uppercased or "exception":
	   sc=Hira or sc=Kata and na!="*SMALL*" and na!="*ITERATION*" and dm=""
	   U+2118 U+213A U+2141-U+2144 U+1F150-U+1F169 U+1F170-U+1F18A
	MIXED if gc == Lt "or both (a) and (b)" meaning probably
	                  "or both Lowercase and Uppercase"
	UNCASED otherwise
	note on LOWER/UPPER exceptions above:  the additional code points
	are not given in a rule or a data file anywhere, but must be
	lifted directly from tr35-##.html.  They are merely described as
	"characters xxcase in form".  Not only that, but they are
	presented as characters rather than numeric code points, so good
	luck figuring them out from printed copies..
      then for original char, combine all.  If all but UNCASED are same,
      use that; otherwise MIXED
    caseFirst=upper/lower/off: force upper/lower case reordering
      when caseLevel=on, see above.  Otherwise, l2.5 is prepended to
      the l3 value, by shifting left 5 and oring with l3 (I think; it's
      hard to say what "ct" means:  it could mean what I said, or it
      could mean that t is multiplied by c) if l2 is non-0.  If l3
      is 0, it stays 0.  If l2 is 0 (and l3 non-0), 4 is prepended
      instead of l2.5. 
    hiraganaQuaternary=on/off: use FFFE instead of FFFF for shifted Hiragana
      Trivial, but requires sc property table (maybe)
        3041-3096, 309D-309F, 1B001, 1F200
    numeric=on/off: replace digit strings with numeric value at l1
      My guess: order them all at same pos as 0 and add 2nd key w/ 
      only primary set to 15 bits at a time, in big-endian order,
      setting bit 0.
    reorder=<set>{,<set>}: reorder major groups
      Need to know where major groups were to reorder them
    collation=<type>
      standard, ducet, locale-defined collation types
        - apparently same as "standard" if unspecified for locale
    locale tailoring (really a superset of collation=)
@

Some operations, such as looking up multi-character collation
elements, require at least one character of lookahead.  In fact, the
reordering requirement requires arbitrary lookahead.  To support this,
a buffer may be filled with lookahead characters.  Any time a NONE
return would happen, if there are buffer characters, the routine
should instead restart itself.  Any time an OK return would happen, an
AGAIN should be returned instead.  Any time an AGAIN return would
happen, the character consumed from the buffer is reinserted, so that
the next time the function is called, that character is consumed again
instead of the next character in the buffer.  If there are no buffered
characters, this is not necessary as no character will be consumed.

This is also the appropriate place to mask out the unneeded levels of
the return value, and to alter the return code as needed.  An OK
return is treated as a NONE return, and an AGAIN return loops without
returning a value.

\lstset{language=C}
<<DUCET lookup state members>>=
uint32_t *bt;
int32_t btp, num_bt, max_bt;
@

<<Look up and return a single key element from DUCET>>=
<<Check for [[l4_ffff]] members>>
restart:
  if(st->num_bt) {
    if(st->btp >= 0)
      c = st->bt[st->btp++];
    else
      st->btp++;
    if(st->btp == st->num_bt)
      st->btp = st->num_bt = 0;
  }
@

<<Free DUCET lookup state members>>=
if(st->bt)
  free(st->bt);
@

<<Private DUCET lookup globals>>=
static void add_bt(uint32_t c, uni_ducet_lookup_state_t *st)
{
  if(!st->bt)
    inisize(st->bt, (st->max_bt = 5));
  if(st->num_bt == st->max_bt)
    resize(st->bt, (st->max_bt *= 2));
  st->bt[st->num_bt++] = c;
}

static void add_bt_buf(const uint32_t *buf, uint32_t len,
                       uni_ducet_lookup_state_t *st)
{
  if(!st->bt)
    inisize(st->bt, (st->max_bt = 5));
  while(st->num_bt + len > st->max_bt)
    resize(st->bt, (st->max_bt *= 2));
  cpybuf(st->bt + st->num_bt, buf, len);
  st->num_bt += len;
}
@

<<Skip [[l4_ffff]] member>>=
<<Return no DUCET values>>
@

<<Return post-[[l4_ffff]] members>>=
*lev123 = l123;
*lev4 = l4;
/* assume that non-default state needed AGAIN */
return st->state ? UNI_DUCET_LOOKUP_AGAIN : UNI_DUCET_LOOKUP_OK;
@

<<Return [[l4_ffff]] members>>=
*lev123 = l123;
*lev4 = l4;
return UNI_DUCET_LOOKUP_AGAIN;
@

<<Return no DUCET values>>=
if(st->num_bt)
  goto restart;
else
  return UNI_DUCET_LOOKUP_NONE;
@

<<Return one DUCET value>>=
<<Mangle DUCET return value>>
if(!l123 && !l4) {
  <<Return no DUCET values>>
}
*lev123 = l123;
*lev4 = l4;
if(st->num_bt)
  return UNI_DUCET_LOOKUP_AGAIN;
else
  return UNI_DUCET_LOOKUP_OK;
@

<<Return more than one DUCET value>>=
if(st->num_bt)
  st->btp--;
<<Mangle DUCET return value>>
if(!l123 && !l4)
  goto restart;
*lev123 = l123;
*lev4 = l4;
return UNI_DUCET_LOOKUP_AGAIN;
@

A private enumeration type is used to represet the current lookup
progress.

<<Private DUCET lookup definitions>>=
typedef enum {
  DUCET_lookup_start
  <<DUCET lookup states>>
} ducet_lookup_state_t;
@

<<Known Data Types>>=
ducet_lookup_state_t,%
@

<<DUCET lookup state members>>=
ducet_lookup_state_t state;
@

<<Look up and return a single key element from DUCET>>=
switch(st->state) {
  case DUCET_lookup_start:
    <<Look up in DUCET given no state>>
  <<Look up in DUCET given state>>
}
@

The first thing to do is check for end-of-string.  At end-of string,
the state is freed, and nothing is returned, unless there are
accumulated [[l4_ffff]] members.

<<DUCET lookup state members>>=
int eoi;
@

<<Look up in DUCET given no state>>=
if(st->eoi || c == UNI_DUCET_LOOKUP_END) {
  <<Check for [[l4_ffff]] members at end>>
  <<Free DUCET lookup state members>>
  free(st);
  *state = NULL;
  return UNI_DUCET_LOOKUP_NONE; /* can't have any more bt */
}
@

<<Return last [[l4_ffff]] member>>=
if(l123 || l4) {
  *lev123 = l123;
  *lev4 = l4;
  <<Free DUCET lookup state members>>
  free(st);
  *state = NULL;
  return UNI_DUCET_LOOKUP_OK;
} else {
  <<Free DUCET lookup state members>>
  free(st);
  *state = NULL;
  return UNI_DUCET_LOOKUP_NONE;
}
@

<<Return [[l4_ffff]] members at end>>=
st->eoi = 1;
*lev123 = l123;
*lev4 = l4;
return UNI_DUCET_LOOKUP_AGAIN;
@

<<Maybe check for [[l4_ffff]] members at end>>=
if(st->eoi) {
  <<Check for [[l4_ffff]] members at end>>
}
@

Otherwise, a lookup is performed in the table.  If the lookup fails, a
key is synthesized.  Note that synthesis requires the UIdeo and blk
properties as specified, although this could be modified to use
explicit range checks instead.  The UIdeo property is complicated
enough and yet small enough (12 ranges/300 bytes) that conversion
would not save much, but the blk checks are just single ranges each
(and the blk table is much larger, at 220 ranges/8032 bytes).  This
assumes that the ranges will never change in the future, which is
actually a pretty safe bet.

The UCA standard gives no hints on how to set the secondary value;
presumably any non-zero number will work, because the primaries will
always differ.  The most common value (0020) is chosen for this.  The
tertiary value has a table, but since no decomposition is involved,
and all special cases are already covered in the DUCET, all characters
get the default (0002) assigned to them.

For the fourth level, the UCA standard states that any non-zero value
should be used on the first key element, and zero on the second.
However, actual DUCET values which duplicate synthesized values (due to
decomposition) show a non-zero level four on the second key element as
well, and in fact both use the code point.  For consistency, that is
what is generated.

Note that while the standard states that FCD form data can be
processed without normalization, this does not cover the implicit
canonical compositions; i.e., the Hangul syllable compositions.  For
this reason, if a lookup fails for such a decomposable character, the
first character of decomposition is processed instead, and any
remaining characters from decomposition are pushed onto the
backtracking buffer.%
\footnote{<sigh> CLDR has additional problems with this.
Specifically, U+0F73 decomposes to U+0F71 U+0F72.  U+0F73 is a starter
(ccc=0), and the other two are non-starters (ccc>0).  This means that
the latter two will be reordered to combine with U+0FB3, but U+0F73
will not.  Having the string in FCD form makes no difference; UCA
simply disallows moving starters around.  I don't know if this is a
bug in the UCD (maybe U+0F73 shouldn't be a starter, since it
decomposes into two non-starters) or a bug in the claim that the UCA
can be applied as-is (maybe it needs changes to support FCD) or a bug
in the CLDR DUCET or what.}


<<DUCET lookup state members>>=
uni_str_ptr_t v;
uint32_t c;
@

<<Look up in DUCET given no state>>=
const uni_str_ptr_t *v;
uni_multi_tab_lookup(st->tab, c * 4, (const uint8_t **)&v, 0);
if(!v || (!v->off && !v->len)) {
  uint32_t hbuf[3];
  int hlen = uni_hangul_syllable_decomp(c, hbuf, 1);
  if(hlen > 0) {
    c = hbuf[0];
    uni_multi_tab_lookup(st->tab, c * 4, (const uint8_t **)&v, 0);
    add_bt(hbuf[1], st);
    if(hlen > 2)
      add_bt(hbuf[2], st);
  }
}
if(v && (v->off || v->len)) {
  st->v = *v;
  <<Process DUCET initial lookup>>
} else {
  st->c = c;
  st->state = DUCET_lookup_synth;
  l123 = ((c >> 15) << 16) + (0x0020 << 7) + (0x0002 << 2);
  if(is_uni_UIdeo(c)) {
#if 0
    uni_blk_t blk = uni_blk_of(c);
    if(blk == UNI_blk_CJK || blk == UNI_blk_CJK_Compat)
#else
    if((c >= 0x4E00 && c <= 0x9FFF) || (c >= 0x3300 && c <= 0x33FF))
#endif
      l123 += 0xFB40 << 16;
    else
      l123 += 0xFB80 << 16;
  } else
    l123 += 0xFBC0 << 16;
  l4 = c; /* actual value does not matter, as long as it's non-0 */
  <<Return more than one DUCET value>>
}
@

<<DUCET lookup states>>=
,DUCET_lookup_synth
@

<<Look up in DUCET given state>>=
case DUCET_lookup_synth:
  l123 = ((st->c & 0x7fff) << 16) | 0x80000000;
  l4 = st->c;
  st->state = 0;
  <<Return one DUCET value>>
@

For a successful lookup, the response for a single-character index is
extracted.  If multi-character indices are available as well, more
characters are requested after saving the work done so far in the
state.  Otherwise, the single-character reponse is returned.

<<DUCET lookup state members>>=
uint32_t curoff;
@

<<Process DUCET initial lookup>>=
const uint32_t *str = st->strs + st->v.off;
uint32_t len = st->v.len, len1;
for(len1 = 0; len1 < len; len1++)
  if((str[len1] & 3) == 3) {
    <<Accumulate and check more collation element characters>>
    <<Return no DUCET values>>
  }
<<Return DUCET entry for single-character index>>
@

<<Accumulate and check more collation element characters>>=
st->state = DUCET_lookup_gotfirst;
st->curoff = len1;
st->c = c;
@

<<DUCET lookup states>>=
,DUCET_lookup_gotfirst
@

If the next character is end-of-string, the match so far is returned.
Otherwise, it is searched for in the subindex strings.   A match
increases the total [[matchlen]], which can be skipped.  Only exact
matches (i.e. only one character after [[matchlen]], and it must match
[[c]]) are considered successful.

<<Look up in DUCET given state>>=
case DUCET_lookup_gotfirst:
  if(c == UNI_DUCET_LOOKUP_END) {
    <<Return DUCET entry for single or multi-character index>>
  } else {
    int32_t l, h, m;
    const uint32_t *str = st->strs + st->v.off;
    m = ducet_lookup_subindex(c, &l, &h, st);
    <<Process index extension search results>>
  }
@

<<DUCET lookup state members>>=
uint32_t matchlen;
@

<<Accumulate and check more collation element characters>>=
st->matchlen = 0;
@

<<Private DUCET lookup globals>>=
static int ducet_lookup_subindex(uint32_t c, int32_t *_l, int32_t *_h,
                                 const uni_ducet_lookup_state_t *st)
{
  uint32_t l = st->curoff, h = st->v.len, m;
  const uint32_t *str = st->strs + st->v.off;
  while(l <= h) {
    <<Find start of middle index extension>>
    m += st->matchlen;
    uint32_t mc = str[m] >> 2;
    if(mc < c) {
      l = m + 1;
      <<Skip to next index extension>>
    } else if(mc > c ||
              /* also greater if longer than one char */
	      (h > m && mc == c &&
	      (str[m + 1] & 3) == 3 && str[m + 1] != 3)) {
      h = m - st->matchlen - 1;
    } else {
      *_l = l;
      *_h = h;
      return m - st->matchlen;
    }
  }
  *_l = l;
  *_h = h;
  return 0;
}
@

<<Find start of middle index extension>>=
m = (l + h) / 2;
while(m > l && ((str[m] & 3) != 3 || str[m] == 3))
  m--;
while(m > l && ((str[m - 1] & 3) == 3 && str[m - 1] != 3))
  m--;
@

<<Skip to next index extension>>=
while(l <= h && (str[l] & 3) == 3 && str[l] != 3)
  l++;
if(l <= h && str[l] == 3)
  l++;
else
  while(l <= h && (str[l] & 3) != 3)
    l++;
@

A successful match is returned if there can be no further extension of
the index.  Otherwise, the search results are narrowed to the
possibilities collected so far, and more input is requested.

<<Process index extension search results>>=
if(l <= h) {
  /* advance search result to match */
  st->v.off += m;
  st->v.len -= m;
  str += m;
  /* find end of match entry */
  l = 1;
  h = st->v.len - 1;
  <<Skip to next index extension>>
  ducet_adjust_search(c, l, st);
  if(st->curoff == st->v.len) {
    /* there are no more possible matches, so just return */
    st->state = DUCET_lookup_start;
    <<Return DUCET entry for multi-character index>>
  } else {
    st->matchlen++;
    <<Return no DUCET values>>
  }
}
@

<<Private DUCET lookup globals>>=
static void ducet_adjust_search(uint32_t c, int32_t l,
                                uni_ducet_lookup_state_t *st)
{
  int32_t h = st->v.len - 1, m;
  const uint32_t *str = st->strs + st->v.off;
  st->curoff = l;
  while(l <= h) {
    <<Find start of middle index extension>>
    if((str[m + st->matchlen] >> 2) != c)
      h = m - 1;
    else {
      l = m + 1;
      <<Skip to next index extension>>
    }
  }
  st->v.len = h + 1;
}
@

When a mismatch occurs, however, it is not guaranteed that this will
never match.  There are two cases where a future character might still
cause a match.  If a longer string is required for a match due to a
DUCET which does not have entries for all intervening-length strings
(the standard DUCET does not), [[h]] always points just below at least
one such string.  If more data is required because a reordering might
cause a match, the ccc of [[c]] and any future characters must be
non-zero and not match the ccc of any potential candidates for
reordering.%
\footnote{But see above:  maybe what should be checked is start-ccc
and end-ccc rather than ccc.}

<<Process index extension search results>>=
else {
  uint32_t ccc;
  /* if c in str, but not only member of str, ill-formed DUCET */
  /* h always points just below longer index */
  h += st->matchlen + 1;
  if(h < st->v.len && str[h] == (c << 2) + 3) {
    <<Enable possible longer lookup>>
  /* check if possible match due to reordering rule */
  } else if((ccc = uni_ccc_of(c))) {
    <<Enable reordering extension>>
  } else {
    /* no possible longer matches, so go ahead and return */
    add_bt(c, st);
    <<Return DUCET entry for single or multi-character index>>
  }
}
@

The first case is easy to detect and manage.  At most one intermediate
is allowed to be missing, so the next character determines if a match
is made.  if so, processing continues just as above.  Otherwise, the
two characters that were not processed are stored, and special states
are entered to feed these back, one at a time.  They will never
collide with each other, because the second character is the first one
which could cause the extension state to reappear, and by then, the
stored characters are done being processed.

Rather than modify the code to loop and retrieve characters from
buffers, a recursive call is used.  Again, this should happen rarely,
and does not affect performance much.

<<DUCET lookup state members>>=
uint32_t long_off;
@

<<DUCET lookup states>>=
,DUCET_lookup_longer
@

<<Enable possible longer lookup>>=
st->state = DUCET_lookup_longer;
st->long_off = st->curoff;
st->c = c;
ducet_adjust_search(c, l, st);
<<Return no DUCET values>>
@

<<Look up in DUCET given state>>=
case DUCET_lookup_longer:
  {
    int32_t m, l, h;
    const uint32_t *str = st->strs + st->v.off;
    st->matchlen++;
    m = ducet_lookup_subindex(c, &l, &h, st);
    if(l > h) { /* assume no even longer matches possible.. */
      add_bt(st->c, st);
      add_bt(c, st);
      st->curoff = st->long_off;
      st->state = DUCET_lookup_start;
      <<Return DUCET entry for single or multi-character index>>
    } else {
      st->v.off += m;
      st->v.len -= m;
      l = 1;
      h = st->v.len - 1;
      <<Skip to next index extension>>
      st->curoff = l;
      <<Return DUCET entry for multi-character index>>
    }
  }
@

The second case requires checking the ccc of all subsequent
characters; if a zero is detected, no further reordering is possible.
Otherwise, as long as a character does not match the ccc of one of the
previously accumulated characters, it is looked up and if it matches,
a reordering can take place.%
\footnote{But see above:  maybe what should be checked is start-ccc
rather than ccc.}


By the time the decision is made, at least one character, and possibly
more, have been accumulated.  A buffer (separate from [[bt]]) needs to
be used to accumulate these and eventually transfer them into [[bt]]. 

<<DUCET lookup state members>>=
uint32_t *buf, buflen, maxbuf;
uint32_t *ccc_buf;
@

<<DUCET lookup states>>=
,DUCET_lookup_reorder
@

<<Enable reordering extension>>=
if(!st->buf) {
  inisize(st->buf, (st->maxbuf = 10));
  inisize(st->ccc_buf, st->maxbuf);
}
st->buf[0] = c;
st->ccc_buf[0] = ccc;
st->buflen = 1;
st->state = DUCET_lookup_reorder;
<<Return no DUCET values>>
@

<<Look up in DUCET given state>>=
case DUCET_lookup_reorder:
  {
    uint32_t ccc = c == UNI_DUCET_LOOKUP_END ? 0 : uni_ccc_of(c);
    uint32_t i;
    if(st->maxbuf == st->buflen) {
      resize(st->buf, st->maxbuf *= 2);
      resize(st->ccc_buf, st->maxbuf);
    }
    st->buf[st->buflen] = c;
    st->ccc_buf[st->buflen++] = ccc;
    if(!ccc) {
      st->state = DUCET_lookup_start;
      add_bt_buf(st->buf, st->buflen, st);
      <<Return DUCET entry for single or multi-character index>>
    }
    for(i = 0; i < st->buflen - 1; i++)
      if(st->ccc_buf[i] == ccc) {
        <<Return no DUCET values>>
      }
    int32_t l, h, m;
    m = ducet_lookup_subindex(c, &l, &h, st);
    if(l <= h) {
      /* advance search result to match */
      st->v.off += m;
      st->v.len -= m;
      /* copy rest of characters accumulated to bt */
      add_bt_buf(st->buf, st->buflen - 1, st);
      /* find end of match entry */
      l = 1;
      h = st->v.len - 1;
      const uint32_t *str = st->strs + st->v.off;
      <<Skip to next index extension>>
      if(l > h) {
        /* there are no more possible matches, so just return */
        st->curoff = l;
	st->state = DUCET_lookup_start;
	<<Return DUCET entry for multi-character index>>
      } else {
        st->state = DUCET_lookup_gotfirst;
	ducet_adjust_search(c, l, st);
	st->matchlen++;
	<<Return no DUCET values>>
      }
    } else {
      <<Return no DUCET values>>
    }
  }
@

<<Free DUCET lookup state members>>=
if(st->buf)
  free(st->buf);
@

Rather than reproduce the code for returning the results everywhere,
it is placed after the switch.  Thus a [[break]] indicates that a
result should be returned.  The only difference between them is that
[[c]] needs to be retrieved from the state for multi-character indices.
Also, [[curoff]] is assumed to be the end of the entry to return, but
it is never assigned for the single-character index case.

<<Return DUCET entry for single or multi-character index>>=
c = st->c;
break;
@

<<Return DUCET entry for multi-character index>>=
c = st->c;
break;
@

<<Return DUCET entry for single-character index>>=
st->curoff = v->len;
break;
@

Due to advancing the pointers above, [[v]] points to the desired result
to return.  The only difficulty is distinguishing between
multi-character index results and blank single-character index
results; this can be done by checking [[curoff]], which points to the
first character after the key.  If this is zero, then it is a blank
single-character index result.

<<Look up and return a single key element from DUCET>>=
if(!st->curoff) {
  <<Return blank single-character index DUCET result>>
}
int i;
const uint32_t *str = st->strs + st->v.off;
for(i = 0; i < st->curoff; i++)
  if((str[i] & 3) != 3)
    break;
<<Return non-blank DUCET result>>
@

The single-character index entries with blank keys denote canonical
decomposition.  This may require more than one result.  All decomposed
characters are guaranteed to have a real key of length one for the
single-character index.

<<DUCET lookup state members>>=
const uint16_t *multi_ret16;
uint32_t multi_ret_len;
@

<<Return blank single-character index DUCET result>>=
const uni_str_ptr_t *v;
uni_multi_tab_lookup(uni_canon_decomp_mtab, c * 4, (const uint8_t **)&v, 0);
st->multi_ret16 = uni_dm_strs + v->off;
st->multi_ret_len = v->len;
<<Return DUCET entry for next decomp character>>
@

<<Return DUCET entry for next decomp character>>=
unsigned int clen;
const uint32_t *str;
c = uni_int_utf16_decode(st->multi_ret16, &clen);
st->multi_ret16 += clen;
st->multi_ret_len -= clen;
<<Look up and return DUCET value for [[c]]>>
if(st->multi_ret_len) {
  st->state = DUCET_lookup_compret;
  <<Return more than one DUCET value>>
} else {
  st->state = DUCET_lookup_start;
  <<Return one DUCET value>>
}
@

<<Look up and return DUCET value for [[c]]>>=
uni_multi_tab_lookup(st->tab, c * 4, (const uint8_t **)&v, 0);
str = st->strs + v->off;
<<Return DUCET value for [[str]]/[[c]]>>
@

<<Return DUCET value for [[str]]/[[c]]>>=
l123 = *str & ~3;
switch(*str & 3) {
  case 0:
    l4 = 0;
    break;
  case 1:
    l4 = c;
    break;
  default:
    l4 = *++str >> 2;
    break;
}
@

<<DUCET lookup states>>=
,DUCET_lookup_compret
@

<<Look up in DUCET given state>>=
case DUCET_lookup_compret:
  {
    <<Return DUCET entry for next decomp character>>
  }
@

The non-blank result only requires work if there is more than one key
entry.  If so, it is returned later.

<<DUCET lookup state members>>=
const uint32_t *multi_ret32;
@

<<Return non-blank DUCET result>>=
str += i;
st->multi_ret32 = str;
st->curoff -= i;
<<Return DUCET value for [[str]]/[[c]]>>
str++;
st->multi_ret_len = st->curoff - (int)(str - st->multi_ret32);
if(!st->multi_ret_len) {
  st->state = DUCET_lookup_start;
  <<Return one DUCET value>>
}
st->multi_ret32 = str;
st->state = DUCET_lookup_multikey;
<<Return more than one DUCET value>>
@

<<DUCET lookup states>>=
,DUCET_lookup_multikey
@

<<Look up in DUCET given state>>=
case DUCET_lookup_multikey:
  {
    const uint32_t *str = st->multi_ret32;
    c = st->c;
    <<Return DUCET value for [[str]]/[[c]]>>
    str++;
    st->multi_ret_len -= str - st->multi_ret32;
    if(st->multi_ret_len) {
      st->multi_ret32 = str;
      <<Return more than one DUCET value>>
    } else {
      st->state = DUCET_lookup_start;
      <<Return one DUCET value>>
    }
  }
@

Note that the multi-level comparison algorithm requires that more than
one pass be made over the keys, unless the strings being compared
happen to generate keys of the same ignorable level.  This can be done
by either calling [[uni_DUCET_lookup]] as many times as there are
levels for each character, or by storing the results of the lookup in
dynamic arrays, to be processed in later passes.  The only entries
which do not need to be stored are all-zero entries.  A simple lookup
function is provided to do the lookups, and return the results as well
as the number of entries in each level (if requested).  While it would
be possible to implement level 2 reversal and literal string appends
(the as-yet unimplemented UCA options) in this function, they are left
for an even higher level function.

<<Unicode property exports>>=
/* negative slen indicates zero termination */
<<[[uni_str_ducet]][[32]] proto>>;
<<[[uni_str_ducet]][[16]] proto>>;
<<[[uni_str_ducet]][[8]] proto>>;
@

<<[[uni_str_ducet]](@sz) proto>>=
uint32_t *uni_str_ducet<<@sz>>(const uint<<@sz>>_t *str, int slen,
                          const uni_uca_opts_t *opts, unsigned int *rlen,
			  unsigned int *llen)
@

<<DUCET lookup functions>>=
<<[[uni_str_ducet]][[32]]>>
<<[[uni_str_ducet]][[16]]>>
<<[[uni_str_ducet]][[8]]>>
@

<<[[uni_str_ducet]](@sz)>>=
<<[[uni_str_ducet]][[<<@sz>>]] proto>>
{
  uint32_t *res, res_max, nres = 0;
  uni_ducet_lookup_state_t *st = NULL;
  int use_lev4 = opts && opts->max_level > 3 ? 1 : 0;
  int ret = UNI_DUCET_LOOKUP_OK;

  if(!str || !slen) {
    *rlen = 0;
    return NULL;
  }
  inisize(res, (res_max = 10));
  if(opts)
    uni_DUCET_lookup_opts(&st, opts);
  if(llen)
    llen[0] = llen[1] = llen[2] = llen[3] = 0;
  while(1) {
    uint32_t lev123, lev4;
    uint32_t cp;
    if(!slen)
      cp = UNI_DUCET_LOOKUP_END;
    else if(ret == UNI_DUCET_LOOKUP_AGAIN)
      cp = 0; /* any old random value */
    else {
      unsigned int clen;
      cp = uni_int_utf<<@sz>>_decode(str, &clen);
      str += clen;
      if(slen > 0)
        slen -= clen; /* underflow if slen was wrong, but I don't care */
      else if(!cp) {
        cp = UNI_DUCET_LOOKUP_END;
	slen = 0;
      }
    }
    ret = uni_DUCET_lookup(cp, &lev123, &lev4, &st);
    if(ret == UNI_DUCET_LOOKUP_NONE) {
      if(cp == UNI_DUCET_LOOKUP_END)
        break;
      else
        continue;
    }
    if(lev123 || lev4) {
      if(nres >= res_max - 1)
        resize(res, (res_max *= 2));
      res[nres++] = lev123;
      if(use_lev4)
        res[nres++] = lev4;
      if(llen) {
        if(lev123 & UNI_DUCET_LEV1_MASK)
          llen[0]++;
        if(lev123 & UNI_DUCET_LEV2_MASK)
          llen[1]++;
        if(lev123 & UNI_DUCET_LEV3_MASK)
          llen[2]++;
        if(lev4)
          llen[3]++;
      }
    }
    if(ret == UNI_DUCET_LOOKUP_OK && cp == UNI_DUCET_LOOKUP_END)
      break;
  }
  *rlen = nres;
  return res;
}
@

The above result could be considered the UCA key, albeit in a
completely different format than recommended by the UCA.  However, the
options need to be passed along with it, in order to determine if
level 4 is present or not, and also to implement any UCA options that
were deferred to a higher level.  A function is provided to perform
the key comparison, but it only adds one UCA option:  reversal of
level 2.  The final option, literal string comparison, is deferred to
an even higher level.

<<Unicode UCA function options>>=
uint8_t reverse_lev2;
@

<<Unicode property exports>>=
int uni_uca_ducet_cmp(const uint32_t *key1, unsigned int len1,
                      const uint32_t *key2, unsigned int len2,
		      const uni_uca_opts_t *opts);
@

<<DUCET lookup functions>>=
static int uni_uca_ducet_cmp123(uint32_t mask,
                                const uint32_t *key1, unsigned int len1,
                                const uint32_t *key2, unsigned int len2,
				int l4, int rev)
{
  uint32_t k1, k2;

  while(1) {
    while(len1 > 0 && !(k1 = *key1 & mask)) {
      key1 += 1 + l4;
      len1 -= 1 + l4;
    }
    while(len2 > 0 && !(k2 = *key2 & mask)) {
      key2 += 1 + l4;
      len2 -= 1 + l4;
    }
    if(!len2)
      return len1 > 0;
    if(!len1)
      return -1;
    if(k1 < k2)
      return -1;
    if(k2 < k1)
      return 1;
    key1 += 1 + l4;
    key2 += 1 + l4;
    len1 -= 1 + l4;
    len2 -= 1 + l4;
  }
}

int uni_uca_ducet_cmp(const uint32_t *key1, unsigned int len1,
                      const uint32_t *key2, unsigned int len2,
		      const uni_uca_opts_t *opts)
{
  int lev = opts && opts->max_level ? opts->max_level : 3;
  int l4 = lev > 3;
  int c;
  c = uni_uca_ducet_cmp123(UNI_DUCET_LEV1_MASK, key1, len1, key2, len2, l4, 0);
  if(c || lev == 1)
    return c;
  c = uni_uca_ducet_cmp123(UNI_DUCET_LEV2_MASK, key1, len1, key2, len2, l4,
                           opts && opts->reverse_lev2);
  if(c || lev == 2)
    return c;
  c = uni_uca_ducet_cmp123(UNI_DUCET_LEV3_MASK, key1, len1, key2, len2, l4, 0);
  if(c || lev == 3)
    return c;
  c = uni_uca_ducet_cmp123(~0, key1 + 1, len1, key2 + 1, len2, 1, 0);
  return c;
}
@

Implementing the final step when literal comparison is to be made is
supported by a simple [[strcmp]]-like function.  Conversion to valid NFD
form is expected to have been done before the call.  The string length
can either be negative, indicating zero termination, or positive,
indicating an absolute length.

<<Unicode I/O Exports>>=
/* "int"ernal strcmp; assumes a & b are valid UTF */
<<[[uni_int_utf-]][[32]][[_strcmp]] proto>>;
<<[[uni_int_utf-]][[16]][[_strcmp]] proto>>;
<<[[uni_int_utf-]][[8]][[_strcmp]] proto>>;
@

<<[[uni_int_utf-]](@sz)[[_strcmp]] proto>>=
int uni_int_utf<<@sz>>_strcmp(const uint<<@sz>>_t *a, int alen, const uint<<@sz>>_t *b, int blen)
@

<<Unicode I/O functions>>=
<<[[uni_int_utf-]][[32]][[_strcmp]]>>
<<[[uni_int_utf-]][[16]][[_strcmp]]>>
<<[[uni_int_utf-]][[8]][[_strcmp]]>>
@

<<[[uni_int_utf-]](@sz)[[_strcmp]]>>=
<<[[uni_int_utf-]][[<<@sz>>]][[_strcmp]] proto>>
{
  while(alen && blen) {
    unsigned int aclen, bclen;
    if(alen < 0 && !*a)
      return blen < 0 && !*b ? 0 : -1;
    if(blen < 0 && !*b)
      return 1;
    uint32_t ac = uni_int_utf<<@sz>>_decode(a, &aclen);
    uint32_t bc = uni_int_utf<<@sz>>_decode(b, &bclen);
    if(ac < bc)
      return -1;
    if(ac > bc)
      return 1;
    a += aclen;
    b += bclen;
    alen -= aclen; /* underflows if alen was wrong, but don't care */
    blen -= bclen; /* likewise with blen */
  }
  return alen > 0 || (alen < 0 && *a) ? 1 :
         blen > 0 || (blen < 0 && *b) ? -1 : 0;
}
@

Another option would be to support [[memcmp]], [[strcmp]] or something
similar, by building a key more like the UCA format.  This might make
storing keys for later use much easier.  However, other than storage
efficiency, I see no advantage, and will not implement this.

For direct UCA string comparison, the above functions could be
combined by generating both keys, comparing them, and freeing them.
However, direct string comparison can also be performed by only
storing levels 2-4, and aborting the comparison if a level 1 mismatch
occurs.  The original string pointers need to be retained as well, in
case a literal comparison needs to be made for the last step.  In
order to avoid passing the literal comparison flag separately, this is
added to the options structure.  While the standard indicates that
literal comparison is selected with [[max_level]] of 5 or higher, in
fact the test data suggests that literal comparison should be possible
with lower levels as well.  If [[max_level]] is 5 or higher, or the
explicit [[literal]] flag is set, literal comparison will be made.

<<Unicode UCA function options>>=
int8_t do_literal;
@

<<Unicode property exports>>=
<<[[uni_uca_strcmp]][[32]] proto>>;
<<[[uni_uca_strcmp]][[16]] proto>>;
<<[[uni_uca_strcmp]][[8]] proto>>;
@

<<[[uni_uca_strcmp]](@sz) proto>>=
int uni_uca_strcmp<<@sz>>(const uint<<@sz>>_t *a, int alen,
                     const uint<<@sz>>_t *b, int blen,
                     const uni_uca_opts_t *opts)
@

<<DUCET lookup functions>>=
<<[[uni_uca_strcmp]][[32]]>>
<<[[uni_uca_strcmp]][[16]]>>
<<[[uni_uca_strcmp]][[8]]>>
@

<<[[uni_uca_strcmp]](@sz)>>=
<<[[uni_uca_strcmp]][[<<@sz>>]] proto>>
{
  const uint<<@sz>>_t *rawa = a, *rawb = b;
  int raw_alen = alen, raw_blen = blen;
  uint32_t *duca, *ducb;
  unsigned int max_duca = 0, max_ducb = 0, duc_lena = 0, duc_lenb = 0;
  int do_literal = opts && opts->do_literal;
  int lev = opts && opts->max_level ? opts->max_level : 3;
  uni_ducet_lookup_state_t *sta = NULL, *stb = NULL;
  int has_l4 = lev > 3;

  if(lev > 4) {
    do_literal = 1;
    lev = 4;
  }
  if(opts) {
    uni_DUCET_lookup_opts(&sta, opts);
    uni_DUCET_lookup_opts(&stb, opts);
  }
  uint32_t lev123a, lev123b, lev4a, lev4b;
  int reta = UNI_DUCET_LOOKUP_OK, retb = UNI_DUCET_LOOKUP_OK;

  <<Get next key element for string [[a]]>>
  <<Get next key element for string [[b]]>>
  if(reta == UNI_DUCET_LOOKUP_NONE && retb == UNI_DUCET_LOOKUP_NONE)
    return do_literal ? uni_int_utf<<@sz>>_strcmp(rawa, raw_alen, rawb, raw_blen) : 0;
  inisize(duca, max_duca = 10);
  inisize(ducb, max_ducb = 10);
  while(1) {
    <<Scan to next non-zero level 1 in string [[a]]>>
    <<Scan to next non-zero level 1 in string [[b]]>>
    if(reta == UNI_DUCET_LOOKUP_NONE && retb == UNI_DUCET_LOOKUP_NONE) {
      int c = 0;
      if(lev > 1)
        c = uni_uca_ducet_cmp123(UNI_DUCET_LEV2_MASK, duca, duc_lena,
                                                      ducb, duc_lenb, has_l4,
						      opts && opts->reverse_lev2);
      if(lev > 2 && !c)
        c = uni_uca_ducet_cmp123(UNI_DUCET_LEV3_MASK, duca, duc_lena,
                                                      ducb, duc_lenb, has_l4, 0);
      if(lev > 3 && !c)
        c = uni_uca_ducet_cmp123(~0, duca + 1, duc_lena,
                                     ducb + 1, duc_lenb, 1, 0);
      free(duca);
      free(ducb);
      return c ? c : do_literal ? uni_int_utf<<@sz>>_strcmp(rawa, raw_alen,
                                                          rawb, raw_blen) : 0;
    }
    if(reta == UNI_DUCET_LOOKUP_NONE) {
      free(duca);
      free(ducb);
      return -1;
    }
    if(retb == UNI_DUCET_LOOKUP_NONE) {
      free(duca);
      free(ducb);
      return 1;
    }
    uint32_t l1a = lev123a & UNI_DUCET_LEV1_MASK,
             l1b = lev123b & UNI_DUCET_LEV1_MASK;
    if(l1a != l1b) {
      free(duca);
      free(ducb);
      return l1a < l1b ? -1 : 1;
    }
    lev123a &= ~UNI_DUCET_LEV1_MASK;
    lev123b &= ~UNI_DUCET_LEV1_MASK;
  }
}
@

<<Scan to next non-zero level 1 in string (@ab)>>=
while(ret<<@ab>> != UNI_DUCET_LOOKUP_NONE && !(lev123<<@ab>> & UNI_DUCET_LEV1_MASK)) {
  /* save l2+ keys obtained so far if non-zero */
  if(lev4<<@ab>> || lev123<<@ab>> & ~(UNI_DUCET_LEV1_MASK | 3)) {
    if(duc_len<<@ab>> >= max_duc<<@ab>> - 1)
      resize(duc<<@ab>>, max_duc<<@ab>> *= 2);
    duc<<@ab>>[duc_len<<@ab>>++] = lev123<<@ab>>;
    if(has_l4)
      duc<<@ab>>[duc_len<<@ab>>++] = lev4<<@ab>>;
  }
  <<Get next key element for string [[<<@ab>>]]>>
}
@

<<Get next key element for string (@ab)>>=
while(1) {
  uint32_t cp;
  if(!<<@ab>>len)
    cp = UNI_DUCET_LOOKUP_END;
  else if(ret<<@ab>> == UNI_DUCET_LOOKUP_AGAIN)
    cp = 0; /* any old random value */
  else {
    unsigned int clen;
    cp = uni_int_utf<<@sz>>_decode(<<@ab>>, &clen);
    <<@ab>> += clen;
    if(<<@ab>>len > 0)
      <<@ab>>len -= clen; /* underflow if <<@ab>>len wrong, but don't care */
    else if(!cp) {
      cp = UNI_DUCET_LOOKUP_END;
      <<@ab>>len = 0;
    }
  }
  ret<<@ab>> = uni_DUCET_lookup(cp, &lev123<<@ab>>, &lev4<<@ab>>, &st<<@ab>>);
  /* only ever return NONE if this was the last */
  if(ret<<@ab>> != UNI_DUCET_LOOKUP_NONE || cp == UNI_DUCET_LOOKUP_END)
    break;
}
@

The above code expects input strings to be in NFD normalization form.
This requires a single decomposition lookup, followed by multuple ccc
lookups and a sort.  However, most strings are already close enough to
being normalized that the DUCET lookup would work anyway.  In
particular, the standard allows a so-called Fast C or D form (FCD)
test.  This requires only the ccc of the first and last character of
each full canonical decomposition.  Since this property is not
provided directly in any other way, it is generated here.

<<Property parsed contents>>=
uni_chrrng_dat16_t *rng_dat16;
@

<<Initialize UCD files>>=
decl_num(FCD);
@

<<Parse UCD files>>=
prop_t *ccc_prop = &parsed_props[prop_ccc];
qsort(ccc_prop->rng_dat8, ccc_prop->len, sizeof(*ccc_prop->rng_dat8), uni_cmp_cp);
prop_t *FCD_prop = &parsed_props[prop_FCD = add_prop("FCD")];
FCD_prop->len = FCD_prop->max_len = ccc_prop->len;
inisize(FCD_prop->rng_dat16, FCD_prop->len);
for(i = 0; i < FCD_prop->len; i++) {
  FCD_prop->rng_dat16[i].low = ccc_prop->rng_dat8[i].low;
  FCD_prop->rng_dat16[i].high = ccc_prop->rng_dat8[i].high;
  FCD_prop->rng_dat16[i].dath = FCD_prop->rng_dat16[i].datl = ccc_prop->rng_dat8[i].dat;
}
for(i = 0; i < dmf_prop->len; i++) {
  uint8_t sccc, eccc;
  int l, h, m;
  uint32_t cp = dmf_prop->str_arr[i].cp;
  if(dmf_prop->str_arr[i].flags)
    continue;
  sccc = uni_chrrng_dat8(dmf_prop->strs[dmf_prop->str_arr[i].off],
                         ccc_prop->rng_dat8, ccc_prop->len, ccc_prop->def);
  eccc = uni_chrrng_dat8(dmf_prop->strs[dmf_prop->str_arr[i].off + 
                                        dmf_prop->str_arr[i].len - 1],
		         ccc_prop->rng_dat8, ccc_prop->len, ccc_prop->def);
  for(l = 0, h = FCD_prop->len - 1; l <= h; ) {
    m = (l + h) / 2;
    if(FCD_prop->rng_dat16[m].high < cp)
      l = m + 1;
    else if(FCD_prop->rng_dat16[m].low > cp)
      h = m - 1;
    else
      break;
  }
  if(l <= h) {
    if(sccc == FCD_prop->rng_dat16[m].dath &&
       eccc == FCD_prop->rng_dat16[m].datl)
      continue;
    if(FCD_prop->rng_dat16[m].low == FCD_prop->rng_dat16[m].high) {
      FCD_prop->rng_dat16[m].dath = sccc;
      FCD_prop->rng_dat16[m].datl = eccc;
      continue;
    }
    /* always ensure room for 2, even if only 1 needed */
    if(FCD_prop->len >= FCD_prop->max_len - 1)
      resize(FCD_prop->rng_dat16, FCD_prop->max_len *= 2);
    if(FCD_prop->rng_dat16[m].low == cp) {
      movebuf(FCD_prop->rng_dat16 + m + 1, FCD_prop->rng_dat16 + m,
              FCD_prop->len - m);
      FCD_prop->len++;
      FCD_prop->rng_dat16[m + 1].low++;
    } else if(FCD_prop->rng_dat16[m].high == cp) {
      movebuf(FCD_prop->rng_dat16 + m + 2, FCD_prop->rng_dat16 + m + 1,
              FCD_prop->len - m - 1);
      FCD_prop->len++;
      FCD_prop->rng_dat16[m++].high--;
    } else {
      movebuf(FCD_prop->rng_dat16 + m + 2, FCD_prop->rng_dat16 + m,
              FCD_prop->len - m);
      FCD_prop->len += 2;
      FCD_prop->rng_dat16[m++].high = cp - 1;
      FCD_prop->rng_dat16[m + 1].low = cp + 1;
    }
    FCD_prop->rng_dat16[m].low = FCD_prop->rng_dat16[m].high = cp;
    FCD_prop->rng_dat16[m].dath = sccc;
    FCD_prop->rng_dat16[m].datl = eccc;
  } else if(sccc || eccc) {
    if(FCD_prop->len == FCD_prop->max_len)
      resize(FCD_prop->rng_dat16, FCD_prop->max_len *= 2);
    movebuf(FCD_prop->rng_dat16 + l + 1, FCD_prop->rng_dat16 + l,
            FCD_prop->len - l);
    FCD_prop->len++;
    FCD_prop->rng_dat16[l].low = FCD_prop->rng_dat16[l].high = cp;
    FCD_prop->rng_dat16[l].dath = sccc;
    FCD_prop->rng_dat16[l].datl = eccc;
  }
}
@

Like the 8-bit and 32-bit data, a post-processing step fixes up the
tables, generates multi-level tables, and prints everything.  The
table testing program can ensure that the two representations are
equal, as well.

<<UCD parser local functions>>=
static void fixup_rng_dat16(prop_t *p)
{
  uint32_t i;
  qsort(p->rng_dat16, p->len, sizeof(uni_chrrng_dat16_t), uni_cmprng_dat16);
  /* starting at top means only optimized entries are memmove'd */
  for(i = p->len - 1; i > 0; i--) {
    uint32_t j = i;
    while(i > 0 && p->rng_dat16[i - 1].high == p->rng_dat16[i].low - 1 &&
          p->rng_dat16[i - 1].dath == p->rng_dat16[i].dath &&
          p->rng_dat16[i - 1].datl == p->rng_dat16[i].datl)
      i--;
    if(i == j)
      continue;
    p->rng_dat16[i].high = p->rng_dat16[j].high;
    if(j < p->len - 1)
        movebuf(p->rng_dat16 + i + 1, p->rng_dat16 + j + 1, p->len - (j + 1));
    p->len -= j - i;
    if(!i)
      break;
  }
}
@

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng_dat16) {
    fixup_rng_dat16(&parsed_props[i]);
    parsed_props[i].mt = uni_rng_dat16_to_multi(parsed_props[i].rng_dat16,
                                                parsed_props[i].len,
                                                &ml_len);
}
@

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng_dat16) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    char nbuf[64];
    sprintf(nbuf, "uni_%s_rng.gen.c", name);
    open_wf(of, nbuf);
    fprintf(of, "#include \"uni_prop.h\"\n\n"
                "const uni_chrrng_dat16_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].len; j++)
      fprintf(of, "\t{ 0x%04X, 0x%02X, 0x%04X, 0x%02X }%s\n",
                  parsed_props[i].rng_dat16[j].low,
                  parsed_props[i].rng_dat16[j].datl,
	          parsed_props[i].rng_dat16[j].high,
	          parsed_props[i].rng_dat16[j].dath,
		  j < parsed_props[i].len - 1 ? "," : "");
    fputs("};\n", of);
    fclose(of);
    fprintf(gen_h, "extern const uni_chrrng_dat16_t uni_%s_rng[];\n"
		   "#define uni_%s_rng_len %d /* %d lookups max */\n"
		   "#define uni_%s_of(x) uni_x_dat16(x, uni_%s_mtab)\n",
		   name, name, parsed_props[i].len,
		   lg2(parsed_props[i].len + 1), name, name);
    print_mtab(name, parsed_props[i].mt, gen_h);
    fprintf(tstf, "dat16(%s);\n", name);
  }
}
@

<<Unicode property exports>>=
uint16_t uni_x_dat16(uint32_t cp, const uint32_t *tab);
@

<<Unicode property functions>>=
uint16_t uni_x_dat16(uint32_t cp, const uint32_t *tab)
{
  const uint8_t *mr;
  uni_multi_tab_lookup(tab, cp * 2, &mr, 0);
  return mr ? *(uint16_t *)mr : 0;
}
@

<<Functions to help test generated tables>>=
#define dat16(x) doit_dat16(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab)

static void doit_dat16(const char *name, const uni_chrrng_dat16_t *rng, uint32_t nent,
                       const uint32_t *mtab)
{
    uint32_t i;

    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      uint16_t r = uni_chrrng_dat16(i, rng, nent);
      uint16_t m = uni_x_dat16(i, mtab);
      if(r != m) {
        fprintf(stderr, "mismatch %s@%d %d %d\n", name, i, (int)r, (int)m);
	exit(1);
      }
    }
    /* check performance */
    int j;
    unsigned long tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        uni_chrrng_dat16(i, rng, nent);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        uni_x_dat16(i, mtab);
    tt = tend();
    printf("  r%ld t%ld %.2fx\n", tr, tt, (double)tr / (double)tt);
}
@

<<Additional property type names>>=
UNI_PROP_TYPE_DAT16,
@

<<Set prop type for export>>=
if(parsed_props[i].rng_dat16)
  t = UNI_PROP_TYPE_DAT16;
@

It would probably be possible to skip normalization up to (but not
including) the last zero start or end ccc, and even to just do one
normalization step and continue the FCD test for the rest of the
string, but that is left as an exercise for the future.  For now, the
FCD algorithm is copied directly from the technical note, and is
therefore full-string only.

<<Unicode normalization support exports>>=
int uni_is_FCD32(const uint32_t *str, int len);
int uni_is_FCD16(const uint16_t *str, int len);
int uni_is_FCD8(const uint8_t *str, int len);
@

<<Unicode normalization support functions>>=
<<[[uni_is_FCD]][[32]]>>
<<[[uni_is_FCD]][[16]]>>
<<[[uni_is_FCD]][[8]]>>
@

<<[[uni_is_FCD]](@sz)>>=
int uni_is_FCD<<@sz>>(const uint<<@sz>>_t *str, int len)
{
  uint8_t prev = 0, curs;

  while(len) {
    unsigned int clen;
    uint32_t cp = uni_int_utf<<@sz>>_decode(str, &clen);
    if(len < 0 && !cp)
      break;
    uint16_t both = uni_FCD_of(cp);
    curs = both >> 8;
    if(curs && curs < prev)
      return 0;
    prev = both & 0xff;
    len -= clen; /* underflow if clen incorrect, but I don't care */
    str += clen;
  }
  return 1;
}
@

To test collation, the CollationTest files are used.  As with the
normalization tests, the files are simply fed into standard input.
There are four separate files, and each requires a different
configuration.  These are selected using command-line arguments.  The
options are DUCET vs. CLDR DUCET, and variable mode non-ignorable vs.
variable mode shifted.

\lstset{language=make}
<<C Test Support Executables>>=
tstuca \
@

<<Additional Tests>>=
./tstuca <$(UCD_LOC)/CollationTest/CollationTest_NON_IGNORABLE.txt
./tstuca -s <$(UCD_LOC)/CollationTest/CollationTest_SHIFTED.txt
./tstuca -c <$(UCD_LOC)/CollationAuxiliary/CollationTest_CLDR_NON_IGNORABLE.txt
./tstuca -cs <$(UCD_LOC)/CollationAuxiliary/CollationTest_CLDR_SHIFTED.txt
@

\lstset{language=C}
<<tstuca.c>>=
<<Common C Header>>
#include "uni_all.h"
#include "mfgets.h"
#include "mallocdef.h"

<<UCA test support>>

int main(int argc, const char **argv)
{
  int do_cldr = 0, do_shift = 0, pr_keys = 0;
  while(argc-- > 1) {
    if(**++argv == '-') {
      const char *s = *argv;
      while(*++s) {
        if(*s == 'c')
	  do_cldr = 1;
	else if(*s == 's')
	  do_shift = 1;
	else if(*s == 'v')
	  pr_keys = 1;
      }
    }
  }
  <<Read and process CollationTest.txt>>
  return 0;
}
@

<<Read and process CollationTest.txt>>=
unsigned int fcd_saved = 0, tot = 0;
uni_uca_opts_t opts;
clearbuf(&opts, 1);
if(do_cldr) {
  opts.tab = uni_DUCET_CLDR_mtab;
  opts.strs = uni_DUCET_CLDR_strs;
  opts.var_top = uni_DUCET_CLDR_var_top;
}
if(do_shift) {
  opts.var_mode = UNI_DUCET_VAR_MODE_SHIFTED;
  opts.max_level = 4;
} else {
  opts.var_mode = UNI_DUCET_VAR_MODE_NON_IGNORABLE;
  opts.max_level = 3; /* undocumented requirement, apparently */
}
opts.do_literal = 1;
char *lbuf = NULL, *s;
unsigned int lbuflen, llen;
uint32_t *buf, *key, *fcd;
unsigned int buf_len, max_buf, key_len, fcd_len;
uint32_t *prev_key = NULL, *prev_buf = NULL;
unsigned int prev_buf_len = 0, prev_key_len = 0;
unsigned int i;
inisize(buf, (max_buf = 10));
#if 1 /* ensure key in comment matches generated key */
char *kbuf;
unsigned int kbuflen;
inisize(kbuf, (kbuflen = 800)); /* just make it big enough rather than resizing */
#endif
while(mfgets(&lbuf, &lbuflen, &llen, 0, stdin)) {
  if(!isxdigit(lbuf[0]))
    continue;
  buf[0] = strtol(lbuf, &s, 16);
  buf_len = 1;
  while(1) {
    while(isspace(*s))
      s++;
    if(!isxdigit(*s))
      break;
    if(buf_len == max_buf)
      resize(buf, (max_buf *= 2));
    buf[buf_len++] = strtol(s, &s, 16);
  }
  tot++;
  if(uni_is_FCD32(buf, buf_len)) {
    fcd_saved++;
    fcd = uni_str_ducet32(buf, buf_len, &opts, &fcd_len, NULL);
  } else
    fcd = NULL;
  /* needs to be NFD for literal append */
  buf_len = uni_NFDt32(&buf, 0, &max_buf, buf_len);
  key = uni_str_ducet32(buf, buf_len, &opts, &key_len, NULL);
  if(fcd && (fcd_len != key_len || memcmp(fcd, key, fcd_len * 4))) {
    fprintf(stderr, "key/fcd mismatch\n%s\n", lbuf);
    s = build_kbuf1234(kbuf, fcd, fcd_len, opts.max_level > 3);
    strcpy(s, "]\n");
    fputs(kbuf, stderr);
    s = build_kbuf1234(kbuf, key, key_len, opts.max_level > 3);
    strcpy(s, "]\n");
    fputs(kbuf, stderr);
    /* CLDR data does not conform! */
    if(!do_cldr)
      exit(1);
  } else
    free(fcd);
#if 1 /* build string to match text in comment */
  s = build_kbuf1234(kbuf, key, key_len, opts.max_level > 3);
  if(pr_keys)
    fputs(kbuf, stdout);
  /* literal */
  strcpy(s, "|]"); /* error in data file: assumes lit level is empty */
  if(pr_keys) {
    fputs("| ", stdout);
    for(i = 0; i < buf_len; i++)
      printf("%04X ", (int)buf[i]);
    puts("]");
  }
  /* comment in test file has UCA key at end of line, formatted as above */
  {
    s = strrchr(lbuf, '[');
    strrchr(s, ']')[1] = 0;
    if(strcmp(s, kbuf)) {
      fputs("comment mismatch\n", stderr);
      for(i = 0; i < key_len; i++)
        fprintf(stderr, "%08X ", key[i]);
      fprintf(stderr, "%s %s\n%s", kbuf, s, lbuf);
      exit(1);
    }
  }
#endif
  if(prev_key) {
    /* note: can't use memcmp except on big-endian machines */
    int c = uni_uca_ducet_cmp(prev_key, prev_key_len, key, key_len, &opts);
    if(!c)
      c = uni_int_utf32_strcmp(prev_buf, prev_buf_len, buf, buf_len);
    if(c > 0) {
      fflush(stdout);
      fprintf(stderr, "keys are out of order at %s\n", lbuf);
      exit(1);
    }
    if(uni_uca_strcmp32(prev_buf, prev_buf_len, buf, buf_len, &opts) > 0) {
      fflush(stdout);
      fprintf(stderr, "strcmp error at %s\n", lbuf);
      exit(1);
    }
    free(prev_key);
    free(prev_buf);
  }
  prev_key = key;
  inisize(prev_buf, buf_len);
  prev_buf_len = buf_len;
  cpybuf(prev_buf, buf, buf_len);
}
if(prev_key) {
  free(prev_key);
  free(prev_buf);
}
printf("%u keys generated; could have skipped NFD on %u tests\n",
       tot, fcd_saved);
@

<<UCA test support>>=
static char *build_kbuf1234(char *s, const uint32_t *key, unsigned int key_len,
                            int has_l4)
{
  unsigned int i;

  *s++ = '[';
  /* lev 1 */
  for(i = 0; i < key_len; i++) {
    uint32_t l1 = (key[i] & UNI_DUCET_LEV1_MASK) >> UNI_DUCET_LEV1_SHIFT;
    if(l1)
      s += sprintf(s, "%04X ", (int)l1);
    if(has_l4)
      i++;
  }
  *s++ = '|';
  *s++ = ' ';
  /* lev 2 */
  for(i = 0; i < key_len; i++) {
    uint32_t l2 = (key[i] & UNI_DUCET_LEV2_MASK) >> UNI_DUCET_LEV2_SHIFT;
    if(l2)
      s += sprintf(s, "%04X ", (int)l2);
    if(has_l4)
      i++;
  }
  *s++ = '|';
  *s++ = ' ';
  /* lev 3 */
  for(i = 0; i < key_len; i++) {
    uint32_t l3 = (key[i] & UNI_DUCET_LEV3_MASK) >> UNI_DUCET_LEV3_SHIFT;
    if(l3)
      s += sprintf(s, "%04X ", (int)l3);
    if(has_l4)
      i++;
  }
  if(has_l4) {
    *s++ = '|';
    *s++ = ' ';
    /* lev 4 */
    for(i = 1; i < key_len; i += 2) {
      uint32_t l4 = key[i];
      if(l4)
        s += sprintf(s, "%04X ", (int)l4);
    }
  }
  return s;
}
@

Since the standard tests only test Shifted and Non-ignorable variable
weighting options, a sample program to exercise the other options is
provided.  It takes the numeric variable treatment type as its first
argument (zero if not present), and if a second argument is present,
the search keys are printed as well.  There is no standard test data;
it must be run and checked manually.  This is not the way to sort a
file.  However, it is sufficient for running a small bit of text
through its paces.

<<C Test Support Executables>>=
tstucavar \
@

<<tstucavar.c>>=
<<Common C Header>>
#include "uni_all.h"
#include "mfgets.h"
#include "mallocdef.h"

static uni_uca_opts_t opts;

struct ustr {
  uint32_t *str, len;
};

static int uca_cmp(const void *_a, const void *_b)
{
  const struct ustr *a = _a, *b = _b;
  return uni_uca_strcmp32(a->str, a->len, b->str, b->len, &opts);
}

int main(int argc, const char **argv)
{
  clearbuf(&opts, 1);
  opts.max_level = 5;
  if(argc > 1)
    opts.var_mode = atoi(argv[1]);
  int pr_keys = argc > 2;
  char *lbuf = NULL, *s;
  unsigned int lbuflen, llen;
  uint32_t *buf32, buf32len, buf32max;
  inisize(buf32, (buf32max = 10));
  struct ustr *strs;
  int nstrs, maxstrs;
  inisize(strs, (maxstrs = 10));
  nstrs = 0;
  while(mfgets(&lbuf, &lbuflen, &llen, 0, stdin)) {
    /* assume utf-8 input */
    if(!*lbuf || *lbuf == '\n')
      continue;
    for(s = lbuf, buf32len = 0; *s; buf32len++) {
      unsigned int len;
      uint32_t c = uni_utf8_decode((uint8_t *)s, &len);
      if(c == '\n')
        break;
      if(buf32len == buf32max)
        resize(buf32, (buf32max *= 2));
      buf32[buf32len] = c;
      s += len;
    }
    if(nstrs == maxstrs)
      resize(strs, (maxstrs *= 2));
    strs[nstrs].str = malloc(buf32len * 4);
    strs[nstrs].len = buf32len;
    cpybuf(strs[nstrs].str, buf32, buf32len);
    nstrs++;
  }
  int i;
  if(pr_keys)
    for(i = 0; i < nstrs; i++) {
      uint32_t *key;
      unsigned int key_len, j;
      uni_utf8_fputs(strs[i].str, strs[i].len, stdout);
      key = uni_str_ducet32(strs[i].str, strs[i].len, &opts, &key_len, NULL);
      for(j = 0; j < key_len; j += 2)
        printf(" %04X", (key[j] & UNI_DUCET_LEV1_MASK) >> UNI_DUCET_LEV1_SHIFT);
      putchar('|');
      for(j = 0; j < key_len; j += 2)
        printf(" %04X", (key[j] & UNI_DUCET_LEV2_MASK) >> UNI_DUCET_LEV2_SHIFT);
      putchar('|');
      for(j = 0; j < key_len; j += 2)
        printf(" %04X", (key[j] & UNI_DUCET_LEV3_MASK) >> UNI_DUCET_LEV3_SHIFT);
      putchar('|');
      for(j = 0; j < key_len; j += 2)
        printf(" %04X", key[j + 1]);
      putchar('\n');
    }
  qsort(strs, nstrs, sizeof(*strs), uca_cmp);
  for(i = 0; i < nstrs; i++) {
    uni_utf8_fputs(strs[i].str, strs[i].len, stdout);
    putchar('\n');
  }
  return 0;
}
@

There is a table titled ``Comparison of Variable Ordering'' in the
standard; it is a good start.  It looks very similar to this (headers
have been shortened for formatting):

\lstset{language=txt}
{\let\Tt\unimono
<<varordtab.txt>>=
non-Ign	Blanked	Shifted	IgnSP	Shft-Tr
de luge	death	death	death	death
de Luge	de luge	de luge	de luge	deluge
de-luge	de-luge	de-luge	de-luge	de luge
de-Luge	deluge	de‐luge	de‐luge	de-luge
de‐luge	de‐luge	deluge	deluge	de‐luge
de‐Luge	de Luge	de Luge	de Luge	deLuge
death	de-Luge	de-Luge	de-Luge	de Luge
deluge	deLuge	de‐Luge	de‐Luge	de-Luge
deLuge	de‐Luge	deLuge	deLuge	de‐Luge
demark	demark	demark	demark	demark
				
☠happy	☠happy	☠happy	☠happy	☠happy
☠sad	♡happy	♡happy	☠sad	♡happy
♡happy	☠sad	☠sad	♡happy	☠sad
♡sad	♡sad	♡sad	♡sad	♡sad
				
@
}

To make the above table appear correctly, this document needs to
switch to UTF-8 mode.  The PDF fonts look a little worse, but at least
such files can display.  In addition, the skull-and-crossbones and
heart characters are not in the default font (Latin Modern), so
something else needs to be used.  I use DejaVu Sans Mono; see the
this noweb file's header for details.

\lstset{language=make}
<<makefile.vars>>=
uni.pdf uni.html: NW_UTF8=1
@

The following script generates a table like the above, given one or
more files to sort.  When more than one file is sorted, a blank line
is printed between them.  [[<<varord1.txt>>]] and [[<<varord2.txt>>]]
correspond to the two lists being sorted in the above table.  In order
to extract the table properly, tabs need to be expanded for it.

<<Test Support Scripts>>=
tstucavartab \
@

<<Plain Build Files>>=
varord1.txt varord2.txt varordtab.txt \
@

<<makefile.vars>>=
varordtab.txt: NOTANGLE_OPTS=-t8
@

<<makefile.rules>>=
test: varord1.txt varord2.txt varordtab.txt
@

\lstset{language=sh}
<<tstucavartab>>=
#!/bin/sh

t="non-Ign Blanked Shifted IgnSP Shft-Tr"
i=0
p=

for tt in $t; do
  (
    echo $tt
    for x; do
      ./tstucavar $i < $x
      echo
    done
  ) > out$$$i
  p="$p out$$$i"
  i=$((i+1))
done
eval "paste $p"
eval "rm $p"
@

<<Additional Tests>>=
./tstucavartab varord[12].txt | diff - varordtab.txt
@

\lstset{language=txt}
<<varord1.txt>>=
de luge
de Luge
de-luge
de-Luge
de‐luge
de‐Luge
death
deluge
deLuge
demark
@

{\let\Tt\unimono
<<varord2.txt>>=
☠happy
☠sad
♡happy
♡sad
@
}

While implementation of the UCA algorithm with the raw tables was not
difficult, there are a few desirable functions which cannot be
accomplished with this data format:%
\footnote{In fact, the first two are the very reason this library
exists.  All of the prior information can be obtained from other
libraries, and in fact the sort keys can be obtained with Single UNIX
Specification functions.  However, no library to my knowledge gives
collation classes or a list of collation elements.  The lack of
locale-specific character classes is less important, since they can at
least be simulated using regular expressions.}

\begin{itemize}
\item Given a collation element, list all collation elements which
have the same key (i.e., determine its collation equivalence class).
This is required for regular expressions.
\item Given two collation elements, list all collation elements which
lie between them.  This is required for regular expressions.
\item Reorder elements.  This is required for static CLDR support.
\item Reorder element blocks.  This is required for static and dynamic
CLDR support.
\end{itemize}

At the minimum, these functions all require a lookup table ordered by
sort key rather than collation element.  Also, the canonical
decomposition entries are there solely to support the FCD shortcut, so
they are dropped (moved to the top by the sort, and then dropped
post-sort, actually).  It would be much more pleasant to drop the
composition entries as well, but they are legitimate multi-character
collation elements.

Similar to the UCA, the sort compares each level separately, ignoring
zeroes.  The first level to mismatch determines the order.

\lstset{language=C}
<<DUCET parser globals>>=
<<DUCET lookup format defs>>

static int cmp_ducet_lev123(const uint32_t *kx, uint32_t xlen,
                            const uint32_t *ky, uint32_t ylen,
			    const uint32_t mask)
{
  uint32_t i = 0, j = 0;
  while(i < xlen && j < ylen) {
    while(i < xlen && !(kx[i] & mask))
      i += 1 + ((kx[i] & 3) == 2);
    while(j < ylen && !(ky[j] & mask))
      j += 1 + ((ky[j] & 3) == 2);
    if(j == ylen)
      return i != xlen; /* 0 if both at end, 1 otherwise */
    if(i == xlen)
      return -1;
    if((kx[i] & mask) > (ky[j] & mask))
      return 1;
    if((kx[i] & mask) < (ky[j] & mask))
      return -1;
    i += 1 + ((kx[i] & 3) == 2);
    j += 1 + ((ky[j] & 3) == 2);
  }
  while(i < xlen && !(kx[i] & mask))
    i += 1 + ((kx[i] & 3) == 2);
  while(j < ylen && !(ky[j] & mask))
    j += 1 + ((ky[j] & 3) == 2);
  return j == ylen ? i != xlen : -1;
}
@

<<DUCET parser globals>>=
static int cmp_by_key(const void *a, const void *b)
{
  const raw_cp_str_t *x = a, *y = b;
  const uint32_t *sx = ducet_strs + x->off, *kx,
                 *sy = ducet_strs + y->off, *ky;
  uint32_t xlen = x->len, ylen = y->len;
  /* push decomps to the top */
  if(!xlen || !ylen)
    return xlen ? -1 : ylen ? 1 : 0;
  /* find end of index extension */
  for(kx = sx; (*kx & 3) == 3; kx++, xlen--);
  for(ky = sy; (*ky & 3) == 3; ky++, ylen--);
  /* always compare level 1 on all keys, then 2 on all keys, etc. */
  int c = cmp_ducet_lev123(kx, xlen, ky, ylen, UNI_DUCET_LEV1_MASK);
  if(c)
    return c;
  c = cmp_ducet_lev123(kx, xlen, ky, ylen, UNI_DUCET_LEV2_MASK);
  if(c)
    return c;
  c = cmp_ducet_lev123(kx, xlen, ky, ylen, UNI_DUCET_LEV3_MASK);
  if(c)
    return c;
  /* level 4 */
  while(xlen && ylen) {
    uint32_t x4, y4;
    switch(*kx & 3) {
      case 0:
        x4 = 0;
	break;
      case 1:
        x4 = x->cp;
	break;
      default: /* can never be 3 */
      /* case 2: */
        x4 = *++kx;
	xlen--;
	break;
    }
    switch(*ky & 3) {
      case 0:
        y4 = 0;
	break;
      case 1:
        y4 = x->cp;
	break;
      default: /* can never be 3 */
      /* case 2: */
        y4 = *++ky;
	ylen--;
	break;
    }
    if(x4 && y4) {
      if(x4 < y4)
        return -1;
      if(x4 > y4)
        return 1;
      x4 = y4 = 0; /* skip */
    }
    if(!x4) {
      kx++;
      xlen--;
    }
    if(!y4) {
      ky++;
      ylen--;
    }
  }
  while(xlen && !(*kx & 3)) {
    kx++;
    xlen--;
  }
  while(ylen && !(*ky & 3)) {
    ky++;
    ylen--;
  }
  if(xlen)
    return 1;
  if(ylen)
    return -1;
  return 0;
}
@

<<Post-process DUCET>>=
qsort(raw_ents, raw_ents_size, sizeof(*raw_ents), cmp_by_key);
/* remove decomp entries */
while(raw_ents_size > 0 && !raw_ents[raw_ents_size - 1].len)
  raw_ents_size--;
@

After sorting, the index and value should be swapped.  Using the full
51-bit key (30 for levels 1-3, plus up to 21 for level 4) as an index
is impractical.  Instead, much like the forward table, there should be
one ``character'' for the index, followed by ``index extensions''
containing the rest of the key.  In this case, the first level is the
most important, so that is the primary index.  The format of the
string at that index is similar to the forward lookup table:  the
first two bits indicate if something is part of the index extension,
or part of the value for that index.  A 3 in the lower two bits
indicates part of the value this time.  Unlike the forward table, the
value must include the first character, and there is little value in
suppressing the first index ``character'' in the ``index extension,''
so the ``index extension'' is the full key.

<<Post-process DUCET>>=
uint32_t *rev_strs, max_rev_strs, rev_strs_len = 0;
inisize(rev_strs, max_rev_strs = 2 * raw_ents_size);
for(i = raw_ents_size - 1; /* i >= 0 */ ; i--) {
  const uint32_t *si = prop->strs + raw_ents[i].off, *kpi, *sj, *kpj;
  int len = raw_ents[i].len + 1; /* + 1 for cp */
  for(kpi = si; (*kpi & 3) == 3; kpi++);
  for(j = i; j > 0; j--) {
    sj = prop->strs + raw_ents[j - 1].off;
    for(kpj = sj; (*kpj & 3) == 3; kpj++);
    if((*kpj & UNI_DUCET_LEV1_MASK) != (*kpi & UNI_DUCET_LEV1_MASK))
      break;
    len += raw_ents[j - 1].len + 1; /* + 1 for cp */
  }
  /* alread sorted by full index, so just append them all */
  while(max_rev_strs < rev_strs_len + len)
    resize(rev_strs, max_rev_strs *= 2);
  uint32_t *dp = rev_strs + rev_strs_len;
  for(low = j; j <= i; j++) {
    sj = prop->strs + raw_ents[j].off;
    /* place key (new index extension) first */
    for(kpj = sj; (*kpj & 3) == 3; kpj++);
    cpybuf(dp, kpj, raw_ents[j].len - (kpj - sj));
    dp += raw_ents[j].len - (kpj - sj);
    /* then append full old index (including cp) */
    *dp++ = (raw_ents[j].cp << 2) + 3;
    cpybuf(dp, sj, kpj - sj);
    dp += kpj - sj;
  }
  raw_ents[low].cp = *kpi >> UNI_DUCET_LEV1_SHIFT;
  raw_ents[low].off = rev_strs_len;
  raw_ents[low].len = len;
  rev_strs_len += len;
  if(low != i)
    movebuf(raw_ents + low + 1, raw_ents + i + 1, raw_ents_size - i - 1);
  raw_ents_size -= i - low;
  if(!(i = low))
    break;
}
@

This transformation alone is already enough to implement the regular
expression requirements, so a property is created using the results.
The string table is rather large (58,099 words, with no redundancy due
to the initial cp differing for each entry), but still barely small
enough for 16-bit offsets.

<<Post-process DUCET>>=
char rname[strlen(prop->name) + 5];
memcpy(rname, "rev_", 4);
strcpy(rname + 4, prop->name);
prop_t *rprop = &parsed_props[add_prop(rname)];
rprop->str_arr = raw_ents;
rprop->len = raw_ents_size;
rprop->strs = rev_strs;
rprop->max_strs = max_rev_strs;
rprop->strs_len = rev_strs_len;
rprop->mt = (uint32_t *)~0; /* force addition to prop table */
rprop->strs_char_size = 32;
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[add_prop("rev_DUCET")], gen_h, tstf);
dump_str_tabs(&parsed_props[add_prop("rev_DUCET_CLDR")], gen_h, tstf);
@

\lstset{language=txt}
<<FIXME>>=
Make function to look up equiv. class:
  - do forward lookup of key
  - do reverse lookup
    - support official synthetic keys
Make function to look up range:
  - again, forward lookup of key
  - do reverse lookup
    - optionally for each end, sort equal entries by cp & start there
    - optionally for each end, instead just include all entries w/ equal key
@

<<FIXME>>=
Make way to modify tables on the fly.  Either supply a customization
  table separately or fully customize existing table?
  E.g.
    a <1 a":
       a: 15EF.0020.0008.0041
       ->
       a: 15EF.0020.0002.0061 0001.0000.0000.0000
       a": 15EF.0020.0002.0061 0002.0000.0000.0000
       [.a.]: <old> 0001.0000.0000.0000
Mark groups for CLDR group reorder function
   note that locale-default reordering is overridden by kr- reordering
   space punct symbol currency digit others/Zzzz script-name
     script-name limited to UAX#31, table 5 except Katakana, Common, Inherited
     Katakana always moves with Hiragana
     Non-UAX31/t5 scripts ordered w/ previous script that is in table
     an ordering sequence w/o space, punct, symbol, currency, and/or
       digit has those in same order before given sequence.  All other
       unmentioned's become "others" and are appended if "others" not
       explicit.
       example: {latn,digit} becomes
                {space,punct,symbol,currency,latn,digit,others}
  Then cldr uca function could do remapping on the fly (expensively?):
   e.g. g .. h reorders before e in a .. j:
      a# .. d# unmodified
      e# .. g# - 1 += (h# - g# + 1)
      g# .. h# -= (g# - e#)
      i# .. j# unmodified
         pass g#, h#, e# triplet for each reordering
	 subseqent reorderings have same modifications as above.
	 e.g.  (e#, j#, j#) changes to (e# + h# - g# + 1, j#, j#)
Mark groups for logical positioning in locale reordering
  {first|last}_{primary_|secondary_|tertiary_|non-}ignorable
  {first|last}_{variable|trailing}
    variable top refers to default, not user setting
@

\subsection{Parsing the UCD -- Others}

[[BidiMirroring.txt]] contains the last officially supported string
property.  However, like the simple case conversion properties, it
always returns just a single code point, and is therefore encoded as a
numeric property.

\lstset{language=C}
<<Initialize UCD files>>=
decl_num(bmg);
@

<<Parse UCD files>>=
open_f("BidiMirroring.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_num(bmg, strtol(fields[1], NULL, 16), 1);
}
fclose(f);
@

While scx is officially an ``Other'' property, it can be treated just
like a string.  The difference is that the string elements are script
enumeration values, rather than code points.  The values fit in a
byte, so it's tempting to encode it as 8 bits, but there are zeroes in
the values, which makes the string table dumper sometimes strip them.
The table is small enough that the savings wouldn't be great, anyway.

<<Initialize UCD files>>=
decl_str(scx);
@

<<Parse UCD files>>=
open_f("ScriptExtensions.txt");
prop_scx = add_prop("scx");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  uint32_t str[32], len = 0;
  <<Parse dotted cp>>
  s = fields[1];
  while(1) {
    char *n = strchr(s, ' ');
    if(n)
      *n = 0;
    str[len++] = enum_val(prop_sc, s);
    if(!n)
      break;
    s = n + 1;
  }
  if(prop_scx < 0)
    prop_scx = add_prop("scx");
  add_str_rng(&parsed_props[prop_scx], low, high, str, len);
}
fclose(f);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_scx], gen_h, tstf);
@

<<Parse UCD files>>=
parsed_props[prop_scx].mt = (uint32_t *)~0; /* force addition to prop table */
@

\lstset{language=txt}
<<FIXME>>=
Change 8-bit array format to require byte length instead of word
length, so scx can be dumped 8-bit (as well as any others with 0 in data)
@

<<FIXME>>=
EmojiSources.txt
  field 1 = index (may be multi-char)
  field 2 = 16-bit DoCoMo Shift-JIS code
  field 3 = 16-bit KDDI Shift-JIS code
  field 4 = 16-bit SoftBank Shift-JIS code
  [may add more fields in the future]
 encoding: all 3 fields as 16-bit numbers (0 if absent), followed by index ext
   # of fields is given as #define
   can't really provide vendor name to field # mapping w/o parsing comments
     '# <n - 1>: <vendor> Shift-JIS code'
 property name = ???
StandardizedVariants.txt
  Field 2: English descriptive text; probably not really useful for computers
  Field 3 could be enumerated, but is useless w/o field 2
USourceData.txt: [note: uses cr for line termination]
  Hard to say if it's useful outside of IRG.  Index is U-source ID,
  not Unicode code point (UTC-x or UCI-x).  See tr45.
@

Unofficially, [[Unihan_Variants]] provides some properties as well.
Their fields are formatted differently, with U+ prefixes on every code
point.  Of these properties, only cjkCompatibilityVariant has an
official property name entry.  Only Traditional, Simplified, and
Compatibility variants are needed for regular expressions.  The others
(Semantic, SpecializedSemantic, and Z) will not be read into a
property unless I find a use for them.  Part of the reason for this is
that these properties contain additional information, in the form of
providence annotations.  The annotations could simply be dropped and
those properties added without issue (after all, static linking
ensures their removal if unused), but for now, they are left out.

\lstset{language=C}
<<UCD parser local definitions>>=
#define add_hstr(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[10]; \
    uint32_t len; \
    for(s = v, len = 0; *s; len++) { \
      while(isspace(*s)) s++; \
      if(*s == 'U') \
        s += 2; \
      str[len] = strtol(s, &s, 16); \
    } \
    add_str_rng(&parsed_props[prop_##n], low, high, str, len); \
  } \
} while(0)
@

<<Initialize Unihan files>>=
decl_str(cjkTraditionalVariant);
decl_str(cjkSimplifiedVariant);
decl_str(cjkCompatibilityVariant);
@

<<Parse Unihan files>>=
open_f("Unihan_Variants.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse Unihan cp>>
  if(!strcmp(fields[1], "kTraditionalVariant"))
    add_hstr(cjkTraditionalVariant, fields[2]);
  else if(!strcmp(fields[1], "kSimplifiedVariant"))
    add_hstr(cjkSimplifiedVariant, fields[2]);
  else if(!strcmp(fields[1], "kCompatibilityVariant"))
    add_hstr(cjkCompatibilityVariant, fields[2]);
}
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_cjkTraditionalVariant], gen_h, tstf);
dump_str_tabs(&parsed_props[prop_cjkSimplifiedVariant], gen_h, tstf);
dump_str_tabs(&parsed_props[prop_cjkCompatibilityVariant], gen_h, tstf);
@

<<Parse Unihan files>>=
/* force addition to prop table */
parsed_props[prop_cjkTraditionalVariant].mt = (uint32_t *)~0;
parsed_props[prop_cjkSimplifiedVariant].mt = (uint32_t *)~0;
parsed_props[prop_cjkCompatibilityVariant].mt = (uint32_t *)~0;
@

\lstset{language=txt}
<<FIXME>>=
Unihan_DictionaryLikeData.txt:  kCangjie, kCheungBaur, kHDZRadBreak
Unihan_Readings.txt: kCantonese, kDefinition, ...
@

Another pair of optional string values comes from the IDNA compatiblity
database%
\footnote{Actually, there is a third field (IDNA2008 status), but it
is informative, and will not be read in as a property unless I find a
use.}%
.  This is available from a separate location
(\url{http://www.unicode.org/Public/idna/}), but is only checked for
in the UCD directory and a few minor variants.  If not present, it is
ignored.  The IDNA\_Status property is technically an enumeration, but
since it has no value aliases, it is instead accumulated as a string
property.  It is then converted to an enumeration.

\lstset{language=C}
<<Initialize UCD files>>=
decl_str(IDNA_Status);
decl_str(IDNA_Mapping);
@

<<Parse UCD files>>=
if((f = fopen("IdnaMappingTable.txt", "r")) ||
   (f = fopen("idna/IdnaMappingTable.txt", "r")) ||
   (f = fopen("../idna/IdnaMappingTable.txt", "r"))) {
  while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    <<Parse dotted cp>>
    add_str8(IDNA_Status, fields[1]);
    if(num_fields > 2)
      add_str(IDNA_Mapping, fields[2]);
  }
  fclose(f);
  str_to_enum(&parsed_props[prop_IDNA_Status], "disallowed");
}
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_IDNA_Mapping], gen_h, tstf);
@

<<Parse UCD files>>=
parsed_props[prop_IDNA_Mapping].mt = (uint32_t *)~0; /* force addition to prop table */
@

<<UCD parser local functions>>=
static void str_to_enum(prop_t *p, const char *def)
{
  uint32_t i, poff = 0, plen = 0, pnum = (uint32_t)(p - parsed_props);
  int32_t seq = -1;

  merge_strs(p);
  p->def = def ? ~0 : 0;
  inisize(val_aliases[pnum], max_val_aliases[pnum] = 5);
  inisize(p->rng_dat8, p->len);
  for(i = 0; i < p->len; i++) {
    if(p->str_arr[i].off == poff && p->str_arr[i].len == plen) {
      p->rng_dat8[i].low = p->rng_dat8[i].high = p->str_arr[i].cp;
      p->rng_dat8[i].dat = seq;
      continue;
    }
    seq++;
    poff = p->str_arr[i].off;
    plen = p->str_arr[i].len;
    if(num_val_aliases[pnum] == max_val_aliases[pnum])
      resize(val_aliases[pnum], max_val_aliases[pnum] *= 2);
    {
      uni_alias_t *va = &val_aliases[pnum][num_val_aliases[pnum]];
      char *n;
      inisize(n, plen * 4 + 1);
      memcpy(n, (char *)(p->strs + poff), plen * 4);
      n[plen * 4] = 0;
      if(def && !strcmp(n, def))
        p->def = seq;
      va->short_name = va->long_name = n;
      va->alt_name = va->alt_name2 = NULL;
      num_val_aliases[pnum]++;
    }
    i--; /* re-run for current item to actually add it */
  }
  if(p->def == (uint8_t)~0) {
    p->def = seq;
    if(num_val_aliases[pnum] == max_val_aliases[pnum])
      resize(val_aliases[pnum], max_val_aliases[pnum] *= 2);
    {
      uni_alias_t *va = &val_aliases[pnum][num_val_aliases[pnum]];
      va->short_name = va->long_name = strdup(def);
      va->alt_name = va->alt_name2 = NULL;
      num_val_aliases[pnum]++;
    }
  }
  inisize(enum_vals[pnum], enum_vals_len[pnum] = num_val_aliases[pnum]);
  for(i = 0; i < enum_vals_len[pnum]; i++) {
    enum_vals[pnum][i].name = strdup(val_aliases[pnum][i].short_name);
    enum_vals[pnum][i].val = i;
  }
  /* while strs was sorted, it was sorted by long word, not byte */
  /* this sort is really only necessary on little-endian systems */
  qsort(enum_vals[pnum], enum_vals_len[pnum], sizeof(*enum_vals[pnum]),
        uni_cmp_valueof);
  free(p->strs);
  p->strs = NULL;
  free(p->str_arr);
  p->str_arr = NULL;
}
@

Similarly, the security information database, available separately at
\url{http://www.unicode.org/Public/security/}, contains two optional
enumeration properties without value aliases.

<<Initialize UCD files>>=
decl_str(ID_Restrict_Status);
decl_str(ID_Restrict_Type);
@

<<Parse UCD files>>=
if((f = fopen("xidmodifications.txt", "r")) ||
   (f = fopen("security/xidmodifications.txt", "r")) ||
   (f = fopen("../security/xidmodifications.txt", "r"))) {
  while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    <<Parse dotted cp>>
    add_str8(ID_Restrict_Status, fields[1]);
    if(num_fields > 2)
      add_str8(ID_Restrict_Type, fields[2]);
  }
  fclose(f);
  str_to_enum(&parsed_props[prop_ID_Restrict_Status], "restricted");
  str_to_enum(&parsed_props[prop_ID_Restrict_Type], "not-chars");
}
@

\lstset{language=txt}
<<FIXME>>=
security/intentional.txt: could be numeric (char-to-char mapping) (propname?)
security/confusables.txt: propname? 4 props or field 3 as part of value?
security/confusablesWholeScript.txt: propname?
  could be plain 32-bit value: <from><to><type>, but there are dup indices
@

\chapter{XML Data Files}

A number of data files outside of the UCD are encoded in XML.  These
include in particular the CLDR files.  One way to deal with this would
be to turn the files into files more like the rest of the UCD using
CLDR.  In fact, an earlier version of this library did just that for
the single XML file it supported.  The CLDR has many more files,
though, and not all are in the same format, so something else needs to
be done.  Since everything else is being parsed in parse-ucd,
parse-ucd will read and parse the XML files directly, usoing libxml2.%
\footnote{\url{http://xmlsoft.org/}}
For maximum simplicity, each file is simply read in using the
document-at-once
parser\footnote{\url{http://xmlsoft.org/html/libxml-parser.html}}.
While the incremental (xmlreader) mode would probably be more
efficient for memory usage, this program is only run once, during
build time, so the memory usage should not matter much.  At least the
XML structure can be freed when finished.  Scanning the tree is done
using manual traversal.  While the associated libxslt could be used to
run the old code directly, the C code is actually much simpler.

\lstset{language=C}
<<UCD parser local definitions>>=
#include <libxml/parser.h>
#include <libxml/xmlerror.h>
@

\lstset{language=make}
<<makefile.vars>>=
XML_CFLAGS := $(shell xml2-config --cflags)
XML_LDFLAGS := $(shell xml2-config --libs)
# adding xml flags only to PARSER_CFLAGS would be nice, but hard
# to integrate w/ static_proto gen
EXTRA_CFLAGS += $(XML_CFLAGS)
PARSER_LDFLAGS += $(XML_LDFLAGS)
@

\lstset{language=C}
<<UCD parser local definitions>>=
#define xml_opts XML_PARSE_NOENT | /* expand known entities */ \
                 XML_PARSE_NONET | /* forbid network access */ \
		 XML_PARSE_NOCDATA | /* merge CDATA as text */ \
                 XML_PARSE_COMPACT | /* compact small text nodes */ \
		 XML_PARSE_HUGE /* no hard-coded limits */
/* avoid type cast in every strcmp */
#define xml_isname(n, s) !strcmp((const char *)n->name, s)
@

<<Parse character data files>>=
xmlInitParser();  /* xmlCleanupParser() when done */
xmlDocPtr doc; /* the document under consideration */
xmlNodePtr n, c; /* for traversing the nodes */
@

\section{XML Entity Names}

As mentioned in the introduction, the XML standard's entity names are
generally much shorter than Unicode names, so a facility to use them
instead is provided.  This requires parsing the XML entity database.

\lstset{language=make}
<<makefile.vars>>=
PARSER_CFLAGS += -DXMLUNI=\"$(XMLUNI)\"
@

\lstset{language=C}
<<Parse character data files>>=
doc = xmlReadFile(XMLUNI, NULL, xml_opts);
if(!doc) {
  perror(XMLUNI);
  exit(1);
}
for(n = doc->children; n; n = n->next)
  if(n->children && xml_isname(n, "unicode"))
    break;
if(!n) {
  perror(XMLUNI);
  exit(1);
}
@


The name database has subsets called groups.  Using every single name
is certain to frequently collide with Unicode names, so a particular
group should be chosen.  The 2007 group is used by my own
applications, and seems like a reasonable default.

\lstset{language=make}
<<makefile.config>>=
# XML entity name group
XML_ENTITY_NAME_GROUP = 2007
@

<<makefile.vars>>=
PARSER_CFLAGS += -DXML_ENTITY_NAME_GROUP=\"$(XML_ENTITY_NAME_GROUP)\"
@

\lstset{language=C}
<<UCD parser local definitions>>=
/* avoid multiple type casts in every attr search */
#define xml_prop(n, p) (char *)xmlGetProp(n, (const xmlChar *)p)
#define xmlprop_free(p) do { \
  if(p) \
    xmlFree((xmlChar *)p); \
} while(0)
@

<<UCD parser local definitions>>=
const char **grp_set;
int ngrp_set;
@

<<Parse character data files>>=
for(c = n->children; c; c = c->next) {
  if(!c->children || !xml_isname(c, "entitygroups"))
    continue;
  for(c = c->children; c; c = c->next) {
    if(!c->children || !xml_isname(c, "group"))
      continue;
    char *gn = xml_prop(c, "name");
    int is_gr = gn && !strcmp(gn, XML_ENTITY_NAME_GROUP);
    xmlprop_free(gn);
    if(is_gr)
      break;
  }
  break;
}
if(!c) {
  perror("group " XML_ENTITY_NAME_GROUP " not in " XMLUNI);
  exit(1);
}
{
  xmlNodePtr cc;
  for(ngrp_set = 0, cc = c->children; cc; cc = cc->next) {
    if(cc->type != XML_ELEMENT_NODE || !xml_isname(cc, "set"))
      continue;
    ngrp_set++;
  }
  inisize(grp_set, ngrp_set);
  for(ngrp_set = 0, cc = c->children; cc; cc = cc->next) {
    if(cc->type != XML_ELEMENT_NODE || !xml_isname(cc, "set"))
      continue;
    grp_set[ngrp_set++] = xml_prop(cc, "name");
  }
  qsort(grp_set, ngrp_set, sizeof(*grp_set), sort_strcmp);
}
@

<<UCD parser local functions>>=
static int sort_strcmp(const void *a, const void *b)
{
  return strcmp(*(const char **)a, *(const char **)b);
}
@

Once the group's set names are known, the charlist is scanned for
character tags with an entity descriptor belonging to one of those
sets.  The id of the entity is the entity name, and the id of the
character is the Unicode code point.  Some names are for
multi-code-point sequences, similar to the Unicode named sequences.
For now, sequences are read into a separate property.

<<Parse character data files>>=
decl_str(na_xml);
decl_str(naseq_xml);
for(n = n->children; n; n = n->next)
  if(n->children && xml_isname(n, "charlist"))
    break;
if(!n) {
  perror("charlist not in " XMLUNI);
  exit(1);
}
for(n = n->children; n; n = n->next) {
  if(!n->children || !xml_isname(n, "character"))
    continue;
  for(c = n->children; c; c = c->next) {
    const char *set = NULL;
    const char *cid = NULL, *eid = NULL;
    if(c->type != XML_ELEMENT_NODE || !xml_isname(c, "entity") ||
       !(set = xml_prop(c, "set")) ||
       !bsearch(&set, grp_set, ngrp_set, sizeof(*grp_set), sort_strcmp) ||
       !(eid = xml_prop(c, "id")) || !(cid = xml_prop(n, "id"))) {
      xmlprop_free(set);
      xmlprop_free(eid);
      xmlprop_free(cid);
      continue;
    }
    if(!strchr(cid, '-')) {
      low = high = strtol(cid + 1, NULL, 16);
      /* note: duplicates will occur: */
      /*   both fully duplicated entries and multiple names for one cp */
      add_str8(na_xml, eid);
    } else
      add_strseq(naseq_xml, cid + 1, eid);
    xmlprop_free(set);
    xmlprop_free(eid);
    xmlprop_free(cid);
  }
}
xmlFreeDoc(doc);
@

Some code points have multiple entity names.  This problem can be
solved the same way as with Name\_Alias:  prepend a length to each
string.  As an optimization, since the word encoding is not done, and
there is no name longer than 127 characters, the length byte has its
high bit set.  The last string does not have a length byte; it just
occupies the remainder of the space.

<<UCD parser local functions>>=
static int cmp_cp_name(const void *a, const void *b)
{
  int c = *(int32_t *)a - *(int32_t *)b;
  return c ? c : cmp_strs(a, b);
}
@

<<Post-process property data>>=
prop_t *xna = &parsed_props[prop_na_xml];
sort_strs = xna->strs;
qsort(xna->str_arr, xna->len, sizeof(*xna->str_arr), cmp_cp_name);
for(i = xna->len - 1; i > 0; i--) {
  unsigned int nlen = xna->str_arr[i].len;
  for(low = i; low > 0; low--) {
    if(xna->str_arr[low - 1].cp != xna->str_arr[i].cp)
      break;
    if(!cmp_strs(&xna->str_arr[low - 1], &xna->str_arr[low]))
      continue;
    nlen += xna->str_arr[low - 1].len + 1;
  }
  if(low == i)
    continue;
  while(xna->max_strs < xna->strs_len + nlen)
    resize(xna->strs, xna->max_strs *= 2);
  uint8_t *strs8 = (uint8_t *)(xna->strs + xna->strs_len);
  unsigned int strs8_len = 0;
  for(j = low; j <= i; j++) {
    while(j < i && !cmp_strs(&xna->str_arr[j], &xna->str_arr[j + 1]))
      j++;
    uint8_t *p8 = (uint8_t *)(xna->strs + xna->str_arr[j].off);
    unsigned int len8 = xna->str_arr[j].len * 4;
    while(!p8[len8 - 1])
      len8--;
    if(j != i)
      strs8[strs8_len++] = 0x80 | (len8 - 1);
    memcpy(strs8 + strs8_len, p8, len8);
    strs8_len += len8;
  }
  while(strs8_len % 4)
    strs8[strs8_len++] = 0;
  movebuf(xna->str_arr + low + 1, xna->str_arr + i + 1,
          xna->len - (i + 1));
  xna->str_arr[low].off = xna->strs_len;
  xna->str_arr[low].len = strs8_len / 4;
  xna->strs_len += strs8_len / 4;
  xna->len -= i - low;
  if(!(i = low))
    break;
}
@

<<Dump character information as C code>>=
dump_str_tabs(xna, gen_h, tstf);
@

Some sequences have multiple entreis with the same starting character.
There is no guarantee that there are no multiple names for the same
sequence, either.  The encoding used for naseq above is good enough to
capture this.

<<Post-process property data>>=
prop_t *xnaseq = &parsed_props[prop_naseq_xml];
sort_strs = xnaseq->strs;
qsort(xnaseq->str_arr, xnaseq->len, sizeof(*xnaseq->str_arr), cmp_cp_name);
for(i = xnaseq->len - 1; i > 0; i--) {
  for(j = i; j > 0 && xnaseq->str_arr[j].cp == xnaseq->str_arr[j - 1].cp &&
             !cmp_strs(&xnaseq->str_arr[j], &xnaseq->str_arr[j - 1]); j--);
  if(j != i) {
    movebuf(xnaseq->str_arr + j + 1, xnaseq->str_arr + i + 1, xnaseq->len - (i + 1));
    xnaseq->len -= i - j;
    if(!(i -= j))
      break;
  }
  while(i > 0 && xnaseq->str_arr[i].cp == xnaseq->str_arr[i - 1].cp) {
    int jlen = xnaseq->str_arr[i - 1].len, ilen = xnaseq->str_arr[i].len;
    while(xnaseq->max_strs < xnaseq->strs_len + ilen + jlen)
      resize(xnaseq->strs, xnaseq->max_strs *= 2);
    uint16_t *newseq = (uint16_t *)(xnaseq->strs + xnaseq->strs_len);
    jlen *= 2;
    cpybuf(newseq, xnaseq->strs + xnaseq->str_arr[i - 1].off, jlen);
    if(!newseq[jlen - 1])
      jlen--;
    ilen *= 2;
    cpybuf(newseq + jlen, xnaseq->strs + xnaseq->str_arr[i].off, ilen);
    if(!newseq[jlen + ilen - 1])
      ilen--;
    if((ilen + jlen) % 2)
      newseq[ilen++ + jlen] = 0;
    for(j = i - 1; j > 0; j--)
      if(cmp_strs(&xnaseq->str_arr[j], &xnaseq->str_arr[j - 1]))
        break;
    xnaseq->str_arr[j].len = (ilen + jlen) / 2;
    xnaseq->str_arr[j].off = xnaseq->strs_len;
    xnaseq->strs_len += (ilen + jlen) / 2;
    movebuf(xnaseq->str_arr + j + 1, xnaseq->str_arr + i + 1, xnaseq->len - (i + 1));
    xnaseq->len--;
    i = j;
  }
  if(!i)
    break;
}
@

<<Dump character information as C code>>=
dump_str_tabs(xnaseq, gen_h, tstf);
@

Finally, although the encoding is fairly simple, a function analogous
to the plain Unicode name lookup function is provided.

<<Library [[uni]] Members>>=
xml_cp_to_name.o
@

<<xml_cp_to_name.c>>=
<<Common C Header>>
#include "uni_prop.h"

@

<<Unicode property exports>>=
/* |*seq_len| is # of chars matched; negative means possible longer match */
/* *alias is alias #: */
/*   if *alias is 0, returned *alias is max alias index */
/*   if *alias is valid non-0, returned *alias is *alias - 1 */
/*   if *alias is invalid non-0, returned *alias is -1 */
int uni_cp_to_xml_name(const uint32_t *cp, unsigned int len, int *seq_len,
                       int *alias, <<Buffer return parameters for UTF-[[8]]>>);
@

<<xml_cp_to_name.c>>=
int uni_cp_to_xml_name(const uint32_t *cp, unsigned int len, int *seq_len,
                       int *alias, <<Buffer return parameters for UTF-[[8]]>>)
{
  if(!len) {
    if(seq_len)
      *seq_len = 0;
    if(alias && *alias)
      *alias = -1;
    return 0;
  }
  if(alias && *alias < 0) {
    if(seq_len)
      *seq_len = 0;
    return 0;
  }
  <<Return XML name for [[*cp]]>>
}
@

Just like with Unicode names, the first thing to do is to see if there
is a matching sequence name.

<<Return XML name for [[*cp]]>>=
if(seq_len)
  *seq_len = 1;
const uni_str_ptr_t *lu = len > 1 || seq_len ? uni_naseq_xml_of(*cp) : NULL;
if(lu && lu->len && len == 1) {
  if(seq_len)
    *seq_len = -1;
} else if(lu && lu->len) {
  const uint8_t *found_na = NULL;
  unsigned int found_nalen = 0, found_seqlen = 0, found_alias = 0, seqlen, nalen;
  const uint8_t *ep = uni_naseq_xml_strs + lu->off, *p = ep + lu->len;
  const uint16_t *seqptr;
  const uint32_t *cp2;
  while(p > ep) {
    nalen = *--p;
    seqlen = (nalen >> 6) + 1;
    nalen = (nalen & 0x3f) + 1;
    if(!p[-1])
      p--;
    p -= nalen + seqlen * 2;
    seqptr = (const uint16_t *)p;
    int sp = 0;
    for(cp2 = cp + 1; cp2 - cp < len; cp2++) {
      unsigned int clen;
      if(*cp2 != uni_int_utf16_decode(seqptr + sp, &clen))
        break;
      sp += clen;
      if(sp == seqlen) {
        if(seqlen > found_seqlen) {
          found_na = p + seqlen * 2;
	  found_nalen = nalen;
	  found_seqlen = seqlen;
	  found_alias = 0;
	} else if(seqlen == found_seqlen) {
	  found_alias++;
	  if(alias && *alias == found_alias) {
	    found_na = p + seqlen * 2;
	    found_nalen = nalen;
	  }
	}
	break;
      }
    }
    /* if at end of string, mark as possibly longer */
    if(seq_len && sp < seqlen && cp2 - cp == len)
      *seq_len = -1;
  }
  if(found_na) {
    if(alias && *alias > found_alias) {
      *alias = -1;
      *seq_len = 0;
      return 0;
    } else if(alias && !*alias)
      *alias = found_alias;
    else if(alias)
      --*alias;
    if(seq_len)
      *seq_len *= found_seqlen + 1; /* 1 or -1 */
    return uni_return8_buf8(found_na, found_nalen, buf, off, buf_len);
  }
}
@

For the single code point case, there is only one table to consult.
It behaves much like the alias table, except the lst entry for each
code point does not have an explicit length, and the explicit lengths
for the others all have their high bit set.

<<Return XML name for [[*cp]]>>=
lu = uni_na_xml_of(*cp);
if(!lu->len)
  return 0;
const uint8_t *str = uni_na_xml_strs + lu->off;
if(!alias)
  return uni_return8_buf8(*str & 0x80 ? str + 1 : str,
                          *str & 0x80 ? (*str & 0x7f) + 1 : lu->len,
			  buf, off, buf_len);
int num_alias = 0;
int alen = lu->len, retlen = 0;
while(1) {
  if(num_alias == *alias) {
    retlen = uni_return8_buf8(*str & 0x80 ? str + 1 : str,
                              *str & 0x80 ? (*str & 0x7f) + 1 : alen,
			      buf, off, buf_len);
    if(num_alias) {
      --*alias;
      return retlen;
    }
  }
  if(!(*str & 0x80))
    break;
  alen -= (*str & 0x7f) + 2;
  str += (*str & 0x7f) + 2;
  num_alias++;
}
if(*alias) /* couldn't find requested alias */
  *alias = -1;
else /* set # of aliases */
  *alias = num_alias;
return retlen;
@

To test this, a simple program just generates every single name.  No
verification is done; this is meant to be verified manually.

<<C Test Support Executables>>=
tstcp_xml_na \
@

<<tstcp_xml_na.c>>=
<<Common C Header>>

#include "uni_prop.h"

int main(void)
{
  uint32_t cp;
  uint8_t *buf = NULL;
  unsigned int buf_len = 0, clen;
  int has_more, aliases;
  for(cp = 0; cp <= UNI_MAX_CP + 1; cp++) {
    clen = uni_cp_to_xml_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
    if(!clen)
      continue;
    printf("%04X %d %.*s\n", cp, has_more, clen, buf);
    while(aliases > 0) {
      clen = uni_cp_to_xml_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
      printf("%04X %d &%.*s\n", cp, has_more, clen, buf);
    }
  }
  /* all sequences */
  int i;
  uint32_t seq[10];
  for(i = 0; i < uni_naseq_xml_arr_len; i++) {
    const uint8_t *ep = uni_naseq_xml_strs + uni_naseq_xml_arr[i].off,
                  *p = ep + uni_naseq_xml_arr[i].len;
    unsigned int seqlen, nalen;
    while(p > ep) {
      uint32_t *sp = seq;
      *sp = uni_naseq_xml_arr[i].cp;
      printf("%04X", (int)*sp++);
      nalen = *--p;
      if(!p[-1])
        p--;
      seqlen = (nalen >> 6) + 1;
      nalen = (nalen & 0x3f) + 1;
      p -= nalen + seqlen * 2;
      const uint16_t *seqp = (const uint16_t *)p;
      while(seqlen > 0) {
        *sp = uni_int_utf16_decode(seqp, &clen);
	printf(":%04X", (int)*sp++);
	seqp += clen;
	seqlen -= clen;
      }
      clen = uni_cp_to_xml_name(seq, (int)(sp - seq), &has_more, &aliases, &buf, 0, &buf_len);
      printf(" %d %.*s\n", has_more, clen, buf);
      while(aliases) {
        clen = uni_cp_to_xml_name(seq, (int)(sp - seq), &has_more, &aliases, &buf, 0, &buf_len);
        printf("\t %d &%.*s\n", has_more, clen, buf);
	nalen = *--p;
	if(!p[-1])
	  p--;
	seqlen = (nalen >> 6) + 1;
	nalen = (nalen & 0x3f) + 1;
	p -= nalen + seqlen * 2;
      }
    }
  }
  return 0;
}
@

\section{The CLDR}

The CLDR and the ICU library go hand-in-hand; the CLDR is basically
the ICU data files somewhat sloppily extracted from ICU.  It may be
best to not support the CLDR data at all, but instead use ICU to query
it.  A few support functions could be provided to use it along with
the UCD (but then again, ICU has all of the UCD as well, so why bother
with libuni at all?).  However, like the POSIX locale functions, the
ICU functions do not provide a way to query the information needed for
a compliant regular expression engine.  It is also a fairly large
library, and I am unsure it does a good job of allowing minimal
linkage.

The CLDR also uses XML rather than simple tables.  Doing hand-coded
XSLT for hundreds of data files seemed excessive, which is the main
reason why I abandoned XSLT.  Note that unlike unicode.xml, there is a
local, valid DTD for every CLDR file.

\lstset{language=make}
<<makefile.vars>>=
PARSER_CFLAGS += -DCLDR_LOC=\"$(CLDR_LOC)\"
@

\lstset{language=C}
<<UCD parser local definitions>>=
#define xml_opts_dtd xml_opts | \
		     XML_PARSE_DTDLOAD | XML_PARSE_DTDATTR | \
		     XML_PARSE_DTDVALID /* use DTD if on local disk */
@

<<Parse character data files>>=
chdir(cwd);
chdir(CLDR_LOC);
<<Parse CLDR files>>
@

The CLDR is, like most XML data, insanely complex and at times poorly
documented%
\footnote{XML was promoted as a self-documenting data format.  The DTD
documents the format not quite as well as a BNF grammar documents a
programming language:  that is, not even well enough to get the syntax
completely right (complex conditions cannot be expressed in BNF or
DTD, so there are often exceptions written in the supplementary text).
Like all data formats, XML needs supplementary documentation.  The
CLDR has such documentation, but it is of medium-to-poor quality.}%
.  Much of its functionality also has dubious value for general
projects.  As such, the only CLDR data provided is that which supports
localization of UCD data, plus one property that is required for
regular expressions.

The first thing necessary for localization is to determine what locale
is being used.  The locale name consists of a language identifier,
followed by optional extensions.  The language identifier consists of
a language subtag, followed by an optional script subtag, followed by
an optional territory subtag, followed by optional variant subtags.
These are all case-insensitive, and are separated by either dashes or
underscores.  The supported language identifier tags are listed in the
supplemental metadata.  As with all of the CLDR data, versioning
information is ignored.\footnote{One day, the versions of all data
files used in building this library should be obtainable as a global
variable or API call.  However, only overall version numbers should be
used, and not the individual constituent file modification times.}

<<Parse CLDR files>>=
doc = xmlReadFile("common/supplemental/supplementalMetadata.xml",
                  NULL, xml_opts_dtd);
if(!doc) {
  perror("supplementalMetadata.xml");
  exit(1);
}
<<Prepare for parsing CLDR supplemental metadata>>
for(n = doc->children; n; n = n->next) {
  if(n->children && xml_isname(n, "supplementalData")) {
    for(n = n->children; n; n = n->next) {
      /* ignore version, generation, cldrVersion */
      if(!n->children || !xml_isname(n, "metadata"))
        continue;
      for(n = n->children; n; n = n->next) {
        if(!n->children)
	  continue;
        <<Parse CLDR supplemental metadata>>
      }
      break;
    }
    break;
  }
}
<<Finish parsing CLDR supplemental metadata>>
xmlFreeDoc(doc);
@

The data we're looking for is under the validity tag.  Each set is
listed as a ``variable'' whose value is the whitespace-separated list
of names.  Technically, they definition in [[attributeValues]] is
used, but for those which have large lists, the value is just an
expansion of one of these variables.  In fact, one could make an
argument that all of the variables of type choice are useful to add as
enumerations.  The main argument against this is that some of the
lists are rather long, and the enumerations' use of binary searching
(rather than, say, hash tables) may be slow.  It also violates my
mandate of only including UCD localization information.

One useful set not covered by variables is provided by the [[type]]
attribute of [[segmentation]], which lists the valid segmentation
types.  This is extracted as well.

<<Parse CLDR supplemental metadata>>=
else if(xml_isname(n, "validity")) {
  for(c = n->children; c; c = c->next) {
    if(c->type != XML_ELEMENT_NODE)
      continue;
    <<Parse CLDR validity entries>>
  }
}
@

<<Parse CLDR validity entries>>=
else if(xml_isname(c, "variable") || xml_isname(c, "attributeValues")) {
  char *t = xml_prop(c, "type");
  if(t && !strcmp(t, "choice")) {
    char *v = xml_prop(c, "id");
    if(!v)
      v = xml_prop(c, "elements");
    if(!c->children || !c->children->content || !v)
      continue; /* should never happpen */
    /* I guess linear search is good enough here */
    if(strcmp(v, "$grandfathered") && /* supplements language */
       strcmp(v, "$language") &&
       strcmp(v, "$territory") &&
       strcmp(v, "$script") &&
       strcmp(v, "$variant") &&
       strcmp(v, "$collationType") &&
       strcmp(v, "segmentation"))
      continue;
    char *name;
    inisize(name, strlen(v) + 5);
    memcpy(name, "CLDR_", 5);
    strcpy(name + 5, *v == '$' ? v + 1 : v);
    xmlprop_free(v);
    int pno = add_prop(name);
    free(name);
    uint32_t str[64];
    for(s = (char *)c->children->content; isspace(*s); s++);
    for(i = 0; *s; i++) {
      char *p = (char *)str;
      while(!isspace(*s) && *s)
        *p++ = *s++;
      while((p - (char *)str) % 4)
        *p++ = 0;
      add_str_rng(&parsed_props[pno], i, i, str, (p - (char *)str) / 4);
      while(isspace(*s))
        s++;
    }
    str_to_enum(&parsed_props[pno], NULL);
    free(parsed_props[pno].rng_dat8);
    parsed_props[pno].rng_dat8 = NULL;
  }
  <<Process other CLDR validity variables>>
  xmlprop_free(t);
}
@

<<Ignore unimplemented enums>>=
/* ignore auto-generated enums for obsolete properties */
if(!parsed_props[i].rng_dat8 && i < num_prop_aliases)
  continue;
@

In addition, the entries in \$grandfathered and some other ISO
language codes are treated differently within Unicode and the CLDR.
These translations are under the alias tag, in languageAlias elements.
Similarly, script, territory, and variants have aliases as well.  In
the actual CLDR, the alias tag always follows the validity tag.  If a
future CLDR does not do this, the following code will bomb out due to
not finding the variable-provided properties.

Since \$grandfathered really only contains language identifiers, it is
merged with \$language before alias processing.

<<Parse CLDR supplemental metadata>>=
else if(xml_isname(n, "alias")) {
  int grandf = add_prop("CLDR_grandfathered"),
      lang = add_prop("CLDR_language"),
      ter = add_prop("CLDR_territory"),
      var = add_prop("CLDR_variant");
  if(!num_val_aliases[lang] || !num_val_aliases[ter] ||
     !num_val_aliases[var]) {
    fputs("variable defs do not preceed aliases!\n", stderr);
    exit(1);
  }
  <<Merge [[grandf]] into [[lang]]>>
  for(c = n->children; c; c = c->next) {
    if(c->type != XML_ELEMENT_NODE)
      continue;
    <<Parse CLDR bcp47 alias entries>>
  }
}
@

The first thing to do is drop the \$grandfathered variable, and merge
it with the \$language variable.  The actual top-level entries should
all be destroyed after aliasing.

<<Merge [[grandf]] into [[lang]]>>=
int nlang = num_val_aliases[lang], ngrandf = num_val_aliases[grandf], ap;
if(nlang + ngrandf > max_val_aliases[lang])
  resize(val_aliases[lang], max_val_aliases[lang] = nlang + ngrandf);
resize(enum_vals[lang], enum_vals_len[lang] = nlang + ngrandf);
num_val_aliases[lang] += ngrandf;
cpybuf(val_aliases[lang] + nlang, val_aliases[grandf], ngrandf);
cpybuf(enum_vals[lang] + nlang, enum_vals[grandf], ngrandf);
for(ap = 0; ap < ngrandf; ap++)
  enum_vals[lang][nlang + ap].val += nlang;
qsort(enum_vals[lang], nlang + ngrandf, sizeof(*enum_vals[lang]), uni_cmp_valueof);
free(enum_vals[grandf]);
enum_vals[grandf] = NULL;
enum_vals_len[grandf] = 0;
free(val_aliases[grandf]);
val_aliases[grandf] = NULL;
num_val_aliases[grandf] = max_val_aliases[grandf] = 0;
@

All properties are handled the same way, so a function is used for
this.

<<Parse CLDR bcp47 alias entries>>=
else if(xml_isname(c, "languageAlias"))
  do_attr_alias(c, lang);
else if(xml_isname(c, "territoryAlias"))
  do_attr_alias(c, ter);
else if(xml_isname(c, "variantAlias"))
  do_attr_alias(c, var);
@

The easy way to do this would be to look up the value to be aliased
([[from]]) and the target ([[to]]) and merge them.  Unfortunately, the
aliases are a mess.  While the variables are supposed to contain all
valid values, sometimes [[from]] doesn't exist in the table.  This
happens so often that I decided to just go ahead and insert it.  In
fact, the language aliases have a few [[from]] values that include
variant or script codes, making it even more difficult to use this
data.  Some targets do not exist, either.  Some of those non-existent
targets are space-separated lists (I'm not sure how to handle those).
Some are language codes with scripts.  Some, however, are just missing
(e.g. ami for the i-ami to ami alias).  Some have no target at all; I
assume this is meant to delete the value, but what is used instead?

One additional problem presented by these aliases is that there are
sometimes more than three aliases for a name.  The alias structure
developed above only supports three aliases.  Since the [[nameof]]
structure was not really meant to be used, there is probably no point
in even keeping the aliases.  They just clutter up the namespace due
to enumeration constants being created for each one.

<<UCD parser local functions>>=
static void do_attr_alias(xmlNodePtr alias, int prop)
{
  char *from = xml_prop(alias, "type");
  char *to = xml_prop(alias, "replacement");
  uni_valueof_t me, *fp, *tp;
  int tval;
  /* don't care about reason */
  /* some aliases have no to.  what does that mean? */
  if(!to) {
#if 0
    fprintf(stderr, "Invalid %s alias %s->%s\n",
                    parsed_props[prop].name, from, to);
#endif
    xmlprop_free(from);
    return;
  }
  me.name = to;
  tp = bsearch(&me, enum_vals[prop], enum_vals_len[prop], sizeof(me),
               uni_cmp_valueof);
  if(tp) {
    tval = tp->val;
    me.name = from;
    fp = bsearch(&me, enum_vals[prop], enum_vals_len[prop], sizeof(me),
                 uni_cmp_valueof);
    if(!fp) {
      /* some aliases are not listed in any original data */
      /* so auto-add them */
      int l, h, m, c;
      for(l = 0, h = enum_vals_len[prop] - 1; l <= h; ) {
        m = (l + h) / 2;
        c = uni_cmp_valueof(&me, &enum_vals[prop][m]);
	if(c < 0)
	  l = m + 1;
	else
	  h = m - 1;
      }
      resize(enum_vals[prop], ++enum_vals_len[prop]);
      movebuf(enum_vals[prop] + l + 1, enum_vals[prop] + l,
              enum_vals_len[prop] - l - 1);
      fp = &enum_vals[prop][l];
      fp->name = strdup(from);
      fp->val = num_val_aliases[prop];
      if(num_val_aliases[prop] == max_val_aliases[prop])
        resize(val_aliases[prop], max_val_aliases[prop] *= 2);
      num_val_aliases[prop]++;
      /* note that val_aliases[prop][fp->val] will just be ignored/deleted */
      val_aliases[prop][fp->val].short_name = NULL; /* prevent freeing */
    }
  }
  if(!tp || !fp) {
#if 0
    fprintf(stderr, "Error looking up %s for %s alias %s->%s\n",
            me.name, parsed_props[prop].name, from, to);
#endif
    /* there are some known issues in CLDR, so just ignore */
    xmlprop_free(from);
    xmlprop_free(to);
    return;
  }
  uni_alias_t *ta = &val_aliases[prop][tval],
              *fa = &val_aliases[prop][fp->val];
#if 0 /* don't bother storing as alias; just make sure lookup works */
  if(!ta->alt_name)
    ta->alt_name = fp->name;
  else if(!ta->alt_name2)
    ta->alt_name2 = fp->name;
  else if(!strcmp(ta->long_name, ta->short_name))
    ta->long_name = fp->name;
  else {
    fprintf(stderr, "No room for %s alias %s->%s\n",
                    parsed_props[prop].name, from, to);
    /* ignore; this just drops enum const & nameof alias */
  }
#endif
  num_val_aliases[prop]--;
  if(fa->short_name) {
    free((char *)fa->short_name);
    if(fa->long_name != fa->short_name)
      free((char *)fa->long_name);
    if(fa->alt_name)
      free((char *)fa->alt_name);
    if(fa->alt_name2)
      free((char *)fa->alt_name2);
  }
  movebuf(fa, fa + 1, num_val_aliases[prop] - fp->val);
  int i, oval = fp->val;
  for(i = 0; i < enum_vals_len[prop]; i++) {
    if(enum_vals[prop][i].val > oval)
      enum_vals[prop][i].val--;
    else if(enum_vals[prop][i].val == oval)
      enum_vals[prop][i].val = tval;
  }
  xmlprop_free(from);
  xmlprop_free(to);
}
@

\lstset{language=txt}
<<FIXME>>=
"unknown" codes:
  $language - und
  $script - Zzzz
  $region - ZZ
translated region (territory?) codes:
  AA -> 958 -> AAA
  QM..QZ -> 959..972 -> QMM..QZZ
  XA..XZ -> 973..998 -> XAA..XZZ
  ZZ -> 999 -> ZZZ
translated script codes:
  Qaaa..Qabx -> 900..949
canonical form of parms (does it really matter?):
  -t- before -u-
  -u-: all attrs in alpha, followed by all keywords in alpha, all lower
       all names are canonical name
  -t-: same canon really
variant translations:
  AALAND -> sv_AX
  BOKMAL -> -nb
  NYNORSK -> -nn
  POSIX -> -u-va-posix
  POLYTONI -> -polyton
  SAAHO -> -ssy
@

The extensions supported by the CLDR are listed in the common/bcp47
data files.

\lstset{language=C}
<<UCD parser local definitions>>=
#include <dirent.h>
@

<<Parse CLDR files>>=
/* note: in most files, version info is silently ignored/dropped */
/* however, when generating files, it might be a good idea to put that */
/* info in to allow strings to determine versioning */

/* bcp47: locale variants */
/* need lookup by extension first (u/t), then name/aliases */
/* for each keyword, need lookup of valid values */

/* need parser function for locale names */
/*  en_US-u-ca-blah */
/*    -> en_US is full locale name (name+script) */
/*       u-ca is a keyword with value blah */
/*  en_US@ca=blah is equivalent */
/*     but keywords with aliases may only use aliases in @-notation */
struct dirent *de;
DIR *d = opendir("common/bcp47");
if(!d) {
  perror("CLDR bcp47");
  exit(1);
}
chdir("common/bcp47");
while((de = readdir(d))) {
  if(de->d_name[0] == '.')
    continue;
  doc = xmlReadFile(de->d_name, NULL, xml_opts_dtd);
  if(!doc) {
    perror(de->d_name);
    exit(1);
  }
  for(n = doc->children; n; n = n->next)
    /* not sure why 1st entry is always empty ldmlBCP47 */
    if(n->children && xml_isname(n, "ldmlBCP47")) {
      for(n = n->children; n; n = n->next) {
        /* ignore version */
	/*  although version/number could be added as literal text to output */
	/* ignore generation */
	/*  although generation/date could be added with version/number */
	/* ignore deprecated cldrVersion */
        if(n->children && xml_isname(n, "keyword")) {
          for(n = n->children; n; n = n->next) {
	    if(xml_isname(n, "key") ||
	       /* attribute is actually 3-letter standalone attribute flag */
	       xml_isname(n, "attribute")) {
	      char *name = xml_prop(n, "name");
	      char *desc = xml_prop(n, "description");
	      char *xalias = xml_prop(n, "alias"), *alias = xalias ? xalias : name;
	      char *depr = xml_prop(n, "deprecated");
	      char *xext = xml_prop(n, "extension");
	      const char *ext = xext;
	      /* ignoring since= attr */
	      if(!ext)
	        ext = "u";
	      /* note: alias may be multiple space-separated aliases */
	      /* if alias present, only alias may be used in loc@var */
	      /* but none observed in wild */
              if(depr && !strcmp(depr, "false")) {
	        xmlprop_free(depr);
	        depr = NULL;
	      }
#if 0
	      printf("%s-%s/%s %s\n", ext, name, alias,
	             depr ? "deprecated" : "");
#endif
	      xmlprop_free(name);
	      xmlprop_free(desc);
	      xmlprop_free(xalias);
	      xmlprop_free(depr);
	      xmlprop_free(xext);
	      /* note: t_i0 may have other valid keys: tags indicating method */
	      /* note: t_k0 is same way */
	      /* note: t_t0 is same way */
	      /* note: t_x0 is generic tags only */
	      for(c = n->children; c; c = c->next) {
	        if(xml_isname(c, "type")) {
		  name = xml_prop(c, "name");
		  xalias = xml_prop(c, "alias");
		  alias = xalias ? xalias : name;
		  desc = xml_prop(c, "description");
		  depr = xml_prop(c, "deprecated");
		  /* ignoring since= attr */
		  /* note: alias may be multiple space-separated aliases */
		  /* 1st alias is preferred */
		  if(depr && !strcmp(depr, "false")) {
		    xmlprop_free(depr);
		    depr = NULL;
		  }
		  /* special names: */
		  /*   REORDER_CODE reordering block name(s) */
		  /*   CODEPOINTS one or more 4-6 hex digits */
#if 0
		  printf("  %s/%s %s\n", name, alias, depr ? "deprecated" : "");
#endif
	           xmlprop_free(name);
		   xmlprop_free(desc);
		   xmlprop_free(xalias);
		   xmlprop_free(depr);
	        }
	      }
	    }
	  }
	  break;
	}
      }
      break;
    }
  xmlFreeDoc(doc);
}
closedir(d);
chdir(cwd);
chdir(CLDR_LOC);
@

\lstset{language=txt}
<<FIXME>>=
 Implement locale name paraser:
   extract lang, script, territory, variant(s)
     lang = 2-3 alpha matching uni_language_valueof[]
            -or- initial 2-part matching uni_grandfathered_valueof[]
     
     script = matching uni_script_valueof[]
     territory = matching uni_territory_valueof[]
     variant = matching uni_variant_valueof[]
   extract extension(s); store in uni_uca_opts_t
   ignore .<encoding> extension, if present
@

The main ldml files contain the actual locale data.  Every ldml file
at least identifies itself.  All ldml files with the same identity
are effectively merged into a single tree.  For alias lookups and data
merging, all available xml files are read in and keyed by their
primary identity.

\lstset{language=C}
<<UCD parser local functions>>=
static void parse_ldml(const char *name)
{
  /* ignore special tags everywhere, since it's undefined */
  xmlDocPtr doc = xmlReadFile(name, NULL, xml_opts_dtd);
  if(!doc) {
    perror(name);
    exit(1);
  }
  xmlNodePtr n, c;
  for(n = doc->children; n; n = n->next)
    /* not sure why 1st entry is always empty ldml */
    if(n->children && xml_isname(n, "ldml")) {
      /* ignore draft attribute */
      /* ignore deprecated fallback tag */
      for(n = n->children; n; n = n->next) {
	if(!n->children)
	  continue;
	<<Parse ldml sections>>
      }
      break;
    }
  xmlFreeDoc(doc);
}
@

<<Parse ldml sections>>=
#define repl_prop(v, n, p) do { \
  xmlprop_free(v); \
  v = xml_prop(n, p); \
} while(0)
else if(xml_isname(n, "identity")) {
  /* ignore version, generation */
  char *lang = NULL;
  char *scr = NULL;
  char *ter = NULL;
  char *var = NULL;
  char *alias = NULL, *alias_path = NULL;
  for(c = n->children; c; c = c->next) {
    /* e.g. zh in zh_Hans_SG */
    if(xml_isname(c, "language"))
      repl_prop(lang, c, "type");
    /* e.g. Hans in zh_Hans_SG */
    else if(xml_isname(c, "script"))
      repl_prop(scr, c, "type");
    /* e.g. SG in zh_Hans_SG */
    else if(xml_isname(c, "territory"))
      repl_prop(ter, c, "type");
    /* e.g. POSIX in en_US_POSIX */
    else if(xml_isname(c, "variant"))
      repl_prop(var, c, "type");
    else if(xml_isname(c, "alias")) {
      /* note: should this even be supported?  is it used? */
      repl_prop(alias, c, "source");
      repl_prop(alias_path, c, "path");
    }
    /* note that draft, references, alt fields ignored above */
  }
  /* file name should be following text */
#if 0
  printf("lang: %s", lang);
  if(scr)
    printf("_%s", scr);
  if(ter)
    printf("_%s", ter);
  if(var)
    printf("_%s", var);
  putchar('\n');
#endif
  xmlprop_free(lang);
  xmlprop_free(scr);
  xmlprop_free(ter);
  xmlprop_free(var);
  xmlprop_free(alias);
  xmlprop_free(alias_path);
}
@

The collations subdirectory contains locale-specific collation order
changes.

<<Parse CLDR files>>=
/* collation: locale-specific collation rules */
d = opendir("common/collation");
if(!d) {
  perror("CLDR collation");
  exit(1);
}
chdir("common/collation");
while((de = readdir(d))) {
  if(de->d_name[0] == '.')
    continue;
  parse_ldml(de->d_name);
}
closedir(d);
chdir(cwd);
chdir(CLDR_LOC);
@

<<Parse ldml sections>>=
else if(xml_isname(n, "collations")) {
  char *subl = xml_prop(n, "validSubLocales");
#if 0
  /* space-separated list of full locale names */
  if(subl)
    printf("sublang: %s\n", subl);
#endif
  /* ignore version, draft attrs */
  const char *def = "standard";
  char *xdef = NULL;
  for(c = n->children; c; c = c->next) {
    /* should be same enum type as types below */
    if(xml_isname(c, "default")) {
      xdef = xml_prop(c, "choice");
      if(!xdef)
        xdef = xml_prop(c, "type"); /* deprecated, but used */
      def = xdef;
      /* ignore draft, references, alt attrs */
      /* can have more than one, but why would you? */
    } else if(xml_isname(c, "collation")) {
      /* store in file as DUCET/rev_DUCET override tables */
      /* just mtab version: */
      /*   <strtab len> */
      /*   <strtab> */
      /*   <mtab len> */
      /*   <mtab> */
      /*   <rev strtab len> */
      /*   <rev strtab> */
      /*   <rev mtab> */
      /* ignore draft, standard, references, alt */
      if(!subl) /* technically, this isn't a req, but it's safe */
        subl = xml_prop(c, "validSubLocales");
      /* ignore draft, standard, references, alt attrs */
      /* ignore visibility attr */
      /* all available types should be an enum */
      /* or at least easily looked up on per-lang basis */
      /* most langs have no alt types, but some have many */
#if 0
      printf("[rules for type %s]\n", xml_prop(c, "type"));
#endif
      xmlNodePtr cc;
      const char *base = NULL; /* DUCET by default */
      for(cc = c->children; cc; cc = cc->next) {
        if(xml_isname(cc, "base")) {
	  xmlNodePtr ccc;
	  for(ccc = cc->children; ccc; ccc = ccc->next)
	    if(xml_isname(ccc, "alias")) {
	      base = xml_prop(ccc, "source");
	      /* path should always be /ldml/collations */
	      break;
	    }
	} else if(xml_isname(cc, "settings")) {
	  xmlAttrPtr a;
	  /* note: only settings in wild are: */
	  /*   backwards=on */
	  /*   caseFirst=upper */
	  /*   normalization=on */
	  /*   alternate=shifted */
	  /*   variableTop=<n> */
	  /*   strength=Tertiary */
	  /*   hiraganaQuaterary=on */
	  /* valid, though: */
	  /*  strength=primary|secondary|tertiary|quaternary|identical */
	  /*  alternate=non-ignorable|shifted */
	  /*  backwards=on|off */
	  /*  normalization=on|off */
	  /*  caseLevel=on|off */
	  /*  caseFirst=upper|lower|off */
	  /*  hiraganaQuarternary=on|off */
	  /*  hiraganaQuaternary=on|off */
	  /*  numeric=on|off */
	  /*  private=true|false */  /* deprecated */
	  /*  variableTop='x' */
	  /*  reorder=? */
#if 0
          fputs("  settings", stdout);
	  for(a = cc->properties; a; a = a->next)
	    printf(" %s=%s", a->name, a->children->content);
	  putchar('\n');
#endif
        } else if(xml_isname(cc, "rules")) {
	  xmlNodePtr r;
	  for(r = cc->children; r; r = r->next) {
	    if(r->type != XML_ELEMENT_NODE)
	      continue;
	    /* r->name may be alias.  not seen in wild. */
	    /* r->name may be import (repeatedly).  not seen in wild. */
	    /* r->name should be enum */
	    /* reset [before=primary/secondary/tertiary] */
	    /* note: reset arg may be tag: */
	    /*    <when>_<lev>_ignorable */
	    /*      <lev>=primary secondary tertiary non */
	    /*      <when>=first last */
	    /*    <when>_variable */
	    /*    <when>_trailing */
	    /*    cp hex=... */
	    /* p pc s sc t tc i ic */ /* q qc are deprecated */
	    /*   each takes interleaved raw data, <cp hex=>, <last_variable/> */
	    /* x [<context>..</>] {p pc s sc ...}* [<extend>..</>] */
	    /*  context & extend are interleaved raw data & <cp hex=> */
#if 0
            printf(" rule: %s%s '%s'\n", r->name,
	           r->properties &&
		     r->properties->children ?
		       r->properties->children->content : "",
		   r->children->type == XML_ELEMENT_NODE ?
		      r->children->name :
		      r->children->content);
#endif
          }
        } else if(xml_isname(cc, "suppress_contractions")) {
#if 0
          printf("  suppress %s\n", cc->children->content);
	  /* note: may be interleaved raw data and <cp hex="...."> tags */
#endif
        } else if(xml_isname(cc, "optimize")) {
	  /* ??? */
	  /* interleaved raw data and <cp hex="..."> tags */
	}
      }
      xmlprop_free(base);
    } else if(xml_isname(c, "alias")) {
      /* path should always be //ldml/collations */
#if 0
      printf("  [copy from %s (%s)]\n", xml_prop(c, "source"), /* leak */
             xml_prop(c, "path")); /* leak */
#endif
    }
  }
  xmlprop_free(subl);
  xmlprop_free(xdef);
#if 0
  printf("[default: %s]\n", def);
#endif
}
@

The main subdirectory contains a number of different common locale
parameters.  The only one we're interested in is the
[[characters/exemplarCharacters]] property.%
\footnote{Maybe [[delimiters/quotation]]* would be useful as well, but
it isn't covered under my UCD-localization-only mandate.  It appears
to be supplemental to sentence boundary determination, but it isn't
really.}

<<Parse CLDR files>>=
/* main: "other" rules */
d = opendir("common/main");
if(!d) {
  perror("CLDR main");
  exit(1);
}
chdir("common/main");
while((de = readdir(d))) {
  if(de->d_name[0] == '.')
    continue;
  parse_ldml(de->d_name);
}
closedir(d);
chdir(cwd);
chdir(CLDR_LOC);
@

<<Parse ldml sections>>=
else if(xml_isname(n, "alias")) {
  /* is this even used?  should it be supported? */
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "characters")) {
  /* exemplarCharacters [type=...]>[uniset]</> */
  /* store root version as cp list or rle cp list plus binary multi-lev */
  /* store in file as :*/
  /*   <rle cp list len> */
  /*   <rle cp list: chars 0-31 indicate run length - 2> */
  /*   <bin multi-lev tab> */
}
@

The segmentation exceptions modify the line, paragraph, and word
breaking rules.

<<Parse CLDR files>>=
/* main: "other" rules */
d = opendir("common/segments");
if(!d) {
  perror("CLDR segmentation");
  exit(1);
}
chdir("common/segments");
while((de = readdir(d))) {
  if(de->d_name[0] == '.')
    continue;
  parse_ldml(de->d_name);
}
closedir(d);
chdir(cwd);
chdir(CLDR_LOC);
@

<<Parse ldml sections>>=
else if(xml_isname(n, "segmentations")) {
  /* segmentation type="<type>" type == CLDR_segmentation_t */
  /* variables/variable id=name: ... -> set vars for rules (uniset) */
  /* segmentRules/rule id=rule_to_replace: rule text */
  /* store in file as overrides for main tables */
  /* need to implement breaking alg with main tables before deciding */
  /* need to compare root.xml with main tables */
}
@

\lstset{language=txt}
<<FIXME>>=
implement

need to decide where data files will go (/usr/share/libuni/<locale>/<data>)
need to decide if an index should be generated (probably yes)

inheritance rules (what a pain)
  bundle:
    strip region, then script, then go to "default" & do same, then root
  item:
    strip region, then script, then go to "root alias" & do same, then root

transforms -> mappings (no real single name for each, though)
  main:transformName(s) even does locale-specific transform name translation
  perhaps for each transorm, add uni_[<var>_]<to>_of_<from>() and vice-versa
  if direction="both"
  can't just encode as table; may be multi-step (e.g. normalize before
  & after).

crappy "set" notation must be implemented for reading LDML files:
 claims UTS18-L1+RL2.5 "with recommended syntax", but then goes on
 to describe a different syntax..  in particular, set ops are different
   "A multi-character string can be in a Unicode set.. uses curly  braces"
     wtf does that mean?  \u{XXXX XXXX}? {c1 c2 c3}?
  literal chars & ranges
  ignore whitespace
  \uXXXX
  \UXXXXXXXX
  \xXX
  \OOO
  \a \b \t \n \v \f \r \\
  \u{X...}
  \u{X.... X...}
  \N{<name>}
  \p{<prop>} [:<prop>:]
  \P{<prop>} [:&<prop>:]
   <prop> is binary or prop-name[: = != =/=]val or script or cat
   script or cat can be ored using |
   val can be ored using |
     General_Category Script Alphabetic Uppercase Lowercase
     White_Space Noncharacter_Code_Point Default_Ignorable_Code_Point
     ANY (0-10FFFF)
     ASCII (0-7F)
     ASSIGNED (!Cn)
     na/name Name_Alias
       only non-binary props are name cat script
       name props must match loosely
  [ ] for grouping (creates a subset)
    adjacent for union
    & for intersection
    - for set difference
      all 3 ops are l-r and same precendence level
      & and - only operate on sets
  [^ ] for inversion
  $<varname> needed for contexts which allow variable assignments
@

\chapter{Additional Testing}

Here is a master driver to call some of the above code for testing.

<<xC Test Support Executables>>=
tst \
@

\lstset{language=C}
<<tst.c>>=
<<Common C Header>>
#include "uni_all.h"

@

For I/O, I'll just use a decent size fixed buffer, and in fact use it
as both a UCS-32 buffer and a character buffer, allowing both at the
same time by marking the divider with [[blen]].

<<tst.c>>=
#define BUF_SIZE 1024
int buf[BUF_SIZE];
int blen;

#define cbuf ((char *)(buf+blen))
#define cblen ((BUF_SIZE - blen) * sizeof(int))
@

The purpose of this program is to take Unicode characters as input,
and do things with them.

<<tst.c>>=
int main(int argc, const char **argv)
{
  <<Get and show characters for [[tst]]>>
  <<Manipulate characters for [[tst]]>>
  return 0;
}
@

For this program, input means command line arguments.  Characters are
specified by either a hexadecimal code point or a Unicode character
name.  They are all appended to [[buf]].

<<Get and show characters for [[tst]]>>=
int cp;

while(--argc > 0) {
  ++argv;
  int l = strlen(*argv);
  const int *mcp;
  cp = cp_of_uni(*argv, l, &mcp);
  if(cp >= 0) {
    /* valid name */
    if(!mcp)
      buf[blen++] = cp;
    else {
      int i;
      for(i = 0; i < cp; i++)
        buf[blen++] = mcp[i];
      cp = mcp[0];
    }
  } else {
    const char *s;
    cp = strtol(*argv, (char **)&s, 16);
    if(!*s) {
      /* valid hex code point */
      buf[blen++] = cp;
    } else {
      /* not valid anything */
      fprintf(stderr, "Can't parse argument %s\n", *argv);
      exit(1);
    }
  }
}
@

Next, we'll print some more information about each character.  This
includes characters artificially added by multi-character sequences.

<<Get and show characters for [[tst]]>>=
int i;
for(i = 0; i < blen; i++) {
  cp = buf[i];
  <<Show character for [[tst]]>>
}
@

First, I'd like to see the Unicode names, as well as the hexadecimal
code points.

<<Show character for [[tst]]>>=
printf("Character %04X\n", cp);
if(uni_gen_name_for(cp, cbuf, cblen))
  printf("Unicode: %s\n", cbuf);
@

Then, maybe some of the character classifications.  These are all
printed out by the [[chartypes]] mainline, but there is no harm in
repeating it.

<<Show character for [[tst]]>>=
fputs("Character class: ", stdout);
#define i cp
<<Print flags and classes of cp [[i]]>>
#undef i
putchar('\n');
@

Next, we'll try some normalization.  But first, we'll print the entire
buffer out before starting, and afer each step.

<<Manipulate characters for [[tst]]>>=
#define prbuf(s) do { \
  fputs(s":", stdout); \
  for(i = 0; i < blen; i++) \
    printf(" %04X", buf[i]); \
  putchar('\n'); \
  putchar('\''); \
  uni_utf8_fputs(buf, blen, stdout); \
  putchar('\''); \
  putchar('\n'); \
} while(0)
prbuf("Start string");
@

Of course these are all destructive.  Another test program should
probably do normalization on copies so that the original can be reused
for more tests.

<<Manipulate characters for [[tst]]>>=
blen = NFD_dec(buf, blen);
prbuf("After decomp");
Canon_Order(buf, blen, 1);
prbuf("After canon-order (NFD)");
blen = NFC_comp(buf, blen, NULL);
prbuf("After canon-comp (NFC)");
blen = NFKD_dec(buf, blen);
prbuf("After compat-decomp");
Canon_Order(buf, blen, 1);
prbuf("After canon-order (NFKD)");
blen = NFC_comp(buf, blen, NULL);
prbuf("After canon-comp (NFKC)");
blen = NFKC_Casefold(buf, blen);
prbuf("After case folding (NFKC_CF)");
@

As a more thorough character name test, we can run every single known
character through the [[tst]] program.  This does not guarantee that
the lists themselves are OK, or that bad names will not trigger false
positives, but it's better than nothing.

<<xTest Scripts>>=
tstchars \
@

\lstset{language=sh}
<<tstchars>>=
#!/bin/sh
<<Common NoWeb Warning>>
@

First, we'll iterate through all of the Unicode names mapping to one
character, both forward and reverse.

<<tstchars>>=
cat charnames.uni{a,} | while IFS=, read a b; do
  b=$(($b))
  hb=`printf %04X $b`
  tr=$(./tst "$a" | head -n 1)
  tr="${tr#* }"
  test $hb = "$tr" || echo "$a,$b $hb"
  tr="`./tst $hb | grep -a '^Unicode:'`"
  tr="${tr#* }"
  test "$a" = "$tr" && continue
  ab=`grep "^$a," charnames.uni{a,} | cut -d: -f2 | cut -d, -f2`
  test "$ab" = "$b" || echo "$hb $a $tr"
done
@

Next, all of the multi-character Unicoded characters.  They can only
be tested for name interpretation, since reverse lookups on
multi-character sequences are impossible.

<<tstchars>>=
cat charnames.uniseq | while IFS=, read a b; do
  tr=
  for x in $(./tst "$a" | grep -a '^Character [0-9A-F]' | cut -d\  -f2); do
    tr=$tr,$((0x$x))
  done
  test ,$b = "$tr" || echo "$a,$b $tr"
done
@

Another useful check is to ensure that the string table's strings are
all actually unique.  This is done by traversing each hash bucket to
see if there are any duplicates.

<<makefile.rules>>=
# explicit strs.gen dep should not be necessary, but gmake flakes on it
hashcheck: uni_strs.o strs.gen
	$(CC) -o $@ $(CFLAGS) $(LDFLAGS) $(EXTRA_CFLAGS) -DHASHCHECK uni_strs.c
@

<<xC Test Executables>>=
hashcheck \
@

\lstset{language=C}
<<uni_strs.c>>=
#ifdef HASHCHECK
int main(void)
{
    int i;
    int err = 0;
    for(i = 0; i < HTAB_SIZE; i++) {
        if(!strhash[i])
            continue;
        int j, k;
        for(j = strhash[i]; builtin_hashe[j - 1].nextent;
	                                    j = builtin_hashe[j - 1].nextent) {
            const char *js = builtin_hashe[j - 1].off + builtin_str;
            int jl = num_before(js, NULL);
            for(k = builtin_hashe[j - 1].nextent; k;
	                                    k = builtin_hashe[k - 1].nextent) {
                const char *ks = builtin_hashe[k - 1].off + builtin_str;
                if(num_before(ks, NULL) == jl && !memcmp(js, ks, jl)) {
                    fprintf(stderr, "duplicate string %d %d %.*s\n",
                            builtin_hashe[j - 1].off, builtin_hashe[k - 1].off,
                            jl, js);
                    err = 1;
                }
            }
        }
    }
    printf("%lu string space; %lu hash entries\n",
           (unsigned long)builtin_str_size,
           (unsigned long)builtin_hashe_size);
    return err;
}
#endif
@

\lstset{language=txt}
<<FIXME>>=
Unihan_IRGSources.txt: kIRG_* (not sure how to encode)
@

\chapter{Code Index}
\nowebchunks

% Some might prefer having the Code Index in an appendix as well.
% For this document, I'm making the appendix the "user documentation"
\appendix

% Begin-doc Users-Guide
\chapter{Building}

Before starting, ensure that all prerequisites are present.  To build
this, build.nw and tjm-ext.nw are required.  If you obtained the PDF
or HTML version of this document, these are included.  Otherwise, they
may be obtained the same place you got this.  In addition, tjm-ext.nw
must be built (using the same build instructions as below, although
the command \texttt{make~libsupt.a} is probably sufficient) in a
separate directory (i.e., without uni.nw in the same directory).  The
other prerequisites are the Unicode data files.  Make sure the file
locations are also set correctly in the config file.

\begin{itemize}
\item The Unicode Character Database; this library was tested using
versions 6.0.0 and 6.2.0.\\
\url{http://www.unicode.org/Public/zipped/}\emph{version}/UCD.zip
\item The Unicode Han Database; this should match the UCD version.\\
\url{http://www.unicode.org/Public/zipped/}\emph{version}/Unihan.zip
\item The Default Unicode Collation Element Table; this should match
the UCD version.  It should be placed in the same directory as the UCD.\\
\url{http://www.unicode.org/Public/UCA/}\emph{version}/*
\item The Unicode Common Locale Data Repository; make sure that the
version is compatible with the UCD version.\\
\url{http://www.unicode.org/Public/cldr/latest/core.zip}
\item The XML entity database; the only version currently supported is
the one at the following URL:\\
\url{http://www.w3.org/2003/entities/2007xml/unicode.xml}
\item The Unicode IDNA compatibility database (optional); this should
match the UCD version.  If present, it should be in the same directory
as the UCD, or in a subdirectory or sibling directory named idna.\\
\url{http://www.unicode.org/Public/idna/}\emph{version}/*
\item The Unicode security information database (optional).  If
present, it should be in the same directory as the UCD, or in a
subdirectory or sibling directory named security.\\
\url{http://www.unicode.org/Public/security/latest/}uts39-data-\emph{version}.zip
\end{itemize}

\input{build-doc.tex} %%% doc
The full, documented [[makefile.config]] is reproduced here for
convenience.  In particular, the non-generic configuration parameters
start at [[UCD_LOC]].

\input{makefile.config.tex} % make

\chapter{API Documentation}

The easiest way to include this library's definitions is using
[[#include "uni_all.h"]].  Linking requires [[-luni]].

\section{Unicode Encodings}

\input{unitypes.tex} %%% doc

To support using valid UTF-8 and UTF-16 for internal storage, macros
are provided to navigate the string.  For consistency, UTF-32
navigation functions are provided as well.  The [[nextc]] and
[[prevc]] macros take a pointer to the start of a character, and
return the offset (in words) to the next or previous character,
respectively. The [[startc]] macro takes a pointer to an arbitrary
word in the string, and returns the offset (in words) to the start of
the character this word belongs to.  Since they are macros, [[buf]]
may be evaluated more than once.

% uni_utf32_startc prototype
% uni_utf32_nextc prototype
% uni_utf32_prevc prototype
% uni_utf16_startc prototype
% uni_utf16_nextc prototype
% uni_utf16_prevc prototype
% uni_utf8_startc prototype
% uni_utf8_nextc prototype
% uni_utf8_prevc prototype

For indexed navigation, functions are provided to convert an offset
into a word array to/from an index into the string.  Again, the
behavior is undefined if the UTF-8 or UTF-16 encoding is not valid up
to (but not necessarily including) the desired offset/index.  These
functions are expensive in that they scan the string from the
beginning for every call; use the macros above for small index
adjustments.  The 32-bit versions are dummy functions that return
[[n]], since offset and index are always identical.  They are mainly
for writing generic code using macros.

% uni_utf32_offset_of prototype
% uni_utf32_index_of prototype
% uni_utf16_offset_of prototype
% uni_utf16_index_of prototype
% uni_utf8_offset_of prototype
% uni_utf8_index_of prototype

To determine the length of a string of fixed word length, the
[[index_of]] function for the appropriate type can be used with the
length as the offset.  To determine the length of a zero-terminated
string, the following functions are provided.

% uni_utf32_strlen prototype
% uni_utf16_strlen prototype
% uni_utf8_strlen prototype

For truly random access to Unicode code points, they should be
converted to UTF-32 first.  For internally generated, guaranteed valid
strings, the following functions can be used to do that.  They return
the code point starting at the valid UTF-8/UTF-16 encoding at [[s]]. 
The optional [[nread]] parameter returns the number of words actually
read, although the [[nextc]] macros above could be used instead.

% uni_valid_utf8_decode prototype
% uni_valid_utf16_decode prototype

In fact, they should always be stored in the architecture's native
endianness.  For this reason, a set of functions without endianness
control is provided.  The fact that they are for internal use only is
emphasized by the ``int'' in the name.  While the UTF-32 versions do
mostly nothing, and the UTF-8 versions just copy other functions, a
complete set is provided to make creation of parallel routines for
differing inputs using preprocessor macros easier.  Each encoder takes
a pointer to a sufficiently large buffer (1 word for UTF-32, 2 for
UTF-16, and 4 for UTF-8) and the code point to encode, and returns the
number of words actually written.  Each decoder takes a pointer to
valid data, and returns the code point stored at that pointer.
Optionally, int can also return the number of words to skip if
[[nread]] is non-[[NULL]].

% uni_int_utf8_decode prototype
% uni_int_utf16_decode prototype
% uni_int_utf32_decode prototype
% uni_int_utf8_encode prototype
% uni_int_utf16_encode prototype
% uni_int_utf32_encode prototype

If the required output buffer length is needed for a character, one of
the following macros may be used.  They may evaluate their arguments
more than once.  These compute the output length of the encoder
functions.  To compute the amount consumed by the decoder functions,
use the [[nextc]] macros above.

% uni_utf8_enclen prototype
% uni_utf16_enclen prototype
% uni_utf32_enclen prototype

To directly compare two strings in a valid UTF format, [[memcmp]] can
be used, given the computed length.  For convenience, the following
functions can be used instead.  Note that these do not use the Unicode
Collation Algorithm or perform any locale-specific transformations;
they simple compare code point values.  Each string length may be
positive to indicate the number of words in the string (not
necessarily the number of characters), and it may be negative to
indicate zero termination.

% uni_int_utf32_strcmp prototype
% uni_int_utf16_strcmp prototype
% uni_int_utf8_strcmp prototype

To transform UTF-32 data to and from UTF-32BE and UTF-32LE, the
following functions are used.  The endianness is specified by
[[bige]]; zero specifies little-endian and non-zero big-endian.  The
encoder takes a [[cp]] and stores its result in a buffer; the stored
length is returned (0 on error).  The decoder takes a character from a
buffer, and returns the decoded character.  The return value is a
signed integer; a negative result indicates an error (i.e., an invalid
Unicode code point).

% uni_utf32_encode prototype
% uni_utf32_decode prototype

Similarly, conversion of UTF-32 to and from UTF-16BE and UTF-16LE is
done using the following functions.  At least two words must be
available in the encoder's return buffer.  If the buffer may be
invalid, ensure that at least two words are available in the decoder's
input buffer as well (padding with zero is safe).  The results and
parameters have the same meaning as for the utf32 functions above,
except that the decoder also returns the number of words it read from
the buffer in [[*nread]] (optionally, if [[nread]] is non-[[NULL]]).

% uni_utf16_encode prototype
% uni_utf16_decode prototype

Similarly, conversion of UTF-32 to and from UTF-8 is done using the
following functions.  At least 4 bytes must be available in the
encoder's return buffer.  If the buffer may be invalid, ensure that at
least 4 characters are available in the decoder's input buffer
(padding with zero is safe).  The results and parameters have the same
meaning as for the utf16 functions above.

% uni_utf8_encode prototype
% uni_utf8_decode prototype

To read and write these types directly from/to a file, the following
functions may be used.  The writers return the number of words in the
target format (e.g. 32-bit words for UTF-32) written to the file on
success, and zero or less on failure.  If the failure was due to a
short write, the return value is the negative of the number of words
actually written.  Use [[ferror]] to distinguish between invalid input
and read/write errors.  The readers return
% Begin-doc utf-reader-ret
$-1$ if no characters could be read, $-2$ on a short (less than a full
code point) read, $-3$ on invalid code points, and otherwise the read
character.
% End-doc utf-reader-ret

Note that due to limitations of the C [[FILE]] API, it may not be
possible to recover from errors reading UTF-16 input.  A read of an
initial surrogate character, followed by anything but an initial
surrogate character will result in the file's read pointer advanced
past the second word, which can no longer be returned to the input
stream.  However, the [[nread]] parameter, if non-[[NULL]], can be
used to determine the number of actual bytes read from the input
stream.  If that is 4, and an error occurred, then recovery could be
done by seeking backwards in the input stream (assuming it is
seekable).

% uni_utf32_putc prototype
% uni_utf16_putc prototype
% uni_utf8_putc prototype
% uni_utf32_getc prototype
% uni_utf16_getc prototype
% uni_utf8_getc prototype

For convenience, UTF-32 strings may also be written to UTF-8 files.  A
UTF-32 buffer [[buf]] of length [[len]] is written to UTF-8 file
[[f]].  Like the single-character output functions above, the return
value is the number of words (bytes in this case) written on success,
or less than or equal to zero on error.  If less than zero, this is
the (short) number of bytes written.

% uni_utf8_fputs prototype

Reading files whose encoding is unknown (e.g. UTF-16 and UTF-32, whose
endianness is determined by the file contents) is supported by a
separate mechanism.  Instead of using the standard C [[FILE]] type, it
uses the opaque type [[uni_file_t]].  Functions to open, close, and
read from such files are provided.

The [[uni_fopen]] function supports multiple input file encodings, as
specified by the [[encoding]] parameter.  If the library was compiled
with [[iconv]] support, all available input encodings for [[iconv]]
are supported.  Otherwise, only the raw input formats, listed below,
are supported.  The default (if [[encoding]] is [[NULL]]) is to use
the current locale's encoding if [[iconv]] support is compiled in and
the current locale's encoding can be obtained, or \verb|UTF|
(generic Unicode) if not.

\paragraph*{Built-in Unicode encodings (case-sensitive):}
\begin{quote}
\begin{description}
\item[ANSI\_X3.4-1968] --- ASCII (i.e., raw bytes; no range checking)
\item[ISO-8859-1] --- ISO Latin-1 (i.e., raw bytes)
\item[UTF] --- Automatically detected UTF mode
\item[UTF-8] --- UTF-8
\item[UTF-16] --- UTF-16; if first character in file is U+FEFF in
inverse native byte order, use that order; otherwise, use native byte order
\item[UTF-16LE] --- Little-endian UTF-16
\item[UTF-16BE] --- Big-endian UTF-16
\item[UTF-32] --- UTF-32; if first character in file is U+FEFF in
inverse native byte order, use that order; otherwise, use native byte order
\item[UTF-32LE] --- Little-endian UTF-32
\item[UTF-32BE] --- Big-endian UTF-32
\end{description}
\end{quote}

Note that unlike [[uni_utf16_getc]], all of the internal encoding
types consume the longest erroneous sequence, and no more, when an
encoding error is encountered.

For generic Unicode input ([["UTF"]]), if the first character could be
Byte Order Mark (U+FEFF) in any supported format, it is considered to
be one, and the format of the file is the format of that Byte Order
Mark:

\begin{quote}
\begin{tabular}{ll}
First bytes of file&Format\\
\texttt{00 00 FE FF}&UTF-32BE\\
\texttt{FF FE 00 00}&UTF-32LE\\
\texttt{FE FF}&UTF-16BE\\
\texttt{FF FE}&UTF-16LE\\
Other&UTF-8\\
\end{tabular}
\end{quote}

Note that a UTF-16LE file whose first character after the Byte Order
Mark is zero is determined to be a UTF-32LE file, instead.  While a
potential conflict exists between all other formats and ``Other'',
there is no valid UTF-8 character which starts with FF or FE, so there
should be no conflict.  Also, the Byte Order Mark is part of the file
data, so it is returned as the first character.

The return value for [[uni_fopen]] is [[NULL]] on error, or a pointer
to the file structure.  This structure is freed by [[uni_fclose]].
The [[uni_fgetc]] return value is
\input{utf-reader-ret.tex} %%% doc

% uni_fopen prototype
% uni_fclose prototype
% uni_fgetc prototype

Since normalization is a common requirement for user input, a function
to read a single normalized character is provided as well.  The [[nt]]
parameter takes a normalization type, corresponding to the standard
normalization type names:  [[UNI_NORM_NFD]], [[UNI_NORM_NFKD]],
[[UNI_NORM_NFC]], [[UNI_NORM_NFKC]], and [[UNI_NORM_NFKC_CF]].  For
completeness, no normalization can be specified using
[[UNI_NORM_NONE]], although it is cheaper to just use [[uni_fgetc]].

The entire file must be read using this function in order for it to
work properly.  Changing the normalization type mid-stream is also
generally not supported, since any normalization in progress will not
be affected.

Errors in the input stream are ignored.  Encoding errors and invalid
Unicode characters (surrogates and code points larger than
[[0x10FFFF]]) are converted to U+FFFD, REPLACEMENT CHARACTER.  There
is no way to determine the difference between the presence of an
actual REPLACEMENT CHARACTER and invalid text.

% uni_fgetc_norm prototype

\section{Data Types for Property Storage}

\subsection{Bit Sets}

The following functions support storage of Unicode boolean properties
in simple bit sets.  [[a]] is an array of any unsigned integer data
type.  The [[UNI_BSET_ELT]] and [[UNI_BSET_BIT]] convert an index
[[i]] into an array index [[e]] and bit number within that array
element [[b]].  The [[UNI_BSET_ENTRY]] function performs the reverse
operation, giving an index [[i]] from an element index [[e]] and bit
index [[b]].  The [[UNI_BSET_MASK]] function converts the bit index
into a mask.  Several common uses for this mask are taken care of:
[[UNI_BSET_IS_SET]] tests if a given index is set, and
[[UNI_BSET_SET]] and [[UNI_BSET_CLEAR]] modify a specific bit.  The
final useful function is [[UNI_BSET_AELT]], which returns the element
at index [[UNI_BSET_ELT]].  This is an addressable lvalue; that is,
you can take its address or assign a value to it.

Normally, extra code would be added to shorten the array by indicating
a lower and upper bound.  For convenience, the common upper bound,
which is the largest valid Unicode code point, is provided as a
preprocessor symbol ([[UNI_MAX_CP]]).

% UNI_BSET_ELT prototype
% UNI_BSET_BIT prototype
% UNI_BSET_ENTRY prototype
% UNI_BSET_MASK prototype
% UNI_BSET_IS_SET prototype
% UNI_BSET_SET prototype
% UNI_BSET_CLEAR prototype
% UNI_BSET_AELT prototype

\subsection{Sorted Code Point Arrays}

An array of any data type whose first member is a 32-bit signed
integer can be searched and sorted using the standard C [[bsearch]]
and [[qsort]] functions with the following comparison routine:

% uni_cmp_cp prototype

Note that valid Unicode code points are always positive when stored in
a signed 32-bit integer.  A convenience wrapper for [[bsearch]] is
provided, but in general a hand-written binary search can perform
significantly faster.

% uni_is_cp prototype

For convenience, a structure is defined for code point arrays with
32-bit values.  As the [[cp]] member is first, [[uni_cmp_cp]] works
with arrays of this type as well, if [[cp]] is a valid Unicode code
point.  If not, care must be taken that the value never exceeds the
maximum [[int32_t]] value ([[0x7fffffff]]).

\input{[[uni_cp_val_t]].tex} % C

\subsection{Sorted Code Point Range Arrays}

Sorted code point range arrays are arrays of structures whose first
two members define an (inclusive) range; the ranges do not overlap, so
sorting can be done using only the low element.

\input{[[uni_chrrng_t]].tex} % C

An array of any data type whose first two members are 32-bit signed
integers indicating a contiguous range of values, as per the
[[uni_chrrng_t]] data type (if the [[low]] and [[high]] values do not
exceed the maximum [[int32_t]] value), may be searched and sorted
using the standard C [[bsearch]] and [[qsort]] functions with the
following comparison routine:

% uni_cmprng prototype

A convenience wrapper for binary searching is provided.  It uses a
hand-written binary search for significant performance improvement
over using [[bsearch]].  However, unlike the macro provided for single
code point arrays, this function can only be used for arrays of the raw
[[uni_chrrng_t]] type.  Since it uses the raw structure, values
exceeding the maximum [[int32_t]] value are treated as unsigned values
still, so care must be taken that sorting either uses a different
comparison function than above or that the values never exceed this
value.

% uni_is_cp_chrrng prototype

In addition to plain ranges, ranges may store data.  One variant,
[[uni_chrrng_dat8_t]], stores a byte of data along side a
24-bit high range value.  This keeps the structure the same size, but
makes it a bit less efficient for searching:

\input{[[uni_chrrng_dat8_t]].tex} % C

The comparison function for [[qsort]] and [[bsearch]] is different,
since it needs to mask out the value:

% uni_cmprng_dat8 prototype

Since the return value is a byte rather than a flag, the wrapper for
binary searching (again, using a hand-written implementation rather
than [[bsearch]]) returns the byte value, or a given default ([[def]])
if the code point is not in the array.

% uni_chrrng_dat8 prototype

16-bit and 32-bit variants are provided as well.  Each attempts to
keep the size of the structure down to 16 bytes.  The 16-bit version
does this the same way as the 8-bit version:  by storing the code
points as 24-bit numbers, and using the remaining 8 for storage.  The
32-bit version uses a low+length method rather than low-high method,
and disallows ranges longer than 256.  Again, special comparison
functions for [[qsort]] and [[bsearch]] are provided, along with
optimized binary searches.

\input{[[uni_chrrng_dat16_t]].tex} % C
\input{[[uni_chrrng_dat32_t]].tex} % C
% uni_cmprng_dat16 prototype
% uni_chrrng_dat16 prototype
% uni_cmprng_dat32 prototype
% uni_chrrng_dat32 prototype

A special variant of the 32-bit type above interprets the 32-bit value
as a rational number with a 24-bit signed numerator and an 8-bit
unsigned denominator.  The 32-bit number is directly cast to this data
type.  Technically, any pair of numbers could be stored in this
structure; the only thing the functions do to support this
interpretation is to set the denominator to one if the numerator is
zero, so that zero can be more conveniently stored as zero over zero.
Also, the default return value is always zero (over one).

\input{[[uni_frac_t]].tex} % C

While the [[uni_cmprng_dat32]] function could be used, the following
function returns the correctly interpreted numerator and denominator.

% uni_chrrng_val prototype

\subsection{Multilevel Tables}

Multi-level tables are bit arrays split into smaller chunks, with
pointers to those chunks in a table at the next level.  Each level
except the top is also split into smaller chunks with the next level
being pointers to that level.  Duplicates in lower levels are removed
by having only one copy with multiple pointers to that copy.  All-zero
data and all-one data are also stored with special pointer values,
making sparse and repetitive bit arrays fairly storage-efficient.

The tables are stored as an opaque sequence of 32-bit words which are
interpreted dynamically.  When reading such a data structure, only the
pointer to the first word is needed.  When creating such a structure,
both the sequence of words and the total length are returned.

Use the following function to convert a plain bit array to a
multi-level table.  It may take a long time to run, so generally it
should be used to pre-generate tables.  The bit array is described by
[[bits]], which is an array of [[len]] bytes.  The [[low]] and
[[high]] numbers are the valid bit numbers; it is up to the user to
ensure that [[high]] - [[low]] $<$ [[len]] times 8.  The data need
not be a bit array; arbitrary data structures are supported.  The
[[minwidth]] parameter is the number of bytes in the data structure;
this is ignored if less than 4.  It is possible to make values outside
of the [[low]] to [[high]] range be either all zeroes or all ones;
[[def]] should be one or zero to indicate which.  The return value,
[[*ml]], is always allocated using [[malloc]], and contains [[*ml_len]]
32-bit words.

% uni_bits_to_multi prototype

Once a table [[dat]] is generated, the first byte of data for a code
point [[val]] can be found using [[uni_multi_tab_lookup]].  For data
structures whose size is not one byte, the code point must be
multiplied by the number of bytes.  For bit arrays, this means
dividing by 8.  For bit arrays, the actual bit must then be obtained
using a bit mask.

The return value from the function is zero for all zeroes, [[~0]] for all
ones, and 1 for anything else.  [[ret]] is [[NULL]] for all zeroes or
all ones, and a pointer to the first byte of the result otherwise.  If
the code point is outside the range defined by the table ([[low]] and
[[high]] during creation), and [[def]] is zero, the default provided
during table creation is returned (i.e., all zeroes or all ones).  If
[[def]] is non-zero, [[def]] is returned by the function, but [[ret]]
is [[NULL]].  Note that if [[def]] is [[~0]], there is no way to tell
the difference between out-of-range and all ones.

% uni_multi_tab_lookup prototype

The above lookup function returns a pointer to raw data.  Since they
are most common, bit array tables can be queried using a simpler
function, which takes the unscaled code point and returns a boolean
flag.

% is_uni_x prototype

To get a list of all members of a multi-level table bit array, it can
be converted into a sorted range list.  The result ([[*ret]]) is always
allocated using [[malloc]].

% uni_multi_bit_to_range prototype

The inverse operation is also possible, but it consumes a large amount
of time and memory.  Internally, the range list is first converted
into a plain bit array, which is then converted using
[[uni_bits_to_multi]] above.  The return value is the multi-level table,
allocated using [[malloc]].  If [[ml_len]] is non-NULL, the storage
size of the table is returned as well.

% uni_rng_to_multi_bit prototype

The multi-level tables can store more information, as well.  The
lookup function merely returns a pointer to a byte.  This byte can be
a collection of bits, or it can be a byte value.  For the latter case,
a separate generic lookup function is provided.  This takes a
byte-sized default value, and returns this if the byte is out-of-range
or zero.  Otherwise it returns the stored value, minus one.  This way,
the default value is stored most efficiently (as a zero), and the rest
only require minor adjustment on return.

% uni_x_of prototype

A function is provided to convert a [[uni_chrrng_dat8_t]] sorted range
list to an offsetted multi-level byte array.  However, no inverse
function is provided.  This operation is not really intended for end
users, but for the static table generator.  Note that while the
default value must be provided, it is not stored, but instead is used
to remove the default value from the table by converting it to zero.

% uni_rng_dat8_to_multi prototype

By multiplying the value size with the lookup index, larger values can
be stored as well.  For an intermediate-sized value, a raw 16-bit or
32-bit value is supported.  Both lookup and conversion from a
[[uni_chrrng_dat16_t]] type or a [[uni_chrrng_dat32_t]] type are
supported.  However, the inverse conversion is not supported.

% uni_x_dat16 prototype
% uni_rng_dat16_to_multi prototype
% uni_x_dat32 prototype
% uni_rng_dat32_to_multi prototype

Another implementation is to store the numerator and denominator of a
rational number as a 32-bit value: 24 bits for a signed numerator, and
8 bits for an unsigned denominator.  A special lookup function is
provided to split out the numerator and denominator from the 32-bit
value; otherwise use the 32-bit functions above.

% uni_x_val prototype

For values where ranges are inappropriate, a function is provided to
convert a sorted code point array with 32-bit values (nominally
[[uni_cp_val_t]]) into a multi-level table.  No inverse function is
provided.  No generic lookup function is provided, either; remember to
multiply code point values by 4 when using [[uni_multi_tab_lookup]].

% uni_cp_val_to_multi prototype

\section{All Properties}

Information on all currently supported properties are provided by
several arrays.  They are intended to be used to scan for desired
properties, find property names and their aliases, and generate data
based on that.  However, there is no harm in using it if you intended
to support every single property, anyway.  Including a reference to
any of these symbols will likely defeat the static linking property of
this library, pulling every single property's data in.

\begin{itemize}
\item [[const uni_propdesc_t uni_propdesc[]]] is an array whose length
is given by the preprocessor symbol [[uni_propdesc_len]].  Each entry
describes a property, and points to its data.  See below for a
description of the elements' data structure.
\item [[uni_propdesc_valueof]] is an array whose length is given by
the preprocessor symbol [[uni_propdesc_valueof_len]].  It is a sorted
array of names for lookup of properties by name rather than sequence
number.  See Enumeration Properties (section \ref{sec:enum}) for
details on the structure and how to use it.
\item [[uni_propdesc_valueof_approx]] is an array whose length is
given by the preprocessor symbol [[uni_propdesc_valueof_approx_len]].
Its usage is similar to the previous item; see Enumeration Properties
(section \ref{sec:enum}) for details.
\end{itemize}


The [[name]] field contains the canonical name and all known aliases
for the property name; see Enumeration Properties (section
\ref{sec:enum}) for details on the [[uni_alias_t]] structure and how
to use it..  The [[mtab]] and [[mtab_len]] fields describe the
multi-level table for the property.  The [[tab]] and [[tab_len]]
fields describe the sorted range table or sorted code point table for
the property.  See the sections below for details on what these tables
contain.  The [[type]] field determines which section describes the
tables:

\begin{itemize}
\item [[UNI_PROP_TYPE_BOOL]]: Boolean Properties
\item [[UNI_PROP_TYPE_ENUM]]: Enumeration Properties; in addition, the
name translation tables are in [[nameof]], [[valueof]],
[[valueof_approx]], [[nameof_len]], and [[valueof_len]].  The default
is in [[def]].
\item [[UNI_PROP_TYPE_NUM]]: Numeric Properties
\item [[UNI_PROP_TYPE_STR]]: String Properties; in addition, the
string table is in [[strs]] and [[strs_len]].
\item [[UNI_PROP_TYPE_DAT16]]: The FCD property (sec. \ref{FCD})
\end{itemize}

\input{[[uni_propdesc_t]].tex} % C

\section{Boolean Properties}

The supported boolean properties are listed in table
\ref{tab:boolprop}.  For each supported boolean property \emph{BP},
the following symbols are defined:

\begin{itemize}
\item [[const uni_chrrng_t uni_]]\emph{BP}[[_rng[]]] is a sorted range
table with entries for that property.
\item [[uni_]]\emph{BP}[[_rng_len]] is the number of entries in that
array (a preprocessor symbol).
\item [[const uint32_t uni_]]\emph{BP}[[_mtab[]]] is a multi-level
table for that property.
\item [[uni_]]\emph{BP}[[_mtab_len]] is the length of that table (a
preprocessor symbol), in case it is to be written out to a file.
\item [[int is_uni_]]\emph{BP}[[(uint32_t cp)]] is a boolean lookup
function for that property.  It is actually a preprocessor macro which
uses the multi-level table and [[is_uni_x]] internally.
\end{itemize}

For static linking, each property is in a separate object file, and
the sorted range table and multi-level table are stored separately, as
well.

\begin{table}
\begin{centering}
\begin{tabular}{lllll}
AHex&ASSIGNED&Alpha&Bidi\_C&Bidi\_M\\
CE&CI&CWCF&CWCM&CWKCF\\
CWL&CWT&CWU&Cased&Comp\_Ex\\
DI&Dash&Dep&Dia&Ext\\
Gr\_Base&Gr\_Ext&Hex&Hyphen&IDC\\
IDS&IDSB&IDST&Ideo&Join\_C\\
LOE&Lower&Math&NChar&NFD\_QC\\
NFKD\_QC&OAlpha&ODI&OIDC&OIDS\\
OLower&OMath&OUpper&Ogr\_Ext&Pat\_Syn\\
Pat\_WS&QMark&Radical&SD&STerm\\
Term&UIdeo&Upper&VS&WSpace\\
XIDC&XIDS\\
\end{tabular}

{\small Note: Hyphen is a deprecated property.}
\caption{\label{tab:boolprop}Boolean properties (Unicode canonical short name)}
\end{centering}
\end{table}

\subsection{Bit Set Operations}

The following functions perform a generic bit operation (see table
\ref{tab:setop2} for a complete list of operations).

\input{op-table.tex} %%% doc
\caption{\label{tab:setop2}Set Operations}
\end{table}

Bit arrays consisting of a single unsigned integer can be operated on
using the following macro:

% UNI_BIT_SET_OP prototype

The [[op]] argument is intended to be one of the [[uni_setop_t]]
enumeration constants.

\input{[[uni_setop_t]].tex} % C

Arbitrary bit arrays of standard C unsigned integer types can be
operated on using the following functions.  The inputs are bit arrays
[[a]] and [[b]], which start at [[a_low]] and [[b_low]], respectively,
and contain [[a_len]] and [[b_len]] elements of the particular bit
size, respectively.  Although the endianness of each element of an
array is irrelevant, the array itself is assumed to be in
little-endian order if the arrays are not of identical size and
offset.  The result ([[res]]) is allocated from new memory using
[[malloc]] if the pointed-to pointer is NULL, or resized using
[[realloc]] if non-NULL and the reported [[res_max]] is too small. 
Its low bit is [[res_low]].  The number of relevant elements of the
bit size is returned in [[res_len]], and the actual number of elements
allocated is returned in [[res_max]].  All return pointers must be
non-[[NULL]].

% uni_setop_bit64 prototype
% uni_setop_bit32 prototype
% uni_setop_bit16 prototype
% uni_setop_bit8 prototype

Since sorted code point arrays are generally inefficient for storing
Unicode boolean properties (sets), no set operations are provided for
this data type.  However, the following sample code may be used if
needed:

\input{Perform set ops on cp array.tex} % C

For sorted code point range arrays, a single function is provided.  It
always allocates its results using [[malloc]].

% uni_chrrng_setop prototype

No set operations are provided for multi-level tables at this time.
Implementation would be insanely complex given the current
implementation, and should not be performed frequently, anyway.  The
only way to perform these operations right now is to convert the
multi-level tables to range arrays first, and then convert the result
back.  Alternately, the lookup function can simply be called for each
table, and the lookup results combined instead, using
[[UNI_BIT_SET_OP]].  For example:

\begin{quote}
\begin{verbatim}
res = UNI_BIT_SET_OP(is_uni_X(cp), op, is_uni_Y(cp)) & 1;
\end{verbatim}
\end{quote}

\section{\label{sec:enum}Enumerated Properties}

For lists of names, there are both the canonical names and aliases.
These are stored in an array of [[uni_alias_t]] structures.  Any
undefined names are [[NULL]].

\input{[[uni_alias_t]].tex} % C

For reverse lookups, the [[uni_valueof_t]] structure's [[val]] field
is the index of that name in the alias array.

\input{[[uni_valueof_t]].tex} % C

An array of [[uni_valueof_t]] structures can be searched or sorted
using the generic comparison function.

% uni_cmp_valueof prototype

If the name entry is stripped of separators and converted to
lower-case, then the Unicode approximate matching technique can be
applied.  This converts the input string to lower-case, strips out
separators (dashes, underscores, and spaces), and, if the raw string
is not found, strips off a leading ``is''.

% uni_x_valueof_approx prototype

It performs its function by first stripping the name; manual stripping
can be done with the following function.  After stripping, the name
can be looked up directly using [[uni_cmp_valueof]] twice:  first
without further modification, and second after striping any leading
``is''.

% uni_name_strip prototype

The list of supported enumeration properties is in table
\ref{tab:enumprop}.  For each enumerated property \emph{EP}, the
following symbols are defined:

\begin{itemize}
\item [[uni_]]\emph{EP}[[_t]] is a C enumeration type whose literals
correspond to the property's values.
\item For each canonical enumeration value name \emph{EN}, the symbol
[[UNI_]]\emph{EP}[[_]]\emph{EN} is defined, which is an enumeration
literal of type [[uni_]]\emph{EP}[[_t]].  Enumeration literals
corresponding to canonical aliases are defined as well, as long as they
do not contain a minus sign or a decimal point.
\item [[UNI_NUM_]]\emph{EP} is another enumeration literal of type
[[uni_]]\emph{EP}[[_t]], which corresponds to the number of unique
values.  Note that where canonical aliases are present, this number
is smaller than the actual number of enumeration symbols.  Also, where
numerical aliases exist, this may be larger than the number of symbols
present.
\item [[const uni_alias_t uni_]]\emph{EP}[[_nameof[]]] is an array of
names corresponding with the enumeration literals.  The index is
simply a valid enumeration literal value, or any positive integer less
than [[UNI_NUM_]]\emph{EP}.
\item [[const uni_valueof_t uni_]]\emph{EP}[[_valueof[]]] is a sorted
array of all unique names (including aliases) for this property; the
[[val]] field in each entry is the associated enumeration literal.
\item [[uni_valueof_t uni_]]\emph{EP}[[_valueof_approx[]]] is a sorted
array of all unique names (including aliases) for this property,
stripped of underscores and converted to lower-case for loose
matching.  Like the [[valueof]] array, the [[val]] field in each entry
is the associated enumeration literal.
\item [[uni_]]\emph{EP}[[_valueof_len]] is the length of the
[[valueof]] and [[valueof_approx]] arrays (a preprocessor symbol).
\item [[uni_]]\emph{EP}[[_t]] [[uni_]]\emph{EP}[[_lookup(const char *n)]]
is a lookup function which will strip underscores, dashes, and spaces
from the lookup string, optionaly strip off any Is, and return the
enumeration constant matching that string.  This is actually a
preprocessor macro which uses the [[uni_x_valueof_approx]] function
with the [[valueof_approx]] array internally.
\item [[const uni_chrrng_dat8_t uni_]]\emph{EP}[[_rng[]]] is a sorted
array of ranges.  The default value is not provided here.
\item [[uni_]]\emph{EP}[[_rng_len]] is the length of the range
array (a preprocessor symbol).
\item [[uint32_t *uni_]]\emph{EP}[[_mtab]] is a multi-level table whose
byte-length values correspond to the enumeration literal values.
\item [[uni_]]\emph{EP}[[_mtab_len]] is the size of the multi-level
table (in words), in case the table is to transferred or written out.
\item [[int uni_]]\emph{EP}[[_of]] is a preprocessor macro which
returns the enumeration value for any code point.  The canonical
default is returned when the table has no entry.  Internally, the
multi-level table and [[uni_x_of]] are used for lookup.
\end{itemize}

For static linking, each property is in a separate object file.  In
addition, the range tables and multi-level tables are stored
separately.  The two plain name tables are stored together in order to
share constant name strings, but are stored separately from the range and
multi-level tables.  The approximate name table is stored separately,
as well.

\begin{table}
\begin{centering}
\begin{tabular}{llll}
age&bc&blk&ccc\\
dt&ea&gc&GCB\\
hst&jg&jt&lb\\
NFC\_QC&NFKC\_QC&nt&sc\\
SB&WB&IDNA\_Status&ID\_Restrict\_Status\\
ID\_Restrict\_Type&&&\\
\end{tabular}
\caption{\label{tab:enumprop}Enumerated properties (Unicode canonical short name)}
\end{centering}
\end{table}

The IDNA\_Status property will not be available if the IDNA data files
were not present when this library was compiled.  The
ID\_Restricted\_Status and ID\_Restricted\_Type fields will not be
available if the security data files were not available when this
library was compiled.  The ID\_Restrict\_Type enumeration literals
change minus signs to underscores; this is the only current
enumeration that supports enumeration names with minus signs.

In addition, the gc property defines a symbol to support aliases which
cover more than one value.  For example, the enumeration literal
[[UNI_gc_Z]] is actually an alias for [[UNI_gc_Zl]], [[UNI_gc_Zp]],
and [[UNI_gc_Zs]].  The [[const uint64_t uni_gc_trans[]]] table takes
an enumeration literal as index, and returns a bit mask with all
appropriate base classes included with this literal set.  The base
classes themselves only have one bit set; to check if a class is a
base class, simply check if the bit is set for itself.

There are two additional support macros for the gc property.
[[is_uni_nl]] checks if a character is any valid newline-type character,
and [[is_uni_bs]] checks if a character is any valid backslash-type
character.  [[is_uni_bs]] is not necessary for text already processed
using NFKC\_CF.

% is_uni_nl prototype
% is_uni_bs prototype

\section{Numeric Properties}

Each numeric property \emph{NP} is supported by the following symbols:

\begin{itemize}
\item [[const uni_chrrng_dat32_t uni_]]\emph{NP}[[_rng[]]] is a sorted
range table whose value is a rational representation of the numeric
value.  You can cast a pointer to the [[dat]] field to a pointer to
[[uni_frac_t]] to retrieve the value.
\item [[int uni_]]\emph{NP}[[_rng_len]] is the length of the sorted
range table.
\item [[const uint32_t *uni_]]\emph{NP}[[_mtab[]]] is a multi-level
table with the same data.
\item [[uni_]]\emph{NP}[[_mtab_len]] is the length of that table (a
preprocessor symbol), in case it is to be written out to a file.
\item [[void uni_]]\emph{NP}[[_of(uint32_t cp, int32_t *num, uint8_t *denom)]]
is a simple wrapper lookup function that uses the multi-level table to
retrieve the numerator and denominator.
\end{itemize}

The age property is officially numeric, but not supported as such. 
Instead, it is only supported as an enumerated property.  The ccc
property is similarly only enumerated, but the value of the
enumeration literal is equal to the numeric value of the property as
well.

The slc, suc, stc and scf properties are technically string properties,
not numeric properties.  However, since they always map to exactly one
character (or nothing), they can be considered numeric properties
whose value is the translated code point (or zero if nothing).

In addition, the value for the cjkRSUnicode field is interpreted as a
series of dotted numbers, with optional exclamation points.  This is
all encoded into the numerator and denominator fields, and can be
extracted using decoder macros.  The [[l]] parameter is set to the
number before the decimal point, and the [[r]] is set to the number
after the decimal point.  If both are zero, the number is not present.
If [[r]] is negative, then an exclamation point precedes the decimal
point, and the right side is the absolute value of [[r]].  When there
is more than one value, the [[val2]] macro will return a non-zero value.

% uni_kRSUnicode_val1 prototype
% uni_kRSUnicode_val2 prototype

\begin{table}
\begin{centering}
\begin{tabular}{llll}
nv&cjkRSUnicode&slc&suc\\
stc&scf&bmg&\\
\end{tabular}

{\small Note: slc, suc, stc, scf, and bmg are technically string
properties, but always return either nothing or a string of length
one.  For the former case, 0 is returned; for the latter, the single
code point.}
\caption{\label{tab:numprop}Numeric properties (Unicode canonical short name)}
\end{centering}
\end{table}

\section{String Properties}

Properties with string values are stored in two parts: a constant
string table, and tables which have offsets and lengths of strings
within that table.  Multiple properties may use the same string table.
The offset and length tables are either sorted code point arrays of
type [[uni_str_arr_t]], or multi-level tables of 32-bit values, which
are interpreted as (and can be cast into) [[uni_str_ptr_t]].

\input{[[uni_str_arr_t]].tex} % C
\input{[[uni_str_ptr_t]].tex} % C

Note that the [[uni_str_arr_t]] data structure can also be cast to the
[[uni_cp_val_t]] type, whose [[val]] element can then be cast to
the [[uni_str_ptr_t]] type.

Searching the multi-level table for a string can be done generically
using the following function; it is used internally by the
per-property lookup functions.  The returned pointer is always valid;
a lookup failure is detected by checking the offset and length.  If
both are zero, then a lookup failure occurred.  Zero-length strings
have non-zero offsets to avoid looking like a failure.

% uni_x_str_of prototype

The supported string properties are listed in table
\ref{tab:stringprop}.  Unless otherwise stated in the subsequent
sections, for each string property \emph{SP}, the following symbols
are defined:

\begin{itemize}
\item [[const uint32_t uni_]]\emph{SP}[[_strs[]]] is a string table
containing all string values.
\item [[uni_]]\emph{SP}[[_strs_len]] is the length of the string table (a
preprocessor symbol), in case it needs to be written out to a file.
\item [[const uni_str_arr_t uni_]]\emph{SP}[[_arr[]]] is a sorted list
of code points with their associated string offsets and lengths.
\item [[uni_]]\emph{SP}[[_arr_len]] is the length of the above array
(a preprocessor symbol).
\item [[const uint32_t uni_]]\emph{SP}[[_mtab[]]] is a multi-level
table of 32-bit [[uni_str_ptr_t]] values.
\item [[uni_]]\emph{SP}[[_mtab_len]] is the length of that table (a
preprocessor symbol), in case it is to be written out to a file.
\item [[const uni_str_ptr_t *uni_]]\emph{SP}[[_of(uint32_t cp)]]
is a simple wrapper lookup function that uses the multi-level table to
retrieve string information as an offset and length in the string
table.  A zero-length string is distinguished from a missed lookup by
the offset field:  only missed lookups have both offset and length of
zero.
\end{itemize}

For static linking, the string table, sorted code point table, and
multi-level table are all in separate object files.

\begin{table}
\begin{centering}
\begin{tabular}{llll}
dm&canon\_decomp&compat\_decomp&canon\_comp\\
lc&uc&tc&cf\\
tcf&NFKC\_CF&cjkTraditionalVariant&cjkSimplifiedVariant\\
cjkCompatibilityVariant&IDNA\_Mapping&DUCET&DUCET\_CLDR\\
rev\_DUCET&rev\_DUCET\_CLDR&scx&JSN\\
\end{tabular}

\small{The following properties are not standard Unicode properties:
canon\_decomp, compat\_decomp, canon\_comp, tcf, DUCET, DUCET\_CLDR.}
\caption{\label{tab:stringprop}String Properties (Unicode canonical short name)}
\end{centering}
\end{table}

\subsection{Normalization}

Normalization data includes the decomposition, composition, full case
folding, and character combining class information.  This is all
combined into convenient functions for normalization.  Each
Unicode-defined normalization method has its own functions:  canonical
decomposition (NFD), canonical composition (NFC), compatibility
decomposition (NFKD), compatibility composition (NFKC), and
compatibility decomposition with case folding (NFKC\_CF).

There are three function pairs for each normalization type.
% Begin-doc UTF buffer return
Their return type is indicated by the trailing number in the function
name: UTF-32 (32), UTF-16 (16), or UTF-8 (8).  The return buffer is
pointed to by [[buf]].  If [[off]] is less than zero, [[buf]] points
to a fixed-length buffer of size [[*buf_len]].  In that case, if
[[*buf_len]] is zero, [[buf]] may be NULL; in fact, [[buf_len]] may be
[[NULL]] as well to indicate a zero-length return buffer.  If [[off]]
is zero or more, the results are placed into a dynamically resizable
buffer starting at [[off]]. [[*buf]] is newly allocated using
[[malloc]] or expanded using [[realloc]] as needed; the current
allocated length is in [[*buf_len]]. A memory allocation failure
causes [[NULL]] to be returned in [[*buf]] along with a non-zero
length.  The return value of the function is the number of words which
would have been written to the output buffer it it had enough space;
on successful dynamic resizing, or if given a large enough fixed-sized
buffer, this is the actual number of words written. Otherwise, for
fixed-sized buffers, only [[*buf_len]] words are written, and the
function must be called again with a larger buffer for the rest of the
results.

% End-doc UTF buffer return

For each return type, input may be specified in one of two ways.  The
first way is to use a separate input buffer, [[ibuf]].  The second way
is to use the output buffer.  The presence of a [[t]] (``transform'')
before the output type indicator selects the latter.  The latter is
simply a wrapper for the former which first copies the input into a
freshly allocated buffer, and then frees that buffer when finished.
This allocation can be avoided by using the former function and
setting [[ibuf]] to [[*buf]], but it is unsafe to do so with more than
one input character.

% uni_NFD32 prototype
% uni_NFDt32 prototype
% uni_NFD16 prototype
% uni_NFDt16 prototype
% uni_NFD8 prototype
% uni_NFDt8 prototype
% uni_NFC32 prototype
% uni_NFCt32 prototype
% uni_NFC16 prototype
% uni_NFCt16 prototype
% uni_NFC8 prototype
% uni_NFCt8 prototype
% uni_NFKD32 prototype
% uni_NFKDt32 prototype
% uni_NFKD16 prototype
% uni_NFKDt16 prototype
% uni_NFKD8 prototype
% uni_NFKDt8 prototype
% uni_NFKC32 prototype
% uni_NFKCt32 prototype
% uni_NFKC16 prototype
% uni_NFKCt16 prototype
% uni_NFKC8 prototype
% uni_NFKCt8 prototype
% uni_NFKC_CF32 prototype
% uni_NFKC_CFt32 prototype
% uni_NFKC_CF16 prototype
% uni_NFKC_CFt16 prototype
% uni_NFKC_CF8 prototype
% uni_NFKC_CFt8 prototype

Most normalization procedures above perform canonical ordering.  This
can also be done outside of those functions.  The following function
takes a buffer ([[buf]]) of a given length ([[ilen]]), and sorts the
characters in-place.  It will never need to expand the length of the
array.  However, it may need more characters to fully sort the input.
The [[last]] parameter indicates that the function must assume there
are no more characters available. Otherwise, it will return the full
length of the input if no more characters are needed, or less than the
full length if more are needed.  If more are needed, the return value
indicates the number of characters which have already been sorted;
these can safely be removed before the next pass.

% uni_Canon_Order32 prototype
% uni_Canon_Order16 prototype
% uni_Canon_Order8 prototype

\label{FCD} In some cases, a chain of transformations begins with NFD
normalization.  However, subsequent steps may succeed even without
fully normalized text.  In particular, if the next step is canonically
closed, it can perform the normalization as part of its
transformation.  For this case, the data only needs to be in what is
called Fast C or D form, or FCD.  A pseudo-property is provided to
perform this test.  The following function performs this test on a
string, returning true if no normalization needs to be performed:

% uni_is_FCD32 prototype
% uni_is_FCD16 prototype
% uni_is_FCD8 prototype

The following symbols are defined for accessing the raw FCD data:

\begin{itemize}
\item [[const uni_chrrng_dat16_t uni_FCD_rng[]]] is a sorted range
table whose 16-bit unsigned data is a ccc pair for the FCD test.
\item [[uni_FCD_rng_len]] is the length of that table (a preprocessor
symbol).
\item [[const uint32_t uni_FCD_mtab[]]] is a multi-level table with
the same data.
\item [[uni_FCD_mtab_len]] is the length of that table, in case it is
to be written out to a file.
\item [[uint16_t uni_FCD_of(uint32_t cp)]] is a function
which returns the 16-bit ccc pair for the given [[cp]].  Internally,
this is a preprocessor macro which uses the multi-level table and
[[uni_x_dat16]].
\end{itemize}

Each 16-bit word has the ccc of the first character of any potential
canonical decomposition as its high byte, and the ccc of the last
character of any potential canonical decomposition as its low byte.
See Unicode Technical Note \#5 for details on the algorithm.  Also see
that technical note for a brief description of what it means for a
transformation to be canonically closed.  The only property currently
claiming to meet this requirement is the DUCET.%
\footnote{However, the supplied CLDR tests have several cases where
passing FCD is not enough to produce a valid sort key.  This may be a
bug in either the UCA or the UCD.}
The UCA is not always guaranteed to meet this requirement; if the
literal text is to be appended to the key, then the literal text must
be in NFD form first.

\subsection{Decomposition}

The raw decomposition data is not meant to be used directly.  Instead,
the following wrapper functions are provided.  They perform just the
decomposition step for the decomposition normal forms; canonical
ordering needs to be performed manually afterwards.  Like the full
normalization functions, they come in two varieties:  ones which
modify ("t"ransform) the input, and those which generate output
separately.  The return value is the length after transformation.

% uni_NFD_dec32 prototype
% uni_NFD_dect32 prototype
% uni_NFD_dec16 prototype
% uni_NFD_dect16 prototype
% uni_NFD_dec8 prototype
% uni_NFD_dect8 prototype
% uni_NFKD_dec32 prototype
% uni_NFKD_dect32 prototype
% uni_NFKD_dec16 prototype
% uni_NFKD_dect16 prototype
% uni_NFKD_dec8 prototype
% uni_NFKD_dect8 prototype

Raw decomposition data is stored in [[uni_dm_strs]].  This includes
the raw dm property, as well as two pseudo properties.  The
canon\_decomp property contains only full (recursive) canonical
decompositions, and the compat\_decomp property contains only full
(recursive) compatibility decompositions when they differ from the
canon\_decomp result.  The raw dm property is not recursively/fully
decomposed.  The length field cannot be used raw; a flag is ored into
the upper bit.  This flag eliminates the need for a dt property lookup
to determine if an entry is canonical or compatibility: compatibilty
decompositions have this flag set.

Lookup with the multi-level tables is simplified with some
preprocessor macros, which return the associated string's offset and
length.  For the compatibility lookups, an optional pointer to a flag
([[compat]]) can be used to check if the returned decomposition was
not canonical.  Unlike the other string properties, the
[[uni_]]\emph{SP}[[_of]] function is not supported for these
properties, in order to force usage of these functions.

% uni_find_canon_decomp prototype
% uni_find_compat_decomp prototype
% uni_find_dm prototype

These macros use the internal-use only [[uni_x_dec]] function to do
their work.  For Hangul syllable strings, the [[h]] flag determines
behavior.  If positive, it acts as the [[full]] flag below, and the
returned offset is always $-1$, but the length is correct.  If
negative, the returned offset is always 1, and the length is zero.

% uni_x_dec prototype

For the Hangul syllables, a separate function must be use to
decompose.  A Hangul syllable string in the form LVT or LV can be
singularly or completely decomposed (dependent on the [[full]] flag)
using the following function.  The function will return nothing (-1)
if the input is not a decomposable Hangul syllable string, so it is
safe to call whenver no decompositionmapping is found in the tables.
Otherwise, it fills in the return buffer ([[res]]) and returns the
number of filled-in values (always 2 or 3).  The return buffer
must contain space for at least 2 characters if the [[full]] flag is
not set, or 3 if it is set.

% uni_hangul_syllable_decomp prototype

To make this easier, a wrapper macro is provided.  This assumes that
at least [[len]] words are available in the passed-in buffer
([[buf]]), and copies out the result.  This is meant to be called
after [[uni_find_]]\emph{xxx}, using its results combined with the
[[uni_hangul_syllable_decomp]] results.

% uni_get_decomp prototype

\subsection{Composition}

The raw composition data is not meant to be used directly.  Instead,
the following wrapper functions are provided.  They perform just the
composition step for the composition normal forms; canonical
decomposition needs to be performed manually beforehand.  Like the
full normalization functions, they modify buffers in-place.  However,
composition never extends the length of the input, so no extra space
needs to be available in the buffer.  However, more characters may be
needed to determine if composition is possible.  if [[nok]] is NULL,
the input buffer is assumed to be complete.  Otherwise, if the
returned [[*nok]] is less than the returned (updated) buffer length,
then the first [[*nok]] characters have been composed successfully,
but the remainder needs additional input before completion.  In any
case, the return value is the updated length of the buffer after
composition.

% uni_NFC_comp32 prototype
% uni_NFC_comp16 prototype
% uni_NFC_comp8 prototype

The raw composition tables, derived from the dm and Comp\_Ex
properties, are designed for multi-step lookup as well. In addition to
having a gap for the Hangul syllable strings, composition takes two
code points for input rather than just one.  The latter problem is
solved by looking up a ``string'' using the first input, which is
actually a sub-table that can be searched using the second input.  The
format of the sub-table is like a [[uni_cp_val_t]] structure; the
value is the composed character.  These sub-tables are stored in the
string table [[uni_canon_comp_strs]] (i.e., \emph{ST} is canon\_comp).
Only one pseudo-property is defined for this: canon\_comp.

The following macro takes the first element of the composition pair
and returns the offset and length of the sub-table by using
[[uni_x_dec]] and the multi-level table.

% uni_find_canon_comp prototype

To look up values in the sub-table, a separate search function is
provided.  Again, a hand-coded binary search is used instead of
[[bsearch]] for speed.

% uni_lookup_compent prototype

To look up Hangul syllable compositions, another separate function is
provided.  It returns zero if there is no valid composition; otherwise
it returns the result of composition.

% uni_hangul_syllable_comp prototype

Combining the above two steps can be done using a wrapper macro.  Thus
the preferred method of looking up compositions is to call
[[uni_find_canon_comp]] for a code point, and then to use the results
for all candidates for the second input to composition with
[[uni_canon_comp]].

% uni_canon_comp prototype

\subsection{Collation Tables}

The Default Unicode Collation Element Table is available as the DUCET
pseudo-property.  The CLDR version of the DUCET is available as the
DUCET\_CLDR pseudo property.  Both of these properties also define the
variable uni\_\emph{property}\_var\_top, which is the default variable
top parameter for that table.  As with the normalization tables, raw
lookup is not encouraged.

To perform a straight string comparison without fully generating keys,
use the following functions.  The [[opts]] parameter may be NULL, or
it may be a set of collation options.  Each string's length may be
either a positive number, indicating absolute length in words, or a
negative number, indicating that the string is zero-terminated.

% uni_uca_strcmp32 prototype
% uni_uca_strcmp16 prototype
% uni_uca_strcmp8 prototype

Collation options are set by the following structure.  The defaults
are selected by zeroing the options structure out first.

\input{[[uni_uca_opts_t]].tex} % C

The [[tab]] and [[strs]] options must be set together, and specify
the static data for the DUCET.  By default, the standard Unicode DUCET
is used.  Normally, the [[var_top]] option would be set at the same
time; by default it is the standard Unicode DUCET's variable top.

The [[max_level]] option is the maximum significant key level; by
default, it is 3.  The second level, if enabled, may be returned in
reverse order using the [[reverse_lev2]] flag. If it is larger than 4,
[[do_literal]] is implied. The [[do_literal]] option, if non-zero,
appends the literal input string to the key.  The UCA requires that
the string be in NFD form for this to work.

Keys whose primary component is non-zero and equal to or less than
[[var_top]] are variable weight keys.  The behavior of variable
weights is controlled by the [[var_mode]] parameter, whose values
correspond to the standard methods.

\input{DUCET variable modes.tex} % C

There is no function to directly generate something like the UCA sort
key.  Users wishing to store these keys are encouraged to encode them
in a more efficient manner for comparison.  However, the raw keys may
be looked up and compared as if they were UCA sort keys.  To retrieve
the sort key for a string, use the following functions.  The string
length is negative for zero-terminated strings, or positive for
strings with an absolute length.  The function returns a sort key
allocated with [[malloc]], whose length is return in [[*rlen]].  If
[[llen]] is non-[[NULL]], it points to a 4-element array into which to
place the number of non-zero elements at each level of the key.
Remember that the string input should be at least FCD form.

% uni_str_ducet32 prototype
% uni_str_ducet16 prototype
% uni_str_ducet8 prototype

To compare the keys returned by the above function, use the following
functions.  Pass in the same options as passed into the key creation
routine; passing different options for different keys or different
options for keys and comparison will result in undefined behavior.
This combination of functions respects all the same options
as the string comparison routines, except for the [[do_literal]]
option.  To obtain the effect of this option, use something equivalent
to [[uni_int_utf32_strcmp]] on the normalized strings if
[[uni_uca_ducet_cmp]] returns zero.

% uni_uca_ducet_cmp prototype

Alternately, the keys can be directly examined.  Each key element is
returned as one or two 32-bit integers.  The first contains a
combination of the first three levels using shifts and masks.  If
[[max_level]] was larger than three when the key was created, a second
word for each key element is the fourth level.

\input{DUCET lookup format defs.tex} % C

Raw DUCET lookups for characters without the variable weight and level
processing can be performed using the following function.  The
[[lev123]] parameter is formatted just like the first word of each
pair returned by [[uni_str_ducet]].  Since there are DUCET entries for
strings of more than one character, the lookup requires string input.
This is done one character at a time, using a state structure to keep
track of progress.  Pass in the next character in [[c]], unless at end
of input, in which case pass in [[UNI_DUCET_LOOKUP_END]].  The return
value is one of [[UNI_DUCET_LOOKUP_AGAIN]], [[UNI_DUCET_LOOKUP_OK]],
or [[UNI_DUCET_LOOKUP_NONE]].  If [[NONE]], [[lev123]] and [[lev4]]
must be ignored.  Otherwise, they contain the next DUCET sort key
element.  If the passed-in character was not [[END]], or the return
value was [[AGAIN]], the function must always be called again to
finish the lookup.  The [[c]] parameter should be the next character
(or [[END]]) for the next call, unless the return value was [[AGAIN]],
in which case [[c]] is ignored.

The [[state]] parameter is a pointer to a pointer to an opaque state
structure.  It must initially point to [[NULL]].  After the last
lookup returns ([[c]] is [[UNI_DUCET_LOOKUP_END]] and the return value
was not [[UNI_DUCET_LOOKUP_AGAIN]]), the structure is automatically freed
and reset to [[NULL]].  It should never be freed manually.

Note that the [[SHIFT_TRIMMED]] variable weight processing method can
be implemented more cheaply in an outer function by using [[SHIFTED]]
instead, and trimming after the entire key has been built.

% uni_DUCET_lookup prototype

The initial state can also be used to set options.  Currently, the
[[reverse_lev2]] and [[do_literal]] options are not supported.  The
options may only be set once; any further attempts will be ignored.

% uni_DUCET_lookup_opts prototype

\subsection{Case Conversion}

The slc, suc, and stc properties are provided as if they were numeric
properties; the numerator result is the single character to which the
input maps.  A lookup failure in the stc table has the result of a suc
lookup as its default.  A lookup failure in either the slc or suc
tables has the input code point itself as its default.

The lc, uc, and sc properties, however, provide not only a mapping,
but also a condition under which the mapping takes place.  The
properties are each stored in separate string tables, so their
\emph{ST} name is the same as their \emph{SP} name.  All currently
possible conditions are mapped to a flag:

\input{Special casing conditions.tex} % C

A flag in [[UNI_SC_FL_LOCALE]] indicates that the translation only
applies in specific locales.  The others ([[UNI_SC_FL_CONTEXT]])
depend on context: generally the entire containing grapheme cluster,
and possibly the next character must be known before deciding on a
translation.

Since each input code point may map to multiple translations
(depending on the condition),  the encoding of the string is to
provide all possible translations, each preceeded by their length.  The
condition flags for that interpretation are then or-ed into the length.

The actual lc, uc, and tc properties combine the slc, suc, and stc
lookups with the contents of the lc, uc, and tc string tables.  To do a
lookup, the [[uni_case_convert]] functions are provided.  A [[cond]]
of [[~0]] indicates that the caller does not know the current
conditions.  The return is the number of potential words in the result
(perhaps even zero), or -1 if no change needs to be made (i.e. the
result is [[cp]], encoded appropriately), or -2 if the condition is
currently unknown, but may affect the result.  For a return value of
-2, the function should be called again with a valid set of condition
flags.  Note that the passed-in condition flags should never set
[[UNI_SC_FL_NOT]]; just leave a flag unset if it is not in effect.

There are three functions.
\input{UTF buffer return.tex} %%% doc

% uni_case_convert32 prototype
% uni_case_convert16 prototype
% uni_case_convert8 prototype

To make this a little easier, wrapper macros are provided which select
the correct set of needed tables.  Note that a return code of $-1$
from the [[uni_tc]] functions indicates no special title casing; in
that case, the caller must manually fall back to the [[uni_uc]]
functions.

% uni_lc32 prototype
% uni_uc32 prototype
% uni_tc32 prototype
% uni_lc16 prototype
% uni_uc16 prototype
% uni_tc16 prototype
% uni_lc8 prototype
% uni_uc8 prototype
% uni_tc8 prototype

Case folding converts to a canonical desired case, usually lower-case.
Like the plain case conversion properties, there are two cases:
simple and full.  In addition, some of the full folding is
conditional.  In this case, though, the only condition is whether or
not the tr or az locale is in use.  Encoding is identical to the case
conversion properties.  The scf property is numeric, and the cf
property can be derived using the [[uni_case_convert]] functions, and
their associated wrapper macros:

% uni_cf32 prototype
% uni_cf16 prototype
% uni_cf8 prototype

The pseudo-property tcf, which includes the Turkic language exceptions
(i.e., the mappings when in the tr or az locale), can be obtained
using a wrapper macro as well:

% uni_tcf32 prototype
% uni_tcf16 prototype
% uni_tcf8 prototype

Note that the cf and tcf properties are actually the same, and are
stored and treated the same way as the other case properties.  The
wrapper functions above filter the low-level cf property based on the
desired locale information.

The only other case conversion string property is NFKC\_CF, which is a
plain string property.  However, for convenience, lookup functions
similar to the decomposition functions is provided as well.

% uni_NFKC_Casefold32 prototype
% uni_NFKC_Casefoldt32 prototype
% uni_NFKC_Casefold16 prototype
% uni_NFKC_Casefoldt16 prototype
% uni_NFKC_Casefold8 prototype
% uni_NFKC_Casefoldt8 prototype

\subsection{Other String Properties}

The scx property is officially not a string property, but it is stored
internally as a string property whose strings are sequences of
enumeration literal values corresponding to the sc property.  For
example, the code point U+0363 has the scx property ``Arab Syrc'',
which is stored as string of length two.  The first character of the
string is [[UNI_sc_Arab]], and the second is [[UNI_sc_Syrc]].

% End-doc Users-Guide

\lstset{language=txt}
<<FIXME>>=
Document:
  CLDR name lookups:
    CLDR_{language,territory,script,variant,currency,tzid,altList,
          casing,apend,calendar,collationType,format,numberSystem}
  enums w/o names:
    DUCET_dt
  strings:
    DUCET_dm, rev_DUCET, rev_DUCET_CLDR, na (and related)
  other:
    uni_DUCET_var_top, uni_DUCET_CLDR_var_top
  functions:
    uni_return_buf{32,16,8}
@

\end{document}

notes:
  page 99: Named Unicode Algorithms
   implemented:
    Canonical Ordering
    Canonical Composition
    Normalization
    Hangul Syllable Composition
    Hangul Syllable Decomposition
    Hangul Syllable Name Generation
       in old code; need to reimplement for new code
    Unicode Collation Algorithm (UCA)
   not implemented:
    Default Case Conversion (depends on Word Segmentation, Normalization)
      toUppercase(X) : map each C in X to Uppercase_Mapping(C)
      toLowercase(X): map each C in X to Lowercase_Mapping(C)
      toTitlecase(X): find word boundaries; use Titlecase_Mapping(C)
                      for 1st char of each word, and Lowercase_Mapping otherwise
    Default Case Folding (technically part of Default Case Conversion)
      toCasefold(X): map each C in X to Case_Folding(C) (only C & F mapping)
      toNFKC_Casefold(X): map each C in X to NFKC_Casefold(C), and then NFC
    Default Case Detection (depends on Default Case Conversion)
      isLowercase(X): X == toLowercase(X)
      isUppercase(X): X == toUppercase(X)
      isTitlecase(X): X == toTitlecase(X)
      isCasefolded(X): X == toCasefold(X)
      isCased(X): !isLowercase(X) || !isUppercase(X) || !isTitlecase(X)
    Line Breaking Algorithm
      complicated; see tr14
    Character Segmentation
      complicated; see tr29
    Word Segmentation (with possible CLDR customizaton)
      complicated; see tr29 & CLDR
    Sentence Segmentation (with possible CLDR customizaton)
      complicated; see tr29 & CLDR
    Hangul Syllable Boundary Determination
      complicated; see tr29
    Default Identifier Determiniation
      ID = ID_Start { ID_Continue }
      XID = XID_Start { XID_Continue }
        note: there are alternatives with format chars & grandfathered chars
	note: optionally NFD, NFKC, NFKC_CF (see tr31)
    Alternative Identifier Determination
      !Pattern_White_Space&!Pattern_Syntax&!Noncharacter_Code_Point&
      !(gc in Private_Use, Surrogate, Control)
    Pattern Syntax Determination
      Pattern_White_Space == space/separator
      Patter_Syntax == syntactic chars
      other = literals or identifiers
    Identifier Normalization
    Identifer Case Folding
   won't implement:
    Bidirectional Algorithm (UBA)
      but should probably do it anyway, to ensure that data files are
      usable, and to exercise BidiTest.txt (see tr9)
    Standard Compression Scheme for Unicode
   other:
     simple strcasecmp(X, Y): toCasefold(X) == toCasefold(Y)
     canonical strcasecmp(X, Y): NFD(toCasefold(NFD(X))) == NFD(toCasefold(NFD(Y)))
     compat strcasecmp(X, Y): NFKD(toCasefold(NFKD(toCasefold(NFD(X))) ==
                              NFKD(toCasefold(NFKD(toCasefold(NFD(Y)))
     id strcasecmp(X, Y): toNFKC_Casefold(NFD(X)) == toNFKC_Casefold(NFD(Y))
Completely untouched & undocumented files:
  IDNA:
    IdnaTest.txt (IDNA algorithm not implemented, so no testing)

  IVD: (doesn't seem all that useful except to the supported vendors)
    IVD_Collections.txt
    IVD_Sequences.txt
    IVD_Stats.txt
