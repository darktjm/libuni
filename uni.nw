% -*- mode: Noweb; noweb-code-mode: c-mode; -*-
% Build with noweb:
%  notangle -t8 build.nw > makefile
%  make
\documentclass[twoside,english]{report}
\usepackage[letterpaper,rmargin=1.5in,bmargin=1in]{geometry}
%%% latex preamble
\RCS $Id$
\RCS $Revision$
\RCS $Date$

%%% requires tjm-ext

\begin{document}

\title{Unicode Support Routines}
\author{Thomas J. Moore}
\date{Version 1.0\\Revision \RCSRevision\\\RCSDate}
\maketitle

\begin{rawhtml}
<!-->
\end{rawhtml}
\iffalse
<<Sources>>=
$Id$
@

<<Version Strings>>=
"$Id$\n"
@

<<Common NoWeb Warning>>=
# $Id$
@
\fi
\begin{rawhtml}
<-->
\end{rawhtml}

\begin{abstract}

This document describes and implements a library which provides
basic Unicode (\url{http://www.unicode.org}) support in a convenient
manner.  Libraries already exist that do this:  GLib, ICU, and others.
However, this library does things my way, which allows me to e.g.
build a regular expression library and a compiler using these routines.

\end{abstract}

% Begin-doc Introduction
\tableofcontents
\listoftables

\chapter{Introduction}

The Unicode standard is a freely available standard describing a large
character set, and the features of those characters.  It also
describes several common transformations and processing methods for
those characters.  The ISO 10646 standard is also freely available at
this time, but like most ISO standards, this was not always so.  The
ISO standard describes the same character set, but has a much smaller
scope overall.  The Unicode Consortium also provides an electronically
readable form of its attribute information: the Unicode Character
Database.%
\footnote{\url{http://www.unicode.org/Public/zipped}}
The purpose of this library is to provide convenient access to that
information in C programs.  The ISO 10646 standard also provides such
information, but again, it is of more limited scope.  To the most
part, the ISO 10646 standard is completely ignored from this point on.

The Unicode consortium also provides a set of rules for interpreting
characters in different locales.  ISO also has standards to this
effect.  Both sets of standards and their data files are freely
available.  However, once again, the ISO data is ignored in favor of
the Unicode data.  The Unicode locale information is available in
electronically processable form in the Unicode Common Locale Data
Repository (CLDR).%
\footnote{\url{http://cldr.unicode.org},
\url{http://unicode.org/Public/cldr/}}
This library provides the CLDR data to C programs as well.
% End-doc Introduction

<<uni_all.h>>=
<<Common C Warning>>
#ifndef UNI_ALL_H
#define UNI_ALL_H

<<Library [[uni]] headers>>

#endif /* UNI_ALL_H */
@

% Begin-doc Introduction
This document does not contain the UCD or CLDR itself; it is expected
that the user download them manually and place them where they can be
found.  Some Linux distributions may already provide at least the UCD
as a package, although the CLDR is probably less common.  This package
was tested with versions 6.0.0 and 6.2.0 of the UCD, but the file
formats are relatively stable and it should work with later versions
as well.  I recommend at least version 6.1.0, as that is the first
version where the control character names were finally made official.
An earlier version of this library actually had hard-coded names for
ASCII control characters optionally added to the name database.  This
package was tested against CLDR version 22.1; I recommend always
getting the latest version available.  In particular, care must be
taken that the CLDR and UCD versions are compatible.
% End-doc Introduction

\lstset{language=make}
<<makefile.config>>=
# The location of the Unicode Character Database (unzipped)
# http://www.unicode.org/Public/zipped/<version>/UCD.zip
UCD_LOC = /usr/share/unicode-data
# The location of the Unicode Han Database (unzipped)
# http://www.unicode.org/Public/zipped/<version>/Unihan.zip
UNIHAN_LOC = $(UCD_LOC)
# The location of the Unicode Common Locale Data Repository (unzipped)
# http://www.unicode.org/Public/cldr/latest/core.zip
CLDR_LOC = /usr/share/unicode-data/cldr
@

% Begin-doc Introduction
Speaking of names, the Unicode character names are rather verbose.%
\footnote{It is recommended in some places that common names be
permitted in a less verbose manner, but that is locale-dependent and
will not be supported by this library.}
The XML standard, on the other hand, has fairly short names.  This
library can use either, or even both (where they do not conflict).  To
do this, the XML entity database is needed.  This is defined by the
W3C XML Entity Definitions for Characters.%
\footnote{\url{http://www.w3.org/TR/xml-entity-names/}}
This was tested with version 01 April 2010.%
\footnote{\url{http://www.w3.org/TR/2010/REC-xml-entity-names-20100401/}}
The only data file needed from that standard is unicode.xml,%
\footnote{\url{http://www.w3.org/2003/entities/2007xml/unicode.xml}}
which is linked from the standard.
% End-doc Introduction

<<makefile.config>>=
# The XML entity database http://www.w3.org/2003/entities/2007xml/unicode.xml
XMLUNI = /usr/share/unicode-data/unicode.xml
@

% Begin-doc Introduction
Other libraries exist which provide similar functionality; I have
looked at GLib\footnote{\url{http://www.gtk.org/}} and
ICU\footnote{\url{http://www.icu-project.org/}} in particular.  The
GLib library provides a number of utility functions for Unicode
character strings.  It provides normalization, classification, and
general string utilities. Of these, this library does not provide
general utf-8 string utilities.  It might have been useful to make
this a GLib extension instead, providing the missing properties (in
particular, the string properties such as character names).  However,
providing all functionality in a consistent way is easier than making
a GLib extension.

The ICU library is primarily C++, with a C wrapper.  While there is
nothing inherently wrong with this, I prefer C with a C++ wrapper over
C++ with a C wrapper in general.  The ICU does everything this library
does, and more.  Unfortunately, it is not very well documented.  Like
many modern library projects, it simply provides a basic API document
auto-generated from source.  This sort of documentation is necessary,
but insufficient.  Another part of the reason with not just using ICU
is that some operations I want to do in other code requires too much
overhead in ICU.

A lesser reason for not using either of these libraries is that I want
to use this library to support my own regular expression library, and
it seems redundant when both provide their own regular expression
implementation as well.  In addition, this library can be trimmed to
the minimal necessary code with static linking, and does not include
heavy wrapper code and additional utilities that the other libraries
include.
% End-doc Introduction

\chapter{Properties}

This library's primary purpose is to preparse the Unicode information
and query it efficiently.  The Unicode Character Database defines
numerous properties for characters.  Since there are many properties,
and any particular application may only need a few of them, an attempt
is made to keep each property in a separate object file.  Static
linking will only pull in the required properties, and there is little
penalty for shared libraries which include everything.  Some code is
shared by all properties, though; this is placed in a common file.

\lstset{language=C}
<<Library [[uni]] headers>>=
#include "uni_prop.h"
@

<<uni_prop.h>>=
<<Common C Warning>>
#ifndef UNI_PROP_H
#define UNI_PROP_H

<<Unicode property exports>>
#endif /* UNI_PROP_H */
@

<<Library [[uni]] Members>>=
uni_prop.o
@

<<uni_prop.c>>=
<<Common C Header>>
#include "uni_prop.h"
// static_proto

<<Unicode property functions>>
@

Some Unicode property exports are generated, and some are hand-coded.
Some of the hand-coded exports may be useful for generation, so they
are in a separate code chunk, which can be included in the generator
without depending on the generated output. 

<<Unicode property exports>>=
<<Unicode property exports for generator>>
@

<<Unicode property functions>>=
<<Unicode property functions for generator>>
@

These properties come in a variety of formats, but for the purpose of
this library, they may be generally classified according to the type
of value:  Boolean, String, Enumerated, or Numeric.  Each class of
values has a particular set of desirable operations related to a
property of that class.

\section{Boolean Properties}

Boolean properties are either true or false.  They define a subset of
characters: those for which the value is true.  As with any set, the
desirable operations are:

%% l2h substitution ominus &ominus;
% tidy doesn't understand &ominus
%% l2h substitution ominus &#8854;
% substitution replaces # with space, so use 0-arg macro instead
% l2h macro ominus 0 &##8854;
% l2h substitution oplus &oplus;
% l2h substitution wedge &and;
% l2h substitution vee &or;
% l2h substitution neg &not;
% l2h macro overline 1 <span style="text-decoration:overline">#1</span>
% l2h substitution emptyset &empty;
% l2h substitution alpha &alpha;
% l2h substitution in &isin;
% l2h substitution notin &notin;
% l2h substitution cap &cap;
% l2h substitution cup &cup;

\begin{itemize}
\item Finding out if a character \emph{is} or is not an element of the
set.
\item Querying the \emph{members} of a set.
\item Applying set \emph{operators} on a pair of sets.  There are
16 possible set operations between two sets (each potential member is
either in or not in each of the 2 sets = $2^2$ membership
combinations; member is either in or not in result = $4^2$ total
combinations).  See table \ref{tab:setop} for a complete list of
operations.

% Begin-doc op-table
\begin{table}
\begin{quote}
\begin{tabular}{lllllcrll}
$x\in A$&false&false&true&true&&~&\\
$x\in B$&false&true&false&true&&&\\
\emph{op}&\multicolumn{4}{c}{$x\in A\textrm{ \emph{op} }B$}&\emph{when}&\multicolumn{2}{l}{\emph{Op \#}}&\emph{Op Name}\\
$\emptyset$&false&false&false&false&never&0&&NIL\\
$\alpha$&true&true&true&true&always&1&&ALL\\
$A$&false&false&true&true&$x\in A$&2&&A\\
$\overline A$&true&true&false&false&$x\notin A$&3&&INV\_A NOT\_A\\
$B$&false&true&false&true&$x\in B$&4&&B\\
$\overline B$&true&false&true&false&$x\notin B$&5&&INV\_B NOT\_B\\
$A\ominus B$&false&true&true&false&$(x\in A)\oplus (x\in B)$&6&&SYM\_DIFF XOR\\
$\overline{A\ominus B}$&true&false&false&true&$(x\in A)\overline\oplus (x\in B)$&7&&INV\_SYM\_DIFF XNOR\\
$A\cap B$&false&false&false&true&$(x\in A)\wedge (x\in B)$&8&&INTER AND\\
$\overline{A\cap B}$&true&true&true&false&$(x\in A)\overline\wedge (x\in B)$&9&&INV\_INTER NAND\\
$A-B$&false&false&true&false&$(x\in A)\wedge (x\notin B)$&10&&A\_MINUS\_B\\
$\overline{A-B}$&true&true&false&true&$(x\notin A)\vee (x\in B)$&11&&INV\_A\_MINUS\_B\\
$B-A$&false&true&false&false&$(x\notin A)\wedge (x\in B)$&12&&B\_MINUS\_A\\
$\overline{B-A}$&true&false&true&true&$(x\in A)\vee (x\notin B)$&13&&INV\_B\_MINUS\_A\\
$A\cup B$&false&true&true&true&$(x\in A)\vee (x\in B)$&14&&UNION OR\\
$\overline{A\cup B}$&true&false&false&false&$(x\in A)\overline\vee (x\in B)$&15&&INV\_UNION NOR\\
\end{tabular}

{\small Operation numbers are assigned using the formula $o_0 \oplus
(x \in A)\wedge o_1 \oplus (x \in B)\wedge o_2 \oplus (x \in A\cap
B)\wedge o_3$, where $o_n$ is true if bit $n$ is one in the operation
number.  Operation names are of the enumeration type [[uni_sop_t]],
and are prefixed with [[UNI_SOP_]].  Where two operations names are
given, either will work.}
\end{quote}
% End-doc op-table
\caption{\label{tab:setop}Set Operations}
\end{table}
\end{itemize}

\subsection{Storage Methods}

All of the above require that every member be stored.  The \emph{is}
test can be stored in a single bit.  However, there are over a million
valid character values, so the total storage for a simple bit array
would still be over 128 kilobytes, even if the array is limited to the
valid code point range.  Some properties only apply to a limited range
of characters, so these might use a little less space.  Even so, with
numerous properties, this adds up quickly, and may even slow things
down by keeping everything out of the cache.  The set
\emph{operations} are easy to perform, but require manipulating up to
384 kilobytes of data.

<<Simple bit set definitions>>=
/* for bit arrays */
/* max bit number for Unicode */
#define MAX_UNI_CP 0x10ffff
/* find array element index from bit number */
#define BSET_ELT(a, b) ((a)[(b)/(sizeof(*(a))*8)])
/* find array element's bit number from full bit number */
#define BSET_BIT(a, b) ((b)%(sizeof(*(a))*8))
/* find full bit number given array element index and element bit number */
#define BSET_ENTRY(a, e, b) ((e)*(sizeof(*(a))*8) + b)
/* convert full bit number into single-bit mask for element */
#define BSET_MASK(a, b) (1 << BSET_BIT(a, b))
/* true if full bit number b is set */
#define BSET_IS_SET(a, b) (BSET_ELT(a, b) & BSET_MASK(a, b))
/* set bit number b */
#define BSET_SET(a, b) BSET_ELT(a, b) |= BSET_MASK(a, b)
/* clear bit number b */
#define BSET_CLEAR(a, b) BSET_ELT(a, b) &= ~BSET_MASK(a, b)
@

<<[[uni_setop_t]]>>=
typedef enum {
  UNI_SOP_NIL, UNI_SOP_ALL, UNI_SOP_A, UNI_SOP_INV_A,
  UNI_SOP_B, UNI_SOP_INV_B, UNI_SOP_SYM_DIFF, UNI_SOP_INV_SYM_DIFF,
  UNI_SOP_INTER, UNI_SOP_INV_INTER, UNI_SOP_A_MINUS_B, UNI_SOP_INV_A_MINUS_B,
  UNI_SOP_B_MINUS_A, UNI_SOP_INV_B_MINUS_A, UNI_SOP_UNION, UNI_SOP_INV_UNION
} uni_set_op_t;
@

<<Unicode property exports for generator>>=
<<Simple bit set definitions>>
/* set operation names */
<<[[uni_setop_t]]>>
/* logical operation names */
#define UNI_SOP_NOT_A UNI_SOP_INV_A
#define UNI_SOP_NOT_B UNI_SOP_INV_B
#define UNI_SOP_XOR UNI_SOP_SYM_DIFF
#define UNI_SOP_XNOR UNI_SOP_INV_SYM_DIFF
#define UNI_SOP_AND UNI_SOP_INTER
#define UNI_SOP_NAND UNI_SOP_INV_INTER
#define UNI_SOP_OR UNI_SOP_UNION
#define UNI_SOP_NOR UNI_SOP_INV_UNION

#define BIT_SET_OP_BIT_(op, n) (~((((op) >> n) & 1) - 1))
#define BIT_SET_OP(A, op, B) \
  (((A) & BIT_SET_OP_BIT_(op, 1)) ^ ((B) & BIT_SET_OP_BIT_(op, 2)) ^ \
   ((A) & (B) & BIT_SET_OP_BIT_(op, 3)) ^ BIT_SET_OP_BIT_(op, 0))
@

<<C Prototypes>>=
uint_t BIT_SET_OP(uint_t A, uni_set_op_t op, uint_t B);
@

<<Known Data Types>>=
uni_set_op_t,%
@

The set \emph{operations} must be performed on arrays of a particular
type.  However, C does not really support generics, so the only way to
support more than one type is to either repeat the code or use
preprocessor macros.  In this case, I will write the function
definition to an include file, and include it multiple times while
redefining the table type.  This prevents the need for a large,
multi-chunk preprocessor macro which does not support [[#line]]
directives.

<<Unicode property exports for generator>>=
#include <stdint.h>
@

<<Unicode property exports>>=
/* uint64_t is not standard, but common */
void setop_bit64(uint64_t *a, uint32_t a_low, uint32_t a_len,
                 uni_set_op_t op,
		 uint64_t *b, uint32_t b_low, uint32_t b_len,
		 uint64_t **res, uint32_t *res_low, uint32_t *res_len,
		 uint32_t *res_max);
void setop_bit32(uint32_t *a, uint32_t a_low, uint32_t a_len,
                 uni_set_op_t op,
		 uint32_t *b, uint32_t b_low, uint32_t b_len,
		 uint32_t **res, uint32_t *res_low, uint32_t *res_len,
		 uint32_t *res_max);
void setop_bit16(uint16_t *a, uint32_t a_low, uint32_t a_len,
                 uni_set_op_t op,
		 uint16_t *b, uint32_t b_low, uint32_t b_len,
		 uint16_t **res, uint32_t *res_low, uint32_t *res_len,
		 uint32_t *res_max);
void setop_bit8(uint8_t *a, uint32_t a_low, uint32_t a_len,
                uni_set_op_t op,
		uint8_t *b, uint32_t b_low, uint32_t b_len,
		uint8_t **res, uint32_t *res_low, uint32_t *res_len,
		uint32_t *res_max);
@

<<makefile.rules>>=
uni_prop.c: setop_bitn.h
@

<<Unicode property functions>>=
#define setop_src_t uint64_t
#define setop_n 64
#define setop_f setop_bit64
#include "setop_bitn.h"
#define setop_src_t uint32_t
#define setop_n 32
#define setop_f setop_bit32
#include "setop_bitn.h"
#define setop_src_t uint16_t
#define setop_n 16
#define setop_f setop_bit16
#include "setop_bitn.h"
#define setop_src_t uint8_t
#define setop_n 8
#define setop_f setop_bit8
#include "setop_bitn.h"
@

<<Known Data Types>>=
setop_src_t,%
@

<<setop_bitn.h>>=
void setop_f(setop_src_t *a, uint32_t a_low, uint32_t a_len,
             uni_set_op_t op,
	     setop_src_t *b, uint32_t b_low, uint32_t b_len,
	     setop_src_t **res, uint32_t *res_low, uint32_t *res_len,
	     uint32_t *res_max)
{
  <<Perform set operation on bit strings>>
}
#undef setop_src_t
#undef setop_n
#undef setop_f
@

First, there are few degenerate cases which do not involve [[a]]
and/or [[b]].  While the general routine will return the same result,
the logic for the degenerate cases is much simpler, so a value can be
returned immediately.  The $\overline A$ and $\overline B$ operations
are degenerate cases by this definition, but they require a bit of
finesse that is provided by the main routine, anyway.  So, instead of
duplicating that code here, the unused input is simply zeroed to
improve its performance.

<<Perform set operation on bit strings>>=
if(op == UNI_SOP_INV_A)
  b_len = 0;
else if(op == UNI_SOP_INV_B)
  a_len = 0;
@

If both inputs are empty, the operation is determined by applying the
operator to two zeroes.  This will always match bit zero of the
operation number, and will be equivalent to applying $\emptyset$ or
$\alpha$.  If just one is empty, then that reduces the operations to
four: the above two, the non-empty set, or the inverse of the
non-empty set.  The operation number is adjusted to reflect this.

<<Perform set operation on bit strings>>=
int zz_op = /* BIT_SET_OP(0, op, 0) */ op & UNI_SOP_ALL;
if(!a_len && !b_len)
  op &= UNI_SOP_ALL;
else if(!a_len)
  op &= UNI_SOP_B | UNI_SOP_ALL;
else if(!b_len)
  op &= UNI_SOP_A | UNI_SOP_ALL;
@

The $\emptyset$ operation returns an empty result.  The $\alpha$
operation returns a result with all bits set.  The $A$ and $B$
operations just return copies of the requested input.

<<Perform set operation on bit strings>>=
if(op == UNI_SOP_NIL) {
  *res_len = 0;
  *res_low = 0;
  if(*res_max) {
    *res_max = 0;
    free(*res);
  }
  return;
}
if(op == UNI_SOP_ALL) {
  *res_len = MAX_UNI_CP / setop_n + 1;
  *res_low = 0;
  if(!*res_max)
    inisize(*res, (*res_max = *res_len));
  else if(*res_max < *res_len)
    resize(*res, (*res_max = *res_len));
  memset(*res, ~0, *res_len * setop_n / 8);
  return;
}
if(op == UNI_SOP_A) {
  *res_len = a_len;
  *res_low = a_low;
  if(!*res_max)
    inisize(*res, (*res_max = a_len));
  else if(*res_max < a_len)
    resize(*res, (*res_max = a_len));
  if(*res != a)
    memcpy(*res, a, *res_len * setop_n / 8);
  return;
}
if(op == UNI_SOP_B) {
  *res_len = b_len;
  *res_low = b_low;
  if(!*res_max)
    inisize(*res, (*res_max = b_len));
  else if(*res_max < b_len)
    resize(*res, (*res_max = b_len));
  if(*res != b)
    memcpy(*res, b, *res_len * setop_n / 8);
  return;
}
@

The bit strings do not necessarily start at the same position.  For
convenience, [[a]] will be reassigned to be the set with the earlier
start position, or only start position if one of the inputs is empty.
If a swap occurs, the operation number must also be adjusted by
swapping the $A$ and $B$ bits.  In addition, if one input is empty, it
is moved past the end of the first so it is not involved in
calculations until the end.

<<Perform set operation on bit strings>>=
if(!a_len || b_low < a_low) {
  setop_src_t *t = a;
  a = b;
  b = t;
  a_len ^= b_len;
  b_len ^= a_len;
  a_len ^= b_len;
  a_low ^= b_low;
  b_low ^= a_low;
  a_low ^= b_low;
  op = (op & ~(UNI_SOP_A|UNI_SOP_B)) | ((op & UNI_SOP_A) << 1) |
                                       ((op & UNI_SOP_B) >> 1);
}
if(!b_len)
  b_low = a_low + a_len * setop_n;
@

The bit strings do not necessarily start at zero.  If neither does,
then the region before the bit strings is all zeroes, meaning that the
result will be all zeroes or all ones depending on [[zz_op]].  If it
is all zeroes, the start of the result can be moved up.  In fact, the
start of the result only needs to be set once ones start rolling in,
so a flag is kept to determine if a one has been encountered yet.

<<Perform set operation on bit strings>>=
uint32_t i;
int did_start = 0;

if(a_low > 0 && zz_op) {
  uint32_t need = a_low / setop_n;
  did_start = 1;
  *res_low = 0;
  if(!*res_max)
    inisize(*res, (*res_max = need + 4));
  else if(*res_max < need) {
    while(*res_max < need)
      *res_max *= 2;
    resize(*res, *res_max);
  }
  if(need > 0)
    memset(*res, ~0, need * sizeof(**res));
}
@

If the inputs do not start at the same place, there is an initial
region where only [[a]] has values other than zero.  Rather than
complicate the case where [[b]] is involved by checking for this case,
a separate loop is performed for this region.  If the result start was
set to zero above, the result may be misaligned with [[a]].  If so,
the bits from the previous operation are shifted into place.  Since
the above set all ones, the first previous value is also all ones.
While it is setting values, it also does not actually increase the
result size or set the value if it is zero.  Instead, it keeps track
of the last position that was set to zero after a non-zero set, so
that the result can be truncated with no excess zeroes.  Naturally,
when a new non-zero value comes along, the zeroes need to be filled in
after all.  The first non-zero value also sets the start position, if
it was not already set above.

<<Perform set operation on bit strings>>=
setop_src_t cur_a = 0, prev_a, r;
uint32_t last_zero = 0;
int misalign_a = did_start ? (a_low - *res_low) % setop_n : 0;
uint32_t end = a_low + a_len * setop_n;
if(end < b_low)
  end = b_low;
for(; (i = a_low) < end - setop_n + 1; a_low += setop_n, a_len--, a++) {
#ifndef update_src
#define update_src(x) do { \
  prev_##x = cur_##x; \
  cur_##x = *x; \
  if(misalign_##x) { \
    prev_##x |= cur_##x << misalign_##x; \
    cur_##x >>= setop_n - misalign_##x; \
  } else \
    prev_##x = cur_##x; \
} while(0)
#endif
  update_src(a);
  r = BIT_SET_OP(prev_a, op, 0);
  <<Set res if non-zero or mark if zero>>
}
@

<<Set res if non-zero or mark if zero>>=
if(!r) {
  if(!last_zero && did_start)
    last_zero = i - *res_low;
} else {
  if(!did_start) {
    did_start = 1;
    *res_low = i;
  }
  if(!*res_max)
    inisize(*res, (*res_max = 4));
  else if(*res_max <= (i - *res_low) / setop_n) {
    while(*res_max <= (i - *res_low) / setop_n)
      *res_max *= 2;
    resize(*res, *res_max);
  }
  if(last_zero) {
    memset(&BSET_ELT(*res, last_zero), 0, (i - *res_low - last_zero) / 8);
    last_zero = 0;
  }
  BSET_ELT(*res, i - *res_low) = r;
}
@

Now, either [[a]] is pointing past the end of the lower input, or it
is pointing at a word which contains the position [[b_low]].  In the
former case, there may be a(nother) region where both [[a]] and [[b]]
are not present.  If so, the region can be set to [[zz_op]] as before,
except for the first word if there was a misalignment.  In either
case, if there was a misalignment, [[cur_a]] contains some of the bits
to be applied to the next word.  Only full words which do not contain
[[b_low]] are processed here.

<<Perform set operation on bit strings>>=
if(misalign_a && i < b_low - setop_n - 1) {
  r = BIT_SET_OP(cur_a, op, 0);
  <<Set res if non-zero or mark if zero>>
  i += setop_n;
  cur_a = 0; /* a_len must be 0 */
}
if(!a_len) {
  misalign_a = 0; /* cur_a is aligned, and remaining 0s do not need align */
  if(did_start)
    i -= (i - *res_low) % setop_n; /* align i with result instead of a */
}
if(i < b_low - setop_n - 1) {
  uint32_t skip = (b_low - i) / setop_n;
  skip *= setop_n;
  if(!zz_op) {
    if(!last_zero && did_start)
      last_zero = i - *res_low;
    if(!did_start)
      i = b_low; /* if b causes a start, *res_low should be b_low */
    else
      i += skip;
  } else {
    <<Set gap to all ones>>
  }
}
@

<<Set gap to all ones>>=
if(!did_start) {
  did_start = 1;
  *res_low = i;
}
uint32_t tot = i - *res_low + skip;
if(!*res_max)
  inisize(*res, (*res_max = tot > setop_n ? tot / setop_n : 4));
else if(*res_max <= tot / setop_n) {
  while(*res_max <= tot / setop_n)
    *res_max *= 2;
  resize(*res, *res_max);
}
if(last_zero) {
  memset(&BSET_ELT(*res, last_zero), 0, (i - *res_low - last_zero) / 8);
  last_zero = 0;
}
memset(&BSET_ELT(*res, i - *res_low), ~0, skip / 8);
i += skip;
@

Now the position is at the word containing [[b_low]], if [[b]] is
non-empty.  If [[a]] has not yet ended, or ended in the last word,
bits remain in [[cur_a]] for this word.  If it ended earlier than
that, [[cur_a]] is zero, so either way [[cur_a]] needs to be applied. 
Both [[b]] and [[a]] need to be aligned to the result before use.  If
the result has not yet started, it will start aligned with [[a]] if
[[a]] has not yet ended (i.e., [[i]] is an integral [[setop_n]]
multiple following [[a_low]]); otherwise, due to the above code, it
will start aligned with [[b]] (i.e., [[i]] was set to [[b_low]]).
First any full words in common with both [[a]] and [[b]] are
processed; then whichever one has words left is processed.  Note that
it is not possible for [[a]] to have data left to process if [[b]] is
absent.

<<Perform set operation on bit strings>>=
if(b_len) {
  uint32_t misalign_b = did_start ? (b_low - *res_low) % setop_n : 
                                    (b_low - i) % setop_n;
  setop_src_t cur_b = 0, prev_b;
  for(; a_len && b_len; b++, b_len--, b_low += setop_n, i += setop_n,
                        a++, a_len--, a_low += setop_n) {
    update_src(b);
    update_src(a);
    r = BIT_SET_OP(prev_a, op, prev_b);
    <<Set res if non-zero or mark if zero>>
  }
  for(; b_len; b++, b_len--, b_low += setop_n, i += setop_n) {
    update_src(b);
    r = BIT_SET_OP(cur_a, op, prev_b);
    <<Set res if non-zero or mark if zero>>
    cur_a = 0;
  }
  for(; a_len; a++, a_len--, a_low += setop_n, i += setop_n) {
    update_src(a);
    r = BIT_SET_OP(prev_a, op, cur_b);
    <<Set res if non-zero or mark if zero>>
    cur_b = 0;
  }
  if(cur_a || cur_b) {
    r = BIT_SET_OP(cur_a, op, cur_b);
    <<Set res if non-zero or mark if zero>>
    i += setop_n;
  }
}
@

Finally, if the upper set did not end at the last valid code point,
the upper region needs to be set the same way as the initial region.

<<Perform set operation on bit strings>>=
if(zz_op) {
  if(did_start)
    i -= (i - *res_low) % setop_n; /* align i with result */
  if(i <= MAX_UNI_CP) {
    uint32_t skip = (MAX_UNI_CP - i) / setop_n + 1;
    skip *= setop_n;
    <<Set gap to all ones>>
  }
}
@

The only thing remaining is to update the length of the result.

<<Perform set operation on bit strings>>=
if(did_start)
  *res_len = ((last_zero ? last_zero : i) - *res_low) / setop_n;
else
  *res_len = 0;
@

A plain bit array also makes it difficult to perform the
\emph{members} operation: every single potential member needs to be
tested.  Other storage strategies are needed.

<<Unicode property exports>>=
/* List members enumeration not directly supported */
@

<<Unicode property functions>>=
#if 0
  <<List bit array members>>
#endif
@

<<List bit array members>>=
int i; elt_t e;
for(i = 0; i < tab_size; i++) {
  e = tab[i];
  while(e) {
    /* strings.h usually selects optimal built-in implementation of ffs() */
    int b = ffs(e); /* or ffsl, or ffsll if glibc */
    add_ent(BSET_ENTRY(tab, i, b - 1));
    e &= ~(1 << (b - 1));
  }
}
@

One of the simplest strategies is to provide a sorted table of
members.  Membership is tested by binary search, so the speed of
lookup is proportional to $\log_2(N)$, where $N$ is the number of set
members.  Querying the \emph{members} of the set is trival: the table
is already the list.  Applying set \emph{operations} only requires
looking at the members already present, with the exception of the
odd-numbered ones:  inversion may be more expensive than with bit
arrays.  This does not save much space for properties which apply to
a large number of characters; in fact, it might even take more space.

<<Unicode property exports for generator>>=
int uni_cmp_cp(const void *a, const void *b); /* for qsort/bsearch */
@

<<Unicode property exports>>=
#define uni_is_cp(c, tab, tab_len) \
  bsearch(&c, tab, tab_len, sizeof(tab[0]), uni_cmp_cp)

/* members are already in list */
/* set operations not directly supported */
@

<<C Prototypes>>=
int uni_is_cp(elt_t c, elt_t *tab, size_t tab_len);
@

<<Unicode property functions for generator>>=
int uni_cmp_cp(const void *a, const void *b)
{
  return *(int32_t *)a - *(int32_t *)b;
}
@

<<Unicode property functions>>=
#if 0
  <<Perform set ops on cp array>>
#endif
@

<<Perform set ops on cp array>>=
uint32_t i = 0;
uint32_t tab[max], tab_len = 0;
int invert = /* BIT_SET_OP(0, op, 0) */ op & 1;
uint32_t aptr, bptr;
for(aptr = bptr = 0; aptr < a_len || bptr < b_len; ) {
  int res;
  uint32_t cp;
  if(bptr == b_len || (aptr < a_len && a[aptr] < b[bptr])) {
    res = BIT_SET_OP(~0, op, 0);
    cp = a[aptr++];
  } else if(aptr == a_len || (bptr < b_len && a[aptr] > b[bptr])) {
    res = BIT_SET_OP(0, op, ~0);
    cp = b[bptr++];
  } else {
    res = BIT_SET_OP(~0, op, ~0);
    cp = a[aptr++];
    bptr++;
  }
  if(invert)
    while(i < cp)
      tab[tab_len++] = i++;
  if(res)
    tab[tab_len++] = cp;
}
if(invert)
  while(i < max)
    tab[tab_len++] = i++;
@

A possible improvement on that strategy is to store ranges instead of
individual code points.  This relies on the fact that most properties
apply to large consecutive ranges; if this is not true, then this is
actually worse than storing individual code points.  Whenever the
number of ranges is less than half of the number of individual code
points, this table will be smaller, requiring less time to search, and
less time to perform set \emph{operations}.  In particular, negation
is usually much faster with a range table than with an individual code
point table.  Not only can absent members be skipped, but present
members within a range can be skipped as well.

<<[[uni_chrrng_t]]>>=
typedef struct {
    uint32_t low, high;
} uni_chrrng_t;
@

<<Unicode property exports for generator>>=
<<[[uni_chrrng_t]]>>
int uni_cmprng(const void *a, const void *b); /* for qsort/bsearch */
int is_cp_chrrng(uint32_t cp, const uni_chrrng_t *tab, uint32_t tab_len);
@

<<Known Data Types>>=
uni_chrrng_t,%
@

<<Unicode property functions for generator>>=
int uni_cmprng(const void *a, const void *b)
{
    const uni_chrrng_t *_a = a, *_b = b;

    if(_a->high < _b->low)
        return -1;
    else if(_b->high < _a->low)
        return 1;
    else
        return 0;
}

int is_cp_chrrng(uint32_t cp, const uni_chrrng_t *tab, uint32_t tab_len)
{
#if 0
  uni_chrrng_t cr = {cp, cp};
  return bsearch(&cr, tab, tab_len, sizeof(uni_chrrng_t), uni_cmprng) ? 1 : 0;
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > tab[j].high)
      l = j + 1;
    else
      return 1;
  }
  return 0;
#endif
}
@

<<Unicode property exports for generator>>=
/* members are already in list, albeit in compact form */
void uni_chrrng_setop(const uni_chrrng_t *a, uint32_t a_len, uni_set_op_t op,
                      const uni_chrrng_t *b, uint32_t b_len,
		      uni_chrrng_t **rtab, uint32_t *r_len);
@

While it would be possible to invert in place (and even use the same
array as the input), if there is enough room, it's easier to just
generate a new array.  At most one range is added (all the ranges
below the current ranges plus one above the top-most range), and at
most one range is removed (replaced by ranges below if top-most is at
top, except if one is already at the bottom).

<<Unicode property functions for generator>>=
static void uni_chrrng_invert(const uni_chrrng_t *a, uint32_t a_len,
		              uni_chrrng_t **rtab, uint32_t *r_len)
{
  uni_chrrng_t *tab;
  uint32_t tab_len = 0;
  inisize(tab, a_len + 1);
  uint32_t i;
  if(a_len && a[0].low) {
    tab[0].low = 0;
    tab[0].high = a[0].low - 1;
    tab_len++;
  }
  for(i = 0; i < a_len - 1; i++, tab_len++) {
    tab[tab_len].low = a[i].high + 1;
    tab[tab_len].high = a[i + 1].low - 1;
  }
  if(a_len && a[a_len - 1].high < MAX_UNI_CP) {
    tab[tab_len].low = a[a_len - 1].high + 1;
    tab[tab_len].high = MAX_UNI_CP;
    tab_len++;
  }
  *rtab = tab;
  *r_len = tab_len;
}
@

For the general operation, as with ordinary bit sets, a few degenerate
cases are handled immediately.  Any operations involving just one set
are done immediately.  Although it would be possible to shortcut
operations against empty (or full) sets as well, the slower generic
routines are used instead of trying to test for those cases.  Again,
rather than supporting a previously allocated result structure, a new
one is always allocated.  Unlike inversion, it is very likely that
this is necessary in order to avoid overwriting either one of the
inputs when they are misaligned.

The generic routine operates by finding points where either input
changes state.  Then, the entire range where the the states where the
same get the result from a single operation call whose result is
almost staticaly calculated using constant inputs.

<<Unicode property functions for generator>>=
void uni_chrrng_setop(const uni_chrrng_t *a, uint32_t a_len, uni_set_op_t op,
                      const uni_chrrng_t *b, uint32_t b_len,
		      uni_chrrng_t **rtab, uint32_t *r_len)
{
  uni_chrrng_t *tab;
  uint32_t tab_len = 0, max_tab = 8;
  /* shortcut all ops that don't involve both sets */
  if(op == UNI_SOP_NIL) {
    inisize(tab, 1); /* so free always works */
    *rtab = tab;
    *r_len = 0;
    return;
  } else if(op == UNI_SOP_ALL) {
    inisize(tab, 1);
    tab[0].low = 0;
    tab[0].high = MAX_UNI_CP;
    *rtab = tab;
    *r_len = 1;
    return;
  } else if(op == UNI_SOP_A) {
    inisize(tab, a_len);
    memcpy(tab, a, a_len * sizeof(*a));
    *rtab = tab;
    *r_len = a_len;
    return;
  } else if(op == UNI_SOP_INV_A) {
    uni_chrrng_invert(a, a_len, rtab, r_len);
    return;
  } else if(op == UNI_SOP_B) {
    inisize(tab, b_len);
    memcpy(tab, b, b_len * sizeof(*b));
    *rtab = tab;
    *r_len = b_len;
    return;
  } else if(op == UNI_SOP_INV_B) {
    uni_chrrng_invert(b, b_len, rtab, r_len);
    return;
  }
  inisize(tab, max_tab);
  int invert = /* BIT_SET_OP(0, op, 0); */ op & 1;
  uint32_t aptr = 0, bptr = 0;
  uint32_t alow, ahigh, blow, bhigh;
  int32_t lasthigh = -1, curlow = -1, curhigh = -1;
#define set_low_high(x) do { \
  x##low = x##ptr < x##_len ? x[0].low : MAX_UNI_CP + 1; \
  x##high = x##ptr < x##_len ? x[0].high : MAX_UNI_CP + 1; \
} while(0)
  set_low_high(a);
  set_low_high(b);
  while(alow <= MAX_UNI_CP || blow <= MAX_UNI_CP) {
    /* find next range where either a or b is different */
    /* compute bit op for that range as well (into res) */
    uint32_t nextlow, nexthigh, res;
    if(alow < blow) {
      nextlow = alow;
      res = BIT_SET_OP(~0, op, 0);
      if(ahigh < blow) {
        nexthigh = ahigh;
	aptr++;
	set_low_high(a);
      } else {
        nexthigh = blow - 1;
	alow = blow;
      }
    } else if(alow > blow) {
      nextlow = blow;
      res = BIT_SET_OP(0, op, ~0);
      if(bhigh < alow) {
        nexthigh = bhigh;
	bptr++;
	set_low_high(b);
      } else {
        nexthigh = alow - 1;
	blow = alow;
      }
    } else {
      nextlow = alow;
      res = BIT_SET_OP(~0, op, ~0);
      nexthigh = ahigh;
      if(ahigh <= bhigh) {
        aptr++;
	set_low_high(a);
      } else
        alow = bhigh + 1;
      if(bhigh <= nexthigh) {
        nexthigh = bhigh;
	bptr++;
	set_low_high(b);
      } else
        blow = nexthigh + 1;
    }
    if(invert && nextlow > lasthigh + 1) {
      if(curlow < 0)
        curlow = lasthigh + 1;
      curhigh = nextlow - 1;
    }
    /* store res into result */
    if(res) {
      if(curlow < 0)
        curlow = nextlow;
      curhigh = nexthigh;
    } else if(curlow >= 0) {
      if(tab_len == max_tab)
        resize(tab, max_tab *= 2);
      tab[tab_len].low = curlow;
      tab[tab_len++].high = curhigh;
      curlow = -1;
    }
    lasthigh = nexthigh;
  }
  if(invert && lasthigh < MAX_UNI_CP) {
    if(curlow < 0)
      curlow = lasthigh + 1;
    curhigh = MAX_UNI_CP;
  }
  if(curlow >= 0) {	
    if(tab_len == max_tab)
      resize(tab, max_tab *= 2);
    tab[tab_len].low = curlow;
    tab[tab_len++].high = curhigh;
  }
  *rtab = tab;
  *r_len = tab_len;
}
@

Another possible simple improvement is to store fixed ranges, for
example 32 characters at at time, with a bit array for that range.
Again, two words are required per entry (start and bit mask), so this
only saves storage space and time if each block has on average two or
more bits set.  This loses the range advantage of being able to skip
large contiguous groups of present members, but also partly loses the
range disadvantage of requiring contiguous members for efficiency.  It
also partly gains the disadvantage of requiring scanning of bit arrays
to produce the \emph{members} list.  No sample implementation is
provided here.

A more complex possible improvement is to split the original bit array
into a multi-level table, with duplicate subtables shared.  The bit
array is partitioned into chunks; these are stored at the lowest
level.  Higher levels are just tables of pointers to lower levels.
This increased indirection is less efficient than the raw bit array,
but having two pointers to the same subtable can significantly reduce
storage requirements.  Note that another name for this structure is an
optimized prefix tree (aka optimized trie) on the code point bit string.

The multi-level table approach requires fewer lookups than the code
point arrays (just as many table lookups as there are levels, vs. a
binary search against a long range list that might take, say, 10 or
more table probes), but even when using space-saving measures will
likely still produce larger tables than even unoptimized range tables
(range tables could be optimized for space by using only 24 bits per
code point, for example.) To share duplicate subtables, additional
management of a list of unique lower-level tables at each level, along
with their users, is required. Making a change of any kind is a
non-trivial operation.  This additional storage and computational
expense makes these more suited to static one-time generation.  On the
other hand, their straight lookup performance benefit over ranged
tables is three-fold or more for many Unicode tables, without a need
to do any special optimizations.

There are probably very efficient algorithms to generate efficient
multi-level tables, but I am not aware of them.  The optimal number of
levels and level size is difficult to find, but a pretty good number
can be found by taking each level in turn, and finding the size which
produces the minimum total length taking into account all uniques and
the length of the array at the next level.  Of course the size of the
next level is determined by redundancy elimination as well, so this
requires a recursive search with backtracking.  In addition, having
too many levels eliminates some of the performance benefit, so the
search depth cannot be infinite.%
\footnote{Then again, a specific structure could be imposed,
eliminating the need for a search, but not always selecting the
optimal size.  For example, choosing a 3-level X/16/8 table might be
good enough for most cases.  However, in order to gain the space
savings, the routine would still need to perform a lot of block
comparisons to eliminate redundancy.  If it's going to take a lot of
time and memory anyway, why not go all the way?}
The initial bit array may be a subset of the desired values, in which
case an initial range can (actually, must) be specified as well.  When
a value lies outside of that range, it can be given a default;
currently only all zeroes and all ones are supported. The structure
may also store multiple bit arrays at once, in which case it may be
necessary to keep more than one byte together contiguously at the
lowest level; this may be specified using [[minwidth]].  By default,
[[minwidth]] can never be lower than 4, since 32-bit words are used.

<<Unicode property exports for generator>>=
#define UNI_MAX_MULTILEV 3
void bits_to_multi(const uint8_t *bits, uint32_t len, uint32_t low,
                   uint32_t high, uint8_t def, uint32_t minwidth,
		   <<Multi-level table type>> **ml, uint32_t *ml_len);
/* return: 0 == all 0, ~0 == all 1, 1 == *ret is ptr to data */
/* *ret is also set to NULL for 0 and ~0 returns */
int multi_tab_lookup(const <<Multi-level table type>> *dat, uint32_t val,
                     const uint8_t **ret, uint8_t def);
@

<<Unicode property functions for generator>>=
void bits_to_multi(const uint8_t *bits, uint32_t len, uint32_t low,
                   uint32_t high, uint8_t def, uint32_t minwidth,
		   <<Multi-level table type>> **ml, uint32_t *ml_len)
{
  <<Split [[bits]] into multi-level table [[*ml]]>>
}
@

<<Unicode property functions>>=
int multi_tab_lookup(const <<Multi-level table type>>*dat, uint32_t val,
                     const uint8_t **ret, uint8_t def)
{
  <<Find multi-level table entry [[val]]>>
}
@

Recursion is actually accomplished using a backtracking stack.

<<Split [[bits]] into multi-level table [[*ml]]>>=
uint32_t i;
<<Saved multi-level table information>>
int curlev = 0;
/* The block size of the block currently being worked on */
uint32_t curblk[UNI_MAX_MULTILEV];

curblk[0] = 0;
while(1) {
  <<Unsaved multi-level table information>>

  <<Compute next [[curblk[curlev]]]>>
  if(!curblk[curlev]) {
    if(!curlev--)
      break;
    <<Free [[curlev]]>>
    continue;
  }
  <<Compute level [[curlev]]>>
  <<Create next level bit array>>
  ++curlev;
  curblk[curlev] = 0;
  <<Save or free [[curlev]]>>
  if(curlev == UNI_MAX_MULTILEV - 1) {
    curlev--;
    <<Free [[curlev]]>>
  }
}
@

The minimum block size is twice the pointer size at the next level.
Any smaller, and the next level could just as well be a copy.
Actually, any smaller than the architecture's memory alignment may
cause performance issues.  For now, this is set to a minimum of 4. 
The pointer size at the next level is 1, 2, or 4 depending on the
number of blocks at this level.  The maximum block size is one half
the total size; if the total size is not binary, the data is padded
with zeroes.

<<Saved multi-level table information>>=
/* raw data array for current set of levels */
uint8_t *lev[UNI_MAX_MULTILEV] = { (uint8_t *)bits };
uint32_t curlen[UNI_MAX_MULTILEV] = { len };
@

<<Unsaved multi-level table information>>=
uint32_t blks;
@

<<Compute next [[curblk[curlev]]]>>=
if(!curblk[curlev]) {
  if(curlen[curlev] <= 256 * 2)
#if 0 /* too small, really */
    curblk[curlev] = 2;
#else
    curblk[curlev] = 4;
#endif
  else if(curlen[curlev] <= 256 * 256 * 4)
    curblk[curlev] = 4;
  else
    curblk[curlev] = 8;
  if(!curlev && curblk[curlev] < minwidth)
    curblk[curlev] = minwidth;
} else {
  curblk[curlev] *= 2;
  if(curblk[curlev] >= curlen[curlev])
    curblk[curlev] = 0;
}
if(curblk[curlev])
  blks = (curlen[curlev] + curblk[curlev] - 1) / curblk[curlev];
@

For each pass, a new pointer array is created.  While this will
eventually be reduced to the pointer size, 32-bit pointers are used
during the search.  In addition, a side array is created to store only
the unique block indices.  This alone prevents having to scan the
entire return array for block matches, speeding things up orders of
magnitude with sparse data.  Additionally, this array is kept sorted,
allowing binary searching to further reduce the number of comparisons
needed.  Using a hash table for this would require extra storage, and
may speed things up further.  In any case, each block is simply added
to the pointer array, checking first for duplicates.  As another
special compensation for sparse (or dense) arrays, all-zero entries
are always encoded as the pointer zero, and all-one entries are always
encoded as the maximum pointer value (all ones).  Not only does this
save a tiny bit of space, but scanning through dense or sparse
sections of the array can be much faster.  Care must be taken during
the comparisons to not access data past the end of the actual bit
array, instead treating them as zeroes.

<<Unsaved multi-level table information>>=
uint32_t j;
uint32_t *ptr, *ublocks, nublocks = 0;
uint8_t *data = lev[curlev];
uint32_t blklen, shortlen;
@

<<Compute level [[curlev]]>>=
blklen = shortlen = curblk[curlev];
ptr = malloc(blks * sizeof(*ptr));
ublocks = malloc(blks * sizeof(*ublocks));
for(i = 0; i < blks; i++) {
  int h, l;
  /* first, check for 0 or 1 */
  int is0 = 1, is1 = 1;
  if(i == blks - 1) {
    shortlen = curlen[curlev] % blklen;
    if(!shortlen)
      shortlen = blklen;
  }
  for(l = 0; l < shortlen && (is0 || is1); l++) {
    if(data[blklen * i + l])
      is0 = 0;
    if(data[blklen * i + l] != (uint8_t)~0)
      is1 = 0;
  }
  if(is0 && (!def || shortlen == blklen)) {
    ptr[i] = 0;
    continue;
  }
  if(is1 && (def || shortlen == blklen)) {
    ptr[i] = ~0;
    continue;
  }
  h = nublocks - 1;
  l = 0;
  while(l <= h) {
    j = (l + h) / 2;
    /* comparing in reverse order to make shortlen cmp easier */
    int c = memcmp(data + blklen * ublocks[j], data + blklen * i, shortlen);
    if(!c) {
      uint32_t k;
      if(shortlen == blklen)
        break;
      /* make c > 0 if there is any non-0 element in ublock */
      for(k = shortlen; k < blklen && !c; k++)
        c = data[blklen * ublocks[j] + k];
      if(!c)
        break;
    }
    if(c > 0)
      h = j - 1;
    else
      l = j + 1;
  }
  if(l > h) {
    if(++h == nublocks)
      ublocks[nublocks++] = i;
    else {
      memmove(ublocks + h + 1, ublocks + h,
              (nublocks - h) * sizeof(*ublocks));
      ++nublocks;
      ublocks[h] = i;
    }
    j = h;
  }
  ptr[i] = ublocks[j] + 1;
}
@

Now that the search is complete, the pointers can be compressed to the
minimum word size required.  The data will eventually be shifted down,
but it is not necessary (or safe) yet.  However, some preparation must
be done here, so that the pointers will eventually point to the
shifted blocks.  The actual pointers need to be adjusted to use the
offset of that pointer in the unique blocks array.  This could not
have been done above, because the array was having members inserted in
the middle, invalidating all of the indices.  To look up the index
more quickly, the block array is stored by block number first (it was
sorted by block contents, instead).

<<Saved multi-level table information>>=
/* unique block pointers */
uint32_t *curublk[UNI_MAX_MULTILEV] = {NULL};
uint32_t curnublk[UNI_MAX_MULTILEV];
@

<<Create next level bit array>>=
curublk[curlev] = ublocks;
curnublk[curlev] = nublocks;
qsort(ublocks, nublocks, sizeof(uint32_t), uni_cmp_cp);
/* for comparison with bsearch, adjust to actual stored value */
for(i = 0; i < nublocks; i++)
  ++ublocks[i];
for(i = 0; i < blks; i++) {
  if(!ptr[i] || ptr[i] == (uint32_t)~0)
    continue;
  uint32_t *p = bsearch(ptr + i, ublocks, nublocks, sizeof(uint32_t), 
                        uni_cmp_cp);
  ptr[i] = p - ublocks + 1;
}
lev[curlev + 1] = (uint8_t *)ptr;
/* note: 0 and ~0 are reserved values, so space for 2 must be reserved */
if(nublocks <= 254) {
  uint8_t *p = (uint8_t *)ptr;
  for(i = 0; i < blks; i++)
    p[i] = ptr[i];
  curlen[curlev + 1] = blks;
} else if(nublocks <= 65534) {
  uint16_t *p = (uint16_t *)ptr;
  for(i = 0; i < blks; i++)
    p[i] = ptr[i];
  curlen[curlev + 1] = blks * 2;
} else
  curlen[curlev + 1] = blks * 4;
@

To retain the minimum-sized level, the current set of minima is
stored.  If the current level is either smaller or the same size, but
with fewer levels, it is saved.  Otherwise, it and all of its children
are freed.

<<Saved multi-level table information>>=
uint32_t cursize[UNI_MAX_MULTILEV];
uint8_t *minlev[UNI_MAX_MULTILEV] = { (uint8_t *)bits };
uint32_t *minublk[UNI_MAX_MULTILEV] = { NULL };
uint32_t minlen[UNI_MAX_MULTILEV] = { len }, minnublk[UNI_MAX_MULTILEV];
uint32_t minblk[UNI_MAX_MULTILEV] = { 0 }, minsize = len;
@

<<Create next level bit array>>=
cursize[curlev] = nublocks * curblk[curlev];
cursize[curlev + 1] = curlen[curlev + 1];
@

<<Save or free [[curlev]]>>=
uint32_t save_size = 0;
for(i = 0; curblk[i]; i++)
  save_size += cursize[i];
save_size += cursize[i];
int save_lev = save_size < minsize;
if(save_size == minsize) {
  for(j = 0; minblk[j]; j++);
  save_lev = i < j;
}
if(save_lev) {
  for(i = 0; i < UNI_MAX_MULTILEV; i++) {
    if(minlev[i] != lev[i] && minlev[i])
      free(minlev[i]);
    if(minublk[i] != curublk[i] && minublk[i])
      free(minublk[i]);
    minblk[i] = curblk[i];
    minlev[i] = lev[i];
    minlen[i] = curlen[i];
    minublk[i] = curublk[i];
    minnublk[i] = curnublk[i];
  }
  minsize = save_size;
}
@

<<Free [[curlev]]>>=
if(curublk[curlev] != minublk[curlev])
  free(curublk[curlev]);
curublk[curlev] = NULL;
if(lev[curlev + 1] != minlev[curlev + 1])
  free(lev[curlev + 1]);
lev[curlev + 1] = NULL;
@

Now, the data can be combined into a single buffer, which is parsed
for every lookup.  This buffer is an opaque buffer of 32-bit integers,
with each data chunk aligned to a 32-bit boundary.  That way, 16-bit
and 32-bit values can be read directly from the buffer without
alignment issues.  The buffer is read from top to bottom, traversing
pointer tables until the data is reached.


<<Multi-level table type>>=
uint32_t
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
#define lev_psz(l) \
  (!(l) ? 1 : minnublk[l - 1] <= 254 ? 1 : minnublk[l - 1] <= 65534 ? 2 : 4)
for(curlev = 0; curlev < UNI_MAX_MULTILEV - 1 && minlev[curlev + 1]; curlev++);
*ml_len = 0<<Multi-level table buffer length>>;
inisize(*ml, *ml_len);
uint32_t *mp = *ml;
@

To determine the size of the buffer, we need to know what the buffer
will contain.  I will approach this by developing the lookup function
at the same time.  This function returns either a pointer into the
lowest-level data arrays, or an integer indicating that this is a zero
or one entry.

<<Find multi-level table entry [[val]]>>=
*ret = NULL;
@

First, we need to filter out the main range.  The default, 0 or 1, is
encoded as the first bit of the low end, after shifting that up.

<<Find multi-level table entry [[val]]>>=
if(val < (*dat >> 1) || val > dat[1])
  return def ? def : *dat & 1 ? ~0 : 0;
val -= *dat >> 1;
dat += 2;
@

<<Multi-level table buffer length>>=
+2
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
*mp++ = (low << 1) + (def & 1);
*mp++ = high;
@

After looking up the value at a particular level, the level needs to
be skipped.  The size of the table can be stored as a compressed
pointer beyond the end of the next level's data.  This is not possible
for the first level, though, so the length needs to be stored
separately.  There is no point in storing a compression length as
well, so this is stored as a 32-bit integer.

<<Find multi-level table entry [[val]]>>=
uint32_t skip = *dat++;
@

<<Multi-level table buffer length>>=
+1
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
*mp++ = minlen[curlev] / lev_psz(curlev);
@

Following this is a list of levels.  The first word of the last level
(i.e., the data level) is zero to terminate the list.

<<Find multi-level table entry [[val]]>>=
<<Prepare to scan multi-level table pointers>>
while(*dat) {
  <<Scan multi-level table pointers>>
}
@

<<Multi-level table buffer length>>=
+1;
for(i = 1; i <= curlev; i++)
  *ml_len += 0<<Multi-level table buffer pointer [[i]] length>>;
*ml_len += 0<<Multi-level table buffer data length>>
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
while(curlev > 0) {
  <<Dump multi-level table pointers>>
  --curlev;
}
*mp++ = 0;
@

Next, we need information regarding this level of the array.  First,
each array corresponds to a particular set of bits; all bits below
that are ignored.  The bit number below which the rest may be ignored
is stored next.  For the lowest level of the array, this is always
zero, so this corresponds with the loop terminator.  Rather than store
the number of bits in an entire 32-bit word, it is stored in a byte.
The other three bytes are available for other information that is only
needed for a pointer level.

<<Scan multi-level table pointers>>=
uint32_t desc = *dat++;
uint8_t shift = desc;
uint32_t idx = val >> shift;

val -= idx << shift;
@

<<Multi-level table buffer pointer [[i]] length>>=
+1
@

<<Dump multi-level table pointers>>=
uint32_t desc;
<<Compute pointer level descriptor>>
*mp++ = desc;
@

<<Compute pointer level descriptor>>=
for(i = 0, desc = 0; i < curlev; i++)
  desc += lg2(minblk[i] / lev_psz(i));
@

Now that the index into the subarray is known, to actually look
something up in the table requires the size of the table entries.
This is stored next.

<<Scan multi-level table pointers>>=
desc >>= 8;
uint8_t psz = desc;
@

<<Compute pointer level descriptor>>=
uint8_t psz = lev_psz(curlev);
desc |= psz << 8;
@

It also needs the offset of the subtable.  Since that is what we are
loooking up, it can be obtained from the previous pass.  For the first
pass, the offset is always zero.

<<Prepare to scan multi-level table pointers>>=
uint32_t toff = 0;
@

Now the lookup can be performed.  First, the offset beyond the table
is read, and then the pointer itself.  Then, the data pointer is
advanced past the end of the entire level.

<<Scan multi-level table pointers>>=
switch(psz) {
  case 1: {
    const uint8_t *p = (const uint8_t *)dat;
    toff = p[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint8_t)~0)
      return ~0;
    dat += skip / 4 + 1; /* aka (skip + 1 + 3) / 4 */
    skip = *p;
    break;
  }
  case 2: {
    const uint16_t *p = (const uint16_t *)dat;
    toff = p[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint16_t)~0)
      return ~0;
    dat += skip / 2 + 1; /* aka (skip + 1 + 1) / 2 */
    skip = *p;
    break;
  }
  case 4:
    toff = dat[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint32_t)~0)
      return ~0;
    toff = *dat;
    dat += skip + 1;
    break;
}
@

<<Multi-level table buffer pointer [[i]] length>>=
+((i != curlev ? minnublk[i] * minblk[i] : minlen[i]) + lev_psz(i) + 3) / 4
@

<<Dump multi-level table pointers>>=
uint8_t *mpp = (uint8_t *)mp;
switch(psz) {
 case 1:
   *mpp++ = minnublk[curlev - 1];
   break;
 case 2:
   *(uint16_t *)mpp = minnublk[curlev - 1];
   mpp += 2;
   break;
 case 4:
   *(uint32_t *)mpp = minnublk[curlev - 1];
   mpp += 4;
   break;
}
<<Copy table data out>>
free(minlev[curlev]);
@

<<Copy table data out>>=
if(minblk[curlev]) {
  /* copy all but last blindly */
  for(i = 0; i < minnublk[curlev] - 1; i++) {
    memcpy(mpp, minlev[curlev] + minblk[curlev] * (minublk[curlev][i] - 1),
           minblk[curlev]);
    mpp += minblk[curlev];
  }
  /* the last may be short */
  uint32_t shortlen;
  if(minublk[curlev][i] == (minlen[curlev] + minblk[curlev] - 1) / minblk[curlev]) {
    shortlen = minlen[curlev] % minblk[curlev];
    if(!shortlen)
      shortlen = minblk[curlev];
  } else
    shortlen = minblk[curlev];
  memcpy(mpp, minlev[curlev] + minblk[curlev] * (minublk[curlev][i] - 1),
         shortlen);
  /* silence valgrind, even though random garbage is perfectly safe */
  if(shortlen < minblk[curlev])
    memset(mpp + shortlen, 0, minblk[curlev] - shortlen);
  mpp += minblk[curlev];
  free(minublk[curlev]);
} else {
  memcpy(mpp, minlev[curlev], minlen[curlev]);
  mpp += minlen[curlev];
}
/* align mpp */
if((mpp - (uint8_t *)mp) & 3) {
  /* silence valgrind here as well */
  memset(mpp, 0, 4 - ((mpp - (uint8_t *)mp) & 3));
  mpp += 4 - ((mpp - (uint8_t *)mp) & 3);
}
mp = (uint32_t *)mpp;
@

The table offset just acquired is a block number (actually, plus one
to allow for zero).  To convert it to a word offset, it needs to be
multiplied by the next block size, in pointer size units.  This is
stored next in [[desc]].  Since there are two bytes left, it is stored
as a 16-bit integer.  If another byte is ever needed, it can be
obtained by changing this to a shift value (it is always a power of
2).

<<Scan multi-level table pointers>>=
desc >>= 8;
toff = (toff - 1) * desc;
skip *= desc;
@

<<Compute pointer level descriptor>>=
desc |= (minblk[curlev - 1] / lev_psz(curlev - 1)) << 16;
@

The final lookup simply finds the byte offset of the remaining value
in the selected subtable.  Note that only bytes may be addressed;
storing more than one byte requires shifting the value left first
before finding the first byte of the value.  Similarly, storing only
one bit requires that the value be shifted right first for addressing,
and then using the shifted out bits to create a bit mask.

<<Find multi-level table entry [[val]]>>=
*ret = (const uint8_t *)dat + 4 + toff + val;
return 1;
@

<<Multi-level table buffer data length>>=
+((curlev ? minnublk[0] * minblk[0] : len) + 3) / 4
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
uint8_t *mpp = (uint8_t *)mp;
<<Copy table data out>>
if(mp - *ml != *ml_len) {
  fprintf(stderr, "len mismatch: %d %d\n", (int)(mp - *ml), (int)*ml_len);
  exit(1);
}
@

Generally, even with the complexity I added by compressing pointers,
the table lookups are faster than equivalent range table lookups.  As
long as duplicates are removed, the space taken up is comparable to
ordinary range tables, and sometimes even better.  With the
suppression of zero and one blocks, it is faster to find the
\emph{members} list than with plain bit tables, but is still not very
fast.  On the other hand, making modifications, or creating new tables
from scratch, is very expensive. Even if we stick with a table's
current structure, there needs to be side storage that deals with the
shared arrays.  If a change is made to an array at any level, then all
pointers at the next level need to be checked: if they point to the
same place, the block needs to be duplicated first, and after updating
the block (duplicate or not), it needs to be compared against all
existing blocks at the same level for re-merging.  The last step is
not strictly necessary for run-time operations, but there is no way
with the current structure to detect shared blocks.  An additional bit
for each pointer could be used to indicate sharing, which would never
be turned off even if the last owner disappears.  There are many
possibilities, and I do not wish to dwell on it any more.  For now,
the multi-level tables are expected to be read-only and only provide
the \emph{in} function.

However, converting a multi-level table to a sorted list of code point
ranges adequately satisfies the [[list]] function, and is relatively
easy to implement.  First, since the entire table is to be scanned
sequentally, the structure of the table is read.  Next, a loop simply
advances counters at each level of the table to get to the data, and
parses the data (slowly) using the generic bit tester.  The only
shortcuts here are when reading all zeroes or all ones, which bypass
the lowest level scan.  The function might get a little faster if it
didn't re-read every level's pointer for every low-level block, but
doing it this way is simpler and probably not a big performance hit.
A bigger speed gain could probably be had by optimizing the bit tester
loop.

<<Unicode property exports>>=
void multi_bit_to_range(const uint32_t *mt, uni_chrrng_t **ret,
                        uint32_t *rlen);
@

<<Unicode property functions>>=
void multi_bit_to_range(const uint32_t *mt, uni_chrrng_t **ret,
                        uint32_t *rlen)
{
  <<Multi-level structure storage>>
  
  <<Gather multi-level structure>>
  <<Convert multi-level structure to range list>>
}
@

<<Multi-level structure storage>>=
const uint32_t *dat[UNI_MAX_MULTILEV + 1];
uint32_t sh[UNI_MAX_MULTILEV + 1], psz[UNI_MAX_MULTILEV + 1],
         bsz[UNI_MAX_MULTILEV + 1];
uint32_t low, high;
int inv;
uint32_t nl;
@

<<Gather multi-level structure>>=
inv = mt[0] & 1;
low = (mt[0] >> 1) * 8;
high = mt[1] * 8 + 7;
uint32_t desc;
uint32_t nskip = mt[2], skip;
uint32_t l;
mt += 3;
bsz[0] = nskip; /* top level is just one block */
for(l = 0; (desc = *mt++); l++) {
  dat[l] = mt;
  psz[l] = (desc >> 8) & 0xff;
  sh[l] = desc & 0xff;
  switch(psz[l]) {
    case 1:
      skip = nskip / 4 + 1;
      nskip = *(uint8_t *)mt;
      break;
    case 2:
      skip = nskip / 2 + 1;
      nskip = *(uint16_t *)mt;
      break;
    default:
      skip = nskip + 1;
      nskip = *mt;
  }
  nskip *= (bsz[l + 1] = desc >> 16);
  mt += skip;
}
dat[l] = mt;
nl = l;
@

<<Convert multi-level structure to range list>>=
uint32_t pno[UNI_MAX_MULTILEV];
uint32_t ret_max;
int32_t rno, in_r;
ret_max = 8;
inisize(*ret, ret_max);
rno = in_r = 0;
if(inv && low > 0) {
  (*ret)[0].low = 0;
  in_r = 1;
}
for(l = 0; l < nl; l++)
  pno[l] = 0;
while(1) {
  uint32_t ptr = 1;
  for(l = 0; l < nl; l++) {
    switch(psz[l]) {
      case 1:
        ptr = ((uint8_t *)dat[l] + (ptr - 1) * bsz[l])[pno[l] + 1];
	if(ptr == 0xff)
	  ptr = ~0;
	break;
      case 2:
        ptr = ((uint16_t *)dat[l] + (ptr - 1) * bsz[l])[pno[l] + 1];
	if(ptr == 0xffff)
	  ptr = ~0;
	break;
      default:
        ptr = (dat[l] + (ptr - 1) * bsz[l])[pno[l] + 1];
    }
    if(!ptr || ptr == (uint32_t)~0)
      break;
  }
  if(!ptr) {
    if(in_r) {
      (*ret)[rno++].high = low - 1;
      in_r = 0;
    }
    low += 8 << sh[l];
  } else if(ptr == (uint32_t)~0) {
    if(!in_r) {
      if(rno == ret_max)
        resize(*ret, ret_max *= 2);
      (*ret)[rno].low = low;
      in_r = 1;
    }
    low += 8 << sh[l];
  } else {
    uint32_t i;
    uint8_t *rdat = (uint8_t *)dat[nl] + (ptr - 1) * bsz[nl];
    for(i = 0; i < bsz[nl] * 8 && low <= high; i++, low++)
      if(!BSET_IS_SET(rdat, i) != !in_r) {
	if(!in_r) {
	  if(rno == ret_max)
	    resize(*ret, ret_max *= 2);
	  (*ret)[rno].low = low;
	  in_r = 1;
	} else {
	  (*ret)[rno++].high = low - 1;
	  in_r = 0;
	}
      }
  }
  if(low > high) {
    low = high + 1;
    break;
  }
  if(l == nl)
    l--;
  while(++pno[l] == bsz[l])
    l--;
  for(l++; l < nl; l++)
    pno[l] = 0;
}
if(in_r) {
  if(!inv)
    (*ret)[rno++].high = low - 1;
  else
    (*ret)[rno++].high = MAX_UNI_CP; /* or ~0? */
} else if(inv && high < MAX_UNI_CP) { /* or ~0? */
  if(rno == ret_max)
    resize(*ret, ret_max *= 2);
  (*ret)[rno].low = low;
  (*ret)[rno++].high = MAX_UNI_CP;  /* or ~0? */
}
*rlen = rno;
@

The inverse operation is not so simple, but is worth implementing for
use with the UCD parser.  The parser can simply read in a range table,
and convert it to a multi-level table.  Since the multi-level table
generation algorithm partitions the bit array, the range table
converter actually constructs the bit array first, and calls the
function already developed above to do the conversion.

This is fairly inelegant and inefficient, but it is not meant to be
called at run-time very often.  Tables are meant to be pre-generated.

<<Unicode property exports for generator>>=
uint32_t *rng_to_multi_bit(const uni_chrrng_t *tab, uint32_t tab_len,
                           uint32_t *ml_len);
@

<<Unicode property functions for generator>>=
uint32_t *rng_to_multi_bit(const uni_chrrng_t *tab, uint32_t tab_len,
                           uint32_t *ml_len)
{
  uint32_t low, high, len, i;
  uint32_t *ml;
  uint8_t *bits, def;

  /* degenerate cases:  all 0, all 1 */
  if(!tab_len) {
    bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  if(tab_len == 1 && !tab[0].low && tab[0].high == MAX_UNI_CP) {
    bits_to_multi(NULL, 0, 1, 0, 1, 0, &ml, ml_len);
    return ml;
  }
  /* not all 0 or all 1 */
  low = tab[0].low;
  high = tab[tab_len - 1].high;
  /* if starts & ends with 1, make it an inverse array */
  /* should also do this if low == 0 && tab[0].high > MAX_UNI_CP - high */
  /* or vice-versa, but it complicates things a bit */
  def = !low && high == MAX_UNI_CP;
  if(def) {
    low = tab[0].high + 1;
    high = tab[tab_len - 1].low - 1;
  }
  /* align with byte boundary */
  low &= ~7;
  len = ((high - low + 1 + 7) / 8);
  inisize(bits, len);
  /* FIXME: only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  if(def)
    /* fill in ones if low not aligned with byte boundary */
    bits[0] = (1 << (tab[0].high + 1) % 8) - 1;
  for(i = def; i < tab_len - def; i++) {
    uint32_t l = tab[i].low - low, h = tab[i].high - low + 1;
    uint8_t hm = (1 << h % 8) - 1;
    uint8_t lm = ~((1 << l % 8) - 1);
    if(h / 8 == l / 8) {
      BSET_ELT(bits, l) |= lm & hm;
      continue;
    }
    if(lm != (uint8_t)~0) {
      BSET_ELT(bits, l) |= lm;
      l += 8 - (l & 7);
    }
    if(l < (h & ~7))
      memset(&BSET_ELT(bits, l), 0xff, h / 8 - l / 8);
    if(hm)
      BSET_ELT(bits, h) |= hm;
  }
  if(def && (tab[tab_len - 1].low - low) % 8)
    /* fill in ones if high + 1 not aligned with byte boundary */
    bits[(tab[tab_len - 1].low - low) / 8] |=
                         ~((1 << (tab[tab_len - 1].low - low) % 8) - 1);
  bits_to_multi(bits, len, low / 8, high / 8, def, 0, &ml, ml_len);
  free(bits);
  return ml;
}
@

As mentioned earlier, I am not aware of any fast, efficient algorithms
for generating the multi-level table structure.  This makes general
bit operations difficult at best.  The general algorithm for unaligned
bit sets above would seem trivial in comparison to the tricks needed
to get unaligned multi-level tables together.  Right now, the only
supported method for set operations is to convert to a range table,
perform the operation, and convert back.  For less frequent
operations, simply performing the set operation on the results of
multiple lookups will do the trick.  This is extremely inefficient,
but at least it does not require creating a huge unmaintanable mess of
code for something that should rarely be done.

Another possibility would be to use more complex data structures.  I
doubt I could come up with a more space-efficient storage method than
the sorted range table, but some are much better at operations like
lookup, scanning, or modification.  An appropriately designed hash
table could be optimized off-line to provide even faster lookups, at
the cost of fragility due to the hash algorithm probably depending on
a particular Unicode release.  Prefix trees (tries, if you prefer)
have good theoretical performance, and in fact the multi-level table
is just a prefix tree of sorts.  It and many other faster tree
approaches achieve their performance by assuming a constant time
parent-to-child traversal with a large set of children, which once
again equates to large storage penalties.  I have reduced the penalty
somewhat in my own implementation by using smaller pointers, at the
expense of slowing the lookup down a bit.  For now, the multi-level
table, sorted range table, and sorted code point table are the only
supported data structures.

\subsection{Testing}

In order to actually compare performance and size, a separate test
program is provided.

<<C Test Support Executables>>=
tsttab \
@

<<makefile.rules>>=
tsttab.o: uni_prop.h
@

<<tsttab.c>>=
<<Common C Header>>

#include "uni_prop.h"
// static_proto

#include <sys/resource.h>
static struct rusage ru;

static void tstart(void)
{
  getrusage(RUSAGE_SELF, &ru);
}

static unsigned long tend(void)
{
  struct rusage ru2;
  getrusage(RUSAGE_SELF, &ru2);
  return (ru2.ru_utime.tv_sec - ru.ru_utime.tv_sec) * 1000000 +
            (ru2.ru_utime.tv_usec - ru.ru_utime.tv_usec);
}

<<Functions to help test generated tables>>

int main(void)
{
  <<Test generated tables>>
  return 0;
}
@

To test a boolean, a function is provided to run the entire test, and
a call to that function is done in the mainline.  The function is
passed the range table, the multi-level table, and the name of the
property.  These can all be generated from the property name with a
preprocessor macro.

<<Functions to help test generated tables>>=
volatile int32_t tres;

#define bool(x) doit_bool(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab)
static void print_mtab_info(const uint32_t *, uint32_t);
static void doit_bool(const char *name, const uni_chrrng_t *rng, uint32_t nent,
                      const uint32_t *mtab)
{
    uint32_t i;
    
    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      int rv = is_cp_chrrng(i, rng, nent);
      int mv = <<range table result for [[i]]>>;
      if(!rv != !mv) {
        fprintf(stderr, "mismatch %s@%d %d %d\n", name, i, rv, mv);
	exit(1);
      }
    }
    /* check performance */
    int j;
    unsigned long tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = is_cp_chrrng(i, rng, nent);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = <<range table result for [[i]]>>;
    tt = tend();
    printf("  r%ld t%ld %.2fx\n", tr, tt, (double)tr / (double)tt);
    /* check conversion */
    uni_chrrng_t *rng2;
    uint32_t nent2;
    multi_bit_to_range(mtab, &rng2, &nent2);
    if(nent2 != nent || memcmp(rng, rng2, nent * sizeof(*rng))) {
      for(j = 0; j < nent && j < nent2; j++)
        if(memcmp(&rng[j], &rng2[j], sizeof(*rng)))
	  break;
      fprintf(stderr, "misconvert %d/%d @%d\n", (int)nent, (int)nent2, j);
      exit(1);
    }
}
@

<<Functions to help test generated tables>>=
static void print_mtab_info(const uint32_t *mt, uint32_t rsize)
{
  uint32_t i, l;
  uint32_t skip, desc, nskip, psz;

  fputs("  mtab: ", stdout);
  if(mt[0] & 1)
    fputs("*inv* ", stdout);
  nskip = mt[2];
  i = 3;
  printf(" %d", nskip);
  for(l = 0; (desc = mt[i]); l++) {
    printf("/%d", desc >> 16);
    psz = (desc >> 8) & 0xff;
    switch(psz) {
      case 1:
	skip = nskip / 4 + 1;
        nskip = *(uint8_t *)&mt[++i];
	break;
      case 2:
	skip = nskip / 2 + 1;
        nskip = *(uint16_t *)&mt[++i];
	break;
      default:
	skip = nskip + 1;
        nskip = mt[++i];
    }
    nskip *= desc >> 16;
    i += skip;
  }
  skip = (nskip + 3) / 4;
  i += skip;
  printf(" (%d bytes, %d lookups max) (%.2fx rng)\n", i * 4, l + 1,
         (double)(i * 4) / rsize);
}
@

\subsection{Parsing the UCD}

A parser program is used to generate these tables as static,
compilable C code from the UCD tables.  One of the advantages of the
range table method is that creating range tables is trival, and could
be done using a simple shell script.  However, during the stages of
this project where that was in use, the generation time began to
dominate compilation time, so this was converted to C.  Since it is in
C anyway, it may as well generate multi-level tables directly, as
well.  In fact, the logic outside of the scripts was becoming
cumbersome as well, so all of that is now incorporated into the C
program.

This program has a special build rule so it can be used to generate
other C code without depending on that C code itself (as with
[[cproto.h]]).  The code itself can't use the standard header macro,
either, since it needs to limit its include files.  One thing it does
need, though, is the ability to read lines of arbitrary length.  This
is provided in my support library, which is referenced rather than
built here, again to avoid building [[cproto.h]].  Since
[[uni_prop.h]] and [[uni_prop.c]] depend on the output of this
program, but this program also needs to use some of the functions
defined therein, the functions and definitions are reinserted directly
into [[parse-ucd.c]] without any dependencies on the generated data.

\lstset{language=make}
<<C Build Executables>>=
parse-ucd \
@

\lstset{language=C}
<<parse-ucd.c>>=
<<Common C Warning>>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <ctype.h>
#include <unistd.h>
#include <stdint.h>
<<Additional parse-ucd includes>>
<<Unicode property exports for generator>>

<<Unicode property functions for generator>>

<<UCD parser local definitions>>

<<UCD parser local functions>>

int main(void)
{
  <<UCD parser variables>>

  <<Parse character data files>>
  <<Post-process property data>>
  <<Dump character information as C code>>
  <<Clean up after parsing UCD files>>
  return 0;
}
@

\lstset{language=C}
<<makefile.vars>>=
PARSE_UCD_DEPS := parse-ucd.c $(SUPT_LIB_LOC)/libsupt.a <<Additional parse-ucd C files>>

PARSER_CFLAGS :=
PARSER_LDFLAGS :=
@

<<makefile.config>>=
# The location of my support library (tjm-ext.nw)
SUPT_LIB_LOC := ../build
@

<<makefile.rules>>=
parse-ucd: $(PARSE_UCD_DEPS)
	$(CC) $(CFLAGS) $(EXTRA_CFLAGS) $(PARSER_CFLAGS) \
	   -o $@ parse-ucd.c -L$(SUPT_LIB_LOC) -lsupt $(LDFLAGS) $(PARSER_LDFLAGS)
@

<<Additional parse-ucd C files>>=
mfgets.h \
@

\lstset{language=C}
<<Additional parse-ucd includes>>=
#include "mfgets.h"
@

All files will be parsed by the same program.  The only necessary
parameters are the locations of the UCD files, which can just be
compiled in based on the configuration parameters.

\lstset{language=make}
<<makefile.vars>>=
PARSER_CFLAGS += -DUCD_LOC=\"$(UCD_LOC)\" -DUNIHAN_LOC=\"$(UNIHAN_LOC)\"
@

Rather than construct file names using the provided paths, the program
just changes to the location directory before reading files, and all
files are read relative to the current directory.  In order to be able
to change back to the starting directory, once again my utility
library has a portable [[getcwd]] that does not use fixed buffer sizes.

\lstset{language=C}
<<UCD parser variables>>=
<<Parser common variables>>
char *cwd = getcwd_full();
@

<<Parser common variables>>=
FILE *f;
char *lbuf = NULL;
unsigned int lbuflen, llen;
#define open_f(fn) do { \
  if(!(f = fopen(fn, "r"))) { \
    perror(fn); \
    exit(1); \
  } \
} while(0)
@

<<Parse character data files>>=
<<Initialize UCD files>>
<<Parse UCD files>>
<<Initialize Unihan files>>
<<Parse Unihan files>>
@

<<Initialize UCD files>>=
chdir(UCD_LOC);
@

<<Initialize Unihan files>>=
chdir(cwd); /* in case unihan_loc is relative */
chdir(UNIHAN_LOC);
@

<<Dump character information as C code>>=
chdir(cwd);
free(cwd);
@

UCD text files consist of semicolon-separated fields, with the code
point(s) in the first field.  Comments are introduced with the number
sign.  Blank lines and purely comment lines have no fields. 
Whitespace is stripped from the beginning and ending of each field.
Comments are stripped off, but kept on the side, since one particular
file stores relevant information in a line comment.  Rather than
process each field as it is encountered, like I would normally do,
each line is split into an array of fields first.  This makes the
parsing routines more readable and obvious as well.

<<Process a line of [[UnicodeData.txt]]>>=
split_line(lbuf);
if(num_fields < 15) { /* should never happen! */
  perror("UnicodeData.txt");
  exit(1);
}
@

<<UCD parser local definitions>>=
static char **fields;
static int num_fields, max_fields = 0;
static char *line_comment;
@

<<Additional parse-ucd C files>>=
mallocdef.h \
@

<<Additional parse-ucd includes>>=
#include "mallocdef.h"
@

<<UCD parser local functions>>=
static void split_line(char *buf)
{
  char fc = 0;

  if(!max_fields)
    inisize(fields, (max_fields = 16));
  num_fields = 0;
  line_comment = NULL;
  while(isspace(*buf)) buf++;
  if(!*buf || *buf == '#')
    return;
  while(1) {
    while(isspace(*buf))
      buf++;
    if(fc == '#')
      line_comment = buf;
    char *f = buf, *nf;
    for(nf = buf; *nf && (fc == '#' || (*nf != ';' && *nf != '#')); nf++);
    fc = *nf;
    buf = nf + 1;
    while(nf > f && isspace(nf[-1])) --nf;
    *nf = 0;
    if(line_comment)
      return;
    if(num_fields == max_fields)
      resize(fields, (max_fields *= 2));
    fields[num_fields++] = f;
    if(!fc)
      return;
  }
}
@

Since named properties are being parsed, and named variables will be
created, it is useful to have a database of all of the property names
and their aliases before starting.  For this reason, the first file to
process is the file which contains those names:  [[PropertyAliases.txt]].
This file is read in and stored in a local table, sorted by name.  The
sorting is done after the fact to reduce complexity.

<<[[uni_alias_t]]>>=
typedef struct {
  const char *short_name, *long_name, *alt_name, *alt_name2;
} uni_alias_t;
@

<<Unicode property exports for generator>>=
<<[[uni_alias_t]]>>
@

<<Known Data Types>>=
uni_alias_t,%
@

<<UCD parser local definitions>>=
static uni_alias_t *prop_aliases;
static int num_prop_aliases = 0, max_prop_aliases = 0;
@

<<Initialize UCD files>>=
open_f("PropertyAliases.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  split_line(lbuf);
  if(num_fields < 2)
    continue;
  if(!max_prop_aliases)
    inisize(prop_aliases, (max_prop_aliases = 128));
  if(max_prop_aliases == num_prop_aliases)
    resize(prop_aliases, (max_prop_aliases *= 2));
  prop_aliases[num_prop_aliases].short_name = strdup(fields[0]);
  prop_aliases[num_prop_aliases].long_name = strdup(fields[1]);
  /* there are no comments on alias lines, so field count is correct */
  prop_aliases[num_prop_aliases].alt_name =
                 num_fields > 2 ? strdup(fields[2]) : NULL;
  prop_aliases[num_prop_aliases].alt_name2 =
                 num_fields > 3 ? strdup(fields[3]) : NULL;
  if(num_fields > 4) {
    perror("PropertyAliases.txt");
    exit(1);
  }
  num_prop_aliases++;
}
fclose(f);
@

There are actually two fields to sort by: long name and short name.
The main table will be sorted by long name, and a side table with
pointers will be used to sort the table by short name as well.  That
way, lookups can use binary searching on either field.

<<UCD parser local functions>>=
static int cmp_longname(const void *a, const void *b)
{
  return strcmp(((uni_alias_t *)a)->long_name, ((uni_alias_t *)b)->long_name);
}
@

<<Initialize UCD files>>=
qsort(prop_aliases, num_prop_aliases, sizeof(uni_alias_t), cmp_longname);
@

<<UCD parser local definitions>>=
uni_alias_t **prop_aliases_short;
@

<<UCD parser local functions>>=
static int cmp_shortname(const void *a, const void *b)
{
  return strcmp((*(uni_alias_t **)a)->short_name,
                (*(uni_alias_t **)b)->short_name);
}
@

<<Initialize UCD files>>=
inisize(prop_aliases_short, num_prop_aliases);
for(i = 0; i < num_prop_aliases; i++)
  prop_aliases_short[i] = &prop_aliases[i];
qsort(prop_aliases_short, num_prop_aliases, sizeof(*prop_aliases_short),
      cmp_shortname);
@


Properties are added to a master property list, containing the name
and the parsed contents.  This is simply tacked on next to the
like-named property in the property name table, if applicable.  Some
properties are artificial, though, so they are appended to the end.  A
separate count is provided for this purpose.

<<UCD parser local definitions>>=
<<[[prop_t]] prerequisites>>
typedef struct {
  const char *name;
  <<Property parsed contents>>
} prop_t;
static prop_t *parsed_props;
static uint32_t nparsed, maxparsed = 0;
@

<<Known Data Types>>=
prop_t,%
@

<<UCD parser local functions>>=
static int add_prop(const char *name)
{
  uni_alias_t *pn, n;
  uint32_t i;
  static char **added_names = NULL;
  static int *added_names_p = NULL;
  static int num_added_names = 0, max_added_names = 0;

  if(!maxparsed) {
    inisize(parsed_props, (maxparsed = num_prop_aliases));
    nparsed = num_prop_aliases;
    clearbuf(parsed_props, nparsed);
  }
  n.long_name = name;
  pn = bsearch(&n, prop_aliases, num_prop_aliases, sizeof(*prop_aliases),
               cmp_longname);
  if(!pn) {
    uni_alias_t **ppn;
    pn = &n;
    n.short_name = name;
    ppn = bsearch(&pn, prop_aliases_short, num_prop_aliases,
                  sizeof(*prop_aliases_short), cmp_shortname);
    if(ppn)
      pn = *ppn;
    else
      pn = NULL;
  }
  if(pn)
    i = (int)(pn - prop_aliases);
  else {
    int h, l, m, c;
    for(l = 0, h = num_added_names - 1; l <= h; ) {
      m = (h + l) / 2;
      c = strcmp(name, added_names[m]);
      if(c < 0)
        h = m - 1;
      else if(c > 0)
        l = m + 1;
      else
        return added_names_p[m];
    }
    if(maxparsed == nparsed) {
      resize(parsed_props, (maxparsed *= 2));
      <<Resize prop-associated arrays>>
    }
    i = nparsed;
    clearbuf(&parsed_props[nparsed], 1);
    <<Clear prop-associated array[i]>>
    nparsed++;
    if(num_added_names == max_added_names) {
      if(!max_added_names) {
        inisize(added_names, (max_added_names = 10));
	inisize(added_names_p, max_added_names);
      } else {
        resize(added_names, (max_added_names *= 2));
	resize(added_names_p, max_added_names);
      }
    }
    if(l < num_added_names) {
      memmove(added_names + l + 1, added_names + l,
              (num_added_names - l) * sizeof(*added_names));
      memmove(added_names_p + l + 1, added_names_p + l,
              (num_added_names - l) * sizeof(*added_names_p));
    }
    added_names[l] = strdup(name);
    added_names_p[l] = i;
    num_added_names++;
  }
  if(!parsed_props[i].name)
    parsed_props[i].name = strdup(name);
  return i;
}
@

The first file to parse for actual data is the main database file,
[[UnicodeData.txt]].

<<Parse UCD files>>=
open_f("UnicodeData.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Process a line of [[UnicodeData.txt]]>>
}
fclose(f);
@

The first field is a code point.  While it appears that each line only
deals with one code point, there are actually ranges as well.  These
are specified by two consecutive entries with names ending in \texttt{,
First>} and \texttt{, Last>}, respectively.

<<Parser common variables>>=
uint32_t low, high;
char *s;
@

<<Process a line of [[UnicodeData.txt]]>>=
low = high = strtol(fields[0], NULL, 16);
if(fields[1][0] == '<' && (s = strchr(fields[1], ',')) &&
   !strcasecmp(s, ", First>")) {
  if(!mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    perror("UnicodeData.txt");
    exit(1);
  }
  split_line(lbuf);
  if(num_fields < 15 ||
     fields[1][0] != '<' || !(s = strchr(fields[1], ',')) ||
     strcasecmp(s, ", Last>")) { /* should also never happen! */
    perror("UnicodeData.txt");
    exit(1);
  }
  high = strtol(fields[0], NULL, 16);
}
@

The parsed contents for boolean properties are the range table and the
associated multi-level table.  In order to build the range table in
place, the storage size of the range table is kept as well.

<<Property parsed contents>>=
uni_chrrng_t *rng;
uint32_t rng_len, max_rng_len;
uint32_t *mt;
@

<<Parser common variables>>=
uint32_t i, j;
@

<<UCD parser local functions>>=
static void add_bool_rng(prop_t *p, uint32_t low, uint32_t high)
{
  if(!p->max_rng_len)
    inisize(p->rng, (p->max_rng_len = 8));
  if(p->rng_len && p->rng[p->rng_len - 1].high == low - 1)
    p->rng[p->rng_len - 1].high = high;
  else {
    if(p->rng_len == p->max_rng_len)
      resize(p->rng, (p->max_rng_len *= 2));
    p->rng[p->rng_len].low = low;
    p->rng[p->rng_len].high = high;
    ++p->rng_len;
  }
}
@

<<UCD parser local definitions>>=
#define decl_bool(n) int prop_##n = -1
#define add_bool(n) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  add_bool_rng(&parsed_props[prop_##n], low, high); \
} while(0)
@

The only directly derived boolean properties in [[UnicodeData.txt]] are
ASSIGNED, which is true if the character is present, and
Bidi\_Mirrored, which is true if field 10 is Y.

<<UCD parser variables>>=
decl_bool(ASSIGNED);
decl_bool(Bidi_Mirrored);
@

<<Process a line of [[UnicodeData.txt]]>>=
add_bool(ASSIGNED);
if(fields[9][0] == 'Y')
  add_bool(Bidi_Mirrored);
@

The [[PropList.txt]] file has more boolean properties, though.  In
fact, all entries in this file specify boolean properties.

<<Parse UCD files>>=
open_f("PropList.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Process a line of [[PropList.txt]]>>
}
fclose(f);
@

Like most of the rest of the UCD files, this file specifies ranges
using double-dot-separated code points.  Also, there are numerous
blank lines and comments, which need to be skipped.

<<Process a line of [[PropList.txt]]>>=
<<Parse dotted cp>>
@

<<Parse dotted cp>>=
split_line(lbuf);
if(!num_fields || !isxdigit(*lbuf))
  continue;
low = high = strtol(lbuf, &s, 16);
if(s && *s == '.' && s[1] == '.')
  high = strtol(s + 2, &s, 16);
if(*s) { /* should never happen, but if it does: ignore */
  fprintf(stderr, "bad col 1: %s\n", lbuf);
  continue;
}
@

Each boolean property is true if field two is the name of that
property.  Since the [[add_prop]] routine finds the correct entry based
on the name (assuming the name exists), it can be called every time,
and the correct entry will be updated.  However, the Hyphen property
needs to be excluded, because it is deprecated.  Also, I have no
applications which require the Other\_* poperties, so these are
ignored as well. They are mostly used to derive other properties, but
it's easier to just process [[DerivedCoreProperties.txt]] than to
generate them manually.

<<Process a line of [[PropList.txt]]>>=
if(!strcmp(fields[1], "Hyphen"))
  continue;
if(!strncmp(fields[1], "Other_", 6))
  continue;
<<Just add field two as binary property>>
@

<<Just add field two as binary property>>=
add_bool_rng(&parsed_props[add_prop(fields[1])], low, high);
@

Speaking of [[DerivedCoreProperties.txt]], this file has entirely
boolean properties as well.  The only deprecated property is
Grapheme\_Link.

<<Parse UCD files>>=
open_f("DerivedCoreProperties.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  if(!strcmp(fields[1], "Grapheme_Link"))
    continue;
  <<Just add field two as binary property>>
}
fclose(f);
@

The [[CompositionExclusions.txt]] file is simply a commmented list of
code points, one per line, each of which has the
Composition\_Exclusion property.  Since comments are processed as
fields, and all lines have trailing comments, the number of fields is
sufficient to use the same parsing technique as for the other files.

<<UCD parser variables>>=
decl_bool(Composition_Exclusion);
@

<<Parse UCD files>>=
open_f("CompositionExclusions.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_bool(Composition_Exclusion);
}
fclose(f);
@

Unfortunately, this file does not have all entries in order. Instead,
it lists multiple groups, and each group is in order.  This requires
post-processing.  In fact, since this might happen with other files as
well, this processing may as well be done for all tables.

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng)
    fixup_rng(&parsed_props[i]);
@

<<UCD parser local functions>>=
static void fixup_rng(prop_t *p)
{
  uint32_t i;
  qsort(p->rng, p->rng_len, sizeof(uni_chrrng_t), uni_cmprng);
  /* starting at top means only optimized entries are memmove'd */
  for(i = p->rng_len - 1; i > 0; i--) {
    uint32_t j = i;
    while(i > 0 && p->rng[i - 1].high == p->rng[i].low - 1)
      i--;
    if(i == j)
      continue;
    p->rng[i].high = p->rng[j].high;
    if(j < p->rng_len - 1) {
        memmove(p->rng + i + 1, p->rng + j + 1,
	        (p->rng_len - (j + 1)) * sizeof(uni_chrrng_t));
    }
    p->rng_len -= j - i;
    if(!i)
      break;
  }
}
@

[[DerivedNormalizationProps.txt]] contains the only remaining required
boolean UCD properties.  Unlike the other three, there are some
non-boolean properties in this file as well.  Luckily, some boolean
values can be detected by simply counting the fields.  The
Expands\_On\_* properties are deprecated.  Technically, some of the
quick check fields could be booleans as well, but they will be taken
care of as enumerations.

<<Parse UCD files>>=
open_f("DerivedNormalizationProps.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  <<Process a line of [[DerivedNormalizationProps.txt]]>>
}
fclose(f);
@

<<Process a line of [[DerivedNormalizationProps.txt]]>>=
if(num_fields == 2) {
  if(!strncmp(fields[1], "Expands_On_", 11))
    continue;
  <<Just add field two as binary property>>
  continue;
}
@

There are no boolean fields exported from Unihan.  The only field
which might be considered boolean (kIICore) is not required by
anything I use.

<<FIXME>>=
add fields from other sources, if necessary.
 XML: none
 CLDR: probably none, and in any case, should probably be loaded at
 run-time
@

Now, to finish up post-processing, the multi-level table is generated.

<<Post-process property data>>=
uint32_t ml_len;
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng)
    parsed_props[i].mt = rng_to_multi_bit(parsed_props[i].rng,
                                          parsed_props[i].rng_len,
                                          &ml_len);
@

\subsection{Generating the Static Data}

The boolean properties can now be printed, using the canonical short
name.  Declarations are all printed to the same file:
[[uni_prop.gen.h]].  However, each boolean property is printed to a
different file, so that static linking will only pull in the desired
properties and representations.  They are then linked into a separate
library from the support routines.  The names of the files are
extracted from the generated header file, and as such, [[parse-ucd]]
is a prerequisite to generating the full makefile.  This complicates
things, and makes builds much slower.  In the future, this may be
changed to use a recursive make, so that only generating the library
is dependent on the generated code.

\lstset{language=make}
<<makefile.rules>>=
uni_prop.gen.h: parse-ucd
	./parse-ucd
@

\lstset{language=C}
<<Library [[uni]] headers>>=
#include "uni_prop.h"
@

<<Unicode property exports>>=
#include "uni_prop.gen.h"
@

<<makefile.rules>>=
uni_prop.h: uni_prop.gen.h
@

<<UCD parser local definitions>>=
#define open_wf(f, fn) \
  FILE *f; \
  if(!(f = fopen(fn, "w"))) { \
    perror(fn); \
    exit(1); \
  }
@

<<Dump character information as C code>>=
open_wf(gen_h, "uni_prop.gen.h");
@

<<Clean up after parsing UCD files>>=
fclose(gen_h);
@

\lstset{language=make}
<<makefile.vars>>=
MAKEFILES+=makefile.unidat
@

<<makefile.rules>>=
makefile.unidat: uni_prop.gen.h
	printf 'RNGDAT_NAMES := ' >$@
	fgrep '[]' $^ | <<Filter generated file names>>
	       sed -e 's/\[.*//;s/.* //' | tr \\n ' ' >>$@
-include makefile.unidat
$(RNGDAT_NAMES:%=%.gen.c): uni_prop.gen.h
	@touch "$@"
@

<<Library [[uni]] Members>>=
$(RNGDAT_NAMES:%=%.gen.o)
@

<<C Headers>>=
uni_prop.gen.h \
@

Converting these to lower-case would be more consistent, but once the
name is known, casing shouldn't really matter.  In addition, a simple
query function (actually, preprocessor macro wrapper) is printed for
the multi-level table lookup functions.  Finally, a list of boolean
properties needs to be converted into calls to [[bool()]] for the test
program.  This is generated directly into a fixed-named C file,
[[tsttab.c.tests]].

<<Plain Built Files>>=
tsttab.c.tests \
@

\lstset{language=C}
<<Test generated tables>>=
#include "tsttab.c.tests"
@

<<Dump character information as C code>>=
open_wf(tstf, "tsttab.c.tests");
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    char nbuf[64];
    sprintf(nbuf, "uni_%s_rng.gen.c", name);
    open_wf(of, nbuf);
    fprintf(of, "#include \"uni_prop.h\"\n\n"
                "const uni_chrrng_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].rng_len; j++)
      fprintf(of, "\t{ %d, %d }%s\n", parsed_props[i].rng[j].low,
                                      parsed_props[i].rng[j].high,
				      j < parsed_props[i].rng_len - 1 ? "," : "");
    fputs("};\n", of);
    fclose(of);
    fprintf(gen_h, "extern const uni_chrrng_t uni_%s_rng[];\n"
		   "#define uni_%s_rng_len %d /* %d lookups max */\n"
                   "#define is_uni_%s(x) is_uni_x(x, uni_%s_mtab)\n",
		   name, name, lg2(parsed_props[i].rng_len + 1),
		   parsed_props[i].rng_len, name, name);
    print_mtab(name, parsed_props[i].mt, gen_h);
    fprintf(tstf, "bool(%s);\n", name);
  }
}
@

<<Clean up after parsing UCD files>>=
fclose(tstf);
@

<<Unicode property exports>>=
int is_uni_x(uint32_t cp, const uint32_t *tab);
@

<<Unicode property functions>>=
int is_uni_x(uint32_t cp, const uint32_t *tab)
{
  const uint8_t *mr;
  uint8_t mv = multi_tab_lookup(tab, cp / 8, &mr, 0);
  if(mr)
    mv = *mr;
  return mv & BSET_MASK(mr, cp) ? 1 : 0;
}
@

<<range table result for [[i]]>>=
is_uni_x(i, mtab)
@

\lstset{language=sh}
<<Clean temporary files>>=
rm -f uni_*_*.gen.[co]
@

When printing the multi-level table, comments are added to indicate
the struture.

\lstset{language=C}
<<UCD parser local functions>>=
static void print_mtab(const char *name, const uint32_t *mt, FILE *gen_h)
{
  char nbuf[64];
  sprintf(nbuf, "uni_%s_mtab.gen.c", name);
  open_wf(mf, nbuf);
  uint32_t i, j, l;
  uint32_t skip, desc, nskip, psz;
  fprintf(mf, "#include <stdint.h>\n\n"
              "const uint32_t uni_%s_mtab[] = {\n", name);
  fprintf(mf, "\t/* low/inv, high, l0len */\n"
              "\t(0x%X << 1) + %d, 0x%X, %d,\n",
	      mt[0] >> 1, mt[0] & 1, mt[1], (nskip = mt[2]));
  i = 3;
  fprintf(gen_h, "extern const uint32_t uni_%s_mtab[]; /* %d", name, nskip);
  for(l = 0; (desc = mt[i]); l++) {
    fprintf(mf, "\t/* level %d (pointers) */\n"
                "\t0x%X, /* shift=%d psz=%d nextblk=%d */\n",
                l, desc, desc & 0xff, (psz = (desc >> 8) & 0xff), desc >> 16);
    fprintf(gen_h, "/%d", desc >> 16);
    switch(psz) {
      case 1:
	skip = nskip / 4 + 1;
        nskip = *(uint8_t *)&mt[++i];
	break;
      case 2:
	skip = nskip / 2 + 1;
        nskip = *(uint16_t *)&mt[++i];
	break;
      default:
	skip = nskip + 1;
        nskip = mt[++i];
    }
    fprintf(mf, "\t/* raw pointers; pointer[0] = l%dlen = %d */\n\t", l + 1, nskip);
    nskip *= desc >> 16;
    for(j = 0; j < skip - 1; j++, i++)
      fprintf(mf, "0x%X,%s", mt[i], !((j + 1) % 8) ? "\n\t" : " ");
    fprintf(mf, "0x%X,\n", mt[i++]);
  }
  skip = (nskip + 3) / 4;
  fprintf(mf, "\t/* level %d (raw data) */\n"
              "\t0%s", l, skip ? ",\n\t" : "\n");
  for(j = 0, i++; j < skip; j++, i++)
    fprintf(mf, "0x%X%s", mt[i], j == skip - 1 ? "\n" :
                                                 !((j + 1) % 8) ? ",\n\t" :
						                  ", ");
  fputs("};\n", mf);
  fclose(mf);
  fprintf(gen_h, " (%d-level); len = %d */\n", l + 1, i);
}
@

\subsection{Additional Properties}

A few specific character classes that are useful for the compiler do
not warrant a full set of tables.  Instead, they are implemented as
macros that compare directly to the limited set they contain.  These
are:  characters which act as line terminators, and characters which
may initiate backslash-escapes.

\lstset{language=C}
<<Unicode property exports>>=
#define uni_is_nl(x) ((x) == '\n' || (x) == '\r' || (x) == '\v' || (x) == '\f' || \
                  (x) == 0x0085 || (x) == 0x2028 || (x) == 0x2029)
/* just use == '\\' for nfkc_cf */
#define uni_is_bs(x) (c == '\\' || c == 0xFE68 || c == 0xFF3C)
@

<<C Prototypes>>=
int uni_is_nl(uint32_t cp);
int uni_is_bs(uint32_t cp);
@

<<FIXME>>=
regex pseudo-booleans:
  punct == gc==P & gc!=S & !alpha
  uni_punct == gc==P
  digit == 0-9
  uni_digit == gc==Nd
  xdigit == 0-9a-fA-F
  uni_xdigit == gc=Nd + Hex_Digit
  alnum == alpha + digit
  uni_alnum == alpha + uni_digit
  blank == gc=Zs + TAB
  graph == anything but White_Space, gc=Cc, gc=Cs, gc=Cn
  print == graph + blank - gc=Cc
  word == alnum + _ (by default)
  uni_word == uni_alnum + gc=M + gc=Pc plus Join_Control
@

\section{Enumerated Properties}

Enumerated properties have a limited number of values, which have
names of their own.  These names may have aliases, as well.  Of course
all properties have a limited number of values, so it is a matter of
interpretation whether it qualifies as an enumeration.  The general
rule followed here is that this only includes properties with an
explicit list of aliases.  That includes one psuedo-property: the list
of properties itself.  An enumerated property can be split into sets,
each of which consists of the characters having one specific
enumerated value.  This means that each enumerated property also
defines a number of boolean properties, which are covered above.
Additional useful operations include:

\begin{itemize}
\item Find out the value \emph{of} the property for a character.  This
should be a numeric value indicating the enumeration literal index.
\item Given a value, find out its \emph{name} and primary \emph{alias}.
\item Given a name or alias, find out its \emph{value}.  This should
support Unicode sloppy matching:  case-insensitive, with dashes and
underscores removed (except as noted).
\item Obtain a \emph{list} of all possible values and aliases, for more
general searching.
\end{itemize}

\subsection{Storage Methods}

Again, for the \emph{of} operation, the multi-level table seems
appropriate.  Rather than storing the actual string value, the
enumeration is simply converted into an integer from zero to the
number of elements, and that integer is stored.  For all enumerations
but the character name, this just one byte.  For the character name, a
different approach is required.  Since every code point has a
different name, there will never be any sharing of lower-level tables.
The only possible sharing would be in the algorithmically generated
code points.  Since this is a significant special case, it gets its
own section, later on.

To obtain a subset for boolean operations, the range list is provided
as well.  It is augmented by a data field.  No enumeration actually
requires 32 bits, so adding another separate field is wasteful.
Instead, an 8-bit bit field is added.  This does slow things down
slightly, but for performance, the multi-level table is better, anyway.

<<[[uni_chrrng_dat_t]]>>=
typedef struct {
    uint32_t low, high:24, dat:8;
} uni_chrrng_dat_t;
@

<<Unicode property exports for generator>>=
#if 1
<<[[uni_chrrng_dat_t]]>>
#else /* 32-bit dat during testing */
typedef struct {
    uint32_t low, high, dat;
} uni_chrrng_dat_t;
#endif
int uni_cmprng_dat(const void *a, const void *b); /* for bsearch/qsort */
uint32_t find_cp_chrrng(uint32_t cp, const uni_chrrng_dat_t *tab,
                        uint32_t tab_len, uint32_t def);
@

<<Known Data Types>>=
uni_chrrng_dat_t,%
@

<<Unicode property functions for generator>>=
int uni_cmprng_dat(const void *a, const void *b)
{
    const uni_chrrng_dat_t *_a = a, *_b = b;

    if(_a->high < _b->low)
        return -1;
    else if(_b->high < _a->low)
        return 1;
    else
        return 0;
}

uint32_t find_cp_chrrng(uint32_t cp, const uni_chrrng_dat_t *tab,
                        uint32_t tab_len, uint32_t def)
{
#if 0
  uni_chrrng_dat_t cr = {cp, cp}, *tab_el;
  tab_el = bsearch(&cr, tab, tab_len, sizeof(uni_chrrng_dat_t), uni_cmprng_dat);
  return tab_el ? tab_el->dat : def;
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > tab[j].high)
      l = j + 1;
    else
      return tab[j].dat;
  }
  return def;
#endif
}
@

For the \emph{name} and \emph{alias} operations, a simple lookup table
can be provided, indexed on the enumeration value (again, except for
the name property).  The table fulfulls the \emph{list} requirement as
well.

For the \emph{value} operation, either a sorted table of names or some
other structure can be provided.  The advantage of the sorted table of
names is that it takes no effort to create, search, and provides the
\emph{list} operation (specifially for searching) as well.  A
statically generated hash function might be faster, though.  Since
applications that need to look up a lot of \emph{value}s are probably
rare, only the simple sorted table is provided, and the library user
is expected to convert that to a hash table, prefix tree, or some
other structure as needed.

\subsection{Name Tables}

The first thing to read in is the list of aliases.  These are keyed on
property short name.  Since there is already a list of property names,
the array of aliases may as well correspond.  As the list of aliases
is read, the comment may be stored as an alias as well.  The group
names for the gc property are stored in this file, and the comment for
the definition line for these names indicates the fundamental values
that are combined into this group.

<<UCD parser local definitions>>=
uni_alias_t **val_aliases;
int *num_val_aliases, *max_val_aliases;
@

<<Initialize UCD files>>=
inisize(val_aliases, num_prop_aliases);
clearbuf(val_aliases, num_prop_aliases);
inisize(num_val_aliases, num_prop_aliases);
clearbuf(num_val_aliases, num_prop_aliases);
inisize(max_val_aliases, num_prop_aliases);
clearbuf(max_val_aliases, num_prop_aliases);
@

<<Resize prop-associated arrays>>=
resize(val_aliases, maxparsed);
resize(num_val_aliases, maxparsed);
resize(max_val_aliases, maxparsed);
@

<<Clear prop-associated array[i]>>=
val_aliases[i] = NULL;
num_val_aliases[i] = max_val_aliases[i] = 0;
@

Then, the alias file can be read and stored in the array.  While it
would be more efficient to only look up the destination when things
change, it's easier to just go ahead and put it in place.

<<Initialize UCD files>>=
open_f("PropertyValueAliases.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  split_line(lbuf);
  if(num_fields < 3)
    continue;
  uni_alias_t me, *mep = &me;
  me.short_name = fields[0];
  uni_alias_t **ret = bsearch(&mep, prop_aliases_short, num_prop_aliases,
                              sizeof(*prop_aliases_short), cmp_shortname);
  if(!ret) {
    perror("PropertyValueAliases.txt");
    exit(1);
  }
  int idx = (int)(*ret - prop_aliases);
  if(!max_val_aliases[idx])
    inisize(val_aliases[idx], (max_val_aliases[idx] = 16));
  if(max_val_aliases[idx] == num_val_aliases[idx])
    resize(val_aliases[idx], (max_val_aliases[idx] *= 2));
  val_aliases[idx][num_val_aliases[idx]].short_name = strdup(fields[1]);
  val_aliases[idx][num_val_aliases[idx]].long_name = strdup(fields[2]);
  /* there are no comments on alias lines, so field count is correct */
  val_aliases[idx][num_val_aliases[idx]].alt_name =
                 num_fields > 3 ? strdup(fields[3]) : NULL;
  val_aliases[idx][num_val_aliases[idx]].alt_name2 =
                 num_fields > 4 ? strdup(fields[4]) : NULL;
  if(num_fields > 5) {
    perror("PropertyValueAliases.txt");
    exit(1);
  }
  if(line_comment && !strcmp(fields[0], "gc"))
    val_aliases[idx][num_val_aliases[idx]].alt_name2 = strdup(line_comment);
  num_val_aliases[idx]++;
}
fclose(f);
@

Since we're about to scan files with the enumerations in text form,
the next logical step is to build a search mechanism.  In this case,
just a simple sorted array, using the standard loose matching.  This
is necessary because some fields do not match any of the literal
values we have gathered so far.  This table will be dumped as well,
for the \emph{value} function.

<<[[uni_valueof_t]]>>=
typedef struct {
  const char *name;
  int val;
} uni_valueof_t;
@

<<Unicode property exports for generator>>=
<<[[uni_valueof_t]]>>
@

<<Known Data Types>>=
uni_valueof_t,%
@

<<UCD parser local definitions>>=
uni_valueof_t **enum_vals;
uint32_t *enum_vals_len;
@

<<Unicode property exports for generator>>=
int cmp_valueof(const void *a, const void *b);
@

<<Unicode property functions for generator>>=
int cmp_valueof(const void *a, const void *b)
{
  return strcmp(((uni_valueof_t *)a)->name, ((uni_valueof_t *)b)->name);
}
@

<<Initialize UCD files>>=
inisize(enum_vals, num_prop_aliases);
clearbuf(enum_vals, num_prop_aliases);
inisize(enum_vals_len, num_prop_aliases);
clearbuf(enum_vals_len, num_prop_aliases);
for(i = 0; i < num_prop_aliases; i++) {
  if(!val_aliases[i])
    continue;
  /* ignore boolean properties */
  if(num_val_aliases[i] == 2 && !strcmp(val_aliases[i][0].short_name, "N") &&
     !strcmp(val_aliases[i][1].short_name, "Y"))
    continue;
  uni_valueof_t *valueof;
  int num_valueof = 0;
  inisize(valueof, num_val_aliases[i] * 4);
  for(j = 0; j < num_val_aliases[i]; j++) {
    const uni_alias_t *va = &val_aliases[i][j];
    valueof[num_valueof].name = strdup(va->short_name);
    valueof[num_valueof++].val = j;
    valueof[num_valueof].name = strdup(va->long_name);
    valueof[num_valueof++].val = j;
    if(va->alt_name) {
      valueof[num_valueof].name = strdup(va->alt_name);
      valueof[num_valueof++].val = j;
    }
    /* need to skip comments stored in alt_name2; all contain | */
    if(va->alt_name2 && !strchr(va->alt_name2, '|')) {
      valueof[num_valueof].name = strdup(va->alt_name2);
      valueof[num_valueof++].val = j;
    }
  }
  for(j = 0; j < num_valueof; j++) {
    char *n = (char *)valueof[j].name, *d = n;
    for(; *n; n++) {
      if(isupper(*n))
        *d++ = tolower(*n);
      else if(*n != '_' && *n != '-' && !isspace(*n))
        *d++ = *n;
    }
    *d = 0;
  }
  qsort(valueof, num_valueof, sizeof(*valueof), cmp_valueof);
  /* remove duplicates now that they are always contiguous */
  for(j = 1; j < num_valueof; j++)
    if(!strcmp(valueof[j].name, valueof[j - 1].name)) {
      free((char *)valueof[j].name);
      if(valueof[j].val != valueof[j - 1].val) {
        perror(valueof[j - 1].name);
	exit(1);
      }
      memmove(valueof + j, valueof + j + 1, (num_valueof - (j + 1)) *
                                               sizeof(*valueof));
      --j;
      --num_valueof;
    }
  enum_vals[i] = valueof;
  enum_vals_len[i] = num_valueof;
}
@

<<Resize prop-associated arrays>>=
resize(enum_vals, maxparsed);
resize(enum_vals_len, maxparsed);
@

<<Clear prop-associated array[i]>>=
enum_vals[i] = NULL;
enum_vals_len[i] = 0;
@

For each value alias, an enumeration needs to be printed to the header
file.  Both the short and long name should be supported, but there is
little value in supporting the secondary aliases.  This excludes
binary values, which have pointless yes/no aliases, and, if possible,
deprecated values.  Special care must be taken with the version
aliases, as the numeric values have decimal points.  The CCC aliases
are numeric as well, but in this case, the short name should be
ignored and used as the value directly instead.

In order to support the \emph{name} and \emph{value} functions, tables
need to be dumped as well.  First, a table similar to the locally
stored table is printed; this does the \emph{name} operation.  Next,
value table built just previously is dumped for the \emph{value}
operation.  Unlike the enumerations, no values may be skipped when
generating the string tables.  The string tables are stored in the
same file, because both files use the same set of string constants,
and most compilers share storage for like string constants.

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  const char *pn = i < num_prop_aliases ? prop_aliases[i].short_name :
                                          parsed_props[i].name;
  if(!enum_vals[i])
    continue;
  /* ignore boolean properties */
  if(parsed_props[i].rng)
    continue;
  <<Ignore unimplemented enums>>
  <<Generate C code for enumeration constants>>
}
@

<<Generate C code for enumeration constants>>=
fprintf(gen_h, "extern const uni_alias_t uni_%s_nameof[];\n", pn);
char nbuf[64];
sprintf(nbuf, "uni_%s_nameof.gen.c", pn);
open_wf(nf, nbuf);
fprintf(nf, "#include \"uni_prop.h\"\n"
            "#include <stdio.h>\n\n" /* for NULL */
	    "const uni_alias_t uni_%s_nameof[] = {\n", pn);
/* generating enum on gen_h */
fputs("typedef enum {\n", gen_h);
int last_nameof = -1;
for(j = 0; j < num_val_aliases[i]; j++) {
  const uni_alias_t *va = &val_aliases[i][j];
  /* print nameof table entry */
  char *e_nameof;
  int cur_nameof = strtol(va->short_name, &e_nameof, 10);
  if(!*e_nameof)
    while(++last_nameof != cur_nameof)
      fputs("\t{ NULL },\n", nf);
  fprintf(nf, "\t{ \"%s\"", va->short_name);
  if(strcmp(va->long_name, va->short_name))
    fprintf(nf, ", \"%s\"", va->long_name);
  if(va->alt_name && strcmp(va->alt_name, va->long_name))
    fprintf(nf, ", \"%s\"", va->alt_name);
  /* again, | is to skip comments */
  if(va->alt_name2 && !strchr(va->alt_name2, '|') &&
     strcmp(va->alt_name, va->alt_name2))
    fprintf(nf, ", \"%s\"", va->alt_name2);
  fputs(" }", nf);
  if(j < num_val_aliases[i] - 1)
    putc(',', nf);
  putc('\n', nf);
  /* filter out bad enum values */
  /* skip version short name */
  if(strchr(va->short_name, '.')) {
    fprintf(gen_h, "\tU_%s_%s,\n", pn, va->long_name);
    continue;
  }
  /* use numeric values directly instead of making into enum */
  if(!isdigit(va->short_name[0]) || !strcmp(va->long_name, va->short_name)) {
    const char *np;
    /* slow, but necessary with e.g. ID_Restrict_Type */
    fprintf(gen_h, "\tU_%s_", pn);
    for(np = va->short_name; *np; np++) {
      putc(isalnum(*np) ? *np : '_', gen_h);
      /* + and - may both appear, so make + different */
      if(*np == '+')
        putc('_', gen_h);
    }
    putc(',', gen_h);
  }
  if(strcmp(va->long_name, va->short_name)) {
    fprintf(gen_h, "%cU_%s_%s = ", isdigit(va->short_name[0]) ? '\t' : ' ',
                    pn, va->long_name);
    if(!isdigit(va->short_name[0]))
      fprintf(gen_h, "U_%s_%s,", pn, va->short_name);
    else
      fprintf(gen_h, "%s,", va->short_name);
  }
  /* filter out alt names with dashes */
  if(va->alt_name && !strchr(va->alt_name, '-') &&
     strcmp(va->alt_name, va->long_name))
    fprintf(gen_h, " U_%s_%s = U_%s_%s,", pn, va->alt_name, pn, va->long_name);
  /* again, | is to skip comments */
  if(va->alt_name2 && !strchr(va->alt_name2, '|') && !strchr(va->alt_name2, '-') &&
     strcmp(va->alt_name, va->long_name) && strcmp(va->alt_name, va->alt_name2))
    fprintf(gen_h, " U_%s_%s = U_%s_%s,", pn, va->alt_name2, pn, va->long_name);
  fputc('\n', gen_h);
}
fputs("};\n", nf);
fprintf(gen_h, "\tU_NUM_%s\n} uni_%s_t;\n", pn, pn);
fprintf(gen_h, "extern const uni_valueof_t uni_%s_valueof[];\n", pn);
fprintf(nf, "const uni_valueof_t uni_%s_valueof[] = {\n", pn);
for(j = 0; j < enum_vals_len[i]; j++) {
  const char *name = val_aliases[i][enum_vals[i][j].val].short_name;
  if(isdigit(*name))
    name = val_aliases[i][enum_vals[i][j].val].long_name;
  fprintf(nf, "\t{ \"%s\", U_%s_", enum_vals[i][j].name, pn);
  /* slow, but necessary with e.g. ID_Restrict_Type */
  /* also, the CLDR ones have /, +, space, etc. */
  for(; *name; name++) {
    putc(isalnum(*name) ? *name : '_', nf);
    /* + and - may both appear, so make + different */
    if(*name == '+')
      putc('_', nf);
  }
  fprintf(nf, " }%s\n", j < enum_vals_len[i] - 1 ? "," : "");
}
fputs("};\n", nf);
fclose(nf);
fprintf(gen_h, "#define uni_%s_valueof_len %d /* %d lookups max */\n",
               pn, enum_vals_len[i], lg2(enum_vals_len[i] + 1));
@

Since the two arrays are stored in the same file, the logic to
generate file names needs to skip the valueof array.

<<Filter generated file names>>=
fgrep -v _valueof | \
@

Another useful array to print is for approximate name matching.  In
this case, approximate is defined by UAX\#44: case-insensitive,
optionally without any prefix of is, and without any spaces,
underscores, and dashes.  A function to perform most of this
transformation in-place is provided as well.  If additional
transformations are required, such as compatibility decomposition, they
must be done before calling this function.  All enumerations are
ASCII-only, so no decomposition is needed, and they are simply stored
lower-case with dashes, spaces, and underscores removed.  Initial
``is'' is left in; that should only be removed from user input.  To
compare using this function, strip, find a match, and if none is found,
search again with initial ``is'' removed if present.  A sample search
function is provided.

<<Unicode property exports for generator>>=
void uni_enum_name_strip(char *n);
@

<<Unicode property functions for generator>>=
void uni_enum_name_strip(char *n)
{
  char *s, *d;
  for(s = d = n; *s; s++)
    if(*s != '-' && *s != '_' && *s != ' ')
      *d++ = tolower(*s);
  *d = 0;
}
@

<<Unicode property exports>>=
/* note: this is not the fastest way to do this */
uint8_t uni_x_valueof_approx(const char *n, const uni_valueof_t *tab, int len,
                             uint8_t def);
@

<<Unicode property functions>>=
uint8_t uni_x_valueof_approx(const char *n, const uni_valueof_t *tab, int len,
                             uint8_t def)
{
  /* yuck - strdup on every call */
  char *s = strdup(n);
  uni_valueof_t v, *f;
  uni_enum_name_strip(s);
  v.name = s;
  /* more yuck - bsearch is 1/2 as fast as hand-coded binary search */
  f = bsearch(&v, tab, len, sizeof(*tab), cmp_valueof);
  if(!f && !strncmp(s, "is", 2) && s[2]) {
    v.name = s + 2;
    /* more yuck - bsearch is 1/2 as fast as hand-coded binary search */
    f = bsearch(&v, tab, len, sizeof(*tab), cmp_valueof);
  }
  free(s);
  return f ? f->val : def;
}
@

<<Generate C code for enumeration constants>>=
sprintf(nbuf, "uni_%s_valueof_approx.gen.c", pn);
open_wf(anf, nbuf);
fprintf(gen_h, "extern const uni_valueof_t uni_%s_valueof_approx[];\n", pn);
fprintf(anf, "const uni_valueof_t uni_%s_valueof_approx[] = {\n", pn);
uni_valueof_t *approx_array;
inisize(approx_array, enum_vals_len[i]);
for(j = 0; j < enum_vals_len[i]; j++) {
  approx_array[j].name = strdup(enum_vals[i][j].name);
  approx_array[j].val = enum_vals[i][j].val;
  uni_enum_name_strip((char *)approx_array[j].name);
}
qsort(approx_array, enum_vals_len[i], sizeof(*approx_array), cmp_valueof);
for(j = 0; j < enum_vals_len[i]; j++) {
  const char *name = val_aliases[i][approx_array[j].val].short_name;
  if(isdigit(*name))
    name = val_aliases[i][approx_array[j].val].long_name;
  fprintf(anf, "\t{ \"%s\", U_%s_", approx_array[j].name, pn);
  /* slow, but necessary with e.g. ID_Restrict_Type */
  /* also, the CLDR ones have /, +, space, etc. */
  for(; *name; name++) {
    putc(isalnum(*name) ? *name : '_', anf);
    /* + and - may both appear, so make + different */
    if(*name == '+')
      putc('_', anf);
  }
  fprintf(anf, " }%s\n", j < enum_vals_len[i] - 1 ? "," : "");
  free((char *)approx_array[j].name);
}
fputs("};\n", anf);
fclose(anf);
free(approx_array);
fprintf(gen_h, "#define uni_%s_lookup(n) "
                   "uni_x_valueof_approx(n, %s_valueof_approx, %d, ~0)\n",
	       pn, pn, enum_vals_len[i]);
@

There is still one nagging issue:  the gc aliases include some that
are actually combinations of the others.  Since each value can only
hold one value, this is not going to work.  To support this, an extra
table is generated which translates a value into a 64-bit mask that
includes all base values.  For the base values themselves, only their
own bit will be set.  The pseudo values are recognized by a comment in
their last field, which is stored in one of the aliases.  This is the
reason the vertical bar was filtered out above:  all of the aliases
have vertical bars in their last field.

<<Generate C code for enumeration constants>>=
if(!strcmp(pn, "gc")) {
  fputs("extern const uint64_t uni_gc_trans[];\n", gen_h);
  open_wf(tf, "uni_gc_trans.gen.c");
  fputs("#include \"uni_prop.h\"\n\n"
	"const uint64_t uni_gc_trans[] = {\n", tf);
  for(j = 0; j < num_val_aliases[i]; j++) {
    const char *desc;
    fprintf(tf, "\t/* %2s */ ", val_aliases[i][j].short_name);
    desc = val_aliases[i][j].alt_name2;
    if(desc && !strchr(desc, '|'))
      desc = NULL;
    if(!desc)
      fprintf(tf, "1ULL << U_gc_%s", val_aliases[i][j].short_name);
    else {
      const char *desce;
      while(1) {
        desce = strchr(desc, ' ');
	if(!desce)
	  desce = desc + strlen(desc);
	fprintf(tf, "(1ULL << U_gc_%.*s)%s", (int)(desce - desc), desc,
	                                     *desce ? " | " : "");
	if(!*desce)
	  break;
	desc = desce + 3; /* past ' | ' */
      }
    }
    if(j < num_val_aliases[i] - 1)
      putc(',', tf);
    putc('\n', tf);
  }
  fputs("};\n", tf);
  fclose(tf);
}
@

Now that the aliases have been taken care of, it's time to read in the
enumeration properties themselves.  As with the boolean types, a range
table will be built first, and then converted to a multi-level table
when finished.

<<Property parsed contents>>=
uni_chrrng_dat_t *rng_dat;
uint8_t def;
@

<<UCD parser local functions>>=
static void add_enum_rng(prop_t *p, uint32_t low, uint32_t high, uint8_t val)
{
  if(p->def == val)
    return;
  if(!p->max_rng_len)
    inisize(p->rng_dat, (p->max_rng_len = 8));
  if(p->rng_len && p->rng_dat[p->rng_len - 1].high == low - 1 &&
     p->rng_dat[p->rng_len - 1].dat == val)
    p->rng_dat[p->rng_len - 1].high = high;
  else {
    if(p->rng_len == p->max_rng_len)
      resize(p->rng_dat, (p->max_rng_len *= 2));
    p->rng_dat[p->rng_len].low = low;
    p->rng_dat[p->rng_len].high = high;
    p->rng_dat[p->rng_len].dat = val;
    ++p->rng_len;
  }
}
@

<<UCD parser local functions>>=
static uint32_t enum_val(int pno, const char *v)
{
  char *s = strdup(v), *t, *d;
  uni_valueof_t me, *vp;
  me.name = s;

  d = t = s;
  for(; *t; t++) {
    if(isupper(*t))
      *d++ = tolower(*t);
    else if(*t != '_' && *t != '-' && !isspace(*t))
      *d++ = *t;
  }
  *d = 0;
  vp = bsearch(&me, enum_vals[pno], enum_vals_len[pno], sizeof(me),
               cmp_valueof);
  /* permit excess prefix of "is" */
  if(!vp && s[0] == 'i' && s[1] == 's' && s[2]) {
    me.name = s + 2;
    vp = bsearch(&me, enum_vals[pno], enum_vals_len[pno], sizeof(me),
                 cmp_valueof);
  }
  free(s);
  if(!vp) {
    perror(v);
    exit(1);
  }
  return vp->val;
}
@

<<UCD parser local definitions>>=
#define decl_enum(n, d) \
  int prop_##n = -1; \
  const char *def_##n = d
#define add_enum(n, v) do { \
  if(prop_##n < 0) { \
    prop_##n = add_prop(#n); \
    parsed_props[prop_##n].def = def_##n ? enum_val(prop_##n, def_##n) : 0; \
  } \
  if(*v) \
    add_enum_rng(&parsed_props[prop_##n], low, high, enum_val(prop_##n, v)); \
} while(0)
#define add_int(n, val) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(isdigit(*val)) \
    add_enum_rng(&parsed_props[prop_##n], low, high, strtol(val, NULL, 0)); \
  else \
    add_enum(n, val); \
} while(0)
@

\subsection{Testing}

To test the table implementations, a different, but nearly identical,
set of routines is used compared to the ones one for booleans.

<<Functions to help test generated tables>>=
#define dat(x, d) doit_dat(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab, d)

static void doit_dat(const char *name, const uni_chrrng_dat_t *rng, uint32_t nent,
                     const uint32_t *mtab, uint8_t def)
{
    uint32_t i;

    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      uint8_t rv = find_cp_chrrng(i, rng, nent, def);
      uint8_t mv = <<range table data for [[i]]>>;
      if(rv != mv) {
        fprintf(stderr, "mismatch %s@%d %d %d\n", name, i, (int)rv, (int)mv);
	exit(1);
      }
    }
    /* check performance */
    int j;
    unsigned long tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = find_cp_chrrng(i, rng, nent, def);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = <<range table data for [[i]]>>;
    tt = tend();
    printf("  r%ld t%ld %.2fx\n", tr, tt, (double)tr / (double)tt);
}
@

\subsection{Parsing the UCD}

First, [[UnicodeData.txt]] has a few fields.  Field 3 is gc.  Field 4
is ccc.  Field 5 provides bc. Field 6 provides dt, albeit indirectly.
Fields 7 through 9 provide nt, even more indirectly.

<<Initialize UCD files>>=
decl_enum(gc, "Cn");
decl_enum(ccc, 0);
decl_enum(bc, "L"); /* default val depends on block */
                    /* probably ought to use extracted/DerivedBidiClass.txt */
decl_enum(dt, "None");
decl_enum(nt, "None");
@

<<Process a line of [[UnicodeData.txt]]>>=
add_enum(gc, fields[2]);
add_int(ccc, fields[3]);
add_enum(bc, fields[4]);
if(fields[5][0] == '<') {
  char *eval = fields[5] + 1;
  while(*eval && *eval != '>')
    eval++;
  if(!*eval) {
    perror("dt");
    exit(1);
  }
  *eval = 0;
  add_enum(dt, fields[5] + 1);
  *eval = '>';
} else if(fields[5][0])
  add_enum(dt, "can");
if(fields[6][0])
  add_enum(nt, "decimal");
else if(fields[7][0])
  add_enum(nt, "digit");
else if(fields[8][0])
  add_enum(nt, "numeric");
@

The other file that has mixed field types is
[[DerivedNormalizationProps.txt]].  The Boolean parser simply looked
at the number of fields, and added if the number of fields was too low
to be boolean.  This parser can't really do the same, without first
loading the expected default values.  There are only four properties,
so it's safer to just do them by hand.  In fact, two of them can only
have two values: Yes and No.  This makes them boolean in my view, so
that they will become.  There is one other caveat:  the default value
for those two is Yes, and the listed value is always No, so they need
to be inverted when finished.

<<Initialize UCD files>>=
decl_enum(NFC_QC, "Y");
decl_bool(NFD_QC);
decl_enum(NFKC_QC, "Y");
decl_bool(NFKD_QC);
@

<<Process a line of [[DerivedNormalizationProps.txt]]>>=
if(num_fields == 3) {
  if(!strcmp(fields[1], "NFC_QC"))
    add_enum(NFC_QC, fields[2]);
  else if(!strcmp(fields[1], "NFD_QC"))
    add_bool(NFD_QC);
  else if(!strcmp(fields[1], "NFKC_QC"))
    add_enum(NFKC_QC, fields[2]);
  else if(!strcmp(fields[1], "NFKD_QC"))
    add_bool(NFKD_QC);
}
@

<<Parse UCD files>>=
uni_chrrng_t *new;
uint32_t new_len;
uni_chrrng_setop(parsed_props[prop_NFD_QC].rng,
                 parsed_props[prop_NFD_QC].rng_len, UNI_SOP_INV_A,
		 NULL, 0, &new, &new_len);
free(parsed_props[prop_NFD_QC].rng);
parsed_props[prop_NFD_QC].rng = new;
parsed_props[prop_NFD_QC].rng_len = new_len;
uni_chrrng_setop(parsed_props[prop_NFKD_QC].rng,
                 parsed_props[prop_NFKD_QC].rng_len, UNI_SOP_INV_A,
		 NULL, 0, &new, &new_len);
free(parsed_props[prop_NFKD_QC].rng);
parsed_props[prop_NFKD_QC].rng = new;
parsed_props[prop_NFKD_QC].rng_len = new_len;
@

Then there are a number of files that describe just one enumerated
property.  Many of these follow here.

<<Initialize UCD files>>=
decl_enum(sc, "Zzzz");
decl_enum(blk, "No_Block");
decl_enum(hst, "NA");
decl_enum(lb, "XX");
decl_enum(GCB, "XX");
decl_enum(SB, "XX");
decl_enum(WB, "XX");
decl_enum(ea, "N");
decl_enum(age, "unassigned");
@

<<Parse UCD files>>=
open_f("Scripts.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  /* need to manually convert U_SC_Hrkt to U_SC_Hira+U_SC_Kana */
  add_enum(sc, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("Blocks.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(blk, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("HangulSyllableType.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(hst, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("LineBreak.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(lb, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("auxiliary/GraphemeBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
   add_enum(GCB, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("auxiliary/SentenceBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(SB, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("auxiliary/WordBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(WB, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("EastAsianWidth.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(ea, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("DerivedAge.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(age, fields[1]);
}
fclose(f);
@

And there is one file that has two properties per line.

<<Initialize UCD files>>=
decl_enum(jt, "U"); /* note: Mn, Me, Cf are T */
decl_enum(jg, "No_Joining_Group");
@

<<Parse UCD files>>=
open_f("ArabicShaping.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(jt, fields[2]);
  add_enum(jg, fields[3]);
}
fclose(f);
@

\subsection{Generating the Static Data}

Once finished, the tables can be dumped.  Just as with boolean
properties, only range tables have been built, so it's time to
generate the multi-level table, assuming that the range data is
correct.  Since the way that the automatic enumerations are generated
does not guarantee that zero is the default value, and zero is stored
more efficiently than any other value, all values are incremented by
one and the default value is aliased to zero as well.  The default
value itself is never stored in the table; it is supplied in the
lookup function as well.

<<Unicode property exports for generator>>=
uint32_t *rng_dat_to_multi(const uni_chrrng_dat_t *tab, uint32_t tab_len,
                           uint32_t *ml_len, uint8_t def);
@

<<Unicode property functions for generator>>=
uint32_t *rng_dat_to_multi(const uni_chrrng_dat_t *tab, uint32_t tab_len,
                           uint32_t *ml_len, uint8_t def)
{
  uint32_t low, high, len, i;
  uint32_t *ml;
  uint8_t *bits;

  /* degenerate case:  always out-of-range */
  if(!tab_len) {
    bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  for(i = tab_len; i > 0; i--)
    if(tab[i - 1].dat != def)
      break;
  high = tab[i - 1].high;
  for(i = 0; i < tab_len; i++)
    if(tab[i].dat != def)
      break;
  low = tab[i].low;
  len = high - low + 1;
  inisize(bits, len);
  /* FIXME: only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  for(; i < tab_len; i++)
    if(tab[i].dat != def)
      memset(bits + tab[i].low - low, tab[i].dat + 1, tab[i].high - tab[i].low + 1);
  bits_to_multi(bits, len, low, high, 0, 0, &ml, ml_len);
  free(bits);
  return ml;
}
@

<<UCD parser local functions>>=
static void fixup_rng_dat(prop_t *p)
{
  uint32_t i;
  qsort(p->rng_dat, p->rng_len, sizeof(uni_chrrng_dat_t), uni_cmprng_dat);
  /* starting at top means only optimized entries are memmove'd */
  for(i = p->rng_len - 1; i > 0; i--) {
    uint32_t j = i;
    while(i > 0 && p->rng_dat[i - 1].high == p->rng_dat[i].low - 1 &&
          p->rng_dat[i - 1].dat == p->rng_dat[i].dat)
      i--;
    if(i == j)
      continue;
    p->rng_dat[i].high = p->rng_dat[j].high;
    if(j < p->rng_len - 1) {
        memmove(p->rng_dat + i + 1, p->rng_dat + j + 1,
	        (p->rng_len - (j + 1)) * sizeof(uni_chrrng_dat_t));
    }
    p->rng_len -= j - i;
    if(!i)
      break;
  }
}
@

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng_dat) {
    fixup_rng_dat(&parsed_props[i]);
    parsed_props[i].mt = rng_dat_to_multi(parsed_props[i].rng_dat,
                                          parsed_props[i].rng_len,
                                          &ml_len, parsed_props[i].def);
}
@

In addition to the property tables, a simple query function is
printed.  This calls a generic search function using the multi-level
table.  The test cases are printed as well, and added to the boolean
tests.

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng_dat) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    char nbuf[64];
    sprintf(nbuf, "uni_%s_rng.gen.c", name);
    open_wf(of, nbuf);
    fprintf(of, "#include \"uni_prop.h\"\n\n"
		"const uni_chrrng_dat_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].rng_len; j++) {
      uint32_t val = parsed_props[i].rng_dat[j].dat;
      uni_alias_t *aliases = val_aliases[i];
      fprintf(of, "\t{ %d, %d, ",
                  parsed_props[i].rng_dat[j].low,
	          parsed_props[i].rng_dat[j].high);
      if(!aliases || (isdigit(aliases[0].short_name[0]) &&
                      !strchr(aliases[0].short_name, '.')))
        fprintf(of, "%d", (int)val);
      else {
        const uni_alias_t *alias = &aliases[val];
	const char *val_name = alias->short_name;
        if(isdigit(val_name[0]))
	  val_name = alias->long_name;
	if(strchr(val_name, '-')) {
	  const char *np;
	  /* slow, but necessary with e.g. ID_Restrict_Type */
	  fprintf(of, "U_%s_", name);
	  for(np = val_name; *np; np++)
	    putc(*np == '-' ? '_' : *np, of);
	} else
	  fprintf(of, "U_%s_%s", name, val_name);
      }
      fprintf(of, " }%s\n", j < parsed_props[i].rng_len - 1 ? "," : "");
    }
    fputs("};\n", of);
    fclose(of);
    fprintf(gen_h, "extern const uni_chrrng_dat_t uni_%s_rng[];\n"
		   "#define uni_%s_rng_len %d /* %d lookups max */\n"
		   "#define uni_%s_of(x) uni_x_of(x, uni_%s_mtab, %d)\n",
		   name, name, lg2(parsed_props[i].rng_len | 1),
		   parsed_props[i].rng_len, name, name,
		   parsed_props[i].def);
    print_mtab(name, parsed_props[i].mt, gen_h);
    fprintf(tstf, "dat(%s, %d);\n", name, parsed_props[i].def);
  }
}
@

<<Unicode property exports>>=
int uni_x_of(uint32_t cp, const uint32_t *tab, uint8_t def);
@

<<Unicode property functions>>=
int uni_x_of(uint32_t cp, const uint32_t *tab, uint8_t def)
{
  const uint8_t *mr;
  uint8_t mv = multi_tab_lookup(tab, cp, &mr, 0);
  if(mr)
    mv = *mr;
  if(!mv)
    return def;
  else
    return mv - 1;
}
@

<<range table data for [[i]]>>=
uni_x_of(i, mtab, def)
@

<<FIXME>>=
add pseudo-enum "prop" == property names & aliases
  perhaps when mtab present, also have val be the mtab?
  limit to supported properties?
@

\section{Numeric Properties}

Numeric properties are a subset of enumerated properties.  The
property's primary values are integers or two dot-separated integers
(i.e., floating point values).  They may have non-numeric values and
aliases as well.  The main difference is that the sloppy matching
should also support matching the plain integer against any like-valued
integer (e.g., 01 matches 1) and the dotted value against any
like-valued floating point number (e.g., 01.10 matches 1.1).  In
addition, the age property is meant to match equal to or less than the
search value.  For example, 3.0 matches 1.1 as well.

\subsection{Storage Methods}

Numeric values are stored as rational numbers (fractions).  Since the
current standard has no numbers requiring denominators over 127, the
denominator is stored in a single byte.  The numerator needs no more
than 19 bits, so it is stored in the remaining 3 bytes.

<<[[uni_chrrng_num_t]]>>=
typedef struct {
    uint32_t low, high;
    int32_t num:24, denom:8;
} uni_chrrng_num_t;
@

<<Unicode property exports for generator>>=
<<[[uni_chrrng_num_t]]>>
void uni_chrrng_val(uint32_t cp, const uni_chrrng_num_t *tab,
                    uint32_t tab_len, int32_t *num, uint8_t *denom);
@

<<Known Data Types>>=
uni_chrrng_num_t,%
@

<<Unicode property functions for generator>>=
void uni_chrrng_val(uint32_t cp, const uni_chrrng_num_t *tab,
                    uint32_t tab_len, int32_t *num, uint8_t *denom)
{
#if 0
  uni_chrrng_num_t cr = {cp, cp}, *tab_el;
  tab_el = bsearch(&cr, tab, tab_len, sizeof(uni_chrrng_num_t), uni_cmprng);
  if(!tab_el)
    *num = 0;
  else {
    *num = tab_el->num;
    *denom = tab_el->denom;
  }
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > tab[j].high)
      l = j + 1;
    else {
      *num = tab[j].num;
      *denom = tab[j].denom;
      return;
    }
  }
  *num = 0;
#endif
  if(!*num)
    *denom = 1;
}
@

<<Property parsed contents>>=
uni_chrrng_num_t *rng_num;
@

<<UCD parser local functions>>=
static void add_num_rng(prop_t *p, uint32_t low, uint32_t high,
                        int32_t num, uint8_t denom)
{
  if(!p->max_rng_len)
    inisize(p->rng_num, (p->max_rng_len = 8));
  if(p->rng_len && p->rng_num[p->rng_len - 1].high == low - 1 &&
     p->rng_num[p->rng_len - 1].num == num &&
     p->rng_num[p->rng_len - 1].denom == denom)
    p->rng_num[p->rng_len - 1].high = high;
  else {
    if(p->rng_len == p->max_rng_len)
      resize(p->rng_num, (p->max_rng_len *= 2));
    p->rng_num[p->rng_len].low = low;
    p->rng_num[p->rng_len].high = high;
    p->rng_num[p->rng_len].num = num;
    p->rng_num[p->rng_len].denom = denom;
    ++p->rng_len;
  }
}
@

<<UCD parser local definitions>>=
#define decl_num(n) int prop_##n = -1
#define add_num(n, num, denom) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  add_num_rng(&parsed_props[prop_##n], low, high, num, denom); \
} while(0)
@

\subsection{Testing}

To test the table implementations, a different, but nearly identical,
set of routines is used compared to the ones one for booleans.

<<Functions to help test generated tables>>=
#define num(x) doit_num(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab)

static void doit_num(const char *name, const uni_chrrng_num_t *rng, uint32_t nent,
                     const uint32_t *mtab)
{
    uint32_t i;

    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 12, lg2(nent + 1));
    print_mtab_info(mtab, nent * 12);
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      int32_t rn, mn;
      uint8_t rd, md;
      uni_chrrng_val(i, rng, nent, &rn, &rd);
      <<range table val for [[i]]>>;
      if(rn != mn || rd != md) {
        fprintf(stderr, "mismatch %s@%d %d/%d %d/%d\n", name, i,
	                (int)rn, (int)rd, (int)mn, (int)md);
	exit(1);
      }
    }
    /* check performance */
    int j;
    unsigned long tr, tt;
    tstart();
    int32_t mn;
    uint8_t md;
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        uni_chrrng_val(i, rng, nent, &mn, &md);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        <<range table val for [[i]]>>;
    tt = tend();
    printf("  r%ld t%ld %.2fx\n", tr, tt, (double)tr / (double)tt);
}
@

\subsection{Parsing the UCD}

Numeric properties from [[UnicodeData.txt]] are ccc (field 4) and nv
(field 9).  Since ccc has aliases, it is already added as an
enumeration.  The nv field is already in normal rational form.

<<Initialize UCD files>>=
decl_num(nv);
@

<<Process a line of [[UnicodeData.txt]]>>=
if(*fields[8]) {
  int32_t num = strtol(fields[8], &s, 10);
  uint8_t denom = *s ? strtol(s + 1, NULL, 10) : 1;
  add_num(nv, num, denom);
}
@

An optional numeric property comes from
[[Unihan_RadicalStrokeCounts.txt]]: cjkRSUnicode.  It is, like all
unihan files, a tab-separated file, with the single code point in U+
notation in field 1.  Note that the actual optional property is
CJK\_Radical, which I interpret as meaning the same as cjkRSUnicode.
However, I will not add a non-normative alias (especially considering
that the library currently only supports four names, and there are
already four names for this property).

<<UCD parser local functions>>=
static void split_line_tab(char *buf)
{
  if(!max_fields)
    inisize(fields, (max_fields = 16));
  num_fields = 0;
  while(*buf != '\t' && isspace(*buf)) buf++;
  if(!*buf || *buf == '#')
    return;
  while(1) {
    while(isspace(*buf) && *buf != '\t')
      buf++;
    char *f = buf, *nf, fc;
    for(nf = buf; *nf && *nf != '\t'; nf++);
    fc = *nf;
    buf = nf + 1;
    while(nf > f && isspace(nf[-1])) --nf;
    *nf = 0;
    if(num_fields == max_fields)
      resize(fields, (max_fields *= 2));
    fields[num_fields++] = f;
    if(!fc)
      return;
  }
}
@

<<Initialize Unihan files>>=
decl_num(cjkRSUnicode);
@

<<Parse Unihan files>>=
open_f("Unihan_RadicalStrokeCounts.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse Unihan cp>>
  if(!strcmp(fields[1], "kRSUnicode")) {
    /* due to complexity of this field, the num/denom is interpreted */
    /* differently: */
    /* denom is number after .; if after !., then it is negated */
    /* num is 3 byte fields; lowest field is left of . or !. */
    /* if there are two values, then the 2nd value is in the upper bytes */
    int32_t num;
    uint8_t denom;
    num = strtol(fields[2], &s, 10);
    if(*s == '!')
      denom = -strtol(s + 2, &s, 10);
    else
      denom = strtol(s + 1, &s, 10);
    if(*s) {
      num |= strtol(s, &s, 10) << 8;
      if(*s == '!')
        num |= -strtol(s + 2, &s, 10) << 16;
      else
        num |= strtol(s + 1, &s, 10) << 16;
    }
    add_num(cjkRSUnicode, num, denom);
  }
}
fclose(f);
@

<<Parse Unihan cp>>=
split_line_tab(lbuf);
if(num_fields < 2)
  continue;
low = high = strtol(lbuf + 2, &s, 16);
if(!low || *s) { /* should never happen, but if it does: ignore */
  fprintf(stderr, "bad col 1: %s\n", lbuf);
  continue;
}
@

<<Unicode property exports>>=
/* extract l.r value; l!.r is encoded as l.-r */
#define uni_kRSUnicode_val1(n, d, l, r) do { \
  l = (uint8_t)(n); \
  r = (int8_t)d; \
} while(0)
#define uni_kRSUnicode_val2(n, d, l, r) do { \
  l = (uint8_t)(n >> 8); \
  r = (int32_t)n >> 16; \
} while(0)
@

<<C Prototypes>>=
void uni_kRSUnicode_val1(int32_t num, uint8_t denom, uint8_t &l,
                         int8_t &r);
void uni_kRSUnicode_val2(int32_t num, uint8_t denom, uint8_t &l,
                         int8_t &r);
@

The only other numeric property currently supported is age, which
comes from [[DerivedAge.txt]].  Since it has aliases, it is already
added as an enumeration.

\subsection{Generating the Static Data}

Once finished, the tables can be dumped.  Just as with other
properties, only range tables have been built, so it's time to
generate the multi-level table, assuming that the range data is correct.

<<Unicode property exports for generator>>=
uint32_t *rng_num_to_multi(const uni_chrrng_num_t *tab, uint32_t tab_len,
                           uint32_t *ml_len);
@

<<Unicode property functions for generator>>=
uint32_t *rng_num_to_multi(const uni_chrrng_num_t *tab, uint32_t tab_len,
                           uint32_t *ml_len)
{
  uint32_t low, high, len, i, j;
  uint32_t *ml;
  uint32_t *bits;

  /* degenerate case:  always out-of-range */
  if(!tab_len) {
    bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  low = tab[0].low;
  high = tab[tab_len - 1].high;
  len = high - low + 1;
  inisize(bits, len);
  /* FIXME: only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  for(i = 0; i < tab_len; i++) {
    uint32_t v = (tab[i].num << 8) + (tab[i].denom & 0xff);
    if(!tab[i].num) /* default value: 0/anything */
      continue;
    for(j = tab[i].low; j <= tab[i].high; j++)
      bits[j - low] = v;
  }
  bits_to_multi((uint8_t *)bits, len * 4, low * 4, high * 4, 0, 4, &ml, ml_len);
  free(bits);
  return ml;
}
@

<<UCD parser local functions>>=
static void fixup_rng_num(prop_t *p)
{
  uint32_t i;
  qsort(p->rng_num, p->rng_len, sizeof(uni_chrrng_num_t), uni_cmprng);
  /* starting at top means only optimized entries are memmove'd */
  for(i = p->rng_len - 1; i > 0; i--) {
    uint32_t j = i;
    while(i > 0 && p->rng_num[i - 1].high == p->rng_num[i].low - 1 &&
          p->rng_num[i - 1].num == p->rng_num[i].num &&
          p->rng_num[i - 1].denom == p->rng_num[i].denom)
      i--;
    if(i == j)
      continue;
    p->rng_num[i].high = p->rng_num[j].high;
    if(j < p->rng_len - 1) {
        memmove(p->rng_num + i + 1, p->rng_num + j + 1,
	        (p->rng_len - (j + 1)) * sizeof(uni_chrrng_num_t));
    }
    p->rng_len -= j - i;
    if(!i)
      break;
  }
}
@

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng_num) {
    fixup_rng_num(&parsed_props[i]);
    parsed_props[i].mt = rng_num_to_multi(parsed_props[i].rng_num,
                                          parsed_props[i].rng_len,
                                          &ml_len);
}
@

In addition to the property tables, a simple query function is
printed.  This calls a generic search function using the multi-level
table.

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng_num) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    char nbuf[64];
    sprintf(nbuf, "uni_%s_rng.gen.c", name);
    open_wf(of, nbuf);
    fprintf(of, "#include \"uni_prop.h\"\n\n"
                "const uni_chrrng_num_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].rng_len; j++)
      fprintf(of, "\t{ %d, %d, %d, %d }%s\n",
                  parsed_props[i].rng_num[j].low,
	          parsed_props[i].rng_num[j].high,
	          parsed_props[i].rng_num[j].num,
		  parsed_props[i].rng_num[j].denom,
		  j < parsed_props[i].rng_len - 1 ? "," : "");
    fputs("};\n", of);
    fclose(of);
    fprintf(gen_h, "extern const uni_chrrng_num_t uni_%s_rng[];\n"
		   "#define uni_%s_rng_len %d /* %d lookups max */\n"
		   "#define uni_%s_val(x, n, d) uni_x_val(x, uni_%s_mtab, n, d)\n",
		   name, name, lg2(parsed_props[i].rng_len + 1),
		   parsed_props[i].rng_len, name, name);
    print_mtab(name, parsed_props[i].mt, gen_h);
    fprintf(tstf, "num(%s);\n", name);
  }
}
@

<<Unicode property exports>>=
void uni_x_val(uint32_t cp, const uint32_t *tab, int32_t *num, uint8_t *denom);
@

<<Unicode property functions>>=
void uni_x_val(uint32_t cp, const uint32_t *tab, int32_t *num, uint8_t *denom)
{
  const uint8_t *mr;
  multi_tab_lookup(tab, cp * 4, &mr, 0);
  if(!mr)
    *num = 0;
  else {
    int32_t v = *(int32_t *)mr;
    *num = v >> 8;
    *denom = v & 0xff;
  }
  if(!*num)
    *denom = 1;
}
@

<<range table val for [[i]]>>=
uni_x_val(i, mtab, &mn, &md);
@

<<FIXME>>=
numeric (UAX44-LM1):
  compare as floating-point number w/ limited precision
  e.g. 01.00 == 1
@

\section{String Properties}

String properties are those which have a value that cannot be
represented using the previously described methods.  Since the UCD is
text, this is generally either an ASCII or Unicode string.  Useful
operations include:

\begin{itemize}
\item Find out the string-valued value \emph{of} the property for a
character.  For variable-length strings, this should support two-part
retrieval: first obtain the length, and then obtain the string.  If
there is a maximum length, it should be listed so that the sizing step
can be avoided.
\end{itemize}

\subsection{Storage Methods}

The storage methods used for boolean properties can be used for
strings as well, with some adjustments.  First of all, there are very
few consecutive characters with the same value (other than the default
value, which is an empty string), so using a range list is wasteful.
Instead, a simple code point list with value is used.  One way to
store the string would be as a pointer to a zero-terminated string (no
strings use zero as a component value).  However, this requires
computing string length at every access, so a better way would be to
store a pointer and a length.  For 64-bit systems, this would require
at least 8 bytes for the pointer, and probably another 8 bytes to
align the structure, so 4 bytes for the code point and 4 bytes for the
length.  However, a more efficient storage method would be to use a
32-bit pointer into a single string containing all possible values,
reducing storage by 4 bytes. Additional savings can be had if the
combined string size can be kept below 64K, in which case 16-bit
integers can represent the offset and length.  The length could be
stored in the string as well, eliminating the need for a separate
length, and generally reducing the space required for that to a byte.
Since fewer than 24 bits are used for the code point, the length could
be stored in the code point's lower byte as well; masking would be
required to test equality, though. Another space-saving measure would
be to store similar properties together.  For example, the main string
properties are either case folding-related or normalization-related,
and some have the others as their default values.  These could be
combined when they have many elements in common.  For now, though,
range tables are built using 32-bit pointers and lengths.  The merging
into a single string table can be done during post-processing.
However, to assist with this, multiple tables sharing the same string
table can use the [[flags]] field to distinguish themselves.

<<[[prop_t]] prerequisites>>=
typedef struct {
  uint32_t cp, off, flags: 8, len: 24;
} raw_cp_str_t;
@

<<Known Data Types>>=
raw_cp_str_t,%
@

<<Property parsed contents>>=
raw_cp_str_t *rng_str;
uint32_t *strs;
uint32_t strs_len, max_strs;
@

<<UCD parser local functions>>=
static void add_str_rng(prop_t *p, uint32_t low, uint32_t high, const uint32_t *val,
                        uint32_t len)
{
  uint32_t off = p->strs_len;
  if(!p->max_rng_len) {
    inisize(p->rng_str, (p->max_rng_len = 8));
    inisize(p->strs, (p->max_strs = 32));
  }
  while(off + len > p->max_strs)
    resize(p->strs, (p->max_strs *= 2));
  memcpy(p->strs + off, val, len * 4);
  p->strs_len += len;
  for(; low <= high; low++) {
    if(p->rng_len == p->max_rng_len)
      resize(p->rng_str, (p->max_rng_len *= 2));
    p->rng_str[p->rng_len].cp = low;
    p->rng_str[p->rng_len].off = off;
    p->rng_str[p->rng_len].len = len;
    p->rng_str[p->rng_len].flags = 0;
    ++p->rng_len;
  }
}
@

<<UCD parser local definitions>>=
#define decl_str(n) \
  int prop_##n = -1
#define add_str(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[64]; /* max known value == 30, so 64 should be enough */ \
    uint32_t len; \
    for(s = v, len = 0; *s; len++) \
      str[len] = strtol(s, &s, 16); \
    add_str_rng(&parsed_props[prop_##n], low, high, str, len); \
  } \
} while(0)
@

\subsection{Parsing the UCD - Decomposition}

The first string field from [[UnicodeData.txt]] is the decomposition
mapping (dm) field.  For compatibility decompositions, the initial
part of the field is the decomposition type in angle brackets, which
has already been encoded as an enumeration (dt), so it is skipped,
along with the space which always follows it.

<<Initialize UCD files>>=
decl_str(dm);
@

<<Process a line of [[UnicodeData.txt]]>>=
s = fields[5];
if(*s == '<')
  s = strchr(s, '>') + 2;
add_str(dm, s);
@

While the default values for dt and dm are normally blank, one range
actually has defined canonical values: the Hangul
Syllables\footnote{Note that I am not Korean, so I may refer to things
incorrectly or inappropriately.  Everything I know about Korean is
what I read about in the Unicode standard (and maybe a little watching
of subtitled Korean TV dramas).}, from AC00 through D7A3.  These are
only described in the standard, and can be generated with a simple
algorithm.  An entry in this range consists of three parts, called L,
V, and T.  There are normal, named code points for each L, V, and T
outside of this range, and every LV and LVT combination is contained
within this range.  There are defined decomposition mappings from LVT
to LV and T, and from LV to L and V. Rather than fill the dm string
table with easily generated values, the value is represented as an
empty result.  A function is then provided to convert that empty
string into the correct decomposition mapping.  The canonical
dm entry for LVT is LV T, so a flag is used to select the full L V T
decomposition directly.

<<Process a line of [[UnicodeData.txt]]>>=
if(low == 0xAC00) {
  /* hangul syllables */
  add_enum_rng(&parsed_props[prop_dt], low, high, enum_val(prop_dt, "can"));
  add_str_rng(&parsed_props[prop_dm], low, high, NULL, 0);
}
@

<<Unicode property exports>>=
/* requires 3-char buf; returns actual len */
int hangul_syllable_decomp(uint32_t cp, uint32_t *res, int full);
@

<<Unicode property functions>>=
int hangul_syllable_decomp(uint32_t cp, uint32_t *res, int full)
{
  int L, V, T;
  if(cp < 0xAC00 || cp > 0xD7A3)
    return -1;
  cp -= 0xac00;
  T = cp % 28;
  V = (cp / 28) % 21;
  L = cp / (28 * 21);
  if(!T) { /* LV -> L V */
    res[0] = 0x1100 + L;
    res[1] = 0x1161 + V;
    return 2;
  }
  if(!full) { /* LVT -> LV T */
    res[0] = 0xAC00 + 28 * (L * 21 + V);
    res[1] = 0x11A7 + T;
    return 2;
  }
  /* LVT -> L V T */
  res[0] = 0x1100 + L;
  res[1] = 0x1161 + V;
  res[2] = 0x11A7 + T;
  return 3;
}
@

The decomposition mappings are more useful if they are fully
decomposed.  Each character is given both a canonical and
compatibility full decomposition.  There is no real problem with
storing two entries with the same key in a sorted code point table, so
both are stored in the same table, with different flags.  Bit zero
indicates compatibility decomposition.

While I think the raw dm value is worthless, the Unicode standard
requires that it be provided for regular expressions.  It is stored in
the same string table as well, marked with bit one set if different
from the full decomposition.  Like the full decomposition, bit zero is
used to indicate compatibility decompositions, in order to avoid an
extra dt table lookup.

<<Initialize UCD files>>=
decl_str(dm_full);
@

<<Parse UCD files>>=
prop_dm_full = add_prop("dm_full");
prop_t *dmf_prop = &parsed_props[prop_dm_full];
prop_t *dm_prop = &parsed_props[prop_dm];
uint8_t can_val = enum_val(prop_dt, "can");
uint8_t none_val = enum_val(prop_dt, "None");
prop_t *dt_prop = &parsed_props[prop_dt];
for(i = 0; i < dm_prop->rng_len; i++) {
  uint32_t cp = dm_prop->rng_str[i].cp;
  uint32_t str[64], len = dm_prop->rng_str[i].len;
  int is_can = find_cp_chrrng(cp, dt_prop->rng_dat, dt_prop->rng_len,
                              none_val) == can_val;
  int do_dec = !is_can;
  int did_repl = 0;
  raw_cp_str_t *unex;

  /* add unexpanded decomp */
  add_str_rng(dmf_prop, cp, cp, NULL, 0);
  if(!dmf_prop->strs_len) { /* deferred until first alloc */
    free(dmf_prop->strs);
    dmf_prop->strs = dm_prop->strs;
    dmf_prop->max_strs = dm_prop->max_strs;
    dmf_prop->strs_len = dm_prop->strs_len;
    dm_prop->strs = NULL;
    dm_prop->max_strs = dm_prop->strs_len = 0;
  }
  unex = &dmf_prop->rng_str[dmf_prop->rng_len - 1];
  unex->off = dm_prop->rng_str[i].off;
  unex->len = len;
  memcpy(str, dmf_prop->strs + unex->off, len * 4);
  if(is_can)
    for(j = 0; j < len; j++) {
      uint8_t dt = find_cp_chrrng(str[j], dt_prop->rng_dat, dt_prop->rng_len,
                                  none_val);
      if(dt == can_val) {
        raw_cp_str_t *dec = bsearch(str + j, dm_prop->rng_str, dm_prop->rng_len,
				    sizeof(raw_cp_str_t), uni_cmp_cp);
        if(dec->len > 1)
          memmove(str + j + dec->len, str + j + 1, (len - j - 1) * 4);
        memcpy(str + j, dec->off + dmf_prop->strs, dec->len * 4);
        len += dec->len - 1;
	did_repl = 1;
        j--; /* recheck 1st char of replacement */
      } else if(dt != none_val)
        do_dec = 1;
  }
  if(did_repl) {
    unex->flags = 2;
    add_str_rng(dmf_prop, cp, cp, str, len);
  }
  if(do_dec) {
    for(j = 0; j < len; j++) {
      uint8_t dt = find_cp_chrrng(str[j], dt_prop->rng_dat, dt_prop->rng_len,
                                  none_val);
      if(dt != none_val) {
        raw_cp_str_t *dec = bsearch(str + j, dm_prop->rng_str, dm_prop->rng_len,
				    sizeof(raw_cp_str_t), uni_cmp_cp);
        if(dec->len > 1)
          memmove(str + j + dec->len, str + j + 1, (len - j - 1) * 4);
        memcpy(str + j, dec->off + dmf_prop->strs, dec->len * 4);
	did_repl = 1;
        len += dec->len - 1;
        j--; /* recheck 1st char of replacement */
      }
    }
    if(did_repl) {
      if(!is_can)
        unex->flags = 3;
      add_str_rng(dmf_prop, cp, cp, str, len);
      dmf_prop->rng_str[dmf_prop->rng_len - 1].flags = 1;
    } else if(!is_can)
      unex->flags = 1;
  }
}
@

\subsection{Generating the Static Data}

This string table can now be printed.  Before doing so, the strings
are sorted, and any strings which are either the same or overlap from
the start share space.  This does not save as much as true common
substring elimination would, but it does save a little.

<<UCD parser local functions>>=
/* qsort has no user data, so this is a static */
const uint32_t *sort_strs;
static int cmp_strs(const void *a, const void *b)
{
   const raw_cp_str_t *A = a, *B = b;
   uint32_t len = A->len;
   int c;

   if(len > B->len)
     len = B->len;
   c = memcmp(sort_strs + A->off, sort_strs + B->off, len * 4);
   if(c)
     return c;
   return (int32_t)A->len - (int32_t)B->len;
}
@

<<UCD parser local functions>>=
static uint32_t merge_strs(prop_t *p)
{
  uint32_t i, saved = 0;

  sort_strs = p->strs;
  qsort(p->rng_str, p->rng_len, sizeof(*p->rng_str), cmp_strs);
  for(i = p->rng_len - 1; i; i--)
    if(p->rng_str[i - 1].len <= p->rng_str[i].len &&
       !memcmp(p->strs + p->rng_str[i - 1].off,
               p->strs + p->rng_str[i].off,
	       p->rng_str[i - 1].len * 4)) {
      p->rng_str[i - 1].off = p->rng_str[i].off;
      saved += p->rng_str[i - 1].len;
  }
  return saved;
}

static void dump_strs(prop_t *p, FILE *gen_h)
{
  uint32_t i, off, saved = merge_strs(p);
  char nbuf[64];

  sprintf(nbuf, "uni_%s_strs.gen.c", p->name);
  open_wf(of, nbuf);
  fprintf(of, "#include <stdint.h>\n\n"
              "const uint32_t uni_%s_strs[] = {\n\t", p->name);
  for(i = 0, off = 0; i < p->rng_len; i++) {
    uint32_t j, len = p->rng_str[i].len;
    if(i < p->rng_len - 1 && p->rng_str[i].off == p->rng_str[i + 1].off) {
      p->rng_str[i].off = len ? off : 1;
      continue;
    }
    for(j = 0; j < len; j++)
      fprintf(of, "0x%X%s", p->strs[p->rng_str[i].off + j],
                            j == len - 1 && i == p->rng_len - 1 ? "\n};\n" :
			          !((off + j + 1) % 8) ? ",\n\t" : ", ");
    p->rng_str[i].off = len ? off : 1;
    off += len;
  }
  fclose(of);
  fprintf(gen_h, "extern const uint32_t uni_%s_strs[]; "
                 "/* %d words (%d bytes) (%d words saved) */\n",
		 p->name, off, off * 4, saved);
}
@

<<Dump character information as C code>>=
dump_strs(dmf_prop, gen_h);
@

Now that the string offsets and lengths are correct, the range table
is ready to convert into a multi-level table, and then both can be
printed.  The range table could simply be the same as the raw table,
using the flags to distinguish entries.  The multi-level table,
though, expects only one value per index, and each value must be the
same size.  Rather than figure out a way to squeeze all three values
into an efficient representation, three separate tables are generated.
While it would definitely be possible to encode the offset and length
into 24 bits, the current multi-level table implementation only
supports sizes which are powers of two.  Encoding into 16 bits might
be possible, but is likely too complex to be useful.  Instead, 32 bits
are used: 16 for the offset, 8 for the length, and 8 for a flag
indicating whether or not this entry is a compatibility decomposition.
The full decomposition tables are similar, but have no flag.  The
compatibility decomposition table uses entries from the canonical
table where no compatibility decomposition exists.  Rather than make
the range tables completely different, three separate range tables
will be generated as well.

<<UCD parser local functions>>=
/* sort by flag first, then cp */
static int cmp_cp_flg(const void *a, const void *b)
{
   const raw_cp_str_t *A = a, *B = b;
   int32_t c;

   c = (int32_t)A->cp - (int32_t)B->cp;
   if(c)
     return c;
   return (int32_t)A->flags - (int32_t)B->flags;
}
@

<<[[uni_str_rng_t]]>>=
typedef struct {
  uint32_t cp;
  uint16_t off;
  uint8_t len, flags;
} uni_str_rng_t;
@

<<[[uni_str_rng_val_t]]>>=
typedef struct {
  uint16_t off;
  uint8_t len, flags;
} uni_str_rng_val_t;
@

<<Unicode property exports for generator>>=
<<[[uni_str_rng_t]]>>
<<[[uni_str_rng_val_t]]>>
@

<<Known Data Types>>=
uni_str_rng_t,uni_str_rng_val_t,%
@

<<Dump character information as C code>>=
qsort(dmf_prop->rng_str, dmf_prop->rng_len, sizeof(*dmf_prop->rng_str), cmp_cp_flg);
uni_str_rng_t *dec;
inisize(dec, dmf_prop->rng_len);
/* the easy table: canon full mapping is all w/ flags == 0 */
open_wf(cf, "uni_canon_decomp_rng.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_rng_t uni_canon_decomp_rng[] = {\n\t", cf);
uint32_t curent;
for(i = 0; dmf_prop->rng_str[i].flags; i++);
for(curent = 0; i < dmf_prop->rng_len; i++, curent++) {
  dec[curent].cp = dmf_prop->rng_str[i].cp;
  dec[curent].off = dmf_prop->rng_str[i].off;
  dec[curent].len = dmf_prop->rng_str[i].len;
  dec[curent].flags = 0;
  while(i < dmf_prop->rng_len - 1 && dmf_prop->rng_str[i + 1].flags)
    ++i;
  fprintf(cf, "{ 0x%X, %d, %d }%s", (int)dec[curent].cp, (int)dec[curent].off,
                                    (int)dec[curent].len,
				    i < dmf_prop->rng_len - 1 ? ",\n\t" : "\n};\n");
}
fclose(cf);
fprintf(gen_h, "extern const uni_str_rng_t uni_canon_decomp_rng[];\n"
	       "#define uni_canon_decomp_rng_len %d /* %d lookups max */\n",
	       curent, lg2(curent + 1));
@

<<Clean up after parsing UCD files>>=
free(dec);
@

<<[[uni_cp_val_t]]>>=
typedef struct {
  uint32_t cp;
  uint32_t val;
} uni_cp_val_t;
@

<<Unicode property exports for generator>>=
<<[[uni_cp_val_t]]>>
uint32_t *cp_val_to_multi(const uni_cp_val_t *tab, uint32_t tab_len,
                           uint32_t *ml_len);
@

<<Known Data Types>>=
uni_cp_val_t,%
@

<<Unicode property functions for generator>>=
uint32_t *cp_val_to_multi(const uni_cp_val_t *tab, uint32_t tab_len,
                           uint32_t *ml_len)
{
  uint32_t low, high, len, i;
  uint32_t *ml;
  uint32_t *bits;

  /* degenerate case:  always out-of-range */
  if(!tab_len) {
    bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  low = tab[0].cp;
  high = tab[tab_len - 1].cp;
  len = high - low + 1;
  inisize(bits, len);
  /* FIXME: only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  for(i = 0; i < tab_len; i++)
    bits[tab[i].cp - low] = tab[i].val;
  bits_to_multi((uint8_t *)bits, len * 4, low * 4, high * 4, 0, 4, &ml, ml_len);
  free(bits);
  return ml;
}
@

<<Dump character information as C code>>=
uint32_t *tmt = cp_val_to_multi((uni_cp_val_t *)dec, curent, &ml_len);
print_mtab("canon_decomp", tmt, gen_h);
free(tmt);
@

<<Dump character information as C code>>=
/* the next harder table: flags == 1 -> full compat decomp */
/* needs to duplicate full canon decomp when empty */
open_wf(kf, "uni_compat_decomp_rng.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_rng_t uni_compat_decomp_rng[] = {\n\t", kf);
for(i = curent = 0; i < dmf_prop->rng_len; i++, curent++) {
  raw_cp_str_t *cur = &dmf_prop->rng_str[i];
  /* could be canon followed by compat, or compat followed by dm */
  if(i < dmf_prop->rng_len - 1 && cur[1].cp == cur->cp &&
     !(cur->flags & 1) && (cur[1].flags & 1)) {
    ++i;
    ++cur;
  }
  dec[curent].cp = cur->cp;
  dec[curent].off = cur->off;
  dec[curent].len = cur->len;
  dec[curent].flags = cur->flags & 1;
  /* could be canon/compat followed by dm */
  if(i < dmf_prop->rng_len - 1 && cur->cp == cur[1].cp)
    ++i;
  fprintf(kf, "{ 0x%X, %d, %d%s }%s",
              (int)dec[curent].cp, (int)dec[curent].off, (int)dec[curent].len,
	      dec[curent].flags ? ", 1" : "",
	      i < dmf_prop->rng_len - 1 ? ",\n\t" : "\n};\n");
}
fclose(kf);
fprintf(gen_h, "extern const uni_str_rng_t uni_compat_decomp_rng[];\n"
	       "#define uni_compat_decomp_rng_len %d /* %d lookups max */\n",
	       curent, lg2(curent + 1));
tmt = cp_val_to_multi((uni_cp_val_t *)dec, curent, &ml_len);
print_mtab("compat_decomp", tmt, gen_h);
free(tmt);
@

<<Dump character information as C code>>=
/* the hardest table: 2/3 -> dm */
/* needs to duplicate full canon decomp when empty */
/* needs to duplicate full compat decomp when empty & full canon empty*/
/* needs to duplicate full canon decomp when empty */
open_wf(dm, "uni_dm_rng.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_rng_t uni_dm_rng[] = {\n\t", dm);
for(i = curent = 0; i < dmf_prop->rng_len; i++) {
  raw_cp_str_t *cur = &dmf_prop->rng_str[i];

  if(i < dmf_prop->rng_len - 1 && cur[1].cp == cur->cp) {
    /* possibilities: canon,compat,dm canon,dm canon,compat compat,dm */
    if(cur[1].flags & 2) {
      ++i; /* compat,dm or canon,dm: skip full */
      cur++;
    } else if(i < dmf_prop->rng_len - 2 && cur->cp == cur[2].cp) {
      i += 2; /* canon,compat,dm: skip both full */
      cur += 2;
    } else /* canon,compat; skip compat when done */
      i++;
  }
  dec[curent].cp = cur->cp;
  dec[curent].off = cur->off;
  dec[curent].len = cur->len;
  dec[curent].flags = cur->flags & 1;
  fprintf(dm, "{ 0x%X, %d, %d%s }%s", (int)dec[curent].cp, (int)dec[curent].off,
                                      (int)dec[curent].len,
				      dec[curent].flags ? ", 1" : "",
				      i < dmf_prop->rng_len - 1 ? ",\n\t" : "\n};\n");
  curent++;
}
fclose(dm);
fprintf(gen_h, "extern const uni_str_rng_t uni_dm_rng[];\n"
	       "#define uni_dm_rng_len %d /* %d lookups max */\n",
	       curent, lg2(curent + 1));
tmt = cp_val_to_multi((uni_cp_val_t *)dec, curent, &ml_len);
print_mtab("dm", tmt, gen_h);
free(tmt);
@

The lookup functions return offset and length.  An additional lookup
function fills in a string given the offset and length.  This is so
that the exact name of the string table and the special value for
Hangul Syllables is encoded in these functions, rather than everywhere
lookups are used.

<<Unicode property exports>>=
/* 2-step lookup: uni_find_xxx followed by uni_get_decomp */
/* off is uint16_t; len and compat are uint8_t; compat is optional */
#define uni_find_canon_decomp(cp, off, len) \
  uni_x_dec(cp, uni_canon_decomp_mtab, off, len, NULL, 1, uni_dm_full_strs)
#define uni_find_compat_decomp(cp, off, len, compat) \
  uni_x_dec(cp, uni_compat_decomp_mtab, off, len, compat, 1, uni_dm_full_strs)
#define uni_find_dm(cp, off, len, compat) \
  uni_x_dec(cp, uni_dm_mtab, off, len, compat, 0, uni_dm_full_strs)
#define uni_get_decomp(cp, buf, off, len) do { \
  if((off) == -1) \
    hangul_syllable_decomp(cp, buf, (len) == 3); \
  else \
    memcpy(buf, uni_dm_full_strs + (off), (len) * 4); \
} while(0)
@

<<C Prototypes>>=
void uni_find_canon_decomp(uint32_t cp, int16_t *off, uint8_t *len);
void uni_find_compat_decomp(uint32_t cp, int16_t *off, uint8_t *len,
                            uint8_t *compat);
void uni_find_dm(uint32_t cp, int16_t *off, uint8_t *len, uint8_t *compat);
void uni_get_decomp(uint32_t cp, uint32_t *buf, int16_t *off, uint8_t *len);
@

<<Unicode property exports>>=
void uni_x_dec(uint32_t cp, const uint32_t *tab, int16_t *off, uint8_t *len,
               uint8_t *compat, int h, const uint32_t *strs);
@

<<Unicode property functions>>=
void uni_x_dec(uint32_t cp, const uint32_t *tab, int16_t *off, uint8_t *len,
               uint8_t *compat, int h, const uint32_t *strs)
{
  const uint8_t *mr;
  multi_tab_lookup(tab, cp * 4, &mr, 0);
  if(!mr) {
    *off = 0;
    *len = 0;
    if(compat)
      *compat = 0;
    return;
  } else {
    uni_str_rng_val_t *v = (void *)mr;
    if(h >= 0 && !v->len && v->off) {
      *off = -1;
      *len = h && (cp - 0xAC00) % 28 ? 3 : 2;
      if(compat)
        *compat = 0;
    } else {
      *off = v->off;
      *len = v->len;
      if(compat)
        *compat = v->flags;
    }
  }
}
@

\subsection{Testing}

To test the table implementations, a different, but nearly identical,
set of routines is used compared to the ones one for booleans.

<<Functions to help test generated tables>>=
#define str(x) doit_str(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab)

static void doit_str(const char *name, const uni_str_rng_t *rng, uint32_t nent,
                     const uint32_t *mtab)
{
    uint32_t i;

    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    uni_str_rng_val_t *ms, *rs, z;
    uni_str_rng_t *r;
    clearbuf(&z, 1);
    for(i = 0; i < 0x110000; i++) {
      r = bsearch(&i, rng, nent, sizeof(*rng), uni_cmp_cp);
      rs = r ? (uni_str_rng_val_t *)&r->off : &z;
      multi_tab_lookup(mtab, i * 4, (const uint8_t **)&ms, 0);
      if(!ms)
        ms = &z;
      if(memcmp(ms, rs, sizeof(*ms))) {
        fprintf(stderr, "mismatch %s@%d %d/%d/%d %d/%d/%d\n", name, i,
	                (int)rs->off, (int)rs->len, (int)rs->flags,
			(int)ms->off, (int)ms->len, (int)ms->flags);
	exit(1);
      }
    }
    /* check performance */
    int j;
    unsigned long tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        r = bsearch(&i, rng, nent, sizeof(*rng), uni_cmp_cp);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        multi_tab_lookup(mtab, i * 4, (const uint8_t **)&ms, 0);
    tt = tend();
    printf("  r%ld t%ld %.2fx\n", tr, tt, (double)tr / (double)tt);
}
@

<<Dump character information as C code>>=
fputs("str(canon_decomp);\n"
      "str(compat_decomp);\n"
      "str(dm);\n", tstf);
@

\subsection{Parsing the UCD - Composition}

As mentioned earlier, composition takes place on pairs.  The canonical
composition of A and B is C if the plain dm value for C is A B, and dt
for C is canonical, and C does not have the Comp\_Ex property.  Given
these conditions, B is removed and A is replaced by C.  After
replacement, C becomes A for a further pass at composition.  It is not
possible to make a ``fully composed'' table that, like the full
decomposition table, iterates thorugh all possible compositions. So,
the composition table is basically a table returning C (or nothing)
given A and B.  

There are a number of possible strategies for storing these values for
efficient retrieval. For sorted arrays, the pair of A and B could
simply be used as the key. When both A and B are known, this is the
most effective form of the sorted array.  However, real normalization
takes place by knowing A, and then scanning all potential candidates
for B.  For the sorted array, this is still not too terrible: the
first and last occurrence of A can be found relatively quickly, and
that limited subset can be searched for B.  The search for the last
and first occurence can be removed by instead having A return a table
(generally an address and a table length) indexed on B.  The B-indexed
table then returns C.  This arrangement can be used regardless of the
actual A-indexed table structure.

Rather than creating a new structure to store these B-indexed tables,
they are stored as if they were strings themselves: A is used to look
up a string, which is actually a code point/value table to look up C
given B.  As with the dm table, the Hangul Syllables are given special
treatment. The A-indexed entries for all LV and L characters point to
zero-length tables, indicating that the [[hangul_syllable_comp]]
function should be used to compose instead.

<<Initialize UCD files>>=
decl_str(cm);
@

<<Parse UCD files>>=
prop_cm = add_prop("canon_comp");
prop_t *fce_prop = &parsed_props[add_prop("Comp_Ex")];
prop_t *cm_prop = &parsed_props[prop_cm];
for(i = 0; i < dm_prop->rng_len; i++) {
  uint32_t str[2];
  uint32_t cp = dm_prop->rng_str[i].cp; /* cp == C */
  if(dm_prop->rng_str[i].len != 2 ||
     find_cp_chrrng(cp, dt_prop->rng_dat, dt_prop->rng_len, none_val) !=
         can_val ||
     is_cp_chrrng(cp, fce_prop->rng, fce_prop->rng_len))
    continue; /* skip if not canonical composition */
  str[0] = dmf_prop->strs[dm_prop->rng_str[i].off + 1]; /* B */
  str[1] = cp;
  cp = dmf_prop->strs[dm_prop->rng_str[i].off]; /* A */
  /* just add; tables are merged later */
  add_str_rng(cm_prop, cp, cp, str, 2);
}
/* add entries for Hangul */
add_str_rng(cm_prop, 0x1100, 0x1112, NULL, 0); /* L */
for(i = 0xAC00; i <= 0xD7A3; i += 28) /* LV */
  add_str_rng(cm_prop, i, i, NULL, 0);
/* merge B->C tables */
qsort(cm_prop->rng_str, cm_prop->rng_len, sizeof(*cm_prop->rng_str), uni_cmp_cp);
for(i = cm_prop->rng_len - 1; i > 0; i--) {
  int off, k;
  j = i;
  while(j > 0 && cm_prop->rng_str[i].cp == cm_prop->rng_str[j - 1].cp)
    j--;
  if(j == i)
    continue;
  while(cm_prop->strs_len + 2 * (i - j + 1) > cm_prop->max_strs)
    resize(cm_prop->strs, (cm_prop->max_strs *= 2));
  for(k = j, off = 0; k <= i; k++) {
    cm_prop->strs[cm_prop->strs_len + off++] = cm_prop->strs[cm_prop->rng_str[k].off];
    cm_prop->strs[cm_prop->strs_len + off++] = cm_prop->strs[cm_prop->rng_str[k].off + 1];
  }
  cm_prop->rng_str[j].off = cm_prop->strs_len;
  cm_prop->rng_str[j].len = 2 * (i - j + 1);
  cm_prop->strs_len += 2 * (i - j + 1);
  /* sort B->C table by B */
  qsort(cm_prop->strs + cm_prop->rng_str[j].off, (i - j) + 1,
        2 * sizeof(*cm_prop->strs), uni_cmp_cp);
  /* remove excess entries */
  memmove(cm_prop->rng_str + j + 1, cm_prop->rng_str + i + 1,
          (cm_prop->rng_len - i - 1) * sizeof(*cm_prop->rng_str));
  cm_prop->rng_len -= i - j;
  if(!(i = j))
    break;
}
@

<<Unicode property exports>>=
/* returns 0 if invalid */
uint32_t hangul_syllable_comp(uint32_t a, uint32_t b);
@

<<Unicode property functions>>=
uint32_t hangul_syllable_comp(uint32_t a, uint32_t b)
{
  if(a >= 0x1100 && a <= 0x1112) /* L */
    if(b >= 0x1161 && b <= 0x1175) /* V */
      return 0xAC00 + 28 * ((a - 0x1100) * 21 + (b - 0x1161)); /* LV */
  if(b > 0x11A7 && b <= 0x11C2) /* T */
    if(a >= 0xAC00 && a <= 0xD7A3 && !((a - 0xAC00) % 28)) /* LV */
      return a + (b - 0x11A7); /* LVT */
  return 0;
}
@

There is probably little value in converting the B-indexed lookup
tables to multi-level tables, so the string table can be printed as is
now. The [[dump_strs]] function is the most convenient method to do
so. The fucntion will pointlessly try to compress the table, but
that's better than writing a dumper function just for this table.

<<Dump character information as C code>>=
int max_len = 0;
for(i = 0; i < cm_prop->rng_len; i++)
  if(cm_prop->rng_str[i].len > max_len)
    max_len = cm_prop->rng_str[i].len;
max_len /= 2;
fprintf(gen_h, "/* %d max B->C len (%d lookups) */\n", max_len, lg2(max_len + 1));
dump_strs(cm_prop, gen_h);
@

The A-indexed range table can now be sorted, converted to a
multi-level table, and printed.  The [[dec]] table built above is
reused as temporary storage to convert to the smaller format for the
multi-level table. It is guaranteed to be large enough, because every
composition entry corresponds to at least one decomposition entry.

<<Dump character information as C code>>=
qsort(cm_prop->rng_str, cm_prop->rng_len, sizeof(*cm_prop->rng_str), uni_cmp_cp);
open_wf(cm, "uni_canon_comp_rng.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_rng_t uni_canon_comp_rng[] = {\n\t", cm);
for(i = 0; i < cm_prop->rng_len; i++) {
  raw_cp_str_t *cur = &cm_prop->rng_str[i];
  dec[i].cp = cur->cp;
  dec[i].off = cur->off;
  dec[i].len = cur->len;
  dec[i].flags = 0;
  fprintf(cm, "{ 0x%X, %d, %d }%s", (int)cur->cp, (int)cur->off, (int)cur->len,
				    i < cm_prop->rng_len - 1 ? ",\n\t" : "\n};\n");
}
fclose(cm);
fprintf(gen_h, "extern const uni_str_rng_t uni_canon_comp_rng[];\n"
	       "#define uni_canon_comp_rng_len %d /* %d lookups max */\n",
	       i, lg2(i + 1));
cm_prop->mt = cp_val_to_multi((uni_cp_val_t *)dec, cm_prop->rng_len, &ml_len);
print_mtab("canon_comp", cm_prop->mt, gen_h);
fputs("str(canon_comp);\n", tstf);
@

The same lookup function as was used for decomposition lookups mostly
works for the A-indexed level as well.  For the B-indexed level, a
binary search is performed.  This is hand-coded for speed.

<<Unicode property exports>>=
/* to compose A+B->C: */
/* look up B->C table for A (cp) */
#define uni_find_canon_comp(cp, off, len) \
  uni_x_dec(cp, uni_canon_comp_mtab, off, len, NULL, -1, uni_canon_comp_strs)
/* then look up C given B->C table info */
#define uni_canon_comp(cpa, cpb, off, len) \
  ((len) > 0 ? uni_lookup_compent(cpb, uni_canon_comp_strs + (off), len) : \
               hangul_syllable_comp(cpa, cpb))
@

<<C Prototypes>>=
void uni_find_canon_comp(uint32_t cp, int16_t *off, uint8_t *len);
uint32_t uni_canon_comp(uint32_t cpa, uint32_t cpb, uint16_t off, uint8_t len);
@

<<Unicode property exports for generator>>=
uint32_t uni_lookup_compent(uint32_t cp, const uint32_t *tab, uint32_t len);
@

<<Unicode property functions for generator>>=
uint32_t uni_lookup_compent(uint32_t cp, const uint32_t *tab, uint32_t len)
{
  int32_t l = 0, h = len / 2 - 1;
  while(h >= l) {
    uint32_t m = (l + h) / 2;
    int32_t c = (int32_t)tab[m * 2] - (int32_t)cp;
    if(!c)
      return tab[m * 2 + 1];
    else if(c < 0)
      l = m + 1;
    else
      h = m - 1;
  }
  return 0;
}
@

\subsection{Parsing the UCD - Case Conversion}

The next strings are the various case conversion tables.
[[UnicodeData.txt]] contains three of them: the simple case mappings
(lower, upper, and title case).  These are always just one character
long, so they are added as a simple value, instead.

It might make sense to make a sort of combined case conversion table,
but it's easier to just dump the three as separate tables.  The only
savings is to suppress redundant entries:  any which match the
character (which should never occur, but it doesn't hurt to check just
in case) or any title case entries which match the upper case entries
(which does occur).

<<Initialize UCD files>>=
decl_num(slc);
decl_num(suc);
decl_num(stc);
@

<<Process a line of [[UnicodeData.txt]]>>=
{
  const char *slcs = fields[13], *sucs = fields[12], *stcs = fields[14];
  uint32_t slc = strtol(slcs, NULL, 16);
  uint32_t suc = strtol(sucs, NULL, 16);
  uint32_t stc = strtol(stcs, NULL, 16);

  if(*slcs && slc != low)
    add_num(slc, slc, 1);
  if(*sucs && suc != low)
    add_num(suc, suc, 1);
  if(*stcs && stc != low && (!*sucs || stc != suc))
    add_num(stc, stc, 1);
}
@

[[SpecialCasing.txt]] lists the exceptions to the above case mappings.
The above values are always either zero or one character (technically
always one character, since the default value is the unmodified code
point); the ones from this file may be more than one character, or may
require more than one character as input, or may only apply under
certain conditions.  The mappings with conditions override the simple
mappings above when the condition is in effect.  It would be nice to
only require one lookup to do case conversion, but combining the above
tables with this one is not that simple.  Instead, this table needs to
always be consulted first, followed by the above tables.

The file is short enough that it can be encoded in a relatively
inefficient format, which simply directly encodes the contents of the
file.  Three tables are generated, corresponding to the desired case
conversion.  If the conversion is already in one of the above tables,
it is simply skipped.  Otherwise, it is encoded as a condition word,
followed by the result string.  Since the condition flags can be
encoded in fewer than 16 bits, the lower 16 bits of the condition word
also encode the length of the conversion.  That way, multiple
conversions with different conditions can simply be appended to the
string.  Unfortunately, multiple entries with the same key are not
always consecutive, so this merging is done after they have all been
gathered.

<<Special casing conditions>>=
/* all known conditions from SpecialCasing.txt, version 6.2 */
/* future revisions may need more flags/parsing */
#define UNI_SC_FL_LT                (1<<16) /* lt locale */
#define UNI_SC_FL_AZ                (1<<17) /* az locale */
#define UNI_SC_FL_TR                (1<<18) /* tr locale */
#define UNI_SC_FL_LOCALE            (0xff << 16) /* any locale flags */

#define UNI_SC_FL_NOT               (1<<24) /* applies to subsequent conditions */
#define UNI_SC_FL_AFTER_I           (1<<25) /* capital I in same grapheme cl. */
#define UNI_SC_FL_AFTER_SOFT_DOTTED (1<<26) /* lower-case i in same gc */
#define UNI_SC_FL_BEFORE_DOT        (1<<27) /* combining dot above in same gc */
#define UNI_SC_FL_MORE_ABOVE        (1<<28) /* grave, acute, etc. in same gc */
#define UNI_SC_FL_FINAL_SIGMA       (1<<29) /* no following letters in same word */
#define UNI_SC_FL_CONTEXT           (0xfe << 24) /* any condition flags */

#define UNI_SC_FL_LEN_MASK          0xffff
@

<<Unicode property exports for generator>>=
<<Special casing conditions>>
@

<<Initialize UCD files>>=
decl_str(lc);
decl_str(uc);
decl_str(tc);
@

<<Parse UCD files>>=
open_f("SpecialCasing.txt");
#define add_sc(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  uint32_t str[64]; /* max known value == 30, so 64 should be enough */ \
  uint32_t len; \
  for(s = v, len = 0; *s; len++) \
    str[len + 1] = strtol(s, &s, 16); \
  str[0] = len | flg; \
  if(len == 1 && low == high) { \
    int32_t num; \
    uint8_t den; \
    prop_t *sc = &parsed_props[prop_s##n]; \
    uni_chrrng_val(low, sc->rng_num, sc->rng_len, &num, &den); \
    if(!den || num != str[1]) \
      add_str_rng(&parsed_props[prop_##n], low, high, str, len + 1); \
  } else \
    add_str_rng(&parsed_props[prop_##n], low, high, str, len + 1); \
} while(0)
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  uint32_t flg = 0;
  if(num_fields > 4) {
    s = fields[4];
    /* slow parse, but at least accurate */
    if(!strncasecmp(s, "lt", 2)) {
      flg |= UNI_SC_FL_LT;
      s += 2;
      if(*s)
        ++s;
    } else if(!strncasecmp(s, "az", 2)) {
      flg |= UNI_SC_FL_AZ;
      s += 2;
      if(*s)
        ++s;
    } else if(!strncasecmp(s, "tr", 2)) {
      flg |= UNI_SC_FL_TR;
      s += 2;
      if(*s)
        ++s;
    }
    if(!strncasecmp(s, "Not_", 4)) {
      flg |= UNI_SC_FL_NOT;
      s += 4;
    }
    if(!strcasecmp(s, "After_I"))
      flg |= UNI_SC_FL_AFTER_I;
    else if(!strcasecmp(s, "After_Soft_Dotted"))
      flg |= UNI_SC_FL_AFTER_SOFT_DOTTED;
    else if(!strcasecmp(s, "Before_Dot"))
      flg |= UNI_SC_FL_BEFORE_DOT;
    else if(!strcasecmp(s, "More_Above"))
      flg |= UNI_SC_FL_MORE_ABOVE;
    else if(!strcasecmp(s, "Final_Sigma"))
      flg |= UNI_SC_FL_FINAL_SIGMA;
    else if(*s) {
      fprintf(stderr, "Unknown case condition: %s\n", s);
      exit(1);
    }
  }
  add_sc(lc, fields[1]);
  add_sc(uc, fields[3]);
  /* note that this condition may be too simple */
  /* it does not account for excess ignorable whitespace */
  /* it does not check if it ends up changing stc */
  /* if(strcmp(fields[3], fields[2]) */
    add_sc(tc, fields[2]);
}
fclose(f);
@

<<Parse UCD files>>=
#define postproc_sc(n) do { \
  prop_t *prop = &parsed_props[prop_##n]; \
  qsort(prop->rng_str, prop->rng_len, sizeof(*prop->rng_str), uni_cmp_cp); \
  /* pass 1: merge */ \
  for(i = 1; i < prop->rng_len; i++) { \
    raw_cp_str_t *prev = &prop->rng_str[i - 1], *cur = &prop->rng_str[i]; \
    if(cur->cp == prev->cp) { \
      uint32_t newstr[cur->len + prev->len]; \
      memcpy(newstr, prop->strs + prev->off, prev->len * 4); \
      memcpy(newstr + prev->len, prop->strs + cur->off, cur->len * 4); \
      prev->cp = 0; \
      add_str_rng(prop, cur->cp, cur->cp, newstr, cur->len + prev->len); \
      cur->off = prop->rng_str[prop->rng_len - 1].off; \
      cur->len += prev->len; \
      prop->rng_len--; \
    } \
  } \
  /* pass 2: remove merged */ \
  /* doing it one at a time is inefficient, but simple and good enough */ \
  for(i = 0; i < prop->rng_len; i++) \
    if(!prop->rng_str[i].cp) { \
      memmove(&prop->rng_str[i], &prop->rng_str[i + 1], (prop->rng_len - i) * 4); \
      prop->rng_len--; \
      i--; \
    } \
} while(0)
postproc_sc(lc);
postproc_sc(uc);
postproc_sc(tc);
@

<<Additional parse-ucd C files>>=
btricks.h
@

<<Additional parse-ucd includes>>=
#include "btricks.h"
@

<<UCD parser local functions>>=
static void dump_str_tabs(prop_t *prop, FILE *gen_h, FILE *tstf)
{
  uint32_t i;
  char nbuf[64];
  uni_str_rng_t *short_str;
  inisize(short_str, prop->rng_len);
  dump_strs(prop, gen_h);
  /* dump_strs "un-sorts" range table */
  qsort(prop->rng_str, prop->rng_len, sizeof(*prop->rng_str), uni_cmp_cp);
  sprintf(nbuf, "uni_%s_rng.gen.c", prop->name);
  open_wf(rt, nbuf);
  fprintf(rt, "#include \"uni_prop.h\"\n\n"
              "const uni_str_rng_t uni_%s_rng[] = {\n\t", prop->name);
  for(i = 0; i < prop->rng_len; i++) {
    short_str[i].cp = prop->rng_str[i].cp;
    short_str[i].off = prop->rng_str[i].off;
    short_str[i].len = prop->rng_str[i].len;
    short_str[i].flags = prop->rng_str[i].flags;
    fprintf(rt, "{ 0x%X, %d, %d}%s", (int)short_str[i].cp, (int)short_str[i].off,
                                     (int)short_str[i].len,
				     i < prop->rng_len - 1 ? ",\n\t" : "\n};\n");
  }
  fclose(rt);
  fprintf(gen_h, "extern const uni_str_rng_t uni_%s_rng[];\n"
	         "#define uni_%s_rng_len %d /* %d lookups max */\n",
	         prop->name, prop->name, i, lg2(i + 1));
  uint32_t ml_len, *tmt = cp_val_to_multi((uni_cp_val_t *)short_str, i, &ml_len);
  print_mtab(prop->name, tmt, gen_h);
  free(tmt);
  free(short_str);
  /* for direct lookup */
  fprintf(gen_h, "#define uni_%s_of(cp) uni_x_str_of(cp, uni_%s_mtab)\n",
                 prop->name, prop->name);
  fprintf(tstf, "str(%s);\n", prop->name);
}
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_lc], gen_h, tstf);
dump_str_tabs(&parsed_props[prop_uc], gen_h, tstf);
dump_str_tabs(&parsed_props[prop_tc], gen_h, tstf);
@

While the properties were named after the final result, these tables
only provide a small part of it.  To get the final result, a helper
function is provided, along with macros to use that function.

<<Unicode property exports>>=
/* return is len; buf filled w/ up to buf_len chars */
/* simple, special are mtabs for simple/special casing */
/* strs is strings for special */
/* cond is bit flags of condtion, or: */
/* cond == ~0 -> return -2 if condition required */
/* return: -1 means no change */
/*         -2 means condition required */
/*         0+ == length of potential return */
int uni_case_convert(uint32_t cp, uint32_t *buf, uint32_t buf_len,
                     const uint32_t *simple, const uint32_t *special,
		     const uint32_t *strs, uint32_t cond);
@

<<Unicode property functions>>=
int uni_case_convert(uint32_t cp, uint32_t *buf, uint32_t buf_len,
                     const uint32_t *simple, const uint32_t *special,
		     const uint32_t *strs, uint32_t cond)
{
  const uint8_t *mr;
  multi_tab_lookup(special, cp * 4, &mr, 0);
  if(!mr) {
    multi_tab_lookup(simple, cp * 4, &mr, 0);
    if(!mr)
      return -1;
    if(buf_len > 0)
      *buf = *(uint32_t *)mr >> 8;
    return 1;
  }
  uni_str_rng_val_t *v = (void *)mr;
  const uint32_t *str = strs + v->off;
  if(!(*str & ~UNI_SC_FL_LEN_MASK) && v->len == (*str & UNI_SC_FL_LEN_MASK) + 1) {
    /* no conditions */
    if(buf_len > v->len - 1)
      buf_len = v->len - 1;
    while(buf_len > 0) {
      *buf++ = *++str;
      buf_len--;
    }
    return v->len - 1;
  }
  if(cond == (uint32_t)~0)
    return -2;
  uint32_t rem = v->len;
  const uint32_t *ncond = NULL;
  while(rem > 0) {
    if(!(*str & UNI_SC_FL_LOCALE) || (*str & UNI_SC_FL_LOCALE & cond)) {
      if(!(*str & UNI_SC_FL_CONTEXT)) {
        /* locale condition is stronger than no condition at all */
        if(!ncond || (*str & UNI_SC_FL_LOCALE))
	  ncond = str;
      } else if(!(*str & cond & UNI_SC_FL_CONTEXT) == !!(*str & UNI_SC_FL_NOT)) {
        /* condition true: return result (assumes no other cond could be true) */
	ncond = str;
	break;
      }
    }
    rem -= (*str & UNI_SC_FL_LEN_MASK) + 1;
    str += (*str & UNI_SC_FL_LEN_MASK) + 1;
  }
  if(!ncond) {
    multi_tab_lookup(simple, cp * 4, &mr, 0);
    if(!mr)
      return -1;
    if(buf_len > 0)
      *buf = *(uint32_t *)mr >> 8;
    return 1;
  }
  uint32_t len = *ncond & UNI_SC_FL_LEN_MASK;
  if(buf_len > len)
    buf_len = len;
  while(buf_len > 0) {
    *buf++ = *++ncond;
    buf_len--;
  }
  return len;
}
@

<<Unicode property exports>>=
/* note for all 3 below: if(uni_xx(cp, buf, ...) < 0) { buf_len = 1; *buf = cp } */
#define uni_lc(cp, buf, buf_len, cond) \
  uni_case_convert(cp, buf, buf_len, uni_slc_mtab, uni_lc_mtab, uni_lc_strs, \
                   cond)
#define uni_uc(cp, buf, buf_len, cond) \
  uni_case_convert(cp, buf, buf_len, uni_suc_mtab, uni_uc_mtab, uni_uc_strs, \
                   cond)
/* don't forget: if((res = uni_tc(cp, ...)) < 0) res = uni_uc(cp, ...) */
#define uni_tc(cp, buf, buf_len, cond) \
  uni_case_convert(cp, buf, buf_len, uni_stc_mtab, uni_tc_mtab, uni_tc_strs, \
                   cond)
@

<<C Prototypes>>=
int uni_lc(uint32_t cp, uint32_t *buf, uint32_t buf_len, uint32_t cond);
int uni_uc(uint32_t cp, uint32_t *buf, uint32_t buf_len, uint32_t cond);
int uni_tc(uint32_t cp, uint32_t *buf, uint32_t buf_len, uint32_t cond);
@

For more case transformation fun, there is [[CaseFolding.txt]].  It
provides four different case folding (lower-case) transformations:
common, simple, full, and Turkic special cases.  The common and simple
cases are guaranteed to always have length one.  There are few enough
full and Turkic cases that it makes sense to just encode them using
the same method as the full case conversion tables above.  Note that
the official cf property ignores the Turkic cases.

<<Initialize UCD files>>=
decl_num(scf);
decl_str(cf);
@

<<Parse UCD files>>=
open_f("CaseFolding.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  uint32_t flg;
  uint32_t scf = strtol(fields[2], NULL, 16);
  switch(fields[1][0]) {
    case 'C': /* default; value == cp */
      add_num(scf, scf, 1);
      break;
    case 'S': /* scf == C+S */
      add_num(scf, scf, 1);
      break;
    case 'F': /* cf == C+F */
      flg = 0;
      add_sc(cf, fields[2]);
      break;
    case 'T': /* tr, az lang only; not used by anything, technically */
      flg = UNI_SC_FL_AZ | UNI_SC_FL_TR;
      add_sc(cf, fields[2]);
      add_str(cf, fields[2]);
      break;
  }
}
fclose(f);
postproc_sc(cf);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_cf], gen_h, tstf);
@

Technically, any entry which is in the S class but not in the F class
should have no translation.  However, I believe that there is no such
case.  Like with the case conversions, a macro is provided for cf
property lookup.  Since there is no condition input, a special macro
is provided for Turkic locales as well.

<<Unicode property exports>>=
#define uni_cf(cp, buf, buf_len) \
  uni_case_convert(cp, buf, buf_len, uni_scf_mtab, uni_cf_mtab, uni_cf_strs, 0)
#define uni_tcf(cp, buf, buf_len) \
  uni_case_convert(cp, buf, buf_len, uni_scf_mtab, uni_cf_mtab, uni_cf_strs, \
                   UNI_SC_FL_TR)
@

<<C Prototypes>>=
int uni_cf(uint32_t cp, uint32_t *buf, uint32_t buf_len, uint32_t cond);
int uni_tcf(uint32_t cp, uint32_t *buf, uint32_t buf_len, uint32_t cond);
@

[[DerivedNormalizationProps.txt]] contains the final case folding
method.  The main difference between it and the other methods is that
it applies multiple transformations simultaneously.  In addition to
case folding, it also removes default ignorable code points and
performs compatibility decomposition on the results repeatedly until
the result is stable.

<<Initialize UCD files>>=
decl_str(NFKC_CF);
@

<<Process a line of [[DerivedNormalizationProps.txt]]>>=
if(num_fields == 3 && !strcmp(fields[1], "NFKC_CF"))
  add_str(NFKC_CF, fields[2]);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_NFKC_CF], gen_h, tstf);
@

It might also quite useful for the other case folding techniques to
include normalization.  This saves a pass in the transformation step.
There are a number of combinations:  NFC\_CF, NFC\_SCF,
NFKC\_SCF.  Whether or not any of those combinations should include
removal of default ignorable code points is difficult to tell.

<<FIXME>>=
 combine case folding and normalization here
   create string tabs NFC_CF, NFC_SCF, NFKC_SCF(?)
@

\subsection{Additional Support for Normalization}

The raw property lookup functions are not meant to be used directly. 
Instead, here are some more high-level functions.  They are stored in
their own object file, even though the actual amount of code is tiny.

<<Library [[uni]] Members>>=
uni_norm.o
@

<<Library [[uni]] headers>>=
#include "uni_norm.h"
@

<<uni_norm.h>>=
<<Common C Warning>>
#ifndef UNI_NORM_H
#define UNI_NORM_H

#include "uni_prop.h"
<<Unicode normalization support exports>>
#endif /* UNI_NORM_H */
@

<<uni_norm.c>>=
<<Common C Header>>
#include "uni_norm.h"

<<Unicode normalization support local definitions>>

<<Unicode normalization support functions>>
@

The first step in any normalization is to decompose.  The only
difference in the three functions is what table they use.

<<FIXME>>=
make these functions take a max_size and offset arg, and allow resizing
@

<<Unicode normalization support exports>>=
/* wrappers */

/* decomp functions can operate on any number of chars, including 1 */
/* decomp functions return final length */
/* remember to leave 18x blen worth of space */
int uni_NFD_dec(uint32_t *buf, int blen);
int uni_NFKD_dec(uint32_t *buf, int blen);
/* NFKC_Casefold assumes NFD has already been run */
int uni_NFKC_Casefold(uint32_t *buf, int blen);
@

<<Unicode normalization support functions>>=
#define any_dec(n, tab, str) \
  int uni_##n(uint32_t *buf, int blen) \
  { \
    int i; \
    for(i = 0; i < blen; i++) { \
      int16_t off; \
      uint8_t len; \
      uint8_t compat; \
      uni_x_dec(buf[i], tab, &off, &len, &compat, 1, str); \
      if(len) { \
        memmove(buf + i + len, buf + i + 1, (blen - i - 1)*sizeof(int)); \
        blen += len - 1; \
	if(off == -1) \
	  hangul_syllable_decomp(buf[i], buf + i, 1); \
	else \
	  memcpy(buf + i, str + off, len * sizeof(uint32_t)); \
        /* result is fully expanded, so skip it */ \
        i += len - 1; \
      } \
    } \
    return blen; \
  }
any_dec(NFKD_dec, uni_compat_decomp_mtab, uni_dm_full_strs)
any_dec(NFD_dec, uni_canon_decomp_mtab, uni_dm_full_strs)
any_dec(NFKC_Casefold, uni_NFKC_CF_mtab, uni_NFKC_CF_strs)
@

Canonical ordering is the second step in any normalization.  This
requires that any consecutive characters with non-zero canonical
combining class be ordered by their canonial combining class.  The
procedure is described as a bubble sort, so the sort must be stable.
If the last character passed in has a non-zero ccc, there may be more,
so a continuation is requested.

<<Unicode normalization support exports>>=
/* canon order function may need more chars than passed in */
/* set last to 1 if no continuations possible */
/* returns less than blen if more chars needed */
/* can skip # of chars equal to return value */
int uni_Canon_Order(uint32_t *buf, int blen, int last);
@

<<Unicode normalization support local definitions>>=
/* uni_ccc_of */
#include "uni_prop.h"

/* for stable sort */
struct ccs {
    int cp, ccc;
    int opos;
};

static int cmpcc(const void *a, const void *b)
{
    const struct ccs *p1 = (const struct ccs *)a;
    const struct ccs *p2 = (const struct ccs *)b;
    if(p1->ccc != p2->ccc)
        return p1->ccc - p2->ccc;
    return p1->opos - p2->opos;
}
@

<<Unicode normalization support functions>>=
/* pass in last=1 if string is whole */
/* otherwise, if return is < blen, needs more chars */
/* but can safely skip return-val chars */
int uni_Canon_Order(uint32_t *buf, int blen, int last)
{
    int i;
    for(i = 0; i < blen; i++) {
        int cc = uni_ccc_of(buf[i]);
        if(cc) {
            int j;
            for(j = i + 1; j < blen; j++) {
                int cc2 = uni_ccc_of(buf[j]);
                if(!cc2)
                    break;
            }
            if(j == blen && !last)
                return i;
            struct ccs ccbuf[j-i];
            ccbuf[0].ccc = cc;
            ccbuf[0].cp = buf[i];
            ccbuf[0].opos = i;
            int k;
            for(k = i + 1; k < j; k++) {
                ccbuf[k-i].ccc = uni_ccc_of(buf[k]);
                ccbuf[k-i].cp = buf[k];
                ccbuf[k-i].opos = k;
            }
            qsort(ccbuf, j - i, sizeof(*ccbuf), cmpcc);
            for(k = i; k < j; k++)
                buf[k] = ccbuf[k-i].cp;
        }
    }
    return blen;
}
@

The last step in any composition normalization is canonical
composition.  Two characers may be combined if the first character has
a canonical combining class of zero and all intervening characters
have a non-zero canonical combining class less than the canonical
combining class of the second character.

<<Unicode normalization support exports>>=
/* canon comp function may need more chars than passed in */
/* set nok to non-NULL if continuation possible */
/* returns nok < blen if more chars needed */
/* can skip nok chars */
/* return value is updated length */
int uni_NFC_comp(uint32_t *buf, int blen, int *nok);
@

<<Unicode normalization support functions>>=
int uni_NFC_comp(uint32_t *buf, int blen, int *nok)
{
  int i, last0 = -1;
  for(i = 0; i < blen; i++) {
    int ccc = uni_ccc_of(buf[i]);
    /* first char must be ccc=0 */
    if(ccc)
      continue;
    int16_t off;
    uint8_t len;
    uni_find_canon_comp(buf[i], &off, &len);
    if(!len && !off)
      continue; /* no compositions for this char */
    last0 = i;
    int lastccc = 0;
    int j;
    for(j = i + 1; j < blen; j++) {
      ccc = uni_ccc_of(buf[j]);
      /* second char must have no intervening equal ccc */
      /* this assumes canonical ordering has been done */
      if(ccc && lastccc && ccc == lastccc)
        continue;
      /* or greater ccc; this only happens if back down to 0 */
      if(!ccc && lastccc) {
        i = j - 1;
        break;
      }
      lastccc = ccc;
      int ccp = uni_canon_comp(buf[i], buf[j], off, len);
      if(ccp > 0) {
        /* composition replaces char 1 and deletes char 2 */
        buf[i] = ccp;
        memmove(buf + j, buf + j + 1, (blen - j - 1)*sizeof(*buf));
        blen--;
	/* start over at first potential 2nd char (i + 1) */
        j = i;
        lastccc = 0;
	/* new 1st char; update table */
	uni_find_canon_comp(buf[i], &off, &len);
	if(!len && !off)
	  break; /* no compositions for this char */
        continue;
      }
      /* if no match and hit a ccc=0, try next 1st char */
      if(!ccc) {
        i = j - 1;
        break;
      }
    }
  }
  if(nok) {
    /* need more if we got a potential start char */
    /* but don't need to look at anything before that char */
    if(last0 < 0)
      *nok = blen;
    else {
      int16_t off;
      uint8_t len;
      uni_find_canon_comp(buf[last0], &off, &len);
      *nok = len || off ? last0 : blen;
    }
  }
  return blen;
}
@

To demonstrate proper normalization, here are some macros which do the
full procedure.

<<C Prototypes>>=
int uni_NFKD(uint32_t *buf, int blen);
int uni_NFD(uint32_t *buf, int blen);
int uni_NFKC(uint32_t *buf, int blen);
int uni_NFC(uint32_t *buf, int blen);
int uni_NFKC_CF(uint32_t *buf, int blen);
@

<<Unicode normalization support exports>>=
#define uni_NFKD(buf, blen) uni_Canon_Order(buf, uni_NFKD_dec(buf, blen), 1)
#define uni_NFD(buf, blen) uni_Canon_Order(buf, uni_NFD_dec(buf, blen), 1)
#define uni_NFKC(buf, blen) uni_NFC_comp(buf, uni_NFKD(buf, blen), NULL)
#define uni_NFC(buf, blen) uni_NFC_comp(buf, uni_NFD(buf, blen), NULL)
#define uni_NFKC_CF(buf, blen) uni_NFKC_Casefold(buf, uni_NFD(buf, blen))
@

\subsection{Testing - Normalization}

To fully demonstrate that the normalization data is correct, the
official test suite is run.  This is provided by the UCD in
NormalizationTest.txt.  This time, rather than compiling the file's
location in, it is simply fed into standard input.

\lstset{language=make}
<<C Test Support Executables>>=
tstnorm \
@

<<Additional Tests>>=
./tstnorm <$(UCD_LOC)/NormalizationTest.txt
@

\lstset{language=C}
<<tstnorm.c>>=
<<Common C Header>>
#include "uni_all.h"

/* longest line == 587 chars */
char lbuf[1024];

/* longest string == 18 chars */
uint32_t ibuf[5][128], ilen[5];
uint32_t obuf[128];

int main(void)
{
  int ret = 0;
  <<Read and process NormalizationTest.txt>>
  return ret;
}
@

First, we need to track when we're in part 1, and mark every code
point encountered there as having been processed.  According to the
test, after exiting part 1, we can run all four standard
normalizations on any code point not encountered and get no effect.

In general, when entering a new section, print the section name.
Otherwise, print a dot after every 50 tests.

<<Read and process NormalizationTest.txt>>=
char *didnorm;
inisize(didnorm, 0x110000);
clearbuf(didnorm, 0x110000);
int inpart1 = 0;
int ntests = 0;
while(fgets(lbuf, sizeof(lbuf), stdin)) {
  if(*lbuf == '#')
    continue;
  if(*lbuf == '@') {
    if(ntests)
      putchar('\n');
    if(lbuf[5] == '1')
      inpart1 = 1;
    else if(inpart1) {
      inpart1 = 0;
      int i;
      for(i = 0; i < 0x110000; i++)
        if(!didnorm[i]) {
	  <<Test normalization does not affect [[i]]>>
	}
    }
    fputs(lbuf, stdout);
    continue;
  }
  if(inpart1) {
    int cp = strtol(lbuf, NULL, 16);
    didnorm[cp] = 1;
  }
  <<Run normalization test for [[lbuf]]>>
  if(!(++ntests % 50)) {
    putchar('.');
    fflush(stdout);
  }
}
putchar('\n');
@

First, let's parse a line into the input buffers.  Each line has five
semicolon-separated fields, terminated by a semicolon.  Each field has
space-separated hexadecimal numbers.

<<Run normalization test for [[lbuf]]>>=
int i;
char *s = lbuf;
for(i = 0; i < 5; i++) {
  int l = 0;
  while(1) {
    ibuf[i][l++] = strtol(s, &s, 16);
    while(isspace(*s))
      s++;
    if(*s == ';')
      break;
  }
  ilen[i] = l;
  if(*s == ';')
    s++;
  else {
    fprintf(stderr, "bad line %s\n", lbuf);
    exit(1);
  }
}
@

Then, run the conformance tests.

<<Run normalization test for [[lbuf]]>>=
/* fields i0 through il normalize to field r using function t */
#define dotst(t, i0, il, r) do { \
  for(i = i0; i < il; i++) { \
    memcpy(obuf, ibuf[i], ilen[i] * sizeof(obuf[0])); \
    int l = t(obuf, ilen[i]); \
    if(l != ilen[r] || memcmp(obuf, ibuf[r], l * sizeof(obuf[0]))) { \
      ret = 1; \
      fprintf(stderr, "Failed "#t" test %d/%d on %s\nGot: ", r + 1, i + 1, lbuf); \
      int j; \
      for(j = 0; j < l; j++) \
        fprintf(stderr, " %04X", obuf[j]); \
      fputs("\nExpected: ", stderr); \
      for(j = 0; j < ilen[r]; j++) \
        fprintf(stderr, " %04X", ibuf[r][j]); \
      fputc('\n', stderr); \
      continue; \
    } \
  } \
} while(0)
@

<<Run normalization test for [[lbuf]]>>=
dotst(uni_NFC, 0, 3, 1);
dotst(uni_NFC, 3, 5, 3);
@

<<Run normalization test for [[lbuf]]>>=
dotst(uni_NFD, 0, 3, 2);
dotst(uni_NFD, 3, 5, 4);
@

<<Run normalization test for [[lbuf]]>>=
dotst(uni_NFKC, 0, 5, 3);
@

<<Run normalization test for [[lbuf]]>>=
dotst(uni_NFKD, 0, 5, 4);
@

<<Test normalization does not affect [[i]]>>=
obuf[0] = i;
if(uni_NFC(obuf, 1) != 1 || obuf[0] != i ||
   uni_NFD(obuf, 1) != 1 || obuf[0] != i ||
   uni_NFKC(obuf, 1) != 1 || obuf[0] != i ||
   uni_NFKD(obuf, 1) != 1 || obuf[0] != i) {
  fprintf(stderr, "Failed Part1 end check at %04x\n", i);
  ret = 1;
}
@

\subsection{Parsing the UCD - Others}

[[BidiMirroring.txt]] contains the last officially supported string
property.

<<Initialize UCD files>>=
decl_str(bmg);
@

<<Parse UCD files>>=
open_f("BidiMirroring.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_str(bmg, fields[1]);
}
fclose(f);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_bmg], gen_h, tstf);
@

While scx is officially an ``Other'' property, it can be treated just
like a string.  The difference is that the string elements are script
enumeration values, rather than code points.

<<Initialize UCD files>>=
decl_str(scx);
@

<<Parse UCD files>>=
open_f("ScriptExtensions.txt");
prop_scx = add_prop("scx");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  uint32_t str[32], len = 0;
  <<Parse dotted cp>>
  s = fields[1];
  while(1) {
    char *n = strchr(s, ' ');
    if(n)
      *n = 0;
    str[len++] = enum_val(prop_sc, s);
    if(!n)
      break;
    s = n + 1;
  }
  if(prop_scx < 0)
    prop_scx = add_prop("scx");
  add_str_rng(&parsed_props[prop_scx], low, high, str, len);
}
fclose(f);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_scx], gen_h, tstf);
@

Another ``Other'' property which can be treated as a string for now is
the na property.  This includes the Name\_Alias property, as well.
However, since this property is large and has some special cases,
dumping it will be handled in a later section.

<<UCD parser local definitions>>=
#define add_str_raw(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[64]; \
    uint32_t len = strlen(v); \
    str[len / 4] = 0; \
    memcpy(str, v, len); \
    len = (len + 3) / 4; \
    add_str_rng(&parsed_props[prop_##n], low, high, str, len); \
  } \
} while(0)
@

<<Initialize UCD files>>=
decl_str(na);
decl_str(Name_Alias);
@

<<Process a line of [[UnicodeData.txt]]>>=
if(fields[1][0] != '<')
  add_str_raw(na, fields[1]);
@

<<Parse UCD files>>=
open_f("NameAliases.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_str_raw(Name_Alias, fields[1]);
}
fclose(f);
@

<<Parse character data files>>=
/* for testing to see if read properly */
#if 0
str_to_enum(&parsed_props[prop_na], NULL);
str_to_enum(&parsed_props[prop_Name_Alias], NULL);
#endif
@

<<FIXME>>=
dump names & name aliases
add name lookup function
name (UAX44-LM2):
  ignore case, whitespace, underscore, and medial hyphens except 1180
name (UTS18-2.5.1):
  ignore case, whitespace, underscore, and all hyphens except 1180,
     0F60, 0FB0
  allow "namespace": prefix to apply if name doesn't match otherwise
    e.g. "LATIN LETTER".  Not sure how useful, but could make rules to
      insert CAPITAL and LOWER into appropriate spot.
@

Unofficially, [[Unihan_Variants]] provides some properties as well.
Their fields are formatted differently, with U+ prefixes on every code
point.  Of these properties, only cjkCompatibilityVariant has an
official property name entry.

<<UCD parser local definitions>>=
#define add_hstr(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[10]; \
    uint32_t len; \
    for(s = v, len = 0; *s; len++) { \
      while(isspace(*s)) s++; \
      if(*s == 'U') \
        s += 2; \
      str[len] = strtol(s, &s, 16); \
    } \
    add_str_rng(&parsed_props[prop_##n], low, high, str, len); \
  } \
} while(0)
@

<<Initialize Unihan files>>=
decl_str(cjkTraditionalVariant);
decl_str(cjkSimplifiedVariant);
decl_str(cjkCompatibilityVariant);
@

<<Parse Unihan files>>=
open_f("Unihan_Variants.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse Unihan cp>>
  if(!strcmp(fields[1], "kTraditionalVariant"))
    add_hstr(cjkTraditionalVariant, fields[2]);
  else if(!strcmp(fields[1], "kSimplifiedVariant"))
    add_hstr(cjkSimplifiedVariant, fields[2]);
  else if(!strcmp(fields[1], "kCompatibilityVariant"))
    add_hstr(cjkCompatibilityVariant, fields[2]);
}
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_cjkTraditionalVariant], gen_h, tstf);
dump_str_tabs(&parsed_props[prop_cjkSimplifiedVariant], gen_h, tstf);
dump_str_tabs(&parsed_props[prop_cjkCompatibilityVariant], gen_h, tstf);
@

Another optional string value comes from the IDNA compatiblity
database.  This is available from a separate location
(\url{http://www.unicode.org/Public/idna/}), but is only checked for
in the UCD directory and a few minor variants.  If not present, it is
ignored.  The IDNA\_Status property is technically an enumeration, but
since it has no value aliases, it is instead accumulated as a string
property.  It is then converted to an enumeration.

<<Initialize UCD files>>=
decl_str(IDNA_Status);
decl_str(IDNA_Mapping);
@

<<Parse UCD files>>=
if((f = fopen("IdnaMappingTable.txt", "r")) ||
   (f = fopen("idna/IdnaMappingTable.txt", "r")) ||
   (f = fopen("../idna/IdnaMappingTable.txt", "r"))) {
  while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    <<Parse dotted cp>>
    add_str_raw(IDNA_Status, fields[1]);
    if(num_fields > 2)
      add_str(IDNA_Mapping, fields[2]);
  }
  fclose(f);
  str_to_enum(&parsed_props[prop_IDNA_Status], "disallowed");
}
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_IDNA_Mapping], gen_h, tstf);
@

<<UCD parser local functions>>=
static void str_to_enum(prop_t *p, const char *def)
{
  uint32_t i, poff = 0, plen = 0, pnum = (uint32_t)(p - parsed_props);
  int32_t seq = -1;

  merge_strs(p);
  p->def = def ? ~0 : 0;
  inisize(val_aliases[pnum], max_val_aliases[pnum] = 5);
  inisize(p->rng_dat, p->rng_len);
  for(i = 0; i < p->rng_len; i++) {
    if(p->rng_str[i].off == poff && p->rng_str[i].len == plen) {
      p->rng_dat[i].low = p->rng_dat[i].high = p->rng_str[i].cp;
      p->rng_dat[i].dat = seq;
      continue;
    }
    seq++;
    poff = p->rng_str[i].off;
    plen = p->rng_str[i].len;
    if(num_val_aliases[pnum] == max_val_aliases[pnum])
      resize(val_aliases[pnum], max_val_aliases[pnum] *= 2);
    {
      uni_alias_t *va = &val_aliases[pnum][num_val_aliases[pnum]];
      char *n;
      inisize(n, plen * 4 + 1);
      memcpy(n, (char *)(p->strs + poff), plen * 4);
      n[plen * 4] = 0;
      if(def && !strcmp(n, def))
        p->def = seq;
      va->short_name = va->long_name = n;
      va->alt_name = va->alt_name2 = NULL;
      num_val_aliases[pnum]++;
    }
    i--; /* re-run for current item to actually add it */
  }
  if(p->def == (uint8_t)~0) {
    p->def = seq;
    if(num_val_aliases[pnum] == max_val_aliases[pnum])
      resize(val_aliases[pnum], max_val_aliases[pnum] *= 2);
    {
      uni_alias_t *va = &val_aliases[pnum][num_val_aliases[pnum]];
      va->short_name = va->long_name = strdup(def);
      va->alt_name = va->alt_name2 = NULL;
      num_val_aliases[pnum]++;
    }
  }
  inisize(enum_vals[pnum], enum_vals_len[pnum] = num_val_aliases[pnum]);
  for(i = 0; i < enum_vals_len[pnum]; i++) {
    enum_vals[pnum][i].name = strdup(val_aliases[pnum][i].short_name);
    enum_vals[pnum][i].val = i;
  }
  free(p->strs);
  p->strs = NULL;
  free(p->rng_str);
  p->rng_str = NULL;
}
@

Similarly, the security information database, available separately at
\url{http://www.unicode.org/Public/security/}, contains two optional
enumeration properties without value aliases.

<<Initialize UCD files>>=
decl_str(ID_Restrict_Status);
decl_str(ID_Restrict_Type);
@

<<Parse UCD files>>=
if((f = fopen("xidmodifications.txt", "r")) ||
   (f = fopen("security/xidmodifications.txt", "r")) ||
   (f = fopen("../security/xidmodifications.txt", "r"))) {
  while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    <<Parse dotted cp>>
    add_str_raw(ID_Restrict_Status, fields[1]);
    if(num_fields > 2)
      add_str_raw(ID_Restrict_Type, fields[2]);
  }
  fclose(f);
  str_to_enum(&parsed_props[prop_ID_Restrict_Status], "restricted");
  str_to_enum(&parsed_props[prop_ID_Restrict_Type], "not-chars");
}
@

\subsection{Parsing the UCD - DUCET}

The most complex optional string value is derived from the Default
Unicode Collation Element Table
(\url{http://unicode.org/Public/UCA/}).  It associates many strings
(collation elements, which are the index to the table) with explicit
sort keys (the data).  This includes several multi-character collation
elements (called contractions) and multi-entry keys (called
expansions).  Each sort key entry consists of a number for each
supported sort level, plus a flag.  Currently, the DUCET supports four
sort levels, although the standard states that more may be added in a
future version.  The flag indicates so-called variable entries.  The
flag does not actually need to be stored, since it is always true for
entries with a non-zero first level up to a certain top.  This top is
stored separately as a single integer.

Rather than make this reader as generic as possible, given the
definitions, it is tailored to the actual data present in the UCA
versions available at the time of this writing.  That means that
recoding may be necessary at a future date.  The first constraint is
that exactly four levels are supported.

While reading, each entry is indexed on the first character of the
collation element.  The raw string value is a four-word sequence for
every sort key entry, followed by the remainder of the collation
element index.  Since no multi-character index has more than four
characters, there is no problem distinguishing the character groups in
the value: the number of key entries is the length divided by four,
and the number of extra characters in the index is the remainder.
Since several multi-character collation elements start with the same
character (and in fact the well-formedness rules require this),
multiple entries with the same [[cp]] are created, but the raw string
table storage method does not care.

In addition to the DUCET itself, the data files include the same data
adjusted for use with the CLDR (\texttt{allkeys\_CLDR.txt} in
\texttt{CollationAuxiliary.zip}).  This file is in the same format. 
It should be unzipped under the same directory.  All CLDR data is based
off of this root locale, and it can be explicitly requested using
\texttt{u-co-standard}.  The CLDR still requires the normal DUCET; it
can be selected manually using \texttt{u-co-ducet}.

<<Initialize UCD files>>=
decl_str(DUCET);
decl_str(DUCET_CLDR);
@

<<Parse UCD files>>=
<<Prepare for reading DUCET>>
static uint32_t ducet_vartop, cldr_vartop;
ducet_vartop = parse_ducet("allkeys.txt", (prop_DUCET = add_prop("DUCET")));
cldr_vartop = parse_ducet("CollationAuxiliary/allkeys_CLDR.txt",
                          (prop_DUCET_CLDR = add_prop("DUCET_CLDR")));
@

<<UCD parser local functions>>=
<<DUCET parser globals>>
static uint32_t parse_ducet(const char *fname, int propn)
{
  <<Parser common variables>>
  prop_t *prop = &parsed_props[propn];
  uint32_t vartop = 0;
  open_f(fname);
  while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    uint32_t str[20*4]; /* current max: 18 * 4 */
    uint32_t len = 0;
    int is_var;

    split_line(lbuf);
    if(!num_fields || !isxdigit(*lbuf))
      continue;
    low = high = strtol(lbuf, &s, 16);
    for(s = fields[1]; s; s = strchr(s, '[')) {
      is_var = s[1] == '*';
      str[len++] = strtol(s + 2, &s, 16);
      if(is_var && str[len - 1] > vartop)
        vartop = str[len - 1];
      str[len++] = strtol(s + 1, &s, 16);
      str[len++] = strtol(s + 1, &s, 16);
      str[len++] = strtol(s + 1, &s, 16);
    }
    /* this assumes that fields[0] will never have more than 4 chars */
    /* that way, len % 4 == extra chars in fields[0]; len / 4 = # of keys */
    if((s = strchr(fields[0], ' '))) {
      do {
        str[len++] = strtol(s, &s, 16);
      } while(*s);
    }
    /* 6.2: 120007 words (6212 saved) */
    /* 6.2-CLDR: 120493 words (5724 saved) */
    add_str_rng(prop, low, high, str, len);
  }
  fclose(f);
  <<Post-process DUCET>>
  return vartop;
}
@

The raw DUCET data is actually of limited use.  Only one optional
poorly defined pseudo-property depends on it (``DUCET primary values''
--- even the property name is undefined).  The Unicode Collation
Algorithm is defined in terms of creating string sort keys from the
raw data as well, but in order to support modifications due to locale,
even that use is probably better served by a different representation.
However, the raw data is provided anyway in case there is a use I have
yet to envision.

The raw data in the Unicode 6.2 DUCET comes to 126,219 words, of which
6,212 can be removed due to redundancy.  This is too much data for the
16-bit offset used by [[uni_str_rng_t]] type, so some adjustment needs
to be made.  Before making adjustments, though, the original raw data
is copied so that it can be converted into a more useful format.

<<Post-process DUCET>>=
uint32_t *strs;
raw_cp_str_t *ents;
inisize(strs, prop->strs_len);
inisize(ents, prop->rng_len);
memcpy(strs, prop->strs, prop->strs_len * sizeof(*strs));
memcpy(ents, prop->rng_str, prop->rng_len * sizeof(*ents));
@

A little space can be saved while storing this by making some strings
identical when they would normally not be, thereby increasing the
removals due to redundancy. Level 4 is usually equal to the code
point, and is never 1, so that case can be stored as 1 to indicate
that it is equal to the code point.  This saves an additional 6,972
words.

<<Post-process DUCET>>=
<<Prepare for DUCET post-processing>>
for(i = 0; i < prop->rng_len; i++) {
  uint32_t len = prop->rng_str[i].len;
  uint32_t *str = prop->strs + prop->rng_str[i].off;
  uint32_t cp = prop->rng_str[i].cp;
  <<Reduce DUCET entry>>
}
@

<<Reduce DUCET entry>>=
for(j = 3; j < len; j += 4)
  if(str[j] == cp)
    str[j] = 1;
/* 6.2: 113035 words (13184 saved) */
/* 6.2-CLDR: 112169 words (14048 saved) */
@

One common expansion is for the sort key to be the concatenation of
the sort keys for each character in the canonical decomposition of the
collation element.  To save more space, this sort key string is
indicated by an empty sort key.  For sanity, the entire key is
checked, even though the current DUCET has no entries which fail the
complete check when just the last element matches the decomposition
characters.  This brings the total down to 117,631 words, with 11,800
removed due to redundancy.  In other words, it saves an additional
7,204 words.

<<Prepare for reading DUCET>>=
/* sort dmf for decomp lookup */
qsort(dmf_prop->rng_str, dmf_prop->rng_len, sizeof(*dmf_prop->rng_str), cmp_cp_flg);
@

<<Prepare for DUCET post-processing>>=
prop_t *dmf_prop = &parsed_props[add_prop("dm_full")];
/* sort ducet for component lookup */
qsort(prop->rng_str, prop->rng_len, sizeof(*prop->rng_str), uni_cmp_cp);
@

<<Reduce DUCET entry>>=
/* if it's decomposed first, then store that case as 0-len string */
if(str[3] && str[3] != 1 && !(len % 4) && len > 4) {
  raw_cp_str_t *dec = bsearch(&cp, dmf_prop->rng_str,
                              dmf_prop->rng_len,
			      sizeof(raw_cp_str_t), uni_cmp_cp);
  if(dec) {
    while(dec->flags && dec > dmf_prop->rng_str && dec->cp == cp)
      dec--; /* select canonical full decomp only */
    if(dec->flags || dec->cp != cp)
      dec = NULL;
  }
  if(dec && dec->len * 4 == len) {
    for(j = 0; j < dec->len; j++) {
      raw_cp_str_t *cd;
      uint32_t ocp = str[3 + 4 * j];
      if(ocp != dmf_prop->strs[dec->off + j])
        break;
      cd = bsearch(&ocp, prop->rng_str,
	           prop->rng_len, sizeof(raw_cp_str_t),
		   uni_cmp_cp);
      if(!cd)
        break;
      /* scan for single-element entry */
      /* some may have ocp as 1st char of multi-char entry */
      while(cd->len != 4 && cd > prop->rng_str && cd[-1].cp == ocp)
        cd--;
      while(cd->len != 4 && cd < prop->rng_str + prop->rng_len - 1 &&
            cd[1].cp == ocp)
	cd++;
      if(cd->len != 4 ||
	 (prop->strs[cd->off + 3] != 1 && prop->strs[cd->off + 3] != ocp) ||
	 memcmp(prop->strs + cd->off, str + 4 * j, 12))
	break;
    }
    if(j == dec->len)
      prop->rng_str[i].len = 0;
  }
}
/* 6.2: 105831 words (11800 saved) */
/* 6.2-CLDR: 104937 words (12688 saved) */
@

Similarly, some multi-character collation elements are the canonical
decomposition of a single character, and the sort key is the sort key
for that character.  That case is also encoded by a zero-length key,
but retains the extra collation element characters as its string. 
This brings the total down to 117,297 words, with 11,881 removed due
to redundancy.  In other words, it saves an additional 415 words.

<<Prepare for DUCET post-processing>>=
prop_t *cm_prop = &parsed_props[add_prop("canon_comp")];
@

<<Reduce DUCET entry>>=
if(len >=5 && len <= 6) { /* support 2 or 3 chars */
  raw_cp_str_t *btab = bsearch(&cp, cm_prop->rng_str, cm_prop->rng_len,
                               sizeof(*cm_prop->rng_str), uni_cmp_cp);
  if(btab && btab->len) { /* ignore missing and hangul */
    cp = uni_lookup_compent(str[4], cm_prop->strs + btab->off, btab->len);
    if(len == 6) {
      btab = bsearch(&cp, cm_prop->rng_str, cm_prop->rng_len,
                     sizeof(*cm_prop->rng_str), uni_cmp_cp);
      if(btab && btab->len)
        cp = uni_lookup_compent(str[5], cm_prop->strs + btab->off, btab->len);
    }
    if(cp == str[3]) {
      prop->rng_str[i].len -= 4;
      prop->rng_str[i].off += 4;
    }
  }
}
/* 6.2: 105416 words (11881 saved) */
/* 6.2-CLDR: 104937 words (12688 saved) */
@

While there is a pattern to most of the rest of the DUCET, it is
difficult to encode, and encoding would make lookup much more
expensive than it probably already is.  The only other features of the
numbers which can be easily taken advantage of are the ranges.  The
first three levels are never more than four digits, or 16 bits.  In
fact, they are quite a bit smaller.  Rather than relying on a single
revision's ranges, the ranges are extracted in the first pass.  This
is mainly to provide a sanity check on the only characteristic that is
likely to change rapidly.

<<Prepare for DUCET post-processing>>=
uint32_t max_0 = 0, max_1 = 0, max_2 = 0;
@

<<Reduce DUCET entry>>=
for(j = 0; j + 3 < len; j += 4) {
  if(str[j] > max_0)
    max_0 = str[j];
  if(str[j + 1] > max_1)
    max_1 = str[j + 1];
  if(str[j + 2] > max_2)
    max_2 = str[j + 2];
}
@

In practice, the first level can be stored in 16 bits, the second in 9
bits, and the third in 5 bits.  In other words, all three levels fit
easily in a single 32-bit word.  While the second and third could be
stored in more bits to allow for more wiggle room, storing minimally
leaves two extra bits for other purposes.  Note that in the current
DUCET, the first level can actually be restricted to 15 bits in case
an additional bit is ever needed.  The default value algorithm
requires the full 16 bits, but these are never stored in the table.
No allowance is made for larger required field sizes; recoding would
be needed in several places.

<<Post-process DUCET>>=
if(lg2(max_0) > 16 || lg2(max_1) > 9 || lg2(max_2) > 5) {
  fputs("FIXME: Can't reduce DUCET\n", stderr);
  exit(1);
}
@

The extra two bits can be used to encode certain common cases,
possibly eliminating the need to store a full 32-bit word for the
fourth level:

\begin{itemize}
\item 0: Level 4 is 0
\item 1: Level 4 is the cp itself (stored above as 1)
\item 2: The next word is level 4
\end{itemize}

When combining like this, each entry drops in length to 2 or fewer
words.  This means that if there is more than two characters in the
index string, it can no longer be detected by length alone.  Since
every character can be encoded in fewer than 30 bits, the extra index
characters are simply shifted left two, leaving the lower two bits for
special encoding as above:

\begin{itemize}
\item 3: This word is actually part of the collation element (index)
\end{itemize}

This also means that locating the extra index characters is more
difficult:  the entire string would need to be scanned rather than
just computing the length of the key and skipping it.  To prevent the
need for scanning, the extra index characters are moved to the front
of the string as well.

For consistency, the explicitly stored level 4 values are also shifted
left by two.  This way, the only characters in the string with the
lower two bits set are the index characters.

After collapsing the keys like this, over two thirds of the required
space is removed.  The result is 33,723 words, with 3,308 words
removed due to redundancy.  This saves an additional 75,416 words.

<<Post-process DUCET>>=
for(i = 0; i < prop->rng_len; i++) {
  int k;
  uint32_t len = prop->rng_str[i].len;
  uint32_t extra[3], extra_len = len % 4;
  uint32_t *str = prop->strs + prop->rng_str[i].off;
  for(j = 0; j < extra_len; j++)
    extra[j] = str[len - extra_len + j];
  for(j = 0, k = extra_len; j + 3 < len; j += 4, k++) {
    str[k] = (str[j] << 16) + (str[j + 1] << 7) + (str[j + 2] << 2);
    if(str[j + 3] == 1)
      str[k] |= 1;
    else if(str[j + 3]) {
      str[k] |= 2;
      str[++k] = str[j + 3] << 2;
    }
  }
  prop->rng_str[i].len = k;
  for(k = 0; j < len; j++, k++)
    str[k] = (extra[k] << 2) | 3;
}
/* 6.2: 30415 words (3308 saved) */
/* 6.2-CLDR: 29489 words (3157 saved) */
@

The final problem to deal with is the possibility of multiple sort
keys with the same starting index character.  Because of this, the
string table cannot be converted to a multi-level table.  After the
last encoding, though, it is easy to simply concatenate all entries
into a single string.  Searching for entries after the first is not
too efficient, but at least it only needs to be done for relatively
few first index characters.  The encoding allows limited binary
searching, as well: the nearest index extension can be located by
checking the lower two bits for 3.

The only possible conflicts are the zero-length canonical composition
and decomposition entries.  Two adjacent entries with zero-length keys
look like one long index string extension.  The entry with no index
extension is always first, so detecting zero-length canonical
decomposition entries is easy: if the first character is an index
extension or the end of the string, then the key is blank. Similarly,
if the very last index extension has a blank key, it is obvious.
However, blank keys in the middle need an explicit separator between
the consecutive index extensions.  A zero can never be part of an
index extension, so a blank key is indicated by adding an index
character of zero.

<<Post-process DUCET>>=
ducet_strs = prop->strs; /* for sort & abbrev */
for(i = prop->rng_len - 1; i > 0; i--) {
  uint32_t cp = prop->rng_str[i].cp;
  for(j = i; j > 0 && prop->rng_str[j - 1].cp == cp; j--);
  if(i == j)
    continue;
  /* sort by index extension */
  low = j;
  qsort(prop->rng_str + low, i - low + 1, sizeof(*prop->rng_str), cmp_ducet_idx);
  if(prop->rng_str[low].len && (ducet_strs[prop->rng_str[low].off] & 3) == 3) {
    fprintf(stderr, "Error: no DUCET entry for %04X\n", (int)cp);
    exit(1);
  }
  uint32_t len = 0;
  for(j = low; j <= i; j++) {
    len += prop->rng_str[j].len;
    if(j > low && j < i && /* in the middle, */
       /* if last char is index char, key is blank */
       (ducet_strs[prop->rng_str[j].off + prop->rng_str[j].len - 1] & 3) == 3)
      len++; /* so a zero needs to be added */
  }
  while(prop->max_strs < prop->strs_len + len) {
    resize(prop->strs, (prop->max_strs *= 2));
    ducet_strs = prop->strs; /* for sort & abbrev */
  }
  memcpy(ducet_strs + prop->strs_len,
         ducet_strs + prop->rng_str[low].off,
	 prop->rng_str[low].len * 4);
  prop->rng_str[low].off = prop->strs_len;
  prop->strs_len += prop->rng_str[low].len;
  prop->rng_str[low].len = len;
  for(j = low + 1; j <= i; j++) {
    uint32_t *str = ducet_strs + prop->rng_str[j].off;
    len = prop->rng_str[j].len;
    memcpy(ducet_strs + prop->strs_len, str, len * 4);
    prop->strs_len += len;
    if(j < i && (str[len - 1] & 3) == 3)
      ducet_strs[prop->strs_len++] = 3; /* zero index ext. separator */
  }
  memmove(prop->rng_str + low + 1, prop->rng_str + i + 1,
          (prop->rng_len - i - 1) * sizeof(*prop->rng_str));
  prop->rng_len -= i - low;
  if(!(i = low))
    break;
}
/* 6.2: 30498 words (3250 saved) */
/* 6.2-CLDR: 29491 words (3155 saved) */
@

<<DUCET parser globals>>=
static uint32_t *ducet_strs;
static int cmp_ducet_idx(const void *a, const void *b)
{
  const raw_cp_str_t *_a = a, *_b = b;
  uint32_t *stra = ducet_strs + _a->off, *strb = ducet_strs + _b->off;
  uint32_t lena = _a->len, lenb = _b->len;

  /* assume all indices are unique, so equality will never be returned */
  while(lena > 0 && lenb > 0) {
    if((*stra & 3) != 3)
      return -1;
    if((*strb & 3) != 3)
      return 1;
    if(*stra != *strb)
      return *stra > *strb ? 1 : -1;
    lena--;
    lenb--;
    stra++;
    strb++;
  }
  return lena ? 1 : -1;
}
@

Unsurprisingly, adding a few zeroes here and there increases the size
a bit, but not much (25 words).  Combining entires also reduces
redundancy a bit (gaining 58 words).  Thus the original 126,219 words
are reduced to 30,498 words.  This allows use of 16-bit offsets, as
supported by the [[uni_str_rng_t]] type.  Note that this is just the
string table; the lookup tables add additional overhead (about 200kb
for the code point table and 100kb for the multi-level table).
Finally, the raw DUCET is ready to be dumped.  The collected default
variable top is dumped to the header as well.

<<Dump character information as C code>>=
fprintf(gen_h, "#define uni_DUCET_var_top %d\n", (int)ducet_vartop);
dump_str_tabs(&parsed_props[prop_DUCET], gen_h, tstf);
fprintf(gen_h, "#define uni_DUCET_CLDR_var_top %d\n", (int)cldr_vartop);
dump_str_tabs(&parsed_props[prop_DUCET_CLDR], gen_h, tstf);
@

In order to compensate for some of the compression above, a special
lookup function is provided.  It is intended for piece-wise lookup of
a full string's key sequence.  This is \emph{not} a key string as
defined by the UCA.  However, it is the set of keys to be considered,
in the order that they are to be considered.  The only work needed to
convert this to a UCA key string is to apply the variable key rule and
split the keys by level.  The returned format is exactly the same as
is stored, except that no blank key strings are returned, and level
four is always present.

Since this function directly references some large property tables, it
is stored in a separate object file.

<<Library [[uni]] Members>>=
ducet_lookup.o
@

<<ducet_lookup.c>>=
<<Common C Header>>
#include "uni_prop.h"
// static_proto

<<DUCET lookup functions>>
@

<<uni_DUCET_lookup parameters>>=
<<DUCET lookup format defs>>

/* function return value */
/* lev123 and lev4 are valid; give me next char */
/* unless char was END, in which case lev123 and lev4 are last return */
#define UNI_DUCET_LOOKUP_OK    0
/* lev123 and lev4 are valid, but don't give me a char next time */
/* in other words, next time c will be ignored */
#define UNI_DUCET_LOOKUP_AGAIN 1
/* lev123 and lev4 are invalid; give me next char */
/* unless char was END, in which case no more lev123 or lev4 */
#define UNI_DUCET_LOOKUP_NONE -1

/* special c parameter */
#define UNI_DUCET_LOOKUP_END 0xffffffff

/* pass in NULL first time to allocate */
/* will be freed by function when done */
typedef struct uni_ducet_lookup_state_t uni_ducet_lookup_state_t;
@

<<DUCET lookup format defs>>=
/* lev123 return value */
#define UNI_DUCET_LEV1_MASK  0xffff0000
#define UNI_DUCET_LEV1_SHIFT 16
#define UNI_DUCET_LEV2_MASK  0x0000ff80
#define UNI_DUCET_LEV2_SHIFT 7
#define UNI_DUCET_LEV3_MASK  0x0000007c
#define UNI_DUCET_LEV3_SHIFT 2
/* low 2 bits are always 0 */
@

<<Known Data Types>>=
uni_ducet_lookup_state_t,%
@

<<Unicode property exports>>=
<<uni_DUCET_lookup parameters>>
int uni_DUCET_lookup(uint32_t c, uint32_t *lev123, uint32_t *lev4,
                     uni_ducet_lookup_state_t **state);
@
                     
<<DUCET lookup functions>>=
<<Private DUCET lookup definitions>>
struct uni_ducet_lookup_state_t {
  <<DUCET lookup state members>>
};
<<Private DUCET lookup globals>>
int uni_DUCET_lookup(uint32_t c, uint32_t *lev123, uint32_t *lev4,
                     uni_ducet_lookup_state_t **state)
{
  <<Look up and return a single key element from DUCET>>
}
@

First of all, there are two DUCET tables that could be used for
lookup.  One way to implement locales would be to load tables from
files, or to construct tables on the fly, so other tables may be used
as well.  The default table to use is the plain DUCET, but other
tables may be specified.  Rather than complicating the lookup function
with a parameter that should only be specified on the first call, a
separate function is provided to set the tables in the state
parameter.

<<DUCET lookup state members>>=
const uint32_t *tab, *strs;
@

<<Unicode property exports>>=
/* set table for DUCET lookups; call before lookup() */
void uni_DUCET_lookup_tab(uni_ducet_lookup_state_t **state,
                          const uint32_t *tab, const uint32_t *strs);
@

<<DUCET lookup functions>>=
void uni_DUCET_lookup_tab(uni_ducet_lookup_state_t **state,
                          const uint32_t *tab, const uint32_t *strs)
{
  if(!*state) {
    inisize(*state, 1);
    clearbuf(*state, 1);
  }
  (*state)->tab = tab;
  (*state)->strs = strs;
}
@

<<Look up and return a single key element from DUCET>>=
/* if(!state) exit(1); */
if(!*state) {
  inisize(*state, 1);
  clearbuf(*state, 1);
}
uni_ducet_lookup_state_t *st = *state;
if(!st->tab) {
  st->tab = uni_DUCET_mtab;
  st->strs = uni_DUCET_strs;
}
@

Some operations, such as looking up multi-character collation
elements, requires at least one character of lookahead.  In fact, the
reordering requirement requires arbitrary lookahead.  To support this,
a buffer may be filled with lookahead characters.  Any time a NONE
return would happen, if there are buffer characters, the routine
should instead restart itself.  Any time an OK return would happen, an
AGAIN should be returned instead.

<<DUCET lookup state members>>=
uint32_t *bt;
int32_t btp, num_bt, max_bt;
@

<<Look up and return a single key element from DUCET>>=
restart:
  if(st->num_bt) {
    if(st->btp >= 0)
      c = st->bt[st->btp++];
    else
      st->btp++;
    if(st->btp == st->num_bt)
      st->btp = st->num_bt = 0;
  }
@

<<Free DUCET lookup state members>>=
if(st->bt)
  free(st->bt);
@

<<Private DUCET lookup globals>>=
static void add_bt(uint32_t c, uni_ducet_lookup_state_t *st)
{
  if(!st->bt)
    inisize(st->bt, (st->max_bt = 5));
  if(st->num_bt == st->max_bt)
    resize(st->bt, (st->max_bt *= 2));
  st->bt[st->num_bt++] = c;
}

static void add_bt_buf(const uint32_t *buf, uint32_t len,
                       uni_ducet_lookup_state_t *st)
{
  if(!st->bt)
    inisize(st->bt, (st->max_bt = 5));
  while(st->num_bt + len > st->max_bt)
    resize(st->bt, (st->max_bt *= 2));
  memcpy(st->bt + st->num_bt, buf, len * sizeof(*st->bt));
  st->num_bt += len;
}
@

<<Return no DUCET values>>=
if(st->num_bt)
  goto restart;
else
  return UNI_DUCET_LOOKUP_NONE;
@

<<Return one DUCET value>>=
if(st->num_bt)
  return UNI_DUCET_LOOKUP_AGAIN;
else
  return UNI_DUCET_LOOKUP_OK;
@

<<Return more than one DUCET value>>=
if(st->num_bt)
  st->btp--;
return UNI_DUCET_LOOKUP_AGAIN;
@

A private enumeration type is used to represet the current lookup
progress.

<<Private DUCET lookup definitions>>=
typedef enum {
  DUCET_lookup_start
  <<DUCET lookup states>>
} ducet_lookup_state_t;
@

<<Known Data Types>>=
ducet_lookup_state_t,%
@

<<DUCET lookup state members>>=
ducet_lookup_state_t state;
@

<<Look up and return a single key element from DUCET>>=
switch(st->state) {
  case DUCET_lookup_start:
    <<Look up in DUCET given no state>>
  <<Look up in DUCET given state>>
}
@

The first thing to do is check for end-of-string.  At end-of string,
the state is freed, and nothing is returned.

<<Look up in DUCET given no state>>=
if(c == UNI_DUCET_LOOKUP_END) {
  <<Free DUCET lookup state members>>
  free(st);
  *state = NULL;
  return UNI_DUCET_LOOKUP_NONE; /* can't have any more bt */
}
@

Otherwise, a lookup is performed in the table.  If the lookup fails, a
key is synthesized.  Note that synthesis requires the UIdeo and blk
properties as specified, although this could be modified to use
explicit range checks instead.  The UIdeo property is complicated
enough and yet small enough (12 ranges/300 bytes) that conversion
would not save much, but the blk checks are just single ranges each
(and the blk table is much larger, at 220 ranges/8032 bytes).  This
assumes that the ranges will never change in the future, which is
actually a pretty safe bet.

<<DUCET lookup state members>>=
uni_str_rng_val_t v;
uint32_t c;
@

<<Look up in DUCET given no state>>=
const uni_str_rng_val_t *v;
multi_tab_lookup(st->tab, c * 4, (const uint8_t **)&v, 0);
if(v && (v->off || v->len)) {
  st->v = *v;
  <<Process DUCET initial lookup>>
} else {
  st->c = c;
  st->state = DUCET_lookup_synth;
  *lev123 = ((c >> 15) << 16) + (0x0020 << 7) + (0x0002 << 2);
  if(is_uni_UIdeo(c)) {
#if 0
    uni_blk_t blk = uni_blk_of(c);
    if(blk == u_blk_CJK || blk == u_blk_CJK_Compat)
#else
    if((c >= 0x4E00 && c <= 0x9FFF) || (c >= 0x3300 && c <= 0x33FF))
#endif
      *lev123 += 0xFB40 << 16;
    else
      *lev123 += 0xFB80 << 16;
  } else
    *lev123 += 0xFBC0 << 16;
  *lev4 = c;
  <<Return more than one DUCET value>>
}
@

<<DUCET lookup states>>=
,DUCET_lookup_synth
@

<<Look up in DUCET given state>>=
case DUCET_lookup_synth:
  *lev123 = ((st->c & 0x7fff) << 16) | 0x80000000;
  *lev4 = st->c;
  st->state = 0;
  <<Return one DUCET value>>
@

For a successful lookup, the response for a single-character index is
extracted.  If multi-character indices are available as well, more
characters are requested after saving the work done so far in the
state.  Otherwise, the single-character reponse is returned.

<<DUCET lookup state members>>=
uint32_t curoff;
@

<<Process DUCET initial lookup>>=
const uint32_t *str = st->strs + st->v.off;
uint32_t len = st->v.len, len1;
for(len1 = 0; len1 < len; len1++)
  if((str[len1] & 3) == 3) {
    <<Accumulate and check more collation element characters>>
    <<Return no DUCET values>>
  }
<<Return DUCET entry for single-character index>>
@

<<Accumulate and check more collation element characters>>=
st->state = DUCET_lookup_gotfirst;
st->curoff = len1;
st->c = c;
@

<<DUCET lookup states>>=
,DUCET_lookup_gotfirst
@

If the next character is end-of-string, the match so far is returned.
Otherwise, it is searched for in the subindex strings.   A match
increases the total [[matchlen]], which can be skipped.  Only exact
matches (i.e. only one character after [[matchlen]], and it must match
[[c]]) are considered successful.

<<Look up in DUCET given state>>=
case DUCET_lookup_gotfirst:
  if(c == UNI_DUCET_LOOKUP_END) {
    <<Return DUCET entry for single or multi-character index>>
  } else {
    int32_t l, h, m;
    const uint32_t *str = st->strs + st->v.off;
    m = ducet_lookup_subindex(c, &l, &h, st);
    <<Process index extension search results>>
  }
@

<<DUCET lookup state members>>=
uint32_t matchlen;
@

<<Accumulate and check more collation element characters>>=
st->matchlen = 0;
@

<<Private DUCET lookup globals>>=
static int ducet_lookup_subindex(uint32_t c, int32_t *_l, int32_t *_h,
                                 const uni_ducet_lookup_state_t *st)
{
  uint32_t l = st->curoff, h = st->v.len, m;
  const uint32_t *str = st->strs + st->v.off;
  while(l <= h) {
    <<Find start of middle index extension>>
    m += st->matchlen;
    uint32_t mc = str[m] >> 2;
    if(mc < c) {
      l = m + 1;
      <<Skip to next index extension>>
    } else if(mc > c ||
              /* also greater if longer than one char */
	      (h > m && mc == c &&
	      (str[m + 1] & 3) == 3 && str[m + 1] != 3)) {
      h = m - st->matchlen - 1;
    } else {
      *_l = l;
      *_h = h;
      return m - st->matchlen;
    }
  }
  *_l = l;
  *_h = h;
  return 0;
}
@

<<Find start of middle index extension>>=
m = (l + h) / 2;
while(m > l && ((str[m] & 3) != 3 || str[m] == 3))
  m--;
while(m > l && ((str[m - 1] & 3) == 3 && str[m - 1] != 3))
  m--;
@

<<Skip to next index extension>>=
while(l <= h && (str[l] & 3) == 3 && str[l] != 3)
  l++;
if(l <= h && str[l] == 3)
  l++;
else
  while(l <= h && (str[l] & 3) != 3)
    l++;
@

A successful match is returned if there can be no further extension of
the index.  Otherwise, the search results are narrowed to the
possibilities collected so far, and more input is requested.

<<Process index extension search results>>=
if(l <= h) {
  /* advance search result to match */
  st->v.off += m;
  st->v.len -= m;
  str += m;
  /* find end of match entry */
  l = 1;
  h = st->v.len - 1;
  <<Skip to next index extension>>
  ducet_adjust_search(c, l, st);
  if(st->curoff == st->v.len) {
    /* there are no more possible matches, so just return */
    st->state = DUCET_lookup_start;
    <<Return DUCET entry for multi-character index>>
  } else {
    st->matchlen++;
    <<Return no DUCET values>>
  }
}
@

<<Private DUCET lookup globals>>=
static void ducet_adjust_search(uint32_t c, int32_t l,
                                uni_ducet_lookup_state_t *st)
{
  int32_t h = st->v.len - 1, m;
  const uint32_t *str = st->strs + st->v.off;
  st->curoff = l;
  while(l <= h) {
    <<Find start of middle index extension>>
    if((str[m + st->matchlen] >> 2) != c)
      h = m - 1;
    else {
      l = m + 1;
      <<Skip to next index extension>>
    }
  }
  st->v.len = h + 1;
}
@

When mismatch occurs, however, it is not guaranteed that this will
never match.  There are two cases where a future character might still
cause a match.  If a longer string is required for a match due to a
DUCET which does not have entries for all intervening-length strings
(the standard DUCET does not), [[h]] always points just below at least
one such string.  If more data is required because a reordering might
cause a match, the ccc of c and any future characters must be non-zero
and not match the ccc of any potential candidates for reordering.

<<Process index extension search results>>=
else {
  uint32_t ccc;
  /* if c in str, but not only member of str, ill-formed DUCET */
  /* h always points just below longer index */
  h += st->matchlen + 1;
  if(h < st->v.len && str[h] == (c << 2) + 3) {
    <<Enable possible longer lookup>>
  /* check if possible match due to reordering rule */
  } else if((ccc = uni_ccc_of(c))) {
    <<Enable reordering extension>>
  } else {
    /* no possible longer matches, so go ahead and return */
    add_bt(c, st);
    <<Return DUCET entry for single or multi-character index>>
  }
}
@

The first case is easy to detect and manage.  At most one intermediate
is allowed to be missing, so the next character determines if a match
is made.  if so, processing continues just as above.  Otherwise, the
two characters that were not processed are stored, and special states
are entered to feed these back, one at a time.  They will never
collide with each other, because the second character is the first one
which could cause the extension state to reappear, and by then, the
stored characters are done being processed.

Rather than modify the code to loop and retrieve characters from
buffers, a recursive call is used.  Again, this should happen rarely,
and does not affect performance much.

<<DUCET lookup state members>>=
uint32_t long_off;
@

<<DUCET lookup states>>=
,DUCET_lookup_longer
@

<<Enable possible longer lookup>>=
st->state = DUCET_lookup_longer;
st->long_off = st->curoff;
st->c = c;
ducet_adjust_search(c, l, st);
<<Return no DUCET values>>
@

<<Look up in DUCET given state>>=
case DUCET_lookup_longer:
  {
    int32_t m, l, h;
    const uint32_t *str = st->strs + st->v.off;
    st->matchlen++;
    m = ducet_lookup_subindex(c, &l, &h, st);
    if(l > h) { /* assume no even longer matches possible.. */
      add_bt(st->c, st);
      add_bt(c, st);
      st->curoff = st->long_off;
      st->state = DUCET_lookup_start;
      <<Return DUCET entry for single or multi-character index>>
    } else {
      st->v.off += m;
      st->v.len -= m;
      l = 1;
      h = st->v.len - 1;
      <<Skip to next index extension>>
      st->curoff = l;
      <<Return DUCET entry for multi-character index>>
    }
  }
@

The second case requires checking the ccc of all subsequent
characters; if a zero is detected, no further reordering is possible.
Otherwise, as long as a character does not match the ccc of one of the
previously accumulated characters, it is looked up and if it matches,
a reordering can take place.

By the time the decision is made, at least one character, and possibly
more, have been accumulated.  A buffer (separate from [[bt]]) needs to
be used to accumulate these and eventually transfer them into [[bt]]. 

<<DUCET lookup state members>>=
uint32_t *buf, buflen, maxbuf;
uint32_t *ccc_buf;
@

<<DUCET lookup states>>=
,DUCET_lookup_reorder
@

<<Enable reordering extension>>=
if(!st->buf) {
  inisize(st->buf, (st->maxbuf = 10));
  inisize(st->ccc_buf, st->maxbuf);
}
st->buf[0] = c;
st->ccc_buf[0] = ccc;
st->buflen = 1;
st->state = DUCET_lookup_reorder;
<<Return no DUCET values>>
@

<<Look up in DUCET given state>>=
case DUCET_lookup_reorder:
  {
    uint32_t ccc = c == UNI_DUCET_LOOKUP_END ? 0 : uni_ccc_of(c);
    uint32_t i;
    if(st->maxbuf == st->buflen) {
      resize(st->buf, st->maxbuf *= 2);
      resize(st->ccc_buf, st->maxbuf);
    }
    st->buf[st->buflen] = c;
    st->ccc_buf[st->buflen++] = ccc;
    if(!ccc) {
      st->state = DUCET_lookup_start;
      add_bt_buf(st->buf, st->buflen, st);
      <<Return DUCET entry for single or multi-character index>>
    }
    for(i = 0; i < st->buflen - 1; i++)
      if(st->ccc_buf[i] == ccc) {
        <<Return no DUCET values>>
      }
    int32_t l, h, m;
    m = ducet_lookup_subindex(c, &l, &h, st);
    if(l <= h) {
      /* advance search result to match */
      st->v.off += m;
      st->v.len -= m;
      /* copy rest of characters accumulated to bt */
      add_bt_buf(st->buf, st->buflen - 1, st);
      /* find end of match entry */
      l = 1;
      h = st->v.len - 1;
      const uint32_t *str = st->strs + st->v.off;
      <<Skip to next index extension>>
      if(l > h) {
        /* there are no more possible matches, so just return */
        st->curoff = l;
	st->state = DUCET_lookup_start;
	<<Return DUCET entry for multi-character index>>
      } else {
        st->state = DUCET_lookup_gotfirst;
	ducet_adjust_search(c, l, st);
	st->matchlen++;
	<<Return no DUCET values>>
      }
    } else {
      <<Return no DUCET values>>
    }
  }
@

<<Free DUCET lookup state members>>=
if(st->buf)
  free(st->buf);
@

Rather than reproduce the code for returning the results everywhere,
it is placed after the switch.  Thus a [[break]] indicates that a
result should be returned.  The only difference between them is that
[[c]] needs to be retrieved from the state for multi-character indices.
Also, [[curoff]] is assumed to be the end of the entry to return, but
it is never assigned for the single-character index case.

<<Return DUCET entry for single or multi-character index>>=
c = st->c;
break;
@

<<Return DUCET entry for multi-character index>>=
c = st->c;
break;
@

<<Return DUCET entry for single-character index>>=
st->curoff = v->len;
break;
@

Due to advancing the pointers above, [[v]] points to the desired result
to return.  The only difficulty is distinguishing between
multi-character index results and blank single-character index
results; this can be done by checking [[curoff]], which points to the
first character after the key.  If this is zero, then it is a blank
single-character index result.

<<Look up and return a single key element from DUCET>>=
if(!st->curoff) {
  <<Return blank single-character index DUCET result>>
}
int i;
const uint32_t *str = st->strs + st->v.off;
for(i = 0; i < st->curoff; i++)
  if((str[i] & 3) != 3)
    break;
if(i == st->curoff) {
  <<Return blank multi-character index DUCET result>>
}
<<Return non-blank DUCET result>>
@

The single-character index entries with blank keys denote canonical
decomposition.  This may require more than one result.  All decomposed
characters are guaranteed to have a real key of length one for the
single-character index.

<<DUCET lookup state members>>=
const uint32_t *multi_ret;
uint32_t multi_ret_len;
@

<<Return blank single-character index DUCET result>>=
const uni_str_rng_val_t *v;
multi_tab_lookup(uni_canon_decomp_mtab, c * 4, (const uint8_t **)&v, 0);
st->multi_ret = uni_dm_full_strs + v->off;
st->multi_ret_len = v->len;
<<Return DUCET entry for next decomp character>>
@

<<Return DUCET entry for next decomp character>>=
const uint32_t *str;
c = *st->multi_ret++;
st->multi_ret_len--;
<<Look up and return DUCET value for [[c]]>>
if(st->multi_ret_len) {
  st->state = DUCET_lookup_compret;
  <<Return more than one DUCET value>>
} else {
  st->state = DUCET_lookup_start;
  <<Return one DUCET value>>
}
@

<<Look up and return DUCET value for [[c]]>>=
multi_tab_lookup(st->tab, c * 4, (const uint8_t **)&v, 0);
str = st->strs + v->off;
<<Return DUCET value for [[str]]/[[c]]>>
@

<<Return DUCET value for [[str]]/[[c]]>>=
*lev123 = *str & ~3;
switch(*str & 3) {
  case 0:
    *lev4 = 0;
    break;
  case 1:
    *lev4 = c;
    break;
  default:
    *lev4 = *++str >> 2;
    break;
}
@

<<DUCET lookup states>>=
,DUCET_lookup_compret
@

<<Look up in DUCET given state>>=
case DUCET_lookup_compret:
  {
    <<Return DUCET entry for next decomp character>>
  }
@

The multi-character index entries with blank keys denote canonical
composition.  This may require more than one composition lookup.  It
always returns exactly one value.

<<Return blank multi-character index DUCET result>>=
int16_t coff;
uint8_t clen;
uni_find_canon_comp(c, &coff, &clen);
c = uni_lookup_compent(*str >> 2, uni_canon_comp_strs + coff, clen);
if(st->curoff > 1 && str[1] != 3 && (str[1] & 3) == 3) {
  uni_find_canon_comp(c, &coff, &clen);
  c = uni_lookup_compent(str[1] >> 2, uni_canon_comp_strs + coff, clen);
}
const uni_str_rng_val_t *v;
<<Look up and return DUCET value for [[c]]>>
st->state = DUCET_lookup_start;
<<Return one DUCET value>>
@

The non-blank result only requires work if there is more than one key
entry.  If so, it is returned later.

<<Return non-blank DUCET result>>=
str += i;
st->multi_ret = str;
st->curoff -= i;
<<Return DUCET value for [[str]]/[[c]]>>
str++;
st->multi_ret_len = st->curoff - (int)(str - st->multi_ret);
if(!st->multi_ret_len) {
  st->state = DUCET_lookup_start;
  <<Return one DUCET value>>
}
st->multi_ret = str;
st->state = DUCET_lookup_multikey;
<<Return more than one DUCET value>>
@

<<DUCET lookup states>>=
,DUCET_lookup_multikey
@

<<Look up in DUCET given state>>=
case DUCET_lookup_multikey:
  {
    const uint32_t *str = st->multi_ret;
    c = st->c;
    <<Return DUCET value for [[str]]/[[c]]>>
    str++;
    st->multi_ret_len -= str - st->multi_ret;
    if(st->multi_ret_len) {
      st->multi_ret = str;
      <<Return more than one DUCET value>>
    } else {
      st->state = DUCET_lookup_start;
      <<Return one DUCET value>>
    }
  }
@

Note that the multi-level comparison algorithm requires that more than
one pass be made over the keys, unless the strings being compared
happen to generate keys of the same ignorable level.  This can be done
by either calling [[uni_DUCET_lookup]] as many times as there are
levels for each character, or by storing the results of the lookup in
dynamic arrays, to be processed in later passes.  The only entries
which do not need to be stored are all-zero entries.  A simple lookup
function is provided to do the lookups, and return the results as well
as the number of entries in each level (if requested).  By passing in
a maximum level number, entries which are ignorable to that level can
be skipped.  By passing in the variable top and variable mode, the
UCA variable entry modifications can be made as well.

<<[[uni_uca_opts_t]]>>=
typedef struct {
  <<Unicode UCA function options>>
} uni_uca_opts_t;
@

<<Unicode UCA function options>>=
const uint32_t *tab, *strs;
uint32_t vartop;
uint8_t level, var_mode;
@

<<Unicode property exports>>=
<<DUCET variable modes>>
@

<<DUCET variable modes>>=
#define UNI_DUCET_VAR_MODE_NON_IGNORABLE 0 /* the default; not UCA def */
#define UNI_DUCET_VAR_MODE_BLANKED       1
#define UNI_DUCET_VAR_MODE_SHIFTED       2 /* the UCA def; must be explicit */
#define UNI_DUCET_VAR_MODE_IGNORE_SP     3
#define UNI_DUCET_VAR_MODE_SHIFT_TRIMMED 4
@

<<Unicode property exports>>=
<<[[uni_uca_opts_t]]>>
uint32_t *uni_str_ducet(const uint32_t *str, uint32_t slen,
                        const uni_uca_opts_t *opts, int *rlen, int *llen);
@

<<DUCET lookup functions>>=
uint32_t *uni_str_ducet(const uint32_t *str, uint32_t slen,
                        const uni_uca_opts_t *opts, int *rlen, int *llen)
{
  uint32_t *res, res_max, nres = 0;
  uni_ducet_lookup_state_t *st = NULL;
  uint32_t mask123;
  int lev = opts ? opts->level : 3;
  uint8_t var_mode = opts ? opts->var_mode : 0; /* not the UCA default! */
  int shifted = var_mode >= UNI_DUCET_VAR_MODE_SHIFTED,
      blanked = var_mode == UNI_DUCET_VAR_MODE_BLANKED;
  uint32_t vartop = opts && opts->vartop ? opts->vartop : uni_DUCET_var_top;
  int in_var = 0;

  if(!str || !slen) {
    *rlen = 0;
    return NULL;
  }
  inisize(res, (res_max = 10));
  if(opts && opts->tab && opts->strs)
    uni_DUCET_lookup_tab(&st, opts->tab, opts->strs);
  if(!lev)
    lev = 3;
  if(lev > 4)
    lev = 4;
  mask123 = UNI_DUCET_LEV1_MASK;
  if(lev > 1)
    mask123 |= UNI_DUCET_LEV2_MASK;
  if(lev > 2)
    mask123 |= UNI_DUCET_LEV3_MASK;
  vartop = (vartop << 16) | 0xffff; /* for direct <= comparison */
  if(llen)
    llen[0] = llen[1] = llen[2] = llen[3] = 0;
  while(1) {
    uint32_t lev123, lev4;
    int ret = uni_DUCET_lookup(slen ? *str : UNI_DUCET_LOOKUP_END,
                               &lev123, &lev4, &st);
    if(ret == UNI_DUCET_LOOKUP_NONE) {
      if(slen) {
        str++;
	slen--;
        continue;
      } else
        break;
    }
    <<Adjust DUCET entries based on shift mode>>
    lev123 &= mask123;
    if(lev < 4)
      lev4 = 0;
    if(lev123 || lev4) {
      if(nres >= res_max - 1)
        resize(res, (res_max *= 2));
      res[nres++] = lev123;
      if(lev >= 4)
        res[nres++] = lev4;
      if(llen) {
        if(lev123 & UNI_DUCET_LEV1_MASK)
          llen[0]++;
        if(lev123 & UNI_DUCET_LEV2_MASK)
          llen[1]++;
        if(lev123 & UNI_DUCET_LEV3_MASK)
          llen[2]++;
        if(lev4)
          llen[3]++;
      }
    }
    if(ret == UNI_DUCET_LOOKUP_OK) {
      if(!slen--)
        break;
      str++;
    }
  }
  *rlen = nres;
  return res;
}
@

<<Adjust DUCET entries based on shift mode>>=
/* UCA variable mode modifies variable entry & anything following */
/* plus some other random ignorables */
if(shifted) {
  if(!lev123)
    lev4 = 0;
  else if(!(lev123 & UNI_DUCET_LEV1_MASK)) {
    if(in_var)
      lev4 = lev123 = 0;
    else
      lev4 = 0xffff;
  } else {
    in_var = 0;
    lev4 = 0xffff;
  }
} else if(blanked && in_var) {
  if(!(lev123 & UNI_DUCET_LEV1_MASK))
    lev123 = lev4 = 0;
  else
    in_var = 0;
}
/* due to modification of vartop above: */
/*    same as lev1 <= vartop && lev1 > 0 */
if(lev123 <= vartop && lev123 > 0xffff) {
  in_var = 1;
  switch(var_mode) {
    /* UNI_DUCET_VAR_MODE_NON_IGNORABLE does nothing */
    case UNI_DUCET_VAR_MODE_BLANKED:
      lev123 = lev4 = 0;
      break;
    case UNI_DUCET_VAR_MODE_SHIFTED:
      lev4 = lev123 >> UNI_DUCET_LEV1_SHIFT;
      /* !!! Undocumented CLDR behavior (required to pass test): !!! */
      /* UCA says lev1 > 0 and lev1 <= vartop is variable */
      /*  but CLDR defines one entry with lev1 == 1, but it is not var */
      /*  yet in shifted mode, it sets lev4 == lev1 as above */
      /*  GRRRR.... */
      /*  at least UCA data files never use lev1 < 0x0200, so */
      /*  just testing for 1 is safe */
      /* who knows what IgnoreSP and Trimmed are supposed to work like */
      if(lev4 > 1)
        lev123 = 0;
      else
        in_var = 0;
      break;
    case UNI_DUCET_VAR_MODE_IGNORE_SP:
      if((uni_gc_trans[U_gc_P] & (1 << uni_gc_of(*str))) ||
         is_uni_WSpace(*str)) {
        lev4 = lev123 >> UNI_DUCET_LEV1_SHIFT;
	lev123 = 0;
      } else
        in_var = 0;
      break;
    case UNI_DUCET_VAR_MODE_SHIFT_TRIMMED:
      lev4 = lev123 >> UNI_DUCET_LEV1_SHIFT;
      lev123 = 0;
      /* FIXME: trim trailing ffff from sort key */
      /* requires delaying 0xffff until known wheter trailing or not */
      break;
  }
}
@

Given the lookup function, it is possible to create simple UCA
wrappers for key creation and string comparison.  To create the UCA
key, the counted lengths are used to size an array and determine the
offset of each level; the array is then filled by scanning the raw
DUCET return one entry at a time, and filling in any present levels.
The returned key string is zero-terminated rather than returning the
actual key length. Levels which are guaranteed to be less than 32 bits
are packed as multiple keys per word while still keeping the key
properly ordered.

One option for creating keys is adding the literal text after the key
in order to make a more deterministic sort; this is added as an option
for UCA functions.  If this option is present, the key has the literal
text appended, even if it contains zeroes.  If this is the case, it is
impossible to determine the length of the array.  While it would be
possible to replace zeroes and ones with ones followed by one or two
to indicate the original value plus one, it is assumed the caller is
smart enough to do that himself.  Zero, one, and two are all
completely ignorable in UCA, so this only affects the appended string.

<<Unicode UCA function options>>=
uint8_t do_literal;
@

<<Unicode property exports>>=
uint32_t *uni_str_uca_key(const uint32_t *str, uint32_t slen,
                          const uni_uca_opts_t *opts);
@

<<DUCET lookup functions>>=
uint32_t *uni_str_uca_key(const uint32_t *str, uint32_t slen,
                          const uni_uca_opts_t *opts)
{
  uint32_t *raw;
  int raw_len;
  int len[4], off[4];
  uint32_t *key, keylen, i;
  int i1, i2, i3, i4;
  int do_literal = opts && opts->do_literal;
  int lev = opts ? opts->level : 3;

  if(lev > 4)
    do_literal = 1;
  else if(lev < 1)
    lev = 3;
  raw = uni_str_ducet(str, slen, opts, &raw_len, len);
  if(!raw_len) {
    if(!do_literal)
      slen = 0;
    inisize(key, slen + 1);
    if(slen)
      memcpy(key, str, slen * 4);
    key[slen] = 0;
    return key;
  }
  /* 0 == 16 bits, 1 == 9 bits, 2 == 5 bits */
  /*  -> 2 per word, 3 per word, 6 per word */
  keylen = len[0] / 2 + 1;
  if(lev > 1)
    keylen += len[1] / 3 + 1;
  if(lev > 2)
    keylen += len[2] / 6 + 1;
  if(lev > 3)
    keylen += len[3] + 1;
  if(do_literal)
    keylen += slen + 1;
  inisize(key, keylen + 1);
  off[0] = len[0] / 2 + 1;
  key[off[0] - 1] = lev > 1 || do_literal;
  if(lev > 1) {
    off[1] = off[0] + len[1] / 3 + 1;
    key[off[1] - 1] = lev > 2 || do_literal;
  } else {
    key[off[0]] = 0;
    off[1] = off[0];
  }
  if(lev > 2) {
    off[2] = off[1] + len[2] / 6 + 1;
    key[off[2] - 1] = lev > 3 || do_literal;
  } else {
    key[off[1]] = 0;
    off[2] = off[1];
  }
  if(lev > 3) {
    off[3] = off[2] + len[3] + 1;
    key[off[3] - 1] = do_literal ? 2 : 0; /* 2 instead of 1 for fffe */
  } else {
    key[off[2]] = 0;
    off[3] = off[2];
  }
  if(do_literal)
    key[off[3] + slen] = 0;
  for(i = i1 = i2 = i3 = i4 = 0; i < raw_len; i++) {
    if(raw[i] & UNI_DUCET_LEV1_MASK) {
      if(i1 % 2)
        key[i1 / 2] |= raw[i] >> UNI_DUCET_LEV1_SHIFT;
      else
        key[i1 / 2] = raw[i] & UNI_DUCET_LEV1_MASK;
      i1++;
    }
    if(raw[i] & UNI_DUCET_LEV2_MASK) {
      uint32_t l2 = (raw[i] & UNI_DUCET_LEV2_MASK) >> UNI_DUCET_LEV2_SHIFT;
      if(i2 % 3)
        key[off[0] + i2 / 3] |= l2 << 9 * (2 - i2 % 3);
      else
        key[off[0] + i2 / 3] = l2 << 18;
      i2++;
    }
    if(raw[i] & UNI_DUCET_LEV3_MASK) {
      uint32_t l3 = (raw[i] & UNI_DUCET_LEV3_MASK) >> UNI_DUCET_LEV3_SHIFT;
      if(i3 % 6)
        key[off[1] + i3 / 6] |= l3 << 5 * (5 - i3 % 6);
      else
        key[off[1] + i3 / 6] = l3 << 25;
      i3++;
    }
    if(lev > 3 && raw[++i]) {
      key[off[2] + i4] = raw[i];
      i4++;
    }
  }
  free(raw);
  if(do_literal)
    memcpy(key + off[3], str, slen * 4);
  return key;
}
@

For string comparison, the raw keys are looked up, and scanned once
per level.

<<Unicode property exports>>=
int uni_uca_strcmp(const uint32_t *stra, uint32_t lena,
                   const uint32_t *strb, uint32_t lenb,
                          const uni_uca_opts_t *opts);
@

<<DUCET lookup functions>>=
<<DUCET strcmp support>>
int uni_uca_strcmp(const uint32_t *stra, uint32_t lena,
                   const uint32_t *strb, uint32_t lenb,
		   const uni_uca_opts_t *opts)
{
  uint32_t *rawa, *rawb;
  int raw_lena, raw_lenb;
  int do_literal = opts && opts->do_literal;
  int lev = opts ? opts->level : 3;

  if(lev > 4) {
    do_literal = 1;
    lev = 4;
  } else if(lev < 1)
    lev = 3;
  rawa = uni_str_ducet(stra, lena, opts, &raw_lena, NULL);
  rawb = uni_str_ducet(strb, lenb, opts, &raw_lenb, NULL);
  if(!raw_lena && !raw_lenb)
    return do_literal ? uni_strcmp_raw(stra, lena, strb, lenb) : 0;
  if(!raw_lena) {
    free(rawb);
    return -1;
  }
  if(!raw_lenb) {
    free(rawa);
    return 1;
  }
  int skip = lev > 3 ? 2 : 1;
  int c;
  if((c = uni_ducet_strcmp_raw(rawa, raw_lena, rawb, raw_lenb,
                               UNI_DUCET_LEV1_MASK, skip))) {
    free(rawa);
    free(rawb);
    return c;
  }
  if(lev > 1 &&
     (c = uni_ducet_strcmp_raw(rawa, raw_lena, rawb, raw_lenb,
                               UNI_DUCET_LEV2_MASK, skip))) {
    free(rawa);
    free(rawb);
    return c;
  }
  if(lev > 2 &&
     (c = uni_ducet_strcmp_raw(rawa, raw_lena, rawb, raw_lenb,
                               UNI_DUCET_LEV3_MASK, skip))) {
    free(rawa);
    free(rawb);
    return c;
  }
  if(lev > 3 &&
     (c = uni_ducet_strcmp_raw(rawa + 1, raw_lena, rawb + 1, raw_lenb, ~0, 2))) {
    free(rawa);
    free(rawb);
    return c;
  }
  free(rawa);
  free(rawb);
  return do_literal ? uni_strcmp_raw(stra, lena, strb, lenb) : 0;
}
@

<<DUCET strcmp support>>=
static int uni_strcmp_raw(const uint32_t *stra, uint32_t lena,
                          const uint32_t *strb, uint32_t lenb)
{
  while(lena && lenb) {
    uint32_t a = *stra++, b = *strb++;
    if(a > b)
      return 1;
    if(b > a)
      return -1;
    lena--;
    lenb--;
  }
  if(lena)
    return 1;
  if(lenb)
    return -1;
  return 0;
}
@

<<DUCET strcmp support>>=
static int uni_ducet_strcmp_raw(const uint32_t *rawa, uint32_t lena,
                                const uint32_t *rawb, uint32_t lenb,
				uint32_t mask, int skip)
{
  while(1) {
    uint32_t a, b;
    while(lena > 0 && !(a = (*rawa & mask))) {
      rawa += skip;
      lena -= skip;
    }
    while(lenb > 0 && !(b = (*rawb & mask))) {
      rawb += skip;
      lenb -= skip;
    }
    if(!lena && !lenb)
      return 0;
    if(!lenb)
      return 1;
    if(!lena)
      return -1;
    if(a > b)
      return 1;
    if(b > a)
      return -1;
    rawa += skip;
    lena -= skip;
    rawb += skip;
    lenb -= skip;
  }
}
@

To test this, the CollationTest files are used.  As with the
normalization tests, the files are simply fed into standard input.
There are four separate files, and each requires a different
configuration.  These are selected using command-line arguments.  The
options are DUCET vs. CLDR DUCET, and variable mode non-ignorable vs.
variable mode shifted.

\lstset{language=make}
<<C Test Support Executables>>=
tstuca \
@

<<Additional Tests>>=
./tstuca <$(UCD_LOC)/CollationTest/CollationTest_NON_IGNORABLE.txt
./tstuca -s <$(UCD_LOC)/CollationTest/CollationTest_SHIFTED.txt
./tstuca -c <$(UCD_LOC)/CollationAuxiliary/CollationTest_CLDR_NON_IGNORABLE.txt
./tstuca -cs <$(UCD_LOC)/CollationAuxiliary/CollationTest_CLDR_SHIFTED.txt
@

\lstset{language=C}
<<tstuca.c>>=
<<Common C Header>>
#include "uni_all.h"
#include "mfgets.h"
#include "mallocdef.h"

int main(int argc, const char **argv)
{
  int do_cldr = 0, do_shift = 0;
  while(argc-- > 1) {
    if(**++argv == '-') {
      const char *s = *argv;
      while(*++s) {
        if(*s == 'c')
	  do_cldr = 1;
	else if(*s == 's')
	  do_shift = 1;
      }
    }
  }
  <<Read and process CollationTest.txt>>
  return 0;
}
@

<<Read and process CollationTest.txt>>=
uni_uca_opts_t opts;
clearbuf(&opts, 1);
if(do_cldr) {
  opts.tab = uni_DUCET_CLDR_mtab;
  opts.strs = uni_DUCET_CLDR_strs;
  opts.vartop = uni_DUCET_CLDR_var_top;
}
if(do_shift) {
  opts.var_mode = UNI_DUCET_VAR_MODE_SHIFTED;
  opts.level = 4;
} else {
  opts.var_mode = UNI_DUCET_VAR_MODE_NON_IGNORABLE;
  opts.level = 3; /* undocumented requirement, apparently */
}
opts.do_literal = 1;
char *lbuf = NULL, *s;
unsigned int lbuflen, llen;
uint32_t *buf, buf_len, max_buf, *key;
uint32_t *prev_key = NULL, *prev_buf = NULL, prev_buf_len = 0;
inisize(buf, (max_buf = 10));
#if 1
char *kbuf;
unsigned int kbuflen;
inisize(kbuf, (kbuflen = 800)); /* just make it big enough rather than resizing */
#endif
while(mfgets(&lbuf, &lbuflen, &llen, 0, stdin)) {
  if(!isxdigit(lbuf[0]))
    continue;
  buf[0] = strtol(lbuf, &s, 16);
  buf_len = 1;
  while(1) {
    while(isspace(*s))
      s++;
    if(!isxdigit(*s))
      break;
    if(buf_len == max_buf)
      resize(buf, (max_buf *= 2));
    buf[buf_len++] = strtol(s, &s, 16);
  }
  while(buf_len * 18 > max_buf)
    resize(buf, (max_buf *= 2));
  buf_len = uni_NFD(buf, buf_len);
  key = uni_str_uca_key(buf, buf_len, &opts);
#if 1
  s = kbuf;
  uint32_t *kp = key;
  *s++ = '[';
  /* lev 1 */
  while(*kp & 0xffff0000) {
    s += sprintf(s, "%04X ", (int)(*kp >> 16));
    if(!(*kp & 0xffff))
      break;
    s += sprintf(s, "%04X ", (int)(*kp & 0xffff));
    kp++;
  }
  kp++;
  *s++ = '|';
  *s++ = ' ';
  /* lev 2 */
  while(*kp & 0x7FC0000) {
    s += sprintf(s, "%04X ", (int)(*kp >> 18));
    if(!(*kp & 0x3FE00))
      break;
    s += sprintf(s, "%04X ", (int)((*kp >> 9) & 0x1ff));
    if(!(*kp & 0x1ff))
      break;
    s += sprintf(s, "%04X ", (int)(*kp & 0x1ff));
    kp++;
  }
  kp++;
  *s++ = '|';
  *s++ = ' ';
  /* lev 3 */
  while(*kp & (0x1f << 25)) {
    int i;
    for(i = 5; i >= 0; i--) {
      uint32_t v = (*kp >> 5 * i) & 0x1f;
      if(!v)
        break;
      s += sprintf(s, "%04X ", (int)v);
    }
    if(i >= 0)
      break;
    kp++;
  }
  kp++;
  if(opts.level > 3) {
    *s++ = '|';
    *s++ = ' ';
    /* lev 4 */
    while(*kp && *kp != 2)
      s += sprintf(s, "%04X ", (int)*kp++);
    kp++;
  }
  fputs(kbuf, stdout);
  /* literal */
  fputs("| ", stdout);
  while(*kp)
    printf("%04X ", (int)*kp++);
  puts("]");
  strcpy(s, "|]"); /* error in data file: assumes lit level is empty */
  /* comment in test file has UCA key at end of line, formatted as above */
  {
    uint32_t i;
    s = strrchr(lbuf, '[');
    strrchr(s, ']')[1] = 0;
    if(strcmp(s, kbuf)) {
      fputs("comment mismatch\n", stderr);
      for(i = 0; key[i]; i++)
        fprintf(stderr, "%08X ", key[i]);
      fprintf(stderr, "%s %s\n%s", kbuf, s, lbuf);
      exit(1);
    }
  }
#endif
  if(prev_key) {
    /* note: can't use memcmp except on big-endian machines */
    uint32_t i;
    for(i = 0; prev_key[i] && key[i]; i++)
      if(prev_key[i] != key[i])
        break;
    if(prev_key[i] > key[i]) {
      fflush(stdout);
      fprintf(stderr, "keys are out of order at %s\n", lbuf);
      exit(1);
    }
    if(uni_uca_strcmp(prev_buf, prev_buf_len, buf, buf_len, &opts) > 0) {
      fflush(stdout);
      fprintf(stderr, "strcmp error at %s\n", lbuf);
      exit(1);
    }
    free(prev_key);
    free(prev_buf);
  }
  prev_key = key;
  inisize(prev_buf, buf_len);
  prev_buf_len = buf_len;
  memcpy(prev_buf, buf, buf_len * 4);
}
if(prev_key)
  free(prev_key);
@

While implementation of the UCA algorithm with the raw tables is
possible, there are a few desirable functions which cannot be
accomplished with this data format:%
\footnote{In fact, the first two are the very reason this library
exists.  All of the prior information can be obtained from other
libraries, and in fact the sort keys can be obtained with Single UNIX
Specification functions.  However, no library to my knowledge gives
collation classes or a list of collation elements.  The lack of
locale-specific character classes is less important, since they can at
least be simulated using regular expressions.}

\begin{itemize}
\item Given a collation element, list all collation elements which
have the same key (i.e., determine its collation equivalence class).
This is required for regular expressions.
\item Given two collation elements, list all collation elements which
lie between them.  This is required for regular expressions.
\item Reorder elements.  This is required for static CLDR support.
\item Reorder element blocks.  This is required for static and dynamic
CLDR support.
\end{itemize}

At the minimum, these functions all require a lookup table ordered by
sort key rather than collation element.

<<Post-process DUCET>>=
free(strs);
free(ents);
@

<<FIXME>>=
DUCET needs more support
  default (derived value):
    Ill-formed: replace all ill-formed sequences with U+FFFD (or assume unassigned)
    Hangul: "simple":
      make all Vs and Ts 1-ignorable
      make L weight same as 1st LVT w/ that L
  full UCA implementation?
      - some levels may be sorted in reverse order, meaning that the
        keys themselves are reversed, not the ordering
CLDR options for run-time reordering
  reorder = space, punct, symbol, currency, digit, <script_id>
@

\chapter{Character Names}

It is difficult to come up with a good storage mechanism for character
names.  For one thing, there are a lot of them, even if the
algorithmically generated names are removed.  Another issue is that
the storage method should match the usage, so a variety of storage
mechanims may need to be supported.  Rather than supporting every use
case, the raw list of names can be easily retrieved and used in a
per-application static data generator similar to parse-ucd.  Instead,
a few common use cases are considered.

<<note>>=
CJK COMPATIBILITY IDEOGRAPH is algorithmically generated, but not a range
Multi-character Unicode names are from NamedSequences.txt
@

So, what do we do with all of these names?  The obvious thing is to
create a hash table to look up the code point using the name, and
another hash table to look up the name using the code point.  Since
the cost of comparing integers is small, a binary search might suffice
for the code point-to-name lookup.  For the hash table, we could
either use some random hash function and hash table implementation, or
we could generate a perfect hash function using [[gperf]].%
\footnote{After I wrote this, a program called [[cmph]]
(\url{http://sourceforge.net/projects/cmph}) caught my attention. This
generates minimal perfect hash functions in linear time (sub second
time for the entire Unicode name set!).  Its main problems are that it
generates data files rather than code (requiring changes to the
library to support code generation instead; see
\url{http://sourceforge.net/tracker/?func=detail\&aid=3590339\&group_id=126608\&atid=706189}),
and that it does not perform the final comparison step, essentially
just returning the integer index of a possible hashed string (easily
correctable).}
My initial implementation used [[gperf]], but generating a perfect
hash for the full Unicode set took 2 hours on my machine, and an
additional 1 hour to compile.  Instead, I decided to go with the first
thing I ever implemented in a compiler: a single hash table used for
all strings in the system, preloaded with known symbols.

Of course a hash function may not even be ideal.  For example, Unicode
regular expressions may allow searching for a name using a pattern.
For that, the ideal structure is probably a suffix table.  Of
course such structures can be much larger than the equivalent hash
table.

\section{Persistent Strings}

Note that GLib provides some of this functionality with its
[[GStringChunk]] and [[GQuark]] datatypes.  The main difference is
that my code stored a length, and GLib stores a terminating zero.  The
terminating zero (rarely) takes up less space, but disallows zeroes in
the strings. For my compiler-wide table, I also stored constant
strings, so zeroes were quite possible (and common when interacting
with C).  For the Unicode library, there is no need to support zeroes,
so the GLib implementation would save coding and debugging effort.
This is one good incentive to make this a GLib extension instead of a
separate library.  Of course the GLib Unicode functions would no
longer need to be duplicated, either.

My original compiler-wide string hash table had a variable-sized hash
table, and an array of all strings in the table.  The array index was
the \emph{string number}, which was the only thing remembered about
the string from that point on, unless the string needed to be printed.
Strings never needed to be compared lexically.  Only a single function
is needed to look up the string number; it will automatically add the
string if not present and [[autoadd]] is non-zero.  For printing, a
second function returns a pointer to the start of the string.  As a
convention, a zero-length string is always string number zero.  Zero
is also returned for a non-zero-length string if it is not present and
was not added.  For unique strings, it may also be desirable to add
data to store additional information.  In the Unicode name
application, for example, the Unicode code point could be stored.  Of
course it could also be stored in a separate table, but that would add
four bytes of overhead minimum per code point (i.e., the string
number) as well as longer lookup times (i.e., having to do a binary
search on the supplemental table).  To support storing information
directly in the string table, [[autoadd]] can be a negative number,
indicating the number of bytes to add before the string to store data.

<<Library [[uni]] Members>>=
uni_strs.o
@

\lstset{language=C}
<<Library [[uni]] headers>>=
#include "uni_strs.h"
@

<<uni_strs.h>>=
<<Common C Warning>>
#ifndef UNI_STRS_H
#define UNI_STRS_H

#include "uni_prop.h"
/* if 0-len string, use 0; don't call this function! */
/* if len == 0, function does strlen() for you */
/* 0 returned if len == 0 or !autoadd and not already there */
/* autoadd < 0 to make -autoadd bytes room before entry */
/* make sure the entry was actually added, though! */
uint32_t strno(const char *str, int len, int autoadd);

/* str is not 0-terminated!  pass non-NULL len if len desired */
const char *str_for(uint32_t no, int *len);

<<Master string table exports>>

#endif /* UNI_STRS_H */
@

<<uni_strs.c>>=
<<Common C Header>>
#include "uni_strs.h"
// static_proto

<<Master string table definitions>>

uint32_t strno(const char *str, int len, int autoadd)
{
  if(!len)
    len = strlen(str);
  if(!len)
    return 0;
  <<Convert [[str]]/[[len]] to string number>>
}

const char *str_for(uint32_t no, int *len)
{
  if(!no) {
    if(len)
      *len = 0;
    <<Return 0-len string>>
  }
  <<Convert [[no]] to string/[[len]]>>
}
@

For this hash table, though, I'll use a fixed-sized table at an
arbitrary size.  A 128K-entry table occupies one megabyte in memory
for a 64-bit system.  Nothing will be stored in the data cache for
very long for such a table.  Of course the comparison only needs to be
made on receiving strings, so it probably does not matter much.

As with my original hash tables, I am using a binary modulus.  I have
found very little difference in hash table performance with actual
program identifiers, and on some processors, there is a significant
performance benefit to using binary moduli.  The hash function simply
xors the length and all characters in the string after rotating the
previous hash value 3 bits left (there is no C function for this, so
equivalent code is used in the hope that the compiler will be smart
enough to figure it out; the compiler I'm using at least seems to
understand).  An xor is used instead of addition to avoid having to
deal with overflow.  The final hash value wraps any bits above 17 down
as well.  This is not done in the loop because there is no instruction
to rotate 17 bits on most processors, leaving the loop fast and the
slower 17-bit operation for the finishing touch.

<<Master string table exports>>=
#define HTAB_SIZE (128*1024)

#ifdef __GNUC__
__attribute__((unused))
#endif
static uint32_t hashfun(const char *nam, int len)
{
  uint32_t hash = len;

  while(--len >= 0)
    /* @<< 3 + @>> 29 should translate to a single rotate instruction */
    hash = ((hash @<< 3) + (hash @>> 29)) ^ *(unsigned char *)nam++;

  /* 17 * 2 == 34, so only need to do this once */
  /* / HTAB_SIZE should translate to >> log2(HTAB_SIZE) */
  /* % HTAB_SIZE should translate to & ~(HTAB_SIZE - 1) */
  return (hash / HTAB_SIZE ^ hash) % HTAB_SIZE;
}
@

The master string hash table requires static storage for strings
(i.e., storage which will not move during the string's lifetime), the
hash table itself, and the array of strings.  The array can store
offsets into the string storage rather than pure pointers, allowing
the strings to move if desired, and removing the need for 64-bit
pointers on 64-bit systems.  In addition, the hash table entries
themselves can be an index into the string array, giving them the same
benefits, as well as removing the need for a 64-bit hash bucket pointer.

<<Master string table exports>>=
extern uint32_t strhash[HTAB_SIZE];

typedef struct {
  uint32_t off; /* offset into string data */
  uint32_t nextent;
} hashent_t;
@

<<Known Data Types>>=
hashent_t,%
@

There are actually two sources of hash table entries:  built-in and
dynamically added.  The built-in strings should be just one large
array of hash table entries along with one large string buffer.  The
most efficient representation would be as arrays (i.e. C [[[]]]), but
in order to allow sharing the same routines with the programs which
build the arrays, I also allow representation as pointers, which
requires an additional read before use rather than a static address.
Rather than introduce this inefficiency into the compiler, I'll use a
[[#define]] to select, and build an extra binary for the alternative.
Similarly, the size of the string can be gotten with [[sizeof]] or
with an external integer, with the former being more efficient.
Taking the size directly only works if the string is included directly
in [[uni_strs.c]], though.

\lstset{language=make}
<<makefile.rules>>=
uni_strs-dyn.c: uni_strs.c cproto.h
	ln -sf $< $@
uni_strs-dyn.o uni_strs.c.static_proto: EXTRA_CFLAGS+=-DDYN_BUILTIN_STR
uni_strs.o uni_strs.c.static_proto: strs.gen
@

<<Clean temporary files>>=
rm -f uni_strs-dyn.[oc]
@

\lstset{language=C}
<<Master string table exports>>=
#ifdef DYN_BUILTIN_STR
extern const char *builtin_str;
extern int builtin_str_size;

extern const hashent_t *builtin_hashe;
extern int builtin_hashe_size;
#else
/* builtin_str and builtin_hashe are local to uni_strs.c */
#define builtin_str_size (sizeof(builtin_str) - 1) /* remove trailing C 0 */
#define builtin_hashe_size (sizeof(builtin_hashe)/sizeof(hashent_t))
#endif
@

<<Master string table definitions>>=
#ifndef DYN_BUILTIN_STR
#include "strs.gen"
#endif
@

The strings themselves are actually arbitrary binary data, so they
cannot be terminated by a sentinel as in normal C strings.  Instead,
the length needs to be stored along with the string.  My old string
routines stored the length before the string, so that the pointer
always points to the first character rather than the length.  This
seems harmless, so I'll do it here as well.  The actual format of the
length is a variable-length integer, with the high bit set on all but
the last byte.  This is obviously just to save space, since most
strings will only require one byte of length.  Since the length is
stored before the string, any additional data stored with the string
must be stored ahead of the length.  To assist in both parsing the
string length and finding a pointer to the space before it,
[[num_before]] can be used.

<<Master string table exports>>=
/*
 * strtab format:
 *  [<parm>] <len> * <utf8-str>
 *  len is big-endian variable-len, with high bit set for all bytes but first
 *  parm is dependent on string #, and interpreted based on that
 *  * indicates position of string offset
 */
/* 1st strtab (built-in) is contiguous string of length BUILTIN_STR_LEN */

/* len of str_for(n) is num_before(str_for(n)). */
/* length of len is returned in len */
#ifdef __GNUC__
__attribute__((unused))
#endif
static uint32_t num_before(const char *s, int *len)
{
  const unsigned char *p = (const unsigned char *)s;
  uint32_t ret = *--p & 0x7f;
  uint32_t shift = 0;
  while(*p-- > 0x7f)
    ret += (*p & 0x7f) << (shift += 7);
  if(len)
    *len = (int)((const unsigned char *)s - p) - 1;
  return ret;
}
@

<<Return 0-len string>>=
return "" + 1; /* in case num_before called on it */
@

The use of an array index or string offset to identify strings and
hash table entries eliminates the need to keep them at fixed
addresses.  This allows the use of simple dynamically resized arrays
for both.  However, as they grow, the expense of potentially copying
every single byte added so far to a newly allocated entry, as well as
the memory fragmentation caused by reallocation, may be too great.
Instead, they are stored in arrays of buffers.  The only resizable
arrays are the buffer pointers, which will grow very slowly.

<<Master string table definitions>>=
#define BUFARRAY_MIN 16 /* # of initial elements in indirect arrays */

#define STRBUF_LEN 65536 /* bytes */
static struct strbuf {
    uint16_t len;
    char buf[STRBUF_LEN];
} **ostrings = NULL;
static int nbufs = 0;

#define HBUF_LEN 1024 /* * sizeof(hashent_t) -> * 8 */
static struct hashebuf {
    uint16_t len;
    hashent_t ent[HBUF_LEN];
} **hashe = NULL;
static int nhashe = 0;
@

The separate storage for built-in and dynamically added strings
requires extra calculation.  Given an index, simply use a builtin
entry if the index is low enough, or subtract the number of builtin
entries and then convert to an indirect index plus an offset into the
buffer.   The other way around simply requires adding the built-in
size to any index returned.  Only the former conversion is presented
here, for hash table entries.  The same method must be applied to
builtin string offsets as well, though.

<<Master string table definitions>>=
static const hashent_t *he_for(uint32_t hn)
{
    if(hn < builtin_hashe_size)
        return &builtin_hashe[hn];
    hn -= builtin_hashe_size;
    return hashe[hn / HBUF_LEN]->ent + hn % HBUF_LEN;
}
@

First, we'll take care of the easy case.  Converting a string number
to a string requires looking up the hash entry, and then looking up
the string specified by its offset.  This routine is not for general
use, so there is no need to check that the string number is within
range first.

<<Convert [[no]] to string/[[len]]>>=
const hashent_t *he = he_for(no - 1);
no = he->off;
const char *ret;
if(no < builtin_str_size)
  ret = builtin_str + no;
else {
  no -= builtin_str_size;
  ret = ostrings[no / STRBUF_LEN]->buf + no % STRBUF_LEN;
}
if(len)
  *len = num_before(ret, NULL);
return ret;
@

The easy case for returning the string number is to find it in the
hash table and just return the hash entry's array index.  This is what
is actually stored in the hash table and the hash bucket next
pointers.

One potential optimization would be to move the found entry to the top
of the hash bucket, on the assumption that it will be accessed again
soon.  This may be revisited in the future, but for now, a simpler
loop structure is used with no modification of built-in hash entries.

<<Convert [[str]]/[[len]] to string number>>=
uint32_t h = hashfun(str, len);
int hen;
/* traverse hash bucket */
for(hen = strhash[h]; hen; ) {
  const hashent_t *he = he_for(hen - 1);
  int helen;
  const char *s = str_for(hen, &helen);
  if(len == helen && !memcmp(str, s, len))
    return hen;
  hen = he->nextent;
}
@

If the string was not found, it is automatically added.

<<Convert [[str]]/[[len]] to string number>>=
/* couldn't find it, so add it */
if(!autoadd)
  return 0;
@

If no strings have been previously added, the expandable arrays are
initialized.  This could have been done in some master initialization
routine instead, but it is not too expensive to do it here.

<<Convert [[str]]/[[len]] to string number>>=
if(!ostrings) {
  inisize(ostrings, BUFARRAY_MIN);
  clearbuf(ostrings, BUFARRAY_MIN);
  nbufs = BUFARRAY_MIN;
  inisize(hashe, BUFARRAY_MIN);
  clearbuf(hashe, BUFARRAY_MIN);
  nhashe = BUFARRAY_MIN;
}
@

Then, we need to compute the amount of memory actually needed for the
string.  In addition to the length of the string, the length of the
length and any requested additional storage must be allocated.

<<Convert [[str]]/[[len]] to string number>>=
uint32_t fulllen = 0, tl;
for(tl = len; tl > 0; tl >>= 7)
   fulllen++;
fulllen += len;
if(autoadd < 0)
  fulllen += -autoadd;
@

To actually store the string, we find the first buffer with enough
room.  If no such buffer exists, we add one.  Just in case the string
is too large to fit in any string buffer, the newly added buffer may
be enlarged to fit the new string.

<<Convert [[str]]/[[len]] to string number>>=
int i;
for(i = 0; i < nbufs && ostrings[i]; i++)
  if(STRBUF_LEN - ostrings[i]->len >= fulllen)
    break;
if(i == nbufs) {
  resize(ostrings, nbufs * 2);
  clearbuf(ostrings + nbufs, nbufs);
  nbufs *= 2;
}
if(!ostrings[i]) {
  if(fulllen > STRBUF_LEN) {
    ostrings[i] = malloc(sizeof(**ostrings) + fulllen - STRBUF_LEN);
    if(!ostrings[i]) {
      perror("string buffers");
      exit(1);
    }
  } else
    inisize(ostrings[i], 1);
  ostrings[i]->len = 0;
}
@

Then, the string is appended to the string buffer, leaving room before
it as needed.  The length is written in as well.  The newly added
string is added to the length, being careful not to overflow the
unsigned short, while allowing the length to equal or exceed 64k.
Since no added string could be just one byte (the only one-byte string
is the empty string), we indicate the buffer is full by stopping at
64k - 1.

<<Convert [[str]]/[[len]] to string number>>=
struct strbuf *sb = ostrings[i];
int off = builtin_str_size + i * STRBUF_LEN + sb->len + (fulllen - len);
unsigned char *p = (unsigned char *)sb->buf + sb->len + (fulllen - len);
memcpy(p, str, len);
while(len > 0) {
  *--p = len & 0x7f;
  len >>= 7;
  if(len)
    *p |= 0x80;
}
if(fulllen + ostrings[i]->len < STRBUF_LEN)
  ostrings[i]->len += fulllen;
else
  ostrings[i]->len = STRBUF_LEN - 1;
@

Next, we add the hash table entry for the string.  Room needs to be
made in the hash entry array for one more entry.

<<Convert [[str]]/[[len]] to string number>>=
for(i = 0; i < nhashe && hashe[i]; i++);
if(i > 0 && hashe[i-1]->len < HBUF_LEN)
  i--;
if(i == nhashe) {
  resize(hashe, nhashe * 2);
  clearbuf(hashe + nhashe, nhashe);
  nhashe *= 2;
}
if(!hashe[i]) {
  inisize(hashe[i], 1);
  hashe[i]->len = 0;
}
hen = builtin_hashe_size + i * HBUF_LEN + hashe[i]->len;
hashent_t *he = hashe[i]->ent + hashe[i]->len;
++hashe[i]->len;
@

Finally, we initialize the entry, link it into the hash bucket, and
update the hash table to point to it.  The index (plus one) is
returned as the string number.

<<Convert [[str]]/[[len]] to string number>>=
he->off = off;
he->nextent = strhash[h];
strhash[h] = hen + 1;
return hen + 1;
@

\section{Storing the Names}

Now that we have the ability to use a table of built-in strings, the
various name tables can be added.  A general string table generator is
needed, since the Unicode names are not the only built-in strings.

\lstset{language=make}
<<makefile.rules>>=
strs.gen: mkstrs <<Built-in string table components>>

	<<Generate built-in string table using [[mkstrs]]>>
	<<Post-process built-in string table>>
@

<<Plain Built Files>>=
strs.gen strs-hash.gen strs-strs.gen \
@

<<C Build Executables>>=
mkstrs \
@

\lstset{language=C}
<<mkstrs.c>>=
<<Common C Header>>
#include "uni_strs.h"

<<[[mkstrs]] definitions>>

int main(int argc, const char **argv)
{
  <<Generate built-in string table>>
}
@

One of the things useful for the character tables is to store the
integer code point along with the name.  This requires, of course,
that all names be unique, since only one number can be stored.  This
is already required by the Unicode standard, so that is not a problem.
In addition, as mentioned above, it might save space to store Unicode
names as individual words, rather than full strings.

\lstset{language=make}
<<makefile.vars>>=
MKSTRS_C := mkstrs.c uni_strs.c
MKSTRS_PROTO := $(MKSTRS_C:%=%.static_proto) cproto.h
@

<<makefile.rules>>=
mkstrs.c.static_proto: EXTRA_CFLAGS+=-DDYN_BUILTIN_STR

mkstrs: $(MKSTRS_C)
	mkdir cp.$$$$; (cd cp.$$$$; touch $(MKSTRS_PROTO)); trap "rm -rf cp.$$$$" 0; \
	$(CC) $(CFLAGS) $(EXTRA_CFLAGS) -Icp.$$$$ $(LDFLAGS) -DDYN_BUILTIN_STR -o $@ $^
@

\lstset{language=C}
<<[[mkstrs]] definitions>>=
#ifdef DYN_BUILTIN_STR /* quiet errors from cproto */
const char *builtin_str;
int builtin_str_size;
const hashent_t *builtin_hashe;
int builtin_hashe_size = 0;
#endif

uint32_t strhash[HTAB_SIZE];
@

There are three outputs of the program.  First is the built-in strings
it reads from the input file(s), in the order given, as C string
constants, one per line (automatically concatenated into a single
string constant by the C compiler).  Next, rather than printing the
hash structure, each entry is printed as a pair of hash value and
string offset.  This allows the outputs to be produced on the fly, as
entries are read in.  A separate program can collect the pairs of
numbers into a hash table.  Finally, in order to use the string
constants in the program without having to look them up first,
[[#define]] statements can be generated for the strings.  The first
string's index and the last string's index can be generated, as well
as an individual index for each string, named after a safe version of
the string itself.  The output files, as well as the names of the
[[#define]]s are all set by command line options. The files can be
either created or appended to, in case of adding to an existing set of
built-in strings.

<<Generate built-in string table>>=
<<[[mkstrs]] option variables>>
int exit_loop = 0;
while(--argc > 0 && !exit_loop && *(*++argv) == '-' && (*argv)[1])
  switch((*argv)[1]) {
    <<Process [[mkstrs]] option>>
    case '-':
      exit_loop = 1;
      break;
  }
@

<<[[mkstrs]] option variables>>=
const char *print_def = NULL, *enum_prefix = NULL,
           *out_fname = "-", *out_fmode = "w",
           *def_fname = "-", *def_fmode = "w",
           *hash_fname = "-", *hash_fmode = "w";
@

<<Process [[mkstrs]] option>>=
case 'o':
  out_fname = *argv + 2;
  if(*out_fname == '+') {
    out_fmode = "a";
    out_fname++;
  }
  break;
case 'd':
  def_fname = *argv + 2;
  if(*def_fname == '+') {
    def_fmode = "a";
    def_fname++;
  }
  break;
case 'h':
  hash_fname = *argv + 2;
  if(*hash_fname == '+') {
    hash_fmode = "a";
    hash_fname++;
  }
  break;
case 'D':
  print_def = *argv + 2;
  break;
case 'a':
  enum_prefix = *argv + 2;
  break;
@

In addition to appending to existing files, multiple input sources are
supported by supplying an initial string and hash table entry number
offset.

<<[[mkstrs]] option variables>>=
uint32_t pos = 1, spos = 0;
@

<<Process [[mkstrs]] option>>=
case 'l':
  pos = strtol(*argv + 2, NULL, 0) + 1;
  break;
case 'L':
  spos = strtol(*argv + 2, NULL, 0);
  break;
@

The remainder of the command-line arguments specify input files.  If
none are specified, standard input is used.

<<Generate built-in string table>>=
int do_stdin = !argc;
@

When merging string tables, the old string table and hash table need
to be read first.  The option to merge tables is selected by using the
append mode for the output files, and specifying a hash entry offset
for validation.  The number of entries in both the string and hash
table files should be equal to the supplied number.  The string offset
need not be supplied, as it is harder to get at (the [[-D]] option
gives the hash entry offset).

<<Generate built-in string table>>=
/* if appending to strings & hash, read old strings */
/* when done, open strings output */
int read_strings = pos > 1 &&
    out_fmode[0] == 'a' && hash_fmode[0] == 'a' &&
    strcmp(out_fname, "-") && strcmp(hash_fname, "-") &&
    strcmp(out_fname, hash_fname) && strcmp(def_fname, hash_fname) &&
    strcmp(out_fname, def_fname);
FILE *in_f;
int lbuf_len = 1024;
char *lbuf;
inisize(lbuf, lbuf_len);
if(read_strings) {
  if(!(in_f = fopen(out_fname, "r"))) {
    perror(out_fname);
    exit(1);
  }
  /* builtin_str is const, so read into regular char * first */
  char *prevstr = NULL;
  /* also, track allocated length rather than actual length */
  int prevstr_size = 0;
  inisize(prevstr, (prevstr_size = 1024));
  /* and track current insertion location instead */
  char *d = prevstr;
  while(1) {
    <<Read a full line into [[lbuf]]>>
    <<Parse and append old builtin string>>
  }
  fclose(in_f);
  builtin_str = prevstr;
  spos = builtin_str_size = d - prevstr;
}
@

<<Read a full line into [[lbuf]]>>=
int off = 0;
while(1) {
  if(!fgets(lbuf + off, lbuf_len - off, in_f) || !*lbuf)
    break;
  off += strlen(lbuf + off);
  if(lbuf[off - 1] == '\n')
    break;
  lbuf_len *= 2;
  resize(lbuf, lbuf_len);
}
if(!off)
  break;
@

The format of each line is a string constant.  The constant may be
followed by a comment, such as the string number and offset.  Rather
than parse an arbitrary C string constant, only the output format is
supported.  The output format is plain ASCII characters unescaped,
backslash and quote characters backslash-escaped, and anything else
escaped as a 3-digit octal escape.

<<[[mkstrs]] definitions>>=
typedef unsigned char uchar;
static void put_str(const char *buf, int len, FILE *f)
{
  for(; len--; buf++) {
    if(*buf == '\\')
      fputs("\\\\", f);
    else if(*buf == '"')
      fputs("\\\"", f);
    else if(*buf < ' ' || *buf > '~')
      fprintf(f, "\\%03o", (uchar)*buf);
    else
      fputc(*buf, f);
  }
}
@

<<Known Data Types>>=
uchar,%
@

<<Parse and append old builtin string>>=
if(*lbuf != '"') /* lines that aren't strings are ignored */
  continue;
char *s = lbuf + 1;
while(1) {
  for( ; *s && *s != '"' && (d - prevstr) < prevstr_size; s++, d++) {
    if(*s == '\\') {
      if(s[1] == '\\' || s[1] == '"')
        *d = *++s;
      else if(s[1] && s[2] && s[3]) {
        char c = s[4];
        s[4] = 0;
        *d = strtol(s + 1, NULL, 8);
        s[4] = c;
        s += 3;
      } else {
        fprintf(stderr, "Invalid previous string file\n");
        exit(1);
      }
    } else
      *d = *s;
  }
  /* here, either at end of string or end of buffer space */
  if(*s == '"')
    break;
  if(d == prevstr + prevstr_size) {
    int l = d - prevstr;
    resize(prevstr, (prevstr_size *= 2));
    d = prevstr + l;
    continue;
  }
  /* or end of input... */
  fprintf(stderr, "Invalid previous string file\n");
  exit(1);
}
@

The old hash entries need to be read in as well, in order to determine
string locations at the very least.  The format of the file is a pair
of numbers per line, separated by a space.  The first is the hash
value of the string, and the second is its offset.

<<Generate built-in string table>>=
if(read_strings) {
  if(!(in_f = fopen(hash_fname, "r"))) {
    perror(hash_fname);
    exit(1);
  }
  hashent_t *hashe = NULL; /* writable version of builtin_hashe */
  int hashe_size = 1024; /* its size */
  inisize(hashe, hashe_size);
  unsigned long h, str;
  uint32_t hen = 0;
  while(fscanf(in_f, "%lu %lu\n", &h, &str) == 2) {
    if(hen == hashe_size) {
      hashe_size *= 2;
      resize(hashe, hashe_size);
    }
    hashent_t *he = &hashe[hen];
    he->off = str;
    h %= HTAB_SIZE;
    he->nextent = strhash[h];
    strhash[h] = ++hen;
  }
  fclose(in_f);
  if(pos != hen + 1) {
    fprintf(stderr, "Invalid hash file; expected %u entries but got %u\n",
            pos - 1, hen);
    exit(1);
  }
  builtin_hashe_size = hen;
  builtin_hashe = hashe;
}
@

Now that the old data has been read in, the output files can be opened
for writing.  Technically, they could have used "r+" to avoid a second
open, but this way is easier to write.  In case any file names are
repeated, the file is only opened once.  The only reason this would
probably happen is if debugging, though.

<<Generate built-in string table>>=
/* open strings output */
FILE *out_f;
if(out_fname && *out_fname && strcmp(out_fname, "-")) {
  out_f = fopen(out_fname, out_fmode);
  if(!out_f) {
    perror(out_fname);
    exit(1);
  }
} else
  out_f = stdout;
/* open definitions output */
FILE *def_f;
if(def_fname && *def_fname && strcmp(def_fname, "-")) {
  if(!strcmp(def_fname, out_fname))
    def_f = out_f;
  else {
    def_f = fopen(def_fname, def_fmode);
    if(!def_f) {
      perror(def_fname);
      exit(1);
    }
  }
} else
  def_f = stdout;
/* open hash output */
FILE *hash_f;
if(hash_fname && *hash_fname && strcmp(hash_fname, "-")) {
  if(!strcmp(hash_fname, out_fname))
    hash_f = out_f;
  else if(!strcmp(hash_fname, def_fname))
    hash_f = def_f;
  else {
    hash_f = fopen(hash_fname, hash_fmode);
    if(!hash_f) {
      perror(hash_fname);
      exit(1);
    }
  }
} else
  hash_f = stdout;
@

The format of the input file is one string per line.  In order to
specify a value to insert before the string, an integer (in C integer
format) may be specified as well.  It is separated from the string by
a custom character.

<<[[mkstrs]] option variables>>=
char sep = 0;
@

<<Process [[mkstrs]] option>>=
case 'v':
  sep = isdigit((*argv)[2]) ? atoi(*argv + 2) : (*argv)[2];
  break;
@

One difficulty is that the built-in strings are massive: nearly 700K
for the Unicode names alone.  How could this be reduced, though?  One
way would be to 5-bit-encode the Unicode names, saving 30\% without
much effort.  However, space on modern machines is not at such a
premium any more, so this probably just wastes time.  Similarly, the
Unicode names can be split out as words, saving nearly 40\% in string
storage space, at the expense of adding 10000 more entries into the
hash table (for a total savings of 25\%, actually).  Again, this is
probably not worth it.  However, I may change my mind some time, so
this program supports splitting Unicode names into words.

<<[[mkstrs]] option variables>>=
int do_words = 0;
@

<<Process [[mkstrs]] option>>=
case 'w':
  /* NOTE: this option is for Unicode character name separation only! */
  /*  - words are separated by spaces */
  /*  - it assumes strings are ASCII (no high bits set!) */
  /*  - the words it adds are not usable for anything else */
  /*    unless already added (so load order is important) */
  do_words = 1;
  break;
@

Before spitting out the first string, we can add a definition to the
file indicating the start of the range.  However, if splitting words,
there may be hash entries for the words themselves first, so only
print it when not doing words.  In addition, it might help debugging
to print a comment before the first string in the output file as well.

<<Generate built-in string table>>=
if(print_def && !do_words) {
  fprintf(def_f, "#define %s_FIRST %u\n", print_def, pos);
  fprintf(out_f, "/* %s_FIRST */\n", print_def);
}
@

Then, we need to loop over all strings read from the inputs.

<<Generate built-in string table>>=
/* now, loop through new strings */
<<Prepare to loop over new builtin strings>>
do { /* for every input file */
  /* open next input file (stdin if none) */
  if(do_stdin || !strcmp(*argv, "-"))
    in_f = stdin;
  else {
    in_f = fopen(*argv, "r");
    if(!in_f) {
      perror(*argv);
      exit(1);
    }
  }
  if(!do_stdin)
    argv++;
  while(1) {
    <<Read a full line into [[lbuf]]>>
    <<Process a new built-in string>>
  }
} while(!do_stdin && --argc);
@

The format is not actually as described before:  in order to allow any
strings to be added, backslash escapes are supported as well.  These
are a simple escaped backslash, an escaped value separator, or a
two-digit hexadecimal character code.  So, the first thing to do is to
parse the string, replacing backslash-escapes and locating the value,
if present.

<<Process a new built-in string>>=
char *s, *d;
uint32_t val = 0;
for(s = d = lbuf; *s; s++, d++) {
  if(*s == '\\') {
    if(*++s == '\\' || *s == sep)
      *d = *s;
    else {
      if(!isxdigit(s[1]) || !isxdigit(s[2])) {
        fprintf(stderr, "Bad line: %s\n", lbuf);
        exit(1);
      }
      char t = s[3];
      s[3] = 0;
      *d = strtol(s, &s, 16);
      *s = t;
    }
  } else if(*s == sep) {
    val = strtol(s + 1, NULL, 0);
    break;
  } else if(*s == '\r' || *s == '\n')
    break;
}
uint32_t len = (uint32_t)(d - lbuf);
/* now, lbuf/len is string, val is value */
@

Now, if not in word mode, we simply spit out the string and move on.
The value and length are of course output first, and the string position
based on how many characters were output for the prefix. Rather than
check for ASCII characters, the length and code number are output as
octal escapes only.

<<[[mkstrs]] definitions>>=
static int put_ln(uint32_t n, FILE *f)
{
  uint32_t vl = 0, v[5], ret;
  while(n > 0x7f) {
    v[vl++] = n & 0x7f;
    n >>= 7;
  }
  ret = vl + 1;
  fprintf(f, "\\%03o", n);
  while(vl > 0)
    fprintf(f, "\\%03o", v[--vl] + 128);
  return ret;
}
@

<<Process a new built-in string>>=
if(!do_words) {
  <<Spit out built-in string>>
  /* spos is updated by prefix above */
  spos += len;
  pos++;
  continue;
}
@

<<Spit out built-in string>>=
fputc('"', out_f);
if(sep)
  spos += put_ln(val, out_f);
spos += put_ln(len, out_f);
put_str(lbuf, len, out_f);
if(sep)
  fprintf(out_f, "\" /* %u/%u%c%u */\n", spos, pos, sep, val);
else
  fprintf(out_f, "\" /* %u/%u */\n", spos, pos);
@

In addition to the string file, the definitions file may need
updating.  The format of the definition name is the prefix, followed
by an underscore, followed by the characters of the string.  If a
character is a space, it is output as double-underscores.  Otherwise,
if it is not a valid C identifier character, it is printed as an
underscore, followed by a lower-case x, follwed by the 2-digit hex
code.  This does not guarantee uniqueness, but it does guarantee that
the identifier is valid C, and in combination with the prefix, can
give a good assurance of uniqueness.

<<[[mkstrs]] definitions>>=
static void pr_enc(const char *s, int len, FILE *f)
{
  while(--len >= 0) {
    if(*s == ' ')
      fputs("__", f);
    else if(*s < ' ' || *s > '~' || (!isalnum(*s) && *s != '_'))
      fprintf(f, "_x%02x", *s);
    else
      fputc(*s, f);
    ++s;
  }
}
@

<<Spit out built-in string>>=
if(enum_prefix) {
  fprintf(def_f, "#define %s_", enum_prefix);
  pr_enc(lbuf, (int)(d - lbuf), def_f);
  fprintf(def_f, " %u\n", pos);
}
@

Finally, the hash table entry needs to be printed.

<<Spit out built-in string>>=
fprintf(hash_f, "%u %u\n", hashfun(lbuf, len), spos);
@

For word mode, processing strings is a little more complicated.
Instead of spitting out words as they arrive, they are added to the
hash table.  The original string is then converted to an encoded form.
The encoding retains spaces, but converts other contiguous strings
into their encoded string number.  Since encoding a single-word
character this way could not possible make it shorter, these are left
unencoded.

<<Process a new built-in string>>=
/* from this point on, it's word mode */
char *we, *ws = lbuf;
for(ws = lbuf; ws < lbuf + len; ws = we + 1) {
  for(we = ws; we < lbuf + len; we++)
    if(*we == ' ')
      break;
  if(we == ws)
    continue;
  int wlen = (int)(we - ws);
  /* don't bother if entire string is one word */
  if(wlen == len)
    break;
  <<Add word to string table and encode>>
}
@

In fact, the encoding of the words requires at least two bytes per
word, so words less than three characters are pointless to encode.

<<Add word to string table and encode>>=
if(wlen <= 2)
  continue; /* too short to bother */
@

Since there is no guarantee that substrings of Unicode names are not
also Unicode names, space needs to be allocated in front of the newly
added word to allow for a value.  Rather than mess with efficient
coding, the value is placed as a 32-bit value, along with a flag
indicating presence.  All newly added words get this.  When writing
the strings out, the words without values will be printed before the
words with values.  This will cause the string numbers to change.  In
order to support this, the updated string number is stored before the
string as a 32-bit number as well.  This means that 9 bytes need to be
allocated before any newly added word.  The first byte is the value
flag; the next four are the value, and the rest are for the updated
string number.  For newly added words, there is never a value
(although it may be added later).

<<Prepare to loop over new builtin strings>>=
int cur_hen = pos, old_hen = pos;
@

<<Add word to string table and encode>>=
/* auto-add and leave 9 bytes of room before word */
int str = strno(ws, wlen, -9);
/* if auto-added, set "value" flag to 0 */
if(str == cur_hen) {
  s = (char *)str_for(str, NULL);
  int ll; /* skip strlen */
  num_before(s, &ll);
  s -= ll + 1;
  *s = 0;
  cur_hen++;
}
@

Since UTF-8 does not cover all 32-bit values, and in order to avoid
conflict with unencoded words and spaces, the encoding of the
string number is a little-endian representation, with 7 bits per byte
and the high bit set on each byte.  This should not increase
the length, but just in case, we'll compute the replacement into a
buffer, and shift the remainder of the string to make room.

<<Add word to string table and encode>>=
/* encode strno(word) and replace word w/ encoding */
/* FIXME: expand lbuf if needed (not needed for Unicode 6.0) */
char ebuf[5];
int eblen;
uint32_t strout = str;
for(eblen = 0; eblen < 5 && strout; eblen++, strout >>=7)
  ebuf[eblen] = 0x80 | (strout & 0x7f);
memmove(ws + eblen, we, len - (int)(we - lbuf));
memcpy(ws, ebuf, eblen);
len -= wlen - eblen;
we -= wlen - eblen;
@

Finally, the encoded form (or unencoded if there was just one word) is
added to the string table.  Again, 9 bytes need to be added if the
string is newly added.  If it isn't, it is assumed to match a word
that was previously added, and so it also has 9 bytes of space.  If
that assumption does not hold, there is a big problem, so print the
unencoded bad string and exit.  To print the unencoded string, the
encoded string numbers are extracted and expanded.

<<Process a new built-in string>>=
int str = strno(lbuf, len, -9);
if(str < old_hen) {
  pr_lw(lbuf, len, stderr);
  fprintf(stderr, ": short name conflict (already there)\n");
  exit(1);
}
@

<<[[mkstrs]] definitions>>=
static void pr_lw_mayenc(const char *buf, int len, FILE *f, int enc)
{
  const char *s;
  
  while(1) {
    for(s = buf; len && !(*s & 0x80); s++, len--);
    if(s != buf) {
      if(enc)
        pr_enc(buf, (int)(s - buf), f);
      else
        fwrite(buf, (int)(s - buf), 1, f);
    }
    if(!len)
      return;

    uint32_t str;

    str = *s & 0x7f;
    int shift = 7;
    while(--len && (*++s & 0x80)) {
      str += (uint32_t)(*s & 0x7f) << shift;
      shift += 7;
    }
    buf = s;
    int l;
    s = str_for(str, &l);
    if(enc)
      pr_enc(s, l, f);
    else
      fwrite(s, l, 1, f);
  }
}
#define pr_lw(a,b,c) pr_lw_mayenc(a,b,c,0)
#define pr_lw_enc(a,b,c) pr_lw_mayenc(a,b,c,1)
@

We can now modify the 9 prefix bytes regardless of whether or not this
was added, but we still need to keep track of how many have been added
in order to detect word additions.  The value flag is set depending on
whether or not there is a value.  Since this is for Unicode character
names, which always have values, this flag also indicates whether it
is a full name or just a word.

<<Process a new built-in string>>=
int ll;
s = (char *)str_for(str, NULL);
num_before(s, &ll);
s -= ll + 1;
if(str == cur_hen) {
  *s = 0;  
  cur_hen++;
}
if(*s) { /* there should not already be a value there */
  pr_lw(lbuf, len, stderr);
  fprintf(stderr, ": short name conflict (already processed)\n");
  exit(1);
}
*s = 1;
if(sep) {
  *--s = val & 0xff;
  val >>= 8;
  *--s = val & 0xff;
  val >>= 8;
  *--s = val & 0xff;
  val >>= 8;
  *--s = val & 0xff;
}
@

After adding all of the strings to string table, they need to be
printed.

<<Generate built-in string table>>=
if(do_words) {
  <<Print word mode builtin strings>>
}
@

The standalone words need to be printed separately from the real
words, so that the range of string numbers printed for the real
numbers is accurate.  While it may seem that it does not matter which
order these are printed in, the words must be printed first.  As they
are added, their string number is updated, and these updated numbers
must be known before an encoded string using that word can be printed.
They are stored in the four bytes beyond the value.

<<Print word mode builtin strings>>=
uint32_t i;
for(i = old_hen; i < cur_hen; i++) {
  const char *str = str_for(i, NULL);
  int ll, len = num_before(str, &ll);
  if(str[-ll - 1]) /* has value */
    continue;
  fputc('"', out_f);
  spos += put_ln(len, out_f);
  put_str(str, len, out_f);
  fprintf(out_f, "\" /* %u/%u>%u */\n", spos, i, pos);
  fprintf(hash_f, "%u %u\n", hashfun(str, len), spos);
  spos += len;
  uint32_t npos = pos++;
  char *s = (char *)str - ll - 5;
  *--s = npos & 0xff;
  npos >>= 8;
  *--s = npos & 0xff;
  npos >>= 8;
  *--s = npos & 0xff;
  npos >>= 8;
  *--s = npos & 0xff;
}
@

Now that the words are all printed, the first element of the range is
known.

<<Print word mode builtin strings>>=
if(print_def) {
  fprintf(def_f, "#define %s_FIRST %u\n", print_def, pos);
  fprintf(out_f, "/* %s_FIRST */\n", print_def);
}
@

All encoded strings are printed next.  However, they must be updated
to new word locations first.  Since some may not really be encoded,
and therefore may be used by other words, the updated string number is
stored in them as well.

<<Print word mode builtin strings>>=
for(i = old_hen; i < cur_hen; i++) {
  const char *str = str_for(i, NULL);
  int ll, len = num_before(str, &ll);
  if(!str[-ll - 1]) /* has no value */
    continue;
  fputc('"', out_f);
  uint32_t val = 0;
  if(sep) {
    val =
      (uint32_t)(uchar)str[-ll - 2] +
      ((uint32_t)(uchar)str[-ll - 3] << 8) +
      ((uint32_t)(uchar)str[-ll - 4] << 16) +
      ((uint32_t)(uchar)str[-ll - 5] << 24);
    spos += put_ln(val, out_f);
  }
  <<Reencode builtin string [[str]] into [[lbuf]]/[[len]]>>
  spos += put_ln(len, out_f);
  put_str(lbuf, len, out_f);
  if(sep)
    fprintf(out_f, "\" /* %u/%u%c%u ", spos, pos, sep, val);
  else
    fprintf(out_f, "\" /* %u/%u ", spos, pos);
  /* print unencoded string */
  pr_lw(str, num_before(str, NULL), out_f);
  fputs(" */\n", out_f);
  fprintf(hash_f, "%u %u\n", hashfun(str, len), spos);
  if(enum_prefix) {
    fprintf(def_f, "#define %s_", enum_prefix);
    pr_lw_enc(str, num_before(str, NULL), def_f);
    fprintf(def_f, " %u\n", pos);
  }
  spos += len;
  uint32_t npos = pos++;
  d = (char *)str - ll - 5;
  *--d = npos & 0xff;
  npos >>= 8;
  *--d = npos & 0xff;
  npos >>= 8;
  *--d = npos & 0xff;
  npos >>= 8;
  *--d = npos & 0xff;
  npos++;
}
@

To reencode the string, all substrings with the high bit set must be
extracted, converted to numbers, looked up, and replaced with the
number that was stored 6-9 bytes before the length.

<<Reencode builtin string [[str]] into [[lbuf]]/[[len]]>>=
const char *s, *e = str + len;
char *d = lbuf;
for(s = str; s < e; s++) {
  if(!(*s & 0x80))
    *d++ = *s;
  else {
    uint32_t os = *s & 0x7f;
    int shift = 7;
    while(++s < e && (*s & 0x80)) {
      os += (uint32_t)(*s & 0x7f) << shift;
      shift += 7;
    }
    s--;
    if(os >= old_hen) {
      int oll;
      const char *ostr = str_for(os, NULL);
      num_before(ostr, &oll);
      os = (uint32_t)(uchar)ostr[-oll - 6] +
           ((uint32_t)(uchar)ostr[-oll - 7] << 8) +
           ((uint32_t)(uchar)ostr[-oll - 8] << 16) +
           ((uint32_t)(uchar)ostr[-oll - 9] << 24);
    }
    while(os > 0) {
      *d++ = 0x80 | (os & 0x7f);
      os >>= 7;
    }
  }
}
len = d - lbuf;
@


Finally, the last string has been printed, so spit out its number if
required.

<<Generate built-in string table>>=
if(print_def) {
  fprintf(def_f, "#define %s_LAST %u\n", print_def, pos - 1);
  fprintf(out_f, "/* %s_LAST */\n", print_def);
}
return 0;
@

Now that the string buffer is generated, we need to generate a hash
table.  This generates both the initial hash entry array and the hash
table itself.

\lstset{language=make}
<<C Build Executables>>=
mkstrs-hash \
@

<<Built-in string table components>>=
mkstrs-hash \
@

<<makefile.rules>>=
mkstrs-hash: mkstrs-hash.c
	mkdir cp.$$$$; touch cp.$$$$/{cproto.h,mkstrs-hash.c.static_proto}; \
	   trap "rm -rf cp.$$$$" 0; \
	$(CC) $(CFLAGS) $(EXTRA_CFLAGS) -Icp.$$$$ $(LDFLAGS) -o $@ $^
@

\lstset{language=C}
<<mkstrs-hash.c>>=
<<Common C Header>>
#include "uni_strs.h"
@

In order to generate the hash table, it needs to be built in memory.
Because the buckets are built by modifying only the newly added entry,
there is no need to keep any entries once they are output.  The only
additional storage kept is a count of how many entries were added to
each slot, so that an estimate of table badness can be printed on
completion.

<<mkstrs-hash.c>>=
uint32_t strhash[HTAB_SIZE], hdepth[HTAB_SIZE];
@

The mainline prints a header for the hash entry array for the static
version (requiring sed to alter it for non-static versions).  It then
reads every line from standard input, assuming it is the hash output
from the previous stage, and prints the hash table entry.  When done
with that, it dumps the hash table itself, as well as any statistics
it accumulated.

<<mkstrs-hash.c>>=
int main(void)
{
  <<Print hash entries>>
  <<Print hash table>>
  <<Print hash statistics>>
  return 0;
}
@

<<Print hash entries>>=
puts("const hashent_t builtin_hashe[] = {");
int he;
unsigned long h, str;
for(he = 0; scanf("%lu %lu", &h, &str) == 2; he++) {
  h %= HTAB_SIZE;
  if(he)
    fputs(",\n", stdout);
  printf("\t{%lu, %u}", str, strhash[h]);
  strhash[h] = he + 1;
  ++hdepth[h];
}
puts("\n};");
@

<<Print hash table>>=
fputs("uint32_t strhash[HTAB_SIZE] = {", stdout);
int i;
for(i = 0; i < HTAB_SIZE - 1; i++) {
  if(!(i & 15))
    fputs("\n\t", stdout);
  else
    putchar(' ');
  printf("%d,", strhash[i]);
}
printf(" %d\n"
       "};\n", strhash[i]);
@

<<Print hash statistics>>=
int maxd = 1, nhid = 0, nfill = 0;
for(i = 0; i < HTAB_SIZE; i++) {
  if(maxd < hdepth[i])
    maxd = hdepth[i];
  if(hdepth[i]) {
    nhid += hdepth[i] - 1;
    ++nfill;
  }
}
printf("/* max depth %d num hidden %d of %d; full %d/%d (%.2f%%) */\n",
       maxd, nhid, he, nfill, HTAB_SIZE, (double)nfill/(double)HTAB_SIZE*100);
@

Now we can start generating string tables.

Multi-character entries will be extracted from another table.  In
order to support this, multi-character entries have their code point
list replaced with a sequence number (the table entry index plus the
max Unicode code point), and the code points are printed as well, in a
format that is easily distinguishable from normal entries (i.e., with
a dash in the name).

<<Prepare for XML filter>>=
int nmulti = 0;
@

<<Print XML character list entry>>=
l = strchr(++s, ',');
if(!l)
  printf("%s,%s", buf, s);
else {
  printf("%s,0x%04X\n", buf, nmulti+0x110000);
  printf("%04X-0x%04X,0x%04X\n", nmulti,
         (int)strtol(s, NULL, 0), 
         (int)strtol(l + 1, NULL, 0));
  ++nmulti;
}
@

The main string table consists of all Unicode character names.
Any multi-character sequences are converted to an offset from
the maximum code point so that the sequences can be stored in a
separate table.

\lstset{language=make}
<<Built-in string table components>>=
charnames.uni charnames.unia charnames.uniseq \
@

<<makefile.config>>=
# Set to -w to use word mode (needs more work)
#UNI_NAME_WORD=-w
@

<<Generate built-in string table using [[mkstrs]]>>=
( \
  cat -n charnames.uniseq | while read -r n s; do \
    echo "$${s%%,*},$$((n+0x10ffff))"; \
  done; \
  cat charnames.uni{,a}; \
) | tr 'A-Z\055' 'a-z ' | \
  ./mkstrs $(UNI_NAME_WORD) -v, -DUNI_CHARS -hstrs-hash.gen -ostrs-strs.gen \
           -dbuiltin-strings.h
@

<<Clean temporary files>>=
rm -f builtin-strings.h
@

<<Post-process built-in string table>>=
./mkstrs-hash <strs-hash.gen >$@
echo "const char builtin_str[] =" >>$@
echo '#include "strs-strs.gen"' >>$@
echo ";" >>$@
@

<<makefile.rules>>=
builtin-strings.h: strs.gen
	touch builtin-strings.h
strs-strs.gen: strs.gen
	touch strs-strs.gen
@

The automatically generated names were not included in the above
lists.  In order to make it easier to look them up in the name-to-code
direction, their prefixes are added to the name table.  In addition, a
range table is added to [[chartypes]] to support the reverse lookup.

First, we'll generate a listing of the ranges, similar to the other
listings.

<<Build Script Executables>>=
mkunirng \
@

<<Plain Built Files>>=
charnames.unirng \
@

<<makefile.rules>>=
charnames.unirng: $(UNIDATA) mkunirng
	./mkunirng $(UNIDATA) >$@
@

\lstset{language=sh}
<<mkunirng>>=
#!/bin/sh
<<Common NoWeb Warning>>

@

There are three groups of automatically generated character names:
the Hangul syllables, the other ranges in UnicodeData.txt, and CJK
COMPATIBILITY IDEOGRAPH groups.  The first has a slightly more
involved generation algorithm; the other two simply append their code
point in hex after a dash.  What sets the CJK COMPATIBILITY IDEOGRAPH
code points apart is that they are listed individually in
UnicodeData.txt rather than as ranges.

The routine grabs both types at once, and outputs a start token (the
prefix followed by a caret) at the start of any range, and an end
token (the prefix followed by a dollar sign) at the end.

<<mkunirng>>=
cut -d\; -f1-2 $1 | egrep '<[^c]|CJK COMPATIBILITY IDEOGRAPH' | \
  dd conv=ucase 2>/dev/null | (
    last=; lastb=
    while IFS=\;, read a b c; do
      an=$((0x$a))
      b="${b%-*}"
      if [ -n "$last" ]; then
        case "$c" in
          *">") echo "$lastb \$,$last"; last= ;;
          *) last=$((last+1))
             if [ $last -ne $an -o "$lastb" != "$b" ]; then
               echo "$lastb \$,$((last-1))"
               last=$an
               lastb="$b"
               echo "$b ^,$an"
             fi ;;
        esac
      fi
      case "$c" in
        " FIRST>") echo "${b#<} ^,0x$a" ;;
        *">") echo "${b#<} \$,0x$a" ;;
        *) if [ -z "$last" ]; then
             last=$an
             lastb="$b"
             echo "$b ^,$an"
           fi ;;
      esac;
    done;
  )
@

For the name lookup, the start name of the range is added.  Rather
than try to look up both ends from the symbol table, further
verification is done using ranges.  In fact, there is no point in
storing the code point at all.

<<Built-in string table components>>=
charnames.unirng \
@

<<Generate built-in string table using [[mkstrs]]>>=
tr 'A-Z-' 'a-z ' < charnames.unirng | fgrep '^' | cut -d, -f1 | sort -u | \
  ./mkstrs $(UNI_NAME_WORD) -DUNI_RANGES -aUNI_RANGE \
           -h+strs-hash.gen -o+strs-strs.gen -d+builtin-strings.h \
           -l`fgrep UNI_CHARS_LAST builtin-strings.h | cut -d\  -f3`
@

The range table is generated using yet a different script, setting the
auxiliary value to the name from the symbol table.  To verify that a
symbol lookup was correct, just ensure that [[uni_range_of]]'s return
value matches the symbol lookup after extracting the code point.

<<Built-in string table components>>=
charnames.unirng \
@

<<Initialize UCD files>>=
int prop_range = -1;
int range_num = 0;
@

<<Process a line of [[UnicodeData.txt]]>>=
if(low != high) {
  if(prop_range < 0)
    prop_range = add_prop("range");
  add_enum_rng(&parsed_props[prop_range], low, high, range_num++);
}
@

<<Parse character data files>>=
parsed_props[prop_range].def = range_num;
@

<<Dump character information as C code>>=
fprintf(gen_h, "#define uni_range_invalid %d\n"
               "#undef uni_range_of\n", range_num);
@

\lstset{language=make}
<<Generate character name data>>=
# Unicode range reverse lookup
echo '#include "builtin-strings.h"' >>$@
echo "const uint32_t unirng_range[] = {" >>$@
tr 'A-Z' 'a-z' < charnames.unirng | sed -n 's/[ -]/__/g;s/\^/_x5e/p' | \
 while IFS=, read n cp; do \
   echo "UNI_RANGE_$${n},"; \
done | sed '$$s/,$$//' >>$@
echo "};" >>$@
@

\lstset{language=C}
<<Character name lookup exports>>=
/* returns string # of low-end of range */
int uni_range_of(int cp);
@

<<Character name lookup functions>>=
int uni_range_of(int cp)
{
  uint8_t i = uni_x_of(cp, uni_range_mtab, uni_range_invalid);
  if(i == uni_range_invalid)
    return 0;
  return unirng_range[i];
}
@

Now enough pieces are together to create some routines for character
name lookup.

<<Library [[uni]] headers>>=
#include "uni_charname.h"
@

<<uni_charname.h>>=
<<Common C Warning>>
#ifndef CHARNAMES_H
#define CHARNAMES_H

#include "uni_prop.h"
<<Character name lookup exports>>

#endif
@

<<Library [[uni]] Members>>=
uni_charname.o
@

<<uni_charname.c>>=
<<Common C Header>>
#include "uni_charname.h"

<<Character name lookup local definitions>>

<<Character name lookup functions>>
@

To find the code point given a name, we need to pass in a string and
its length, and expect to get a code point in return.  For the
multi-character sequences, we instead receive an array.  The length of
the array is returned instead of a code point; the existence of the
array distinguishes between the two.  If nothing is found, a negative
value is returned (since zero is a valid code point, and negative
numbers aren't).

<<Character name lookup exports>>=
/* returns < 0 if no match */
/* returns array in arr and length of arr if > 1 char */
/* otherwise returns cp and sets arr to NULL */
int cp_of_uni(const char *name, int len, const int **arr);
@

<<Character name lookup functions>>=
int cp_of_uni(const char *name, int len, const int **arr)
{
  <<Look up code point of Unicode character [[name]]/[[len]]>>
}
@

Both lookup functions use the number stored before the length in the
string table.

<<Character name lookup local definitions>>=
static int cp_for(uint32_t str)
{
#if 1
    return 0;
#else
    int llen;
    const char *cns = str_for(str, NULL);
    num_before(cns, &llen);
    return num_before(cns - llen, NULL);
#endif
}
@

<<makefile.rules>>=
uni_charname.o uni_charname.c.static_proto: builtin-strings.h
@

<<Character name lookup local definitions>>=
#include "builtin-strings.h"
@

<<Character name lookup local definitions>>=
<<Character name generated structures>>
#include "charnames.gen"
@

\lstset{language=make}
<<makefile.rules>>=
charnames.gen: <<Character name data generation dependencies>>

	echo >$@
	<<Generate character name data>>
uni_charname.o uni_charname.c.static_proto: charnames.gen
@

<<Plain Built Files>>=
charnames.gen \
@

Looking up Unicode names is much more difficult.  On a lookup failure,
we need to downcase the entire string and remove dashes and
underscores.  This requires allocating a buffer.  For now, this is
done on the stack.  To avoid stack overflow, no character names larger
than 128 characters are accepted (the actual maximum is around 83).

If the transformed name still can't be looked up, it needs to be
looked up as a range.

<<Look up code point of Unicode character [[name]]/[[len]]>>=
if(len > 128)
  return -1;
#if 0
int cn = strno(name, len, 0);
#else
int cn = UNI_CHARS_FIRST + 14;
#endif
if(cn < UNI_CHARS_FIRST || cn > UNI_CHARS_LAST) {
  char buf[len + 1];
  int i;
  for(i = 0; i < len; i++)
    buf[i] = name[i] == '_' || name[i] == '-' ? ' ' :
             isupper(name[i]) ? tolower(name[i]) : name[i];
#if 1
  cn = UNI_CHARS_FIRST + 15;
#else
  cn = strno(buf, len, 0);
#endif
  if(cn < UNI_CHARS_FIRST || cn > UNI_CHARS_LAST) {
    <<Look up code point of Unicode range element [[buf]]>>
  }
}
int cp = cp_for(cn);
@

To look up a range entry, the last word needs to be replaced by a
caret.  If that lookup fails, it isn't a range entry, so return $-1$.

<<Look up code point of Unicode range element [[buf]]>>=
char *s;
for(s = buf + len - 1; --s > buf; )
  if(*s == ' ')
    break;
if(s <= buf)
  return -1;
char c = *++s;
*s = '^';
#if 0
cn = strno(buf, (int)(s - buf + 1), 0);
#else
cn = 0;
#endif
if(cn < UNI_RANGES_FIRST || cn > UNI_RANGES_LAST)
  return -1;
*s = c;
@

Then, that last word needs to be validated.  For all but the Hangul
syllables, the last word is a hexadecimal code point of at least four
characters with no superfluous leading zeroes.  There are no
multi-character sequences matching these names.  The table of ranges
returns the low end of the range, which is what we looked up, so if
they match, we're good to go.

<<Character name lookup local definitions>>=
#include "uni_prop.h"
@

<<Look up code point of Unicode range element [[buf]]>>=
buf[len] = 0;
if(cn != UNI_RANGE_hangul__syllable___x5e) {
  if(len - (s - buf) < 4)
    return -1;
  if(len - (s - buf) > 4 && *s == '0')
    return -1;
  char *es;
  int cp = strtol(s, &es, 16);
  if(*es || uni_range_of(cp) != cn)
    return -1;
  *arr = NULL;
  return cp;
}
@

Hangul is more difficult, requiring character-by-character parsing.
It might be possible to come up with a regular expression to do the
match, but that would not help with returning the correct code point.
Instead, lookup tables are used for all three parts.  The tables are
filled from the abbreviations listed in Jamo.txt.

\lstset{language=make}
<<makefile.vars>>=
UNIJAMO = $(UCD_LOC)/Jamo.txt
@

<<Character name data generation dependencies>>=
$(UNIJAMO) \
@

<<Generate character name data>>=
for y in CHO JUNG JONG; do \
  echo "static const struct abent $${y}_dec[] = {"; \
  fgrep $${y}SEONG $(UNIJAMO) | cut -d\# -f1 | fgrep \; | \
    dd conv=lcase 2>/dev/null | cut -d\; -f2 | cat -n | while read a b; do \
      echo " {\"$$b\", $$((a-1))},"; \
    done | sort | sed '$$s/,$$//'; \
  echo "};"; \
done >>$@
@

\lstset{language=C}
<<Character name generated structures>>=
struct abent {
    const char *name;
    uint32_t cp;
};
@

<<Character name lookup local definitions>>=
#define NUM_CHO (sizeof(CHO_dec)/sizeof(CHO_dec[0]))
#define NUM_JUNG (sizeof(JUNG_dec)/sizeof(JUNG_dec[0]))
#define NUM_JONG (sizeof(JONG_dec)/sizeof(JONG_dec[0]))
@

The L portion is always either one character or one doubled character
or blank.  The blank can be detected by the fact that V shares no
characters with L.  In order to search the table quickly, the first
character, and, if doubled, the second cahracter are joined into a
string and searched using plain binary search.

<<Look up code point of Unicode range element [[buf]]>>=
/* L is 0 char or 1 char or 1 doubled char */
if(*s == s[1]) {
  c = s[2];
  s[2] = 0;
} else {
  c = s[1];
  s[1] = 0;
}
/* look up in table */
/* could use bsearch() but seems silly */
int l, h, m, cmp = 0;
l = 0; h = NUM_CHO - 1;

while(l <= h) {
  m = (l + h) / 2;
  cmp = strcmp(s, CHO_dec[m].name);
  if(cmp < 0)
    h = m - 1;
  else if(cmp > 0)
    l = m + 1;
  else
    break;
}
/* if not found, assume L is blank */
if(cmp)
  m = 0;
int L = CHO_dec[m].cp;
s += strlen(s);
*s = c;
@

Binary search fails too easily due to possible T presence, and the V
table is short with short strings, so the search for V is linear.

<<Look up code point of Unicode range element [[buf]]>>=
/* find first match */
int ll = 0;
for(m = 0; m < NUM_JUNG; m++) {
  ll = strlen(JUNG_dec[m].name);
  if(!strncmp(s, JUNG_dec[m].name, ll))
    break;
}
/* there is no blank V, so not found == error */
if(m == NUM_JUNG)
  return -1;
/* find longest match */
int m2;
for(m2 = m + 1; m2 < NUM_JUNG; m2++) {
  int ll2 = strlen(JUNG_dec[m2].name);
  if(ll2 <= ll || memcmp(JUNG_dec[m2].name, JUNG_dec[m].name, ll))
    break;
  if(!strncmp(s, JUNG_dec[m2].name, ll2)) {
    ll = ll2;
    m = m2;
  }
}
int V = JUNG_dec[m].cp;
s += strlen(JUNG_dec[m].name);
@

Finally, the remaining characters are a full string again, so binary
search is the easiest.

<<Look up code point of Unicode range element [[buf]]>>=
/* last part can be binary searched again, if present */
int T = 0;
if(*s) {
  l = 0; h = NUM_JONG - 1;
  while(l <= h) {
    m = (l + h) / 2;
    cmp = strcmp(s, JONG_dec[m].name);
    if(cmp < 0)
      h = m - 1;
    else if(cmp > 0)
      l = m + 1;
    else
      break;
  }
  /* unrecognized suffix is error */
  if(cmp)
    return -1;
  T = JONG_dec[m].cp + 1;
}
*arr = NULL;
return 0xAC00 + T + (NUM_JONG + 1) * (V + NUM_JUNG * L);
@

None of the ranged characters are multi-character, but any of the
others might be.  The sequences are loaded into an array.  The
sequence length can vary from 2--4, so table entry is zero-terminated.

<<Look up code point of Unicode character [[name]]/[[len]]>>=
if(cp < 0x110000) {
  *arr = NULL;
  return cp;
}
const int *ccp = &charseq[cp-0x110000][0];
int i;
for(i = 0; i < 4; i++)
  if(!ccp[i])
    break;
*arr = ccp;
return i;
@

\lstset{language=make}
<<Generate character name data>>=
# unicode character sequences
echo "const int charseq[][4] = {" >>$@
cut -d, -f2- < charnames.uniseq | sed 's/.*/{&},/;$$s/,$$//' >>$@
echo "};" >>$@
@

To do reverse lookups, the static string should be returned.  However,
for Unicode, we have to generate a name in case the name is
algorithmically generated.

\lstset{language=C}
<<Character name lookup exports>>=
/* always returns target length, even if len is too small */
int uni_gen_name_for(int cp, char *buf, int len);
@

<<Character name lookup functions>>=
int uni_gen_name_for(int cp, char *buf, int len)
{
  <<Return Unicode name of [[cp]]>>
}
@

<<Character name generated structures>>=
struct chrnm {
    uint32_t name, cp;
};
@

<<Character name lookup local definitions>>=
static int nmcmp(const void *a, const void *b)
{
    return ((struct chrnm *)a)->cp - ((struct chrnm *)b)->cp;
}
@

\lstset{language=make}
<<Character name data generation dependencies>>=
strs.gen \
@

Unicode is more difficult, of course, and not just because the name
needs to be copied into an output buffer.  The first part is simple,
and the same as for XML, though:  look up the code point in the name
table.  The table is only filled with entries which are not
automatically generated, so this might fail even if there is a good
name.  Like with XML, the table is generated from the string data.

\lstset{language=C}
<<Return Unicode name of [[cp]]>>=
uint32_t str;
struct chrnm cps = { 0, cp }, *res;

/* FIXME: change to manual bsearch */
if((res = bsearch(&cps, uni_nmord, sizeof(uni_nmord)/sizeof(*uni_nmord),
                  sizeof(*uni_nmord), nmcmp)))
  str = res->name;
else
  str = 0;
@

\lstset{language=make}
<<Generate character name data>>=
# cp -> string reverse lookup
echo 'static const struct chrnm uni_nmord[] = {' >>$@
sed -n -e '/UNI_CHARS_FIRST/,/UNI_CHARS_LAST/{ \
                  s%.*/\* [0-9]*/\([0-9]*,[0-9]*\) .*\*/$$%  {\1},%;T;p;}' \
  strs-strs.gen | sort -t, -k 2n | sed '$$s/,$$//' >>$@
echo '};' >>$@
@

For strings that are found, we are not done, though.  Unless of course
no characters are actually to be copied out, since the length is
indeed accurate.

\lstset{language=C}
<<Return Unicode name of [[cp]]>>=
if(str) {
  int nlen = 0;
  const char *s = NULL;

#if 0
  s = str_for(str, &nlen);
#endif
  if(!len)
    return nlen;
  <<Post-process regular Unicode character name>>
  return nlen;
}
@

The first thing to do is zero-terminate the return string, as required
by the API.  At this point there is at least one character available
in the output buffer, so setting it blindly is OK.

<<Post-process regular Unicode character name>>=
if(len > nlen + 1)
  len = nlen + 1;
buf[--len] = 0;
@

Next, we copy out the characters, converting them to upper-case.  They
are stored lower-case in the symbol table because NFKC\_CF converts
strings to lower-case for comparison.

<<Post-process regular Unicode character name>>=
int i;
for(i = 0; i < len; i++, s++)
   buf[i] = islower(*s) ? toupper(*s) : *s;
@

The other transformation which was done for easy comparison is to
strip the dashes out.  To fix this, a table of dash locations is used.
This table stores the offsets into the name of up to three dashes; no
name was found with more than three.  The offset actually starts at one
so that the array of three can be zero-terminated.  This actually
requires a fourth byte as well.

% FIXME: use signed offset instead; sign == negative for dash, pos for
% space; also, remove cp and store offset into this table with name
% string, and expand d to 12 chars, and end loop at 12

<<Character name generated structures>>=
struct dashloc {
    int cp;
    unsigned char d[4];
};
@

<<Post-process regular Unicode character name>>=
const struct dashloc dl = {cp}, *dash;

/* FIXME: change to manual bsearch */
dash = bsearch(&dl, dashloc, sizeof(dashloc)/sizeof(*dashloc), sizeof(*dashloc),
               cmpdl);
if(dash) {
  const unsigned char *d = dash->d;
  while(*d && *d <= len)
    buf[*d++ - 1] = '-';
}
@

<<Character name lookup local definitions>>=
static int cmpdl(const void *a, const void *b)
{
    return ((struct dashloc *)a)->cp - ((struct dashloc *)b)->cp;
}
@

To generate the array, some shell code is used to find dashes and
count characters.

\lstset{language=make}
<<makefile.vars>>=
UNIDATA = $(UCD_LOC)/UnicodeData.txt
@

<<Generate character name data>>=
# dash locations (stripped out for stored name)
echo "const struct dashloc dashloc[] = {" >>$@
cut -d\; -f1,2 $(UNIDATA) | fgrep -v 'CJK COMPATIBILITY IDEOGRAPH' | \
  fgrep -- - | while IFS=\; read cp x; do \
    d="$${x%%-*}"; \
    n=$$(($${#d}+1)); \
    ns=$$n; \
    while :; do \
      x="$${x#*-}"; \
      case "$$x" in \
        *-*) d="$${x%%-*}"; \
	     n=$$((n+$${#d}+1)); \
             ns=$$ns,$$n ;; \
          *) break ;; \
      esac; \
    done; \
    echo "    {0x$$cp,{$$ns}},"; \
  done | sed '$$s/,$$//' >>$@
echo "};" >>$@
@

Once again, the ranges add more effort.  At least if it is not one of
the range code points, we can return immediately.  Otherwise, we may
as well copy out as much of the name as we already have.  There are no
dashes in any of these names, except just before the hexadecimal code
point on the ones which have this.

\lstset{language=C}
<<Return Unicode name of [[cp]]>>=
int low = uni_range_of(cp);
if(!low)
  return 0;
int nlen = 0;
const char *s = NULL;
#if 0
s = str_for(low, &nlen);
#endif
int fulllen = nlen - 1; /* remove the ^ */
if(len > 1) {
  /* back up to last space */
  if(--nlen > len - 1)
    nlen = len - 1;
  else
    --nlen;
  len -= nlen;
  while(--nlen >= 0) {
    if(islower(*s))
      *buf++ = toupper(*s++);
    else
      *buf++ = *s++;
  }
}
@

Speaking of which, those now require appending the hexadecimal code
point, and a dash.  Precomputing the length is pretty easy, since
there are either 4, 5, or 6 digits depending on the code point range.
That means we can finally return a value if no output buffer was
given.  Otherwise, we write the suffix, followed by a 0, and return
the computed length.

<<Return Unicode name of [[cp]]>>=
if(low != UNI_RANGE_hangul__syllable___x5e) {
  fulllen += 4;
  if(cp > 0xffff)
    fulllen++;
  if(cp > 0xfffff)
    fulllen++;
  if(!len)
    return fulllen;
  if(len > 1) {
    *buf++ = '-';
    --len;
  }
  if(--len)
    snprintf(buf, len + 1, "%04X", cp);
  else
    *buf = 0;
  return fulllen;
}
@

For Hangul, the computation of the letters to append is pretty simple,
but the table already generated does not have the letters in the right
order, so they are added again, but this time in all caps and in the
right order, and without any additional data.

<<Return Unicode name of [[cp]]>>=
int t = (cp - 0xac00) % 28;
int v = (cp - 0xac00) / 28;
int l = v / 21;
v %= 21;
const char *ls = CHO_abbrev[l];
const char *vs = JUNG_abbrev[v];
const char *ts = t ? JONG_abbrev[t - 1] : "";
@

\lstset{language=make}
<<Generate character name data>>=
# hangul syllable lookup table
for y in CHO JUNG JONG; do \
  echo "static const char *$${y}_abbrev[] = {"; \
  fgrep $${y}SEONG $(UNIJAMO) | cut -d\# -f1 | fgrep \; | \
    while read a b; do \
      echo " \"$$b\","; \
    done | sed '$$s/,$$//'; \
  echo "};"; \
done >>$@
@

Having the letters makes computation of the full length trivial.

\lstset{language=C}
<<Return Unicode name of [[cp]]>>=
int ll = strlen(ls);
int vl = strlen(vs);
int tl = strlen(ts);
fulllen += ll + vl + tl;
if(!len)
  return fulllen;
@

Now to tack on what fits:  a space, followed by the L abbreviation,
followed by the V abbreviation, followed by the T abbreviation.  Then
we can return the computed length and it's over.

<<Return Unicode name of [[cp]]>>=
if(len > 1) {
  *buf++ = ' ';
  --len;
}
/* append L */
if(ll > len)
  ll = len;
len -= ll;
while(--ll >= 0)
  *buf++ = *ls++;
/* append V */
if(vl > len)
  vl = len;
len -= vl;
while(--vl >= 0)
  *buf++ = *vs++;
/* append T */
if(tl > len)
  tl = len;
while(--tl >= 0)
  *buf++ = *ts++;
/* terminate and return */
*buf = 0;
return fulllen;
@

\chapter{XML Data Files}

As mentioned in the introduction, the XML standard's entity names are
generally much shorter than Unicode names, so a facility to use them
instead is provided.  This requires parsing the XML entity database.
One way to deal with this would be to turn it into a simple-to-parse
format using XSLT.  An earlier version of this library did just that.
However, since there are other XML files to parse as well, and
everything else is being done in parse-ucd, it is now read in directly
using libxml2.%
\footnote{\url{http://xmlsoft.org/}}
For maximum simplicity, each file is simply read in using the
document-at-once
parser\footnote{\url{http://xmlsoft.org/html/libxml-parser.html}}.
While the incremental (xmlreader) mode would probably be more
efficient for memory usage, this program is only run once, during
build time, so the memory usage should not matter much.  At least the
XML structure can be freed when finished.  Scanning the tree is done
using manual traversal.  While the associated libxslt could be used to
run the old code directly, the C code is actually much simpler.

<<UCD parser local definitions>>=
#include <libxml/parser.h>
#include <libxml/xmlerror.h>
@

\lstset{language=make}
<<makefile.vars>>=
XML_CFLAGS := $(shell xml2-config --cflags)
XML_LDFLAGS := $(shell xml2-config --libs)
# adding xml flags only to PARSER_CFLAGS would be nice, but hard
# to integrate w/ static_proto gen
PARSER_CFLAGS += -DXMLUNI=\"$(XMLUNI)\"
EXTRA_CFLAGS += $(XML_CFLAGS)
PARSER_LDFLAGS += $(XML_LDFLAGS)
@

\lstset{language=C}
<<UCD parser local definitions>>=
#define xml_opts XML_PARSE_NOENT | /* expand known entities */ \
                 XML_PARSE_NONET | /* forbid network access */ \
		 XML_PARSE_NOCDATA | /* merge CDATA as text */ \
                 XML_PARSE_COMPACT | /* compact small text nodes */ \
		 XML_PARSE_HUGE /* no hard-coded limits */
/* avoid type cast in every strcmp */
#define xml_isname(n, s) !strcmp((const char *)n->name, s)
@

<<Parse character data files>>=
xmlInitParser();  /* xmlCleanupParser() when done */
xmlDocPtr doc = xmlReadFile(XMLUNI, NULL, xml_opts);
if(!doc) {
  perror(XMLUNI);
  exit(1);
}
xmlNodePtr n;
for(n = doc->children; n; n = n->next)
  if(n->children && xml_isname(n, "unicode"))
    break;
if(!n) {
  perror(XMLUNI);
  exit(1);
}
@

The name database has subsets called groups.  Using every single name
is certain to frequently collide with Unicode names, so a particular
group should be chosen.  The 2007 group is used by my own
applications, and seems like a reasonable default.

\lstset{language=make}
<<makefile.config>>=
# XML entity name group
XML_ENTITY_NAME_GROUP = 2007
@

<<makefile.vars>>=
PARSER_CFLAGS += -DXML_ENTITY_NAME_GROUP=\"$(XML_ENTITY_NAME_GROUP)\"
@

\lstset{language=C}
<<UCD parser local definitions>>=
/* avoid multiple type casts in every attr search */
#define xml_prop(n, p) (const char *)xmlGetProp(n, (const xmlChar *)p)
@

<<Parse character data files>>=
xmlNodePtr c;
const char **grp_set;
int ngrp_set;
for(c = n->children; c; c = c->next) {
  if(!c->children || !xml_isname(c, "entitygroups"))
    continue;
  for(c = c->children; c; c = c->next) {
    if(!c->children || !xml_isname(c, "group"))
      continue;
    const char *gn = xml_prop(c, "name");
    if(gn && !strcmp(gn, XML_ENTITY_NAME_GROUP))
      break;
  }
  break;
}
if(!c) {
  perror("group " XML_ENTITY_NAME_GROUP " not in " XMLUNI);
  exit(1);
}
{
  xmlNodePtr cc;
  for(ngrp_set = 0, cc = c->children; cc; cc = cc->next) {
    if(cc->type != XML_ELEMENT_NODE || !xml_isname(cc, "set"))
      continue;
    ngrp_set++;
  }
  inisize(grp_set, ngrp_set);
  for(ngrp_set = 0, cc = c->children; cc; cc = cc->next) {
    if(cc->type != XML_ELEMENT_NODE || !xml_isname(cc, "set"))
      continue;
    grp_set[ngrp_set++] = xml_prop(cc, "name");
  }
  qsort(grp_set, ngrp_set, sizeof(*grp_set), sort_strcmp);
}
@

<<UCD parser local functions>>=
static int sort_strcmp(const void *a, const void *b)
{
  return strcmp(*(const char **)a, *(const char **)b);
}
@

Once the group's set names are known, the charlist is scanned for
character tags with an entity descriptor belonging to one of those
sets.  The id of the entity is the entity name, and the id of the
character is the Unicode code point.  Some of these define
multi-character code points; these will be added along with the named
multi-character Unicode groups later.

<<Parse character data files>>=
decl_str(xmlname);
for(n = n->children; n; n = n->next)
  if(n->children && xml_isname(n, "charlist"))
    break;
if(!n) {
  perror("charlist not in " XMLUNI);
  exit(1);
}
for(n = n->children; n; n = n->next) {
  if(!n->children || !xml_isname(n, "character"))
    continue;
  for(c = n->children; c; c = c->next) {
    const char *set;
    const char *cid, *eid;
    if(c->type != XML_ELEMENT_NODE || !xml_isname(c, "entity") ||
       !(set = xml_prop(c, "set")) ||
       !bsearch(&set, grp_set, ngrp_set, sizeof(*grp_set), sort_strcmp) ||
       !(eid = xml_prop(c, "id")) || !(cid = xml_prop(n, "id")))
      continue;
    if(!strchr(cid, '-')) {
      low = high = strtol(cid + 1, NULL, 16);
      /* note: duplicates will occur: */
      /*   both fully duplicated entries and multiple names for one cp */
      add_str_raw(xmlname, eid);
    } else {
      <<Add multi-char xml entity>>
    }
  }
}
xmlFreeDoc(doc);
@

<<Parse character data files>>=
/* for testing to see if read properly */
#if 0
str_to_enum(&parsed_props[prop_xmlname], NULL);
#endif
@

The CLDR also uses XML rather than simple tables.  Doing hand-coded
XSLT for hundreds of data files seemed excessive, which is the main
reason why I abandoned XSLT.  Note that unlike unicode.xml, there is a
local, valid DTD for every CLDR file.

\lstset{language=make}
<<makefile.vars>>=
PARSER_CFLAGS += -DCLDR_LOC=\"$(CLDR_LOC)\"
@

\lstset{language=C}
<<UCD parser local definitions>>=
#define xml_opts_dtd xml_opts | \
		     XML_PARSE_DTDLOAD | XML_PARSE_DTDATTR | \
		     XML_PARSE_DTDVALID /* use DTD if on local disk */
@

<<Parse character data files>>=
chdir(cwd);
chdir(CLDR_LOC);
<<Parse CLDR files>>
@

The CLDR is, like most XML data, insanely complex and at times poorly
documented, and much of its functionality has dubious value for
general projects.  Part of the purpose of this project was to provide
information not obtainable from the POSIX locale functions.  As such,
everything will be ignored except for collation data.

Since the CLDR deals with locale data, the first thing to do is
determine what locale is being used.  The locale name consists of a
language identifier, followed by optional extensions.  The language
identifier consists of a language subtag, followed by an optional
script subtag, followed by an optional territory subtag, followed by
optional variant subtags.  These are all case-insensitive, and are
separated by either dashes or underscores.  The supported language
identifier tags are listed in the supplemental metadata.  As with all
of the CLDR data, versioning information is ignored.\footnote{One day,
the versions of all data files used in building this library should be
obtainable as a global variable or API call.  However, only overall
version numbers should be used, and not the individual constituent
file modification times.}

<<Parse CLDR files>>=
doc = xmlReadFile("common/supplemental/supplementalMetadata.xml",
                  NULL, xml_opts_dtd);
if(!doc) {
  perror("supplementalMetadata.xml");
  exit(1);
}
<<Prepare for parsing CLDR supplemental metadata>>
for(n = doc->children; n; n = n->next) {
  if(n->children && xml_isname(n, "supplementalData")) {
    for(n = n->children; n; n = n->next) {
      /* ignore version, generation, cldrVersion */
      if(!n->children || !xml_isname(n, "metadata"))
        continue;
      for(n = n->children; n; n = n->next) {
        if(!n->children)
	  continue;
        <<Parse CLDR supplemental metadata>>
      }
      break;
    }
    break;
  }
}
<<Finish parsing CLDR supplemental metadata>>
xmlFreeDoc(doc);
@

The data we're looking for is under the validity tag.  Each set is
listed as a ``variable'' whose value is the whitespace-separated list
of names.  In fact, one could make an argument that all of the
variables of type choice are useful to add as enumerations.  The main
argument against this is that some of the lists are rather long, and
the enumerations' use of binary searching (rather than, say, hash
tables) may be slow.

<<Parse CLDR supplemental metadata>>=
else if(xml_isname(n, "validity")) {
  for(c = n->children; c; c = c->next) {
    if(!c->children)
      continue;
    <<Parse CLDR validity entries>>
  }
}
@

<<Parse CLDR validity entries>>=
else if(xml_isname(c, "variable")) {
  const char *t = xml_prop(c, "type");
  if(!strcmp(t, "choice")) {
    const char *v = xml_prop(c, "id");
    if(!c->children || !c->children->content || !v || v[0] != '$')
      continue; /* should never happpen */
    int pno = add_prop(v + 1);
    uint32_t str[64];
    for(s = (char *)c->children->content; isspace(*s); s++);
    for(i = 0; *s; i++) {
      char *p = (char *)str;
      while(!isspace(*s) && *s)
        *p++ = *s++;
      while((p - (char *)str) % 4)
        *p++ = 0;
      add_str_rng(&parsed_props[pno], i, i, str, (p - (char *)str) / 4);
      while(isspace(*s))
        s++;
    }
    str_to_enum(&parsed_props[pno], NULL);
    free(parsed_props[pno].rng_dat);
    parsed_props[pno].rng_dat = NULL;
  }
  <<Process other CLDR validity variables>>
}
@

<<Ignore unimplemented enums>>=
/* ignore auto-generated enums for obsolete properties */
if(!parsed_props[i].rng_dat && i < num_prop_aliases)
  continue;
@

In addition, the entries in \$grandfathered and some other ISO
language codes are treated differently within Unicode and the CLDR.
These translations are under the alias tag, in languageAlias elements.
Similarly, script, territory, and variants have aliases as well.

<<Parse CLDR supplemental metadata>>=
else if(xml_isname(n, "alias")) {
  
}
@

<<FIXME>>=
"unknown" codes:
  $language - und
  $script - Zzzz
  $region - ZZ
  $currency - XXX
  $tzid - unk
translated region (territory?) codes:
  AA -> 958 -> AAA
  QM..QZ -> 959..972 -> QMM..QZZ
  XA..XZ -> 973..998 -> XAA..XZZ
  ZZ -> 999 -> ZZZ
translated script codes:
  Qaaa..Qabx -> 900..949
canonical form of parms (does it really matter?):
  -t- before -u-
  -u-: all attrs in alpha, followed by all keywords in alpha, all lower
       all names are canonical name
  -t-: same canon really
variant translations:
  AALAND -> sv_AX
  BOKMAL -> -nb
  NYNORSK -> -nn
  POSIX -> -u-va-posix
  POLYTONI -> -polyton
  SAAHO -> -ssy
@

The extensions supported by the CLDR are listed in the common/bcp47
data files.

<<UCD parser local definitions>>=
#include <dirent.h>
@

<<Parse CLDR files>>=
/* note: in most files, version info is silently ignored/dropped */
/* however, when generating files, it might be a good idea to put that */
/* info in to allow strings to determine versioning */

/* bcp47: locale variants */
/* need lookup by extension first (u/t), then name/aliases */
/* for each keyword, need lookup of valid values */

/* need parser function for locale names */
/*  en_US-u-ca-blah */
/*    -> en_US is full locale name (name+script) */
/*       u-ca is a keyword with value blah */
/*  en_US@ca=blah is equivalent */
/*     but keywords with aliases may only use aliases in @-notation */
struct dirent *de;
DIR *d = opendir("common/bcp47");
if(!d) {
  perror("CLDR bcp47");
  exit(1);
}
chdir("common/bcp47");
while((de = readdir(d))) {
  if(de->d_name[0] == '.')
    continue;
  doc = xmlReadFile(de->d_name, NULL, xml_opts_dtd);
  if(!doc) {
    perror(de->d_name);
    exit(1);
  }
  for(n = doc->children; n; n = n->next)
    /* not sure why 1st entry is always empty ldmlBCP47 */
    if(n->children && xml_isname(n, "ldmlBCP47")) {
      for(n = n->children; n; n = n->next) {
        /* ignore version */
	/*  although version/number could be added as literal text to output */
	/* ignore generation */
	/*  although generation/date could be added with version/number */
	/* ignore deprecated cldrVersion */
        if(n->children && xml_isname(n, "keyword")) {
          for(n = n->children; n; n = n->next) {
	    if(xml_isname(n, "key") ||
	       /* attribute is actually 3-letter standalone attribute flag */
	       xml_isname(n, "attribute")) {
	      const char *name = xml_prop(n, "name");
	      const char *desc = xml_prop(n, "description");
	      const char *alias = xml_prop(n, "alias");
	      const char *depr = xml_prop(n, "deprecated");
	      const char *ext = xml_prop(n, "extension");
	      /* ignoring since= attr */
	      if(!ext)
	        ext = "u";
	      if(!alias)
	        alias = name;
	      /* note: alias may be multiple space-separated aliases */
	      /* if alias present, only alias may be used in loc@var */
	      /* but none observed in wild */
              if(depr && !strcmp(depr, "false"))
	        depr = NULL;
#if 0
	      printf("%s-%s/%s %s\n", ext, name, alias,
	             depr ? "deprecated" : "");
#endif
	      /* note: t_i0 may have other valid keys: tags indicating method */
	      /* note: t_k0 is same way */
	      /* note: t_t0 is same way */
	      /* note: t_x0 is generic tags only */
	      for(c = n->children; c; c = c->next) {
	        if(xml_isname(c, "type")) {
		  name = xml_prop(c, "name");
		  alias = xml_prop(c, "alias");
		  desc = xml_prop(c, "description");
		  depr = xml_prop(c, "deprecated");
		  /* ignoring since= attr */
		  if(!alias)
		    alias = name;
		  /* note: alias may be multiple space-separated aliases */
		  /* 1st alias is preferred */
		  if(depr && !strcmp(depr, "false"))
		    depr = NULL;
		  /* special names: */
		  /*   REORDER_CODE reordering block name(s) */
		  /*   CODEPOINTS one or more 4-6 hex digits */
#if 0
		  printf("  %s/%s %s\n", name, alias, depr ? "deprecated" : "");
#endif
	        }
	      }
	    }
	  }
	  break;
	}
      }
      break;
    }
  xmlFreeDoc(doc);
}
chdir(cwd);
chdir(CLDR_LOC);
@

The main lldml files contain the actual locale data.  Every lldml file
at least identifies itself.  All lldml files with the same identity
are effectively merged into a single tree.  For alias lookups and data
merging, all available xml files are read in and keyed by their
primary identity.

<<UCD parser local functions>>=
static void parse_ldml(const char *name)
{
  /* ignore special tags everywhere, since it's undefined */
  xmlDocPtr doc = xmlReadFile(name, NULL, xml_opts_dtd);
  if(!doc) {
    perror(name);
    exit(1);
  }
  xmlNodePtr n, c;
  for(n = doc->children; n; n = n->next)
    /* not sure why 1st entry is always empty ldml */
    if(n->children && xml_isname(n, "ldml")) {
      /* ignore draft attribute */
      /* ignore deprecated fallback tag */
      for(n = n->children; n; n = n->next) {
	if(!n->children)
	  continue;
	<<Parse ldml sections>>
      }
      break;
    }
  xmlFreeDoc(doc);
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "identity")) {
  /* ignore version, generation */
  const char *lang = NULL;
  const char *scr = NULL;
  const char *ter = NULL;
  const char *var = NULL;
  const char *alias = NULL, *alias_path = NULL;
  for(c = n->children; c; c = c->next) {
    /* e.g. zh in zh_Hans_SG */
    if(xml_isname(c, "language"))
      lang = xml_prop(c, "type");
    /* e.g. Hans in zh_Hans_SG */
    else if(xml_isname(c, "script"))
      scr = xml_prop(c, "type");
    /* e.g. SG in zh_Hans_SG */
    else if(xml_isname(c, "territory"))
      ter = xml_prop(c, "type");
    /* e.g. POSIX in en_US_POSIX */
    else if(xml_isname(c, "variant"))
      var = xml_prop(c, "type");
    else if(xml_isname(c, "alias")) {
      /* note: should this even be supported?  is it used? */
      alias = xml_prop(c, "source");
      alias_path = xml_prop(c, "path");
    }
    /* note that draft, references, alt fields ignored above */
  }
  /* file name should be following text */
#if 0
  printf("lang: %s", lang);
  if(scr)
    printf("_%s", scr);
  if(ter)
    printf("_%s", ter);
  if(var)
    printf("_%s", var);
  putchar('\n');
#endif
}
@

The casing subdirectory contains internal-use-only casing rules for
user interfaces.

<<Parse CLDR files>>=
/* casing: special casing rules */
/* need to convert this to a binary file format */
/* need to load at run-time based on locale var or desired locale */
/* is this even useful?  supposedly internal-use-only */
d = opendir("common/casing");
if(!d) {
  perror("CLDR casing");
  exit(1);
}
chdir("common/casing");
while((de = readdir(d))) {
  if(de->d_name[0] == '.')
    continue;
  parse_ldml(de->d_name);
}
chdir(cwd);
chdir(CLDR_LOC);
@

<<Parse ldml sections>>=
else if(xml_isname(n, "metadata")) {
  for(c = n->children; c; c = c->next)
    if(xml_isname(c, "casingData")) {
      for(c = c->children; c; c = c->next) {
        if(!xml_isname(c, "casingItem"))
	  continue;
	/* type should be enum of all potential types */
	    /* calendar-field day-format-except-narrow */
	    /* day-narrow day-standalone-except-narrow */
	    /* displayName displayName-count era-abbr */
	    /* era-name era-narrow language key */
	    /* metazone-long month-format-except-narrow */
	    /* month-standalone-except-narrow */
	    /* month-narrow quarter-abbreviated */
	    /* quarter-format-wide tense territory */
	    /* quarter-standalone-wide script symbol */
	    /* type metazone-short zone-exemplarCity */
	  /* not sure if should allow "other" types */
	/* content should be enum of values: */
	  /* titlecase/uppercase/lowercase */
#if 0
        printf(" casing %s: %s\n", xml_prop(c, "type"),
	                           c->children->content);
#endif
      }
      break;
    }
}
@

The collations subdirectory contains locale-specific collation order
changes.

<<Parse CLDR files>>=
/* collation: locale-specific collation rules */
d = opendir("common/collation");
if(!d) {
  perror("CLDR collation");
  exit(1);
}
chdir("common/collation");
while((de = readdir(d))) {
  if(de->d_name[0] == '.')
    continue;
  parse_ldml(de->d_name);
}
chdir(cwd);
chdir(CLDR_LOC);
@

<<Parse ldml sections>>=
else if(xml_isname(n, "collations")) {
  const char *subl = xml_prop(n, "validSubLocales");
#if 0
  /* space-separated list of full locale names */
  if(subl)
    printf("sublang: %s\n", subl);
#endif
  /* ignore version, draft attrs */
  const char *def = "standard";
  for(c = n->children; c; c = c->next) {
    /* should be same enum type as types below */
    if(xml_isname(c, "default")) {
      def = xml_prop(c, "choice");
      if(!def)
        def = xml_prop(c, "type"); /* deprecated, but used */
      /* ignore draft, references, alt attrs */
      /* can have more than one, but why would you? */
    } else if(xml_isname(c, "collation")) {
      /* ignore draft, standard, references, alt */
      if(!subl) /* technically, this isn't a req, but it's safe */
        subl = xml_prop(c, "validSubLocales");
      /* ignore draft, standard, references, alt attrs */
      /* ignore visibility attr */
      /* all available types should be an enum */
      /* or at least easily looked up on per-lang basis */
      /* most langs have no alt types, but some have many */
#if 0
      printf("[rules for type %s]\n", xml_prop(c, "type"));
#endif
      xmlNodePtr cc;
      const char *base = NULL; /* DUCET by default */
      for(cc = c->children; cc; cc = cc->next) {
        if(xml_isname(cc, "base")) {
	  xmlNodePtr ccc;
	  for(ccc = cc->children; ccc; ccc = ccc->next)
	    if(xml_isname(ccc, "alias")) {
	      base = xml_prop(ccc, "source");
	      /* path should always be /ldml/collations */
	      break;
	    }
	} else if(xml_isname(cc, "settings")) {
	  xmlAttrPtr a;
	  /* note: only settings in wild are: */
	  /*   backwards=on */
	  /*   caseFirst=upper */
	  /*   normalization=on */
	  /*   alternate=shifted */
	  /*   variableTop=<n> */
	  /*   strength=Tertiary */
	  /*   hiraganaQuaterary=on */
	  /* valid, though: */
	  /*  strength=primary|secondary|tertiary|quaternary|identical */
	  /*  alternate=non-ignorable|shifted */
	  /*  backwards=on|off */
	  /*  normalization=on|off */
	  /*  caseLevel=on|off */
	  /*  caseFirst=upper|lower|off */
	  /*  hiraganaQuarternary=on|off */
	  /*  hiraganaQuaternary=on|off */
	  /*  numeric=on|off */
	  /*  private=true|false */  /* deprecated */
	  /*  variableTop='x' */
	  /*  reorder=? */
#if 0
          fputs("  settings", stdout);
	  for(a = cc->properties; a; a = a->next)
	    printf(" %s=%s", a->name, a->children->content);
	  putchar('\n');
#endif
        } else if(xml_isname(cc, "rules")) {
	  xmlNodePtr r;
	  for(r = cc->children; r; r = r->next) {
	    if(r->type != XML_ELEMENT_NODE)
	      continue;
	    /* r->name may be alias.  not seen in wild. */
	    /* r->name may be import (repeatedly).  not seen in wild. */
	    /* r->name should be enum */
	    /* reset [before=primary/secondary/tertiary] */
	    /* note: reset arg may be tag: */
	    /*    <when>_<lev>_ignorable */
	    /*      <lev>=primary secondary tertiary non */
	    /*      <when>=first last */
	    /*    <when>_variable */
	    /*    <when>_trailing */
	    /*    cp hex=... */
	    /* p pc s sc t tc i ic */ /* q qc are deprecated */
	    /*   each takes interleaved raw data, <cp hex=>, <last_variable/> */
	    /* x [<context>..</>] {p pc s sc ...}* [<extend>..</>] */
	    /*  context & extend are interleaved raw data & <cp hex=> */
#if 0
            printf(" rule: %s%s '%s'\n", r->name,
	           r->properties &&
		     r->properties->children ?
		       r->properties->children->content : "",
		   r->children->type == XML_ELEMENT_NODE ?
		      r->children->name :
		      r->children->content);
#endif
          }
        } else if(xml_isname(cc, "suppress_contractions")) {
#if 0
          printf("  suppress %s\n", cc->children->content);
	  /* note: may be interleaved raw data and <cp hex="...."> tags */
#endif
        } else if(xml_isname(cc, "optimize")) {
	  /* ??? */
	  /* interleaved raw data and <cp hex="..."> tags */
	}
      }
    } else if(xml_isname(c, "alias")) {
      /* path should always be //ldml/collations */
#if 0
      printf("  [copy from %s (%s)]\n", xml_prop(c, "source"),
             xml_prop(c, "path"));
#endif
    }
  }
#if 0
  printf("[default: %s]\n", def);
#endif
}
@

The main subdirectory contains common locale parameters such as
calendar, money, and numeric format changes.

<<Parse CLDR files>>=
/* main: "other" rules */
d = opendir("common/main");
if(!d) {
  perror("CLDR main");
  exit(1);
}
chdir("common/main");
while((de = readdir(d))) {
  if(de->d_name[0] == '.')
    continue;
  parse_ldml(de->d_name);
}
chdir(cwd);
chdir(CLDR_LOC);
@

<<Parse ldml sections>>=
else if(xml_isname(n, "dates")) {
  /* ignore draft/standard/references/alt tags */
  const char *subl = xml_prop(n, "validSubLocales");
  for(c = n->children; c; c = c->next) {
    if(xml_isname(c, "alias")) {
    } else if(xml_isname(c, "localizedPatternChars")) {
      /* deprecated */
      /* ignore draft/standard/references/alt */
      if(!subl)
        subl = xml_prop(c, "validSubLocales");
      /* data is interleaved raw data & <cp hex=> */
    } else if(xml_isname(c, "dateRangePattern")) {
      /* deprecated */
      /* ignore draft/standard/references/alt */
      if(!subl)
        subl = xml_prop(c, "validSubLocales");
      /* data is interleaved raw data & <cp hex=> */
    } else if(xml_isname(c, "calendars")) {
      /* calendar type=gregorian */
      /*      months */
      /*        monthContext type="format" */
      /*          monthWidth type="wide" */
      /*            <month type="8">Leqeeni</month> */
    } else if(xml_isname(c, "timeZoneNames")) {
    }
  }
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "alias")) {
  /* is this even used?  should it be supported? */
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "localeDisplayNames")) {
  /* <languages><language type="..">...</>.... */
  /* <scripts><script type="..">...</>... */
  /* <territories><territory type='..'>...</>...*/
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "layout")) {
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "contextTransforms")) {
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "characters")) {
  /* exemplarCharacters [type=...]>[uniset]</> */
  /* note: this is an optional Unicode property */
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "delimiters")) {
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "measurement")) {
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "numbers")) {
  /* <currenceis><currency type="DJF"><symbol>Fdj</symbol>...*/
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "units")) {
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "listPatterns")) {
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "posix")) {
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "segmentations")) {
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "rbnf")) {
}
@

<<Parse ldml sections>>=
else if(xml_isname(n, "references")) {
}
@

<<FIXME>>=
implement

inheritance rules (what a pain)
  bundle:
    strip region, then script, then go to "default" & do same, then root
  item:
    strip region, then script, then go to "root alias" & do same, then root

rbnf -> rbnf for generating number names from numbers
  doctype = ldml
segments -> patterns for text segmentation
  doctype = ldml
supplemental -> various
  doctype = ldmlSupplemental
transforms -> mappings (no real single name for each, though)

crappy "set" notation must be implemented for reading LDML files:
 claims UTS18-L1+RL2.5 "with recommended syntax", but then goes on
 to describe a different syntax..  in particular, set ops are different
   "A multi-character string can be in a Unicode set.. uses curly  braces"
     wtf does that mean?  \u{XXXX XXXX}? {c1 c2 c3}?
  literal chars & ranges
  ignore whitespace
  \uXXXX
  \UXXXXXXXX
  \xXX
  \OOO
  \a \b \t \n \v \f \r \\
  \u{X...}
  \u{X.... X...}
  \N{<name>}
  \p{<prop>} [:<prop>:]
  \P{<prop>} [:&<prop>:]
   <prop> is binary or prop-name[: = != =/=]val or script or cat
   script or cat can be ored using |
   val can be ored using |
     General_Category Script Alphabetic Uppercase Lowercase
     White_Space Noncharacter_Code_Point Default_Ignorable_Code_Point
     ANY (0-10FFFF)
     ASCII (0-7F)
     ASSIGNED (!Cn)
     na/name Name_Alias
       only non-binary props are name cat script
       name props must match loosely
  [ ] for grouping (creates a subset)
    adjacent for union
    & for intersection
    - for set difference
      all 3 ops are l-r and same precendence level
      & and - only operate on sets
  [^ ] for inversion
@

<<FIXME>>=
for regex:
  string property "exemplar" is UnicodeSet ldml/characters/exmplarCharacters
@

\chapter{File Input}

Basic I/O support is provided in the form of UTF decoders and
encoders.  More advanced I/O is provided in the form of arbitrary
encoding input.

<<Library [[uni]] Members>>=
uni_io.o
@

\lstset{language=C}
<<uni_io.c>>=
<<Common C Header>>
#include "uni_io.h"

<<Unicode I/O local definitions>>

<<Unicode I/O functions>>
@

<<Library [[uni]] headers>>=
#include "uni_io.h"
@

<<uni_io.h>>=
<<Common C Warning>>
#ifndef UNI_IO_H
#define UNI_IO_H

#include "uni_prop.h"
<<Unicode I/O Exports>>
#endif
@

\section{Basic Unicode I/O}

Unicode defines several file formats:  UTF-8, UTF-16, and UTF-32.  No
special conversion needs to be made from UTF-32 to integer code
points, other than to filter out invalid values and possibly byte-swap
the code point.

<<Common C Includes>>=
#include <stdint.h>
/* FIXME: this is glibc-specific */
/* BSD apparently uses <sys/endian.h> */
/* OpenBSD additionally uses different fn names */
/* others may not even have any equivalent functions */
#include <endian.h>
@

\lstset{language=make}
<<makefile.vars>>=
# for endian
EXTRA_CFLAGS += -D_BSD_SOURCE
@

\lstset{language=C}
<<Unicode I/O Exports>>=
int utf32_encode(uint32_t *buf, int cp, int bige);
@

<<Unicode I/O functions>>=
int utf32_encode(uint32_t *buf, int cp, int bige)
{
  if(cp > 0x10ffff || (cp >= 0xd800 && cp < 0xe000))
    return 0;
  *buf = bige ? htobe32(cp) : htole32(cp);
  return 1;
}
@

<<Unicode I/O Exports>>=
int utf32_decode(const uint32_t *s, int bige);
@

<<Unicode I/O functions>>=
int utf32_decode(const uint32_t *s, int bige)
{
  uint32_t c = bige ? be32toh(*s) : le32toh(*s);
  if(c > 0x10ffff || (c >= 0xd800 && c < 0xe000))
    return -1;
  return c;
}
@

<<Unicode I/O Exports>>=
int utf32_putc(int c, FILE *f, int bige);
@

<<Unicode I/O functions>>=
int utf32_putc(int c, FILE *f, int bige)
{
  uint32_t w;

  if(!utf32_encode(&w, c, bige))
    return 0;
  return fwrite(&w, 4, 1, f);
}
@

<<Unicode I/O Exports>>=
int utf32_getc(FILE *f, int bige); /* always reads 4 bytes */
@

<<Unicode I/O functions>>=
int utf32_getc(FILE *f, int bige)
{
  uint32_t w;
  int ret;

  if((ret = fread(&w, 1, 4, f)) != 4)
    return ret ? -2 : -1;
  ret = utf32_decode(&w, bige);
  return ret < 0 ? -3 : ret;
}
@

For UTF-16, a slightly more complex scheme is used, involving the
surrogate code points (D800 through DFFF) in pairs.  The first half of
the surrogate code points (D800 through DBFF) is always used for the
first member of the pair, and the second half is used for the other.
The first member gives the first 5 bits, and the second member gives
the remainder. Since this is for representing code points 10000 and
up, an extra bit can be gained giving the full range through 10FFFF.

<<Unicode I/O Exports>>=
/* make sure at least 2 words avail in buf */
/* returns # of words written */
int utf16_encode(uint16_t *buf, int cp, int bige);
@

<<Unicode I/O functions>>=
int utf16_encode(uint16_t *buf, int cp, int bige)
{
  if((cp >= 0xD800 && cp < 0xE000) || cp > 0x10FFFF)
    return 0;
  if(cp < 0x10000) {
    *buf = bige ? htobe16(cp) : htole16(cp);
    return 1;
  }
  int u = (cp & 0x1f0000) >> 16;
  cp &= 0xffff;
  uint16_t w;
  w = 0xd800 + ((u - 1) @<< 10) + (cp @>> 10);
  *buf++ = bige ? htobe16(w) : htole16(w);
  w = 0xdc00 + (cp > 0x3fff);
  *buf = bige ? htobe16(w) : htole16(w);
  return 2;
}
@

<<Unicode I/O Exports>>=
/* this function returns -1 on errors */
/* if the buffer may be invalid, ensure at least 2 words available */
int utf16_decode(const uint16_t *s, int *nread, int bige);
@

<<Unicode I/O functions>>=
int utf16_decode(const uint16_t *s, int *nread, int bige)
{
  uint16_t w = bige ? be16toh(*s) : le16toh(*s);
  if(nread)
    *nread = 1;
  if(w < 0xd800 || w >= 0xe000)
    return w;
  if(w >= 0xdc00)
    return -1;
  uint16_t w2 = bige ? be16toh(s[1]) : le16toh(s[1]);
  if(w2 >= 0xe000 || w2 < 0xdc00)
    return -1;
  if(nread)
    *nread = 2;
  int c = w2 & 0x3ff;
  c += (w & 0x3f) << 10;
  w = (w >> 10) & 0xf;
  c += (w + 1) << 16;
  return c;
}
@

<<Unicode I/O Exports>>=
int utf16_putc(int c, FILE *f, int bige);
@

<<Unicode I/O functions>>=
int utf16_putc(int c, FILE *f, int bige)
{
  unsigned char buf[4];
  uint16_t enc[2], encl;
  
  encl = utf16_encode(enc, c, bige);
  if(!encl)
    return 0;
  if(bige) {
    buf[0] = enc[0] >> 8;
    buf[1] = enc[0];
    if(encl > 1) {
      buf[2] = enc[1] >> 8;
      buf[3] = enc[1];
    }
  } else {
    buf[1] = enc[0] >> 8;
    buf[0] = enc[0];
    if(encl > 1) {
      buf[3] = enc[1] >> 8;
      buf[2] = enc[1];
    }
  }
  return fwrite(buf, 2, encl, f);
}
@

The input function needs to read ahead to get the second member of a
pair.  If that character is not what was expected, the correct
behavior is to undo the readahead and return the current code point as
an error.  However, it is not possible in C standard I/O to push more
than one character back into the stream.  For now, this is going to
have to be erroneous behavior.

<<Unicode I/O Exports>>=
int utf16_getc(FILE *f, int bige, int *nread);  /* nread == # of bytes read */
@

<<Unicode I/O functions>>=
int utf16_getc(FILE *f, int bige, int *nread)
{
  int c = fgetc(f), c2;
  if(c == EOF) {
    if(nread)
      *nread = 0;
    return -1;
  }
  c2 = fgetc(f);
  if(c == EOF) {
    if(nread)
      *nread = 1;
    return -2;
  }
  if(bige)
    c = c2 + (c << 8);
  else
    c += c2 << 8;
  if(c < 0xd800 || c >= 0xE000) {
    if(nread)
      *nread = 1;
    return c;
  }
  if(c >= 0xdc00) {
    if(nread)
      *nread = 2;
    return -3;
  }
  int res = (c & 0x3f) << 10;
  int w = (c >> 10) & 0xf;
  res += (w + 1) << 16;
  if((c = fgetc(f)) == EOF) {
    if(nread)
      *nread = 2;
    return -2;
  }
  if((c2 = fgetc(f)) == EOF) {
    if(nread)
      *nread = 3;
    return -2;
  }
  if(bige)
    c = c2 + (c << 8);
  else
    c += c2 << 8;
  if(c < 0xdc00 || c >= 0xe000) {
#if 0 /* FIXME: this does not work; ungetc() can't be called twice */
    ungetc(c2, f);
    ungetc(bige ? c >> 8 : c & 0xff, f);
    if(nread)
      *nread = 2;
#else
    if(nread)
      *nread = 4;
#endif
    return -3;
  }
  if(nread)
    *nread = 4;
  res += c & 0x3ff;
  return res + (c & 0x3ff);
}
@

For UTF-8, an even more complex encoding scheme is used.  Again, a
standalone memory codec is provided.  All code points over 007F are
encoded using a multi-byte sequence.  All characters in a multi-byte
sequence have their high bit set; the first non-zero bit determines
the byte's role.  All but the first byte have only one high bit set,
and encode 6 bits.  The first byte determines how many trailing bytes
there are, and also encode 3--5 bits.  The number of high bits set in
the first byte is the total number of bytes in the sequence.

<<Unicode I/O Exports>>=
/* make sure at least 4 bytes avail in buf */
/* returns # of bytes written */
int utf8_encode(char *buf, int cp);
@

<<Unicode I/O functions>>=
int utf8_encode(char *buf, int cp)
{
    if((cp >= 0xD800 && cp < 0xE000) || cp > 0x10FFFF)
        return 0;
    if(cp < 128) {
        *buf = cp;
        return 1;
    } else if(cp < 0x800) {
        *buf = 0xc0 + (cp >> 6);
        buf[1] = 0x80 + (cp & 0x3f);
        return 2;
    } else if(cp < 0x10000) {
        *buf = 0xe0 + (cp >> 12);
        buf[1] = 0x80 + ((cp >> 6) & 0x3f);
        buf[2] = 0x80 + (cp & 0x3f);
        return 3;
    } else {
        *buf = 0xf0 + (cp >> 18);
        buf[1] = 0x80 + ((cp >> 12) & 0x3f);
        buf[2] = 0x80 + ((cp >> 6) & 0x3f);
        buf[3] = 0x80 + (cp & 0x3f);
        return 4;
    }
}
@

<<Unicode I/O Exports>>=
/* this function returns -1 on errors */
/* if the buffer may be invalid, ensure at least 4 chars available */
int utf8_decode(const char *buf, int *nread);
@

<<Unicode I/O functions>>=
int utf8_decode(const char *buf, int *nread)
{
    if(*buf < 0x7f) {
        *nread = 1;
        return (unsigned char)*buf;
    }
    int c = (unsigned char)*buf++, ec;
    if((c & 0xf8) == 0xf0) {
        c &= 0x07;
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 1;
            return -1;
        }
        c = (c << 6) + (ec & 0x3f);
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 2;
            return -1;
        }
        c = (c << 6) + (ec & 0x3f);
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 3;
            return -1;
        }
        *nread = 4;
        c = (c << 6) + (ec & 0x3f);
        if(c < 0x10000 || c > 0x10FFFF)
            return -1;
        return c;
    }
    if((c & 0xf0) == 0xe0) {
        c &= 0x0f;
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 1;
            return -1;
        }
        c = (c << 6) + (ec & 0x3f);
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 2;
            return -1;
        }
        *nread = 3;
        c = (c << 6) + (ec & 0x3f);
        if(c < 0x800 || (c >= 0xD800 && c < 0xE000))
            return -1;
        return c;
    }
    if((c & 0xe0) == 0xc0) {
        c &= 0x1f;
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 1;
            return -1;
        }
        *nread = 2;
        c = (c << 6) + (ec & 0x3f);
        if(c < 0x80)
            return -1;
        return c;
    }
    *nread = 1;
    return -1;
}
@

<<Unicode I/O Exports>>=
int utf8_putc(int c, FILE *f);
@

<<Unicode I/O functions>>=
int utf8_putc(int c, FILE *f)
{
    char obuf[4];
    int nout = utf8_encode(obuf, c);
    return fwrite(obuf, 1, nout, f);
}
@

UTF-8 output is common enough to warrant a string version of the
output function as well.

<<Unicode I/O Exports>>=
int utf8_fputs(int *buf, int len, FILE *f);
@

<<Unicode I/O functions>>=
int utf8_fputs(int *buf, int len, FILE *f)
{
  int ret = 0;

  while(len-- > 0)
    ret += utf8_putc(*buf++, f);
  return ret;
}
@

Like UTF-16 input, UTF-8 input requires some readahead.  For anything
more than one character, C once again disallows returning the
characters to the stream.  So, once again, bad input causes erroneous
behavior.

<<Unicode I/O Exports>>=
int utf8_getc(FILE *f, int *nread);  /* nread == # of bytes read */
@

<<Unicode I/O functions>>=
int utf8_getc(FILE *f, int *nread)
{
  int c;
  int chrread = 1;

  if((c = getc(f)) == EOF) {
    c = -1;
    chrread = 0;
  } else if(c >= 0x80) {
    int ec;
    chrread = 1;
    if((c & 0xf8) == 0xf0) {
      <<Read 4-char utf-8>>
      if(c < 0x10000 || c > 0x10ffff)
        c = -3;
    } else if((c & 0xf0) == 0xe0) {
      <<Read 3-char utf-8>>
      if(c < 0x800 || (c >= 0xD800 && c < 0xE000))
        c = -3;
    } else if((c & 0xe0) == 0xc0) {
      <<Read 2-char utf-8>>
      if(c < 0x80)
        c = -3;
    } else
      c = -3;
  }
  if(nread)
    *nread = chrread;
  return c;
}
@

<<Read 4-char utf-8>>=
c &= 0x07;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
  if((ec = getc(f)) == EOF)
    c = -2;
  else if((ec & 0xc0) != 0x80) {
    ungetc(ec, f);
    c = -3;
  } else {
    chrread = 3;
    c = (c << 6) + (ec & 0x3f);
    if((ec = getc(f)) == EOF)
      c = -2;
    else if((ec & 0xc0) != 0x80) {
      ungetc(ec, f);
      c = -3;
    } else {
      chrread = 4;
      c = (c << 6) + (ec & 0x3f);
    }
  }
}
@

<<Read 3-char utf-8>>=
c &= 0x0f;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
  if((ec = getc(f)) == EOF)
    c = -2;
  else if((ec & 0xc0) != 0x80) {
    ungetc(ec, f);
    c = -3;
  } else {
    chrread = 3;
    c = (c << 6) + (ec & 0x3f);
  }
}
@

<<Read 2-char utf-8>>=
c &= 0x1f;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
}
@

\section{General File Input}

A more general UTF reader would have to scan for a byte order mark and
remember what it said.  This requires retaining state.

<<Unicode I/O Exports>>=
typedef struct {
  FILE *f;
  char *name;
  <<Unicode file buffer state>>
} unifile_t;
@

<<Known Data Types>>=
unifile_t,%
@

Rather than automatically attaching state to a file on first access
and never really knowing when to free it, explicit routines are
provided to create and remove the state.   

<<Unicode I/O Exports>>=
/* encoding = NULL for automatic detection */
/* automatically detects utf16/utf32 if encoding == UTF-8 */
unifile_t *uni_fopen(const char *name, const char *encoding);
void uni_fclose(unifile_t *uf);
@

<<Unicode I/O functions>>=
unifile_t *uni_fopen(const char *name, const char *encoding)
{
  unifile_t *uf;

  inisize(uf, 1);
  clearbuf(uf, 1);
  uf->name = strdup(name);
  if(!uf->name) {
    free(uf);
    return NULL;
  }
  uf->f = fopen(name, "rb");
  if(!uf->f) {
    free(uf->name);
    free(uf);
    return NULL;
  }
  <<Initialize unicode file buffer state>>
  return uf;
}
@

<<Unicode I/O functions>>=
void uni_fclose(unifile_t *uf)
{
  free(uf->name);
  fclose(uf->f);
  <<Free unicode file buffer state>>
  free(uf);
}
@

One limitation of standard I/O is that only one character may be
pushed back into the stream.  To correct this, a 4-byte buffer is
kept, along with a count.  In order to avoid reading from the file
after the buffered characters have been removed, an end-of-file flag
is kept as well.  In order to allow error messages to display the
correct file offset, that is also kept.

<<Unicode file buffer state>>=
size_t fpos;
char readahead[4];
unsigned char raptr;
unsigned char eof;
@

The first thing to do is to decide the file's encoding.  ASCII and
ISO-Latin-1 can be read raw, since they map directly to Unicode.
UTF-8 is taken to mean automatic Unicode detection, with UTF-8 as a
default.  For all other input types, the [[iconv]] function should be
used.%
\footnote{The C99 library provides something similar in the
[[mbstowcs]] function, but the POSIX [[iconv]] function is commonly
available (perhaps even as a third party library), and could be
provided as wrapper around [[mbstowcs]] if all else fails.  Unlike
[[mbstowcs]], the type of the output can always be set to Unicode
encodings, and the input type can be controlled more easily as well.}
This function should be in the standard C library, but may be provided
externally as well.  If so, [[CFLAGS]] and [[LDFLAGS]] may need to be
modified appropriately.  In order to accomodate the case where a
minimal C library has no working [[iconv]], or to otherwise avoid code
bloat, this can be disabled with a configuration variable.

\lstset{language=make}
<<makefile.config>>=
# Set to non-empty to enable iconv input support
#USE_ICONV=y
@

<<makefile.vars>>=
ifneq ($USE_ICONV,)
EXTRA_CFLAGS += -DUNI_USE_ICONV
endif
@

<<Unicode I/O local definitions>>=
#ifdef UNI_USE_ICONV
#include <langinfo.h>
#include <locale.h>
#include <iconv.h>
#endif
@


<<Unicode file buffer state>>=
unsigned char enctype;
/*
 * 0 = ASCII/Latin-1
 * 1 = iconv (if UNI_USE_ICONV)
 * 8/16/32 = utf-8/16/32
 */
#ifdef UNI_USE_ICONV
void *enchelp;
#endif
@

<<Unicode I/O local definitions>>=
#ifdef UNI_USE_ICONV
#define ICBUF_SIZE 1024
typedef struct {
  iconv_t ic;
  char ibuf[ICBUF_SIZE];
  int ilen;
  uint32_t obuf[ICBUF_SIZE];
  int optr, olen;
} iconv_supt_t;
#endif
@

<<Known Data Types>>=
iconv_supt_t,%
@

<<Initialize unicode file buffer state>>=
#ifdef UNI_USE_ICONV
if(!encoding) {
  setlocale(LC_CTYPE, "");
  encoding = nl_langinfo(CODESET);
}
#endif
if(!encoding)
  encoding = "UTF-8";
if(encoding && (!strcmp(encoding, "ISO-8859-1") ||
                !strcmp(encoding, "ANSI_X3.4-1968"))) /* ASCII */
  return uf; /* enctype == 0 */
#ifdef UNI_USE_ICONV
else if(encoding && strcmp(encoding, "UTF-8")) {
  iconv_supt_t *ics;
  inisize(ics, 1);
#if __BYTE_ORDER == __LITTLE_ENDIAN
#define bo "LE"
#else
#define bo "BE"
#endif
  ics->ic = iconv_open("UTF-32" bo, encoding);
  if(ics->ic == (iconv_t)-1) {
    perror(encoding);
    exit(1);
  }
  uf->enchelp = ics;
  uf->enctype = 1;
  return uf;
}
#endif
/* otherwise it's Unicode */
@

<<Free unicode file buffer state>>=
#ifdef UNI_USE_ICONV
if(uf->enctype == 1 && uf->enchelp) {
  iconv_supt_t *ics = uf->enchelp;
  iconv_close(ics->ic);
  free(ics);
}
#endif
@

The first thing to do on opening a new Unicode file is to check for
the byte order mark (U+FEFF).  Flags are kept to indicate what was
found.  Note that the byte order mark is still returned as the first
character, so all read characters must be backed out.

<<Unicode file buffer state>>=
unsigned char bige;
@

<<Initialize unicode file buffer state>>=
uf->enctype = 8; /* default is UTF-8 */
/* Read and process BOM (FEFF) */
int c;
<<Read and store a char from [[uf]]>>
if(c == 0xff) {
  <<Read and store a char from [[uf]]>>
  if(c == 0xfe) { /* FFFE UTF-16BE */
    uf->bige = 1;
    uf->enctype = 16;
  }
} else if(c == 0xfe) {
  <<Read and store a char from [[uf]]>>
  if(c == 0xff) {
    <<Read and store a char from [[uf]]>>
    if(!c) {
      <<Read and store a char from [[uf]]>>
      if(!c) /* FEFF0000 UTF-32LE */
        uf->enctype = 32;
      else /* FEFF */
        uf->enctype = 16;
    } else /* FEFF UTF-16LE */
      uf->enctype = 16;
  }
} else if(!c) {
  <<Read and store a char from [[uf]]>>
  if(!c) {
    <<Read and store a char from [[uf]]>>
    if(c == 0xfe) {
      <<Read and store a char from [[uf]]>>
      if(c == 0xff) { /* 0000FEFF UTF-32BE */
        uf->enctype = 32;
	uf->bige = 1;
      }
    }
  }
}
@

<<Read and store a char from [[uf]]>>=
c = getc(uf->f);
if(c == EOF)
  uf->eof = 1;
else
  uf->readahead[uf->raptr++] = c;
@

The reader then uses the appropriate method to read a character based
on the [[enctype]].

<<Unicode I/O Exports>>=
int uni_fgetc(unifile_t *uf);
@

<<Unicode I/O functions>>=
int uni_fgetc(unifile_t *uf)
{
  if(!uf->enctype) {
    int c = getc(uf->f);
    if(c == EOF)
      uf->eof = 1;
    else
      uf->fpos++;
    return c;
#ifdef UNI_USE_ICONV
  } else if(uf->enctype == 1) {
    <<Return a character using iconv>>
#endif
  }
  <<Return a Unicode character>>
}
@

Rather than using the potentially flawed direct I/O routines, the
Unicode reader always reads raw bytes and then calls the decoder
instead.  That way, the readahead issue disappears.  The readahead
buffer is always flushed to four characters every time, since UTF-32
and UTF-16 both may require that much.  UTF-8 only requires three, but
there is little point in making that a special case.

<<Return a Unicode character>>=
while(!uf->eof && uf->raptr < 4) {
  int nr = fread(uf->readahead + uf->raptr, 1, 4 - uf->raptr, uf->f);
  if(nr <= 0)
    uf->eof = 1;
  else
    uf->raptr += nr;
}
if(uf->eof && !uf->raptr)
  return -1;
int cp, l;
if(uf->enctype == 32) {
  cp = utf32_decode((uint32_t *)uf->readahead, uf->bige);
  l = 4;
} else if(uf->enctype == 16) {
  cp = utf16_decode((uint16_t *)uf->readahead, &l, uf->bige);
  l *= 2;
} else
  cp = utf8_decode(uf->readahead, &l);
if(l > uf->raptr) {
  fprintf(stderr, "Ill-formed character @ %lu\n", uf->fpos);
  uf->raptr = 0;
  return -2;
}
if(l < 4)
  memmove(uf->readahead, uf->readahead + l, 4 - l);
if(cp < 0)
  fprintf(stderr, "Ill-formed character @ %lu\n", uf->fpos);
uf->raptr -= l;
uf->fpos += l;
return cp < 0 ? -2 : cp;
@

The [[iconv]] conversion proceeds much differently.  If a character is
availble in the output buffer, it validates and returns that character.
Otherwise, it fills up the input buffer from the file, and then
attempts to fill the output buffer using [[iconv]].  Any unprocessed
input characters are then moved to the start of the input buffer for
the next pass.

<<Return a character using iconv>>=
iconv_supt_t *ics = uf->enchelp;
while(1) {
  if(ics->olen) {
    --ics->olen;
    return utf32_decode(&ics->obuf[ics->optr++], __BYTE_ORDER == __BIG_ENDIAN);
  }
  char *ibuf = ics->ibuf;
  size_t ilen = ics->ilen;
  char *obuf = (char *)ics->obuf;
  size_t olen = ICBUF_SIZE * 4;
  while(!uf->eof && ilen < ICBUF_SIZE) {
    int nr = fread(ibuf, 1, ICBUF_SIZE - ilen, uf->f);
    if(nr <= 0)
      uf->eof = 1;
    else
      ilen += nr;
  }
  while(1) {
    iconv(ics->ic, &ibuf, &ilen, &obuf, &olen);
    if(ibuf != ics->ibuf)
      break;
    /* ignore bad chars */
    if(!--ilen)
      break;
    memmove(ics->ibuf, ics->ibuf + 1, ilen);
  }
  if(ilen)
    memmove(ics->ibuf, ibuf, ilen);
  else if(uf->eof)
    iconv(ics->ic, NULL, NULL, &obuf, &olen);
  ics->olen = olen / 4;
  ics->optr = 0;
  ics->ilen = ilen;
}
@

\section{Generic Normalized Unicode Input}

To make things even easier, we can make a function which returns a
normalized character.  Any normalization type we have code for, we
support.

<<Unicode normalization support exports>>=
typedef enum {
  UN_NONE, UN_NFD, UN_NFKD, UN_NFC, UN_NFKC, UN_NFKC_CF
} uni_normtype_t;
@

<<Known Data Types>>=
uni_normtype_t,%
@

<<Unicode normalization support exports>>=
#include "uni_io.h"

int uni_fgetc_norm(unifile_t *uf, uni_normtype_t nt);
@

<<Unicode normalization support functions>>=
int uni_fgetc_norm(unifile_t *uf, uni_normtype_t nt)
{
  <<Get normalized character>>
}
@

A simple function which returns a normalized character requires that
the current decomposition and composition state be kept.  It would be
ideal to extend the [[unifile_t]] structure with the extra information
rather than allocating yet another wrapper, but for now I would prefer
[[unifile_t]] to stay isolated to unnormalized I/O.

In order to make this transparent, the extra support structure is
automatically created and placed on a linked list.  It is assumed that
the file will not be closed and reopened without an intervening EOF.
It is also assumed that the function will not be called any more after
it returns EOF.

<<Unicode normalization support local definitions>>=
typedef struct unifile_norm {
  struct unifile_norm *next;
  unifile_t *uf;
  <<Normalized input buffer state>>
} unifile_norm_t;
static unifile_norm_t *norm_files = NULL;
@

<<Known Data Types>>=
unifile_norm_t,%
@

<<Get normalized character>>=
unifile_norm_t *ufn = norm_files;

while(ufn && ufn->uf != uf)
  ufn = ufn->next;
if(!ufn) {
  inisize(ufn, 1);
  clearbuf(ufn, 1);
  ufn->next = norm_files;
  norm_files = ufn;
}

<<Return normalized character if not EOF>>

unifile_norm_t **bufp;
for(bufp = &norm_files; *bufp != ufn; bufp = &(*bufp)->next);
*bufp = ufn->next;
free(ufn);

return -1;
@

For support, we'll need to store at least the maximal decomposition of
a character.  Technically, it is possible to require an infinite
buffer to support canonical ordering and composition, but we'll only
support a finite number and hope for the best.  Some future revision
should make the buffer automatically expand when necessary.

<<Normalized input buffer state>>=
/* maximal decomposition of a single char is 18 chars */
/* leave room in buf for at least 2 */
uint32_t buf[128];
@

We'll also need to know how many characters are in the buffer, and how
many of those are good to go.

<<Normalized input buffer state>>=
int blen, oklen;
@

To return a single character, we'll need to normalize until at least
one character could not possibly be modified any more.

<<Return normalized character if not EOF>>=
int oklen = ufn->oklen;
uint32_t *buf = ufn->buf;
while(!oklen && !uf->eof) {
  /* read a char into end of buf, if needed & possible */
  <<Read and normalize a character>>
}
if(oklen) {
  /* now we have oklen chars in buf that are ready to return */
  int c = buf[0];
  memmove(buf, buf + 1, (--ufn->blen) * sizeof(*buf));
  ufn->oklen = oklen - 1;
  return c;
}
@

To add more characters, append a character to the buffer and
decompose it.  The Unicode standard states that case folding
requires an additional NFD step beforehand, but reading of the data
suggests that instead, it needs canonical ordering and composition
afterwards, just like the NFC and NFKC.  This makes case folding just
another decomposition method.

int nread;

<<Read and normalize a character>>=
int c;
int bp = ufn->blen;
c = buf[bp] = uni_fgetc(uf);
if(nt == UN_NONE) {
  oklen = ufn->blen = ++bp;
  break;
}
if(c != -1)
  switch(nt) {
    case UN_NONE: /* here to remove warning */
    case UN_NFD:
    case UN_NFC:
      bp += uni_NFD_dec(buf + bp, 1);
      break;
    case UN_NFKD:
    case UN_NFKC:
      bp += uni_NFKD_dec(buf + bp, 1);
      break;
    case UN_NFKC_CF:
      bp += uni_NFKC_Casefold(buf + bp, 1);
  }
@

Then, the entire set of characters already read in can be ordered.  If
no characters can be obtained from canonical ordering, try to read
more right away.

<<Read and normalize a character>>=
int oblen = uni_Canon_Order(buf, bp, uf->eof);
if(!oblen) {
  ufn->blen = bp;
  continue;
}
@

Then, all characters which have been ordered can be composed.  If
composition might need more characters, the next trip around the loop
will get them.

<<Read and normalize a character>>=
switch(nt) {
  case UN_NONE: /* here to remove warning */
  case UN_NFD:
  case UN_NFKD:
    /* no composition */
    oklen = oblen;
    break;
  case UN_NFC:
  case UN_NFKC:
  case UN_NFKC_CF: {
    int cblen = uni_NFC_comp(buf, oblen, uf->eof ? NULL : &oklen);
    if(uf->eof)
      oklen = cblen;
    /* move unordered characters down if composition removed chars */
    if(cblen < oblen) {
      if(bp > oblen)
        memmove(buf + cblen, buf + oblen, (bp - oblen) * sizeof(*buf));
      bp -= oblen - cblen;
    }
    break;
  }
}
ufn->blen = bp;
@

\chapter{Testing}

Here is a master driver to call some of the above code for testing.

<<C Test Support Executables>>=
tst \
@

<<tst.c>>=
<<Common C Header>>
#include "uni_all.h"

@

For I/O, I'll just use a decent size fixed buffer, and in fact use it
as both a UCS-32 buffer and a character buffer, allowing both at the
same time by marking the divider with [[blen]].

<<tst.c>>=
#define BUF_SIZE 1024
int buf[BUF_SIZE];
int blen;

#define cbuf ((char *)(buf+blen))
#define cblen ((BUF_SIZE - blen) * sizeof(int))
@

The purpose of this program is to take Unicode characters as input,
and do things with them.

<<tst.c>>=
int main(int argc, const char **argv)
{
  <<Get and show characters for [[tst]]>>
  <<Manipulate characters for [[tst]]>>
  return 0;
}
@

For this program, input means command line arguments.  Characters are
specified by either a hexadecimal code point or a Unicode character
name.  They are all appended to [[buf]].

<<Get and show characters for [[tst]]>>=
int cp;

while(--argc > 0) {
  ++argv;
  int l = strlen(*argv);
  const int *mcp;
  cp = cp_of_uni(*argv, l, &mcp);
  if(cp >= 0) {
    /* valid name */
    if(!mcp)
      buf[blen++] = cp;
    else {
      int i;
      for(i = 0; i < cp; i++)
        buf[blen++] = mcp[i];
      cp = mcp[0];
    }
  } else {
    const char *s;
    cp = strtol(*argv, (char **)&s, 16);
    if(!*s) {
      /* valid hex code point */
      buf[blen++] = cp;
    } else {
      /* not valid anything */
      fprintf(stderr, "Can't parse argument %s\n", *argv);
      exit(1);
    }
  }
}
@

Next, we'll print some more information about each character.  This
includes characters artificially added by multi-character sequences.

<<Get and show characters for [[tst]]>>=
int i;
for(i = 0; i < blen; i++) {
  cp = buf[i];
  <<Show character for [[tst]]>>
}
@

First, I'd like to see the Unicode names, as well as the hexadecimal
code points.

<<Show character for [[tst]]>>=
printf("Character %04X\n", cp);
if(uni_gen_name_for(cp, cbuf, cblen))
  printf("Unicode: %s\n", cbuf);
@

Then, maybe some of the character classifications.  These are all
printed out by the [[chartypes]] mainline, but there is no harm in
repeating it.

<<Show character for [[tst]]>>=
fputs("Character class: ", stdout);
#define i cp
<<Print flags and classes of cp [[i]]>>
#undef i
putchar('\n');
@

Next, we'll try some normalization.  But first, we'll print the entire
buffer out before starting, and afer each step.

<<Manipulate characters for [[tst]]>>=
#define prbuf(s) do { \
  fputs(s":", stdout); \
  for(i = 0; i < blen; i++) \
    printf(" %04X", buf[i]); \
  putchar('\n'); \
  putchar('\''); \
  utf8_fputs(buf, blen, stdout); \
  putchar('\''); \
  putchar('\n'); \
} while(0)
prbuf("Start string");
@

Of course these are all destructive.  Another test program should
probably do normalization on copies so that the original can be reused
for more tests.

<<Manipulate characters for [[tst]]>>=
blen = NFD_dec(buf, blen);
prbuf("After decomp");
Canon_Order(buf, blen, 1);
prbuf("After canon-order (NFD)");
blen = NFC_comp(buf, blen, NULL);
prbuf("After canon-comp (NFC)");
blen = NFKD_dec(buf, blen);
prbuf("After compat-decomp");
Canon_Order(buf, blen, 1);
prbuf("After canon-order (NFKD)");
blen = NFC_comp(buf, blen, NULL);
prbuf("After canon-comp (NFKC)");
blen = NFKC_Casefold(buf, blen);
prbuf("After case folding (NFKC_CF)");
@

As a more thorough character name test, we can run every single known
character through the [[tst]] program.  This does not guarantee that
the lists themselves are OK, or that bad names will not trigger false
positives, but it's better than nothing.

<<Test Scripts>>=
tstchars \
@

\lstset{language=sh}
<<tstchars>>=
#!/bin/sh
<<Common NoWeb Warning>>
@

First, we'll iterate through all of the Unicode names mapping to one
character, both forward and reverse.

<<tstchars>>=
cat charnames.uni{a,} | while IFS=, read a b; do
  b=$(($b))
  hb=`printf %04X $b`
  tr=$(./tst "$a" | head -n 1)
  tr="${tr#* }"
  test $hb = "$tr" || echo "$a,$b $hb"
  tr="`./tst $hb | grep -a '^Unicode:'`"
  tr="${tr#* }"
  test "$a" = "$tr" && continue
  ab=`grep "^$a," charnames.uni{a,} | cut -d: -f2 | cut -d, -f2`
  test "$ab" = "$b" || echo "$hb $a $tr"
done
@

Next, all of the multi-character Unicoded characters.  They can only
be tested for name interpretation, since reverse lookups on
multi-character sequences are impossible.

<<tstchars>>=
cat charnames.uniseq | while IFS=, read a b; do
  tr=
  for x in $(./tst "$a" | grep -a '^Character [0-9A-F]' | cut -d\  -f2); do
    tr=$tr,$((0x$x))
  done
  test ,$b = "$tr" || echo "$a,$b $tr"
done
@

Another useful check is to ensure that the string table's strings are
all actually unique.  This is done by traversing each hash bucket to
see if there are any duplicates.

<<makefile.rules>>=
# explicit strs.gen dep should not be necessary, but gmake flakes on it
hashcheck: uni_strs.o strs.gen
	$(CC) -o $@ $(CFLAGS) $(LDFLAGS) $(EXTRA_CFLAGS) -DHASHCHECK uni_strs.c
@

<<C Test Executables>>=
hashcheck \
@

\lstset{language=C}
<<uni_strs.c>>=
#ifdef HASHCHECK
int main(void)
{
    int i;
    int err = 0;
    for(i = 0; i < HTAB_SIZE; i++) {
        if(!strhash[i])
            continue;
        int j, k;
        for(j = strhash[i]; builtin_hashe[j - 1].nextent;
	                                    j = builtin_hashe[j - 1].nextent) {
            const char *js = builtin_hashe[j - 1].off + builtin_str;
            int jl = num_before(js, NULL);
            for(k = builtin_hashe[j - 1].nextent; k;
	                                    k = builtin_hashe[k - 1].nextent) {
                const char *ks = builtin_hashe[k - 1].off + builtin_str;
                if(num_before(ks, NULL) == jl && !memcmp(js, ks, jl)) {
                    fprintf(stderr, "duplicate string %d %d %.*s\n",
                            builtin_hashe[j - 1].off, builtin_hashe[k - 1].off,
                            jl, js);
                    err = 1;
                }
            }
        }
    }
    printf("%lu string space; %lu hash entries\n",
           (unsigned long)builtin_str_size,
           (unsigned long)builtin_hashe_size);
    return err;
}
#endif
@

\chapter{Code Index}
\nowebchunks

% Some might prefer having the Code Index in an appendix as well.
% For this document, I'm making the appendix the "user documentation"
\appendix

% Begin-doc Users-Guide
\chapter{Building}

Before starting, ensure that all prerequisites are present.  To build
this, build.nw and tjm-ext.nw are required.  If you obtained the PDF
or HTML version of this document, these are included.  Otherwise, they
may be obtained the same place you got this.  The other prerequisites
are the Unicode data files.  Make sure the file locations are also set
correctly in the config file.

\begin{itemize}
\item The Unicode Character Database; this library was tested using
versions 6.0.0 and 6.2.0.\\
\url{http://www.unicode.org/Public/zipped/}\emph{version}/UCD.zip
\item The Unicode Han Database; this should match the UCD version.\\
\url{http://www.unicode.org/Public/zipped/}\emph{version}/Unihan.zip
\item The Default Unicode Collation Element Table; this should match
the UCD version.  It should be placed in the same directory as the UCD.\\
\url{http://www.unicode.org/Public/UCA/}\emph{version}/*
\item The Unicode Common Locale Data Repository; make sure that the
version is compatible with the UCD version.\\
\url{http://www.unicode.org/Public/cldr/latest/core.zip}
\item The XML entity database; the only version currently supported is
the one at the following URL:\\
\url{http://www.w3.org/2003/entities/2007xml/unicode.xml}
\item The Unicode IDNA compatibility database (optional); this should
match the UCD version.  If present, it should be in the same directory
as the UCD, or in a subdirectory or sibling directory named idna.\\
\url{http://www.unicode.org/Public/idna/}\emph{version}/*
\item The Unicode security information database (optional).  If
present, it should be in the same directory as the UCD, or in a
subdirectory or sibling directory named security.\\
\url{http://www.unicode.org/Public/security/latest/}uts39-data-\emph{version}.zip
\end{itemize}

\input{build-doc.tex} %%% doc
The full, documented [[makefile.config]] is reproduced here for
convenience.  In particular, the non-generic configuration parameters
start at [[UCD_LOC]].

\input{makefile.config.tex} % make

\chapter{API Documentation}

The easiest way to include this library's definitions is using
[[#include "uni_all.h"]].  Linking requires [[-luni]].

\section{Data Types for Storage}

\subsection{Bit Sets}

The following functions support storage of Unicode boolean properties
in simple bit sets.  [[a]] is an array of any unsigned integer data
type.  [[b]] is a bit number, and [[e]] is an index into [[a]].
Normally, extra code would be added to shorten the array by indicating
a lower and upper bound.

\input{Simple bit set definitions.tex} % C

\subsection{Sorted Code Point Arrays}

An array of any data type whose first member is a 32-bit signed
integer can be searched and sorted using the standard C [[bsearch]]
and [[qsort]] functions with the following comparison routine:

% uni_cmp_cp prototype

Note that valid Unicode code points are always positive when stored in
a signed 32-bit integer.  A convenience wrapper for [[bsearch]] is
provided, but in general a hand-written binary search can perform
significantly faster.

% uni_is_cp prototype

For convenience, a structure is defined for code point arrays with
32-bit values.  As the [[cp]] member is first, [[uni_cmp_cp]] works
with arrays of this type as well, if [[cp]] is a valid Unicode code
point.  If not, care must be taken that the value never exceeds the
maximum [[int32_t]] value ([[0x7fffffff]]).

\input{[[uni_cp_val_t]].tex} % C

\subsection{Sorted Code Point Range Arrays}

Sorted code point range arrays are arrays of structures whose first
two members define an (inclusive) range; the ranges do not overlap, so
sorting can be done using only the low element.

\input{[[uni_chrrng_t]].tex} % C

An array of any data type whose first two members are 32-bit signed
integers indicating a contiguous range of values, as per the
[[uni_chrrng_t]] data type (if the [[low]] and [[high]] values do not
exceed the maximum [[int32_t]] value), may be searched and sorted
using the standard C [[bsearch]] and [[qsort]] functions with the
following comparison routine:

% uni_cmprng prototype

A convenience wrapper for binary searching is provided.  It uses a
hand-written binary search for significant performance improvement
over using [[bsearch]].  However, unlike the macro provided for single
code point arrays, this function can only be used for arrays of the raw
[[uni_chrrng_t]] type.  Since it uses the raw structure, values
exceeding the maximum [[int32_t]] value are treated as unsigned values
still, so care must be taken that sorting either uses a different
comparison function than above or that the values never exceed this
value.

% is_cp_chrrng prototype

In addition to plain ranges, ranges may store data.  One variant,
[[uni_chrrng_dat_t]], stores a byte of data along side a
24-bit high range value.  This keeps the structure the same size, but
makes it a bit less efficient for searching:

\input{[[uni_chrrng_dat_t]].tex} % C

The comparison function for [[qsort]] and [[bsearch]] is different,
since it needs to mask out the value:

% uni_cmprng_dat prototype

Since the return value is a byte rather than a flag, the wrapper for
binary searching (again, using a hand-written implementation rather
than [[bsearch]]) returns the byte value, or a given default ([[def]])
if the code point is not in the array.

% find_cp_chrrng prototype

Another variant, [[uni_chrrng_num_t]], stores a rational number as a
24-bit numerator ([[num]]), and 8-bit denominator ([[denom]]).  This
extends the structure size to 3 32-bit words.  Technically, any pair
of numbers could be stored in this structure; the only thing the
functions do to support this interpretation is to set the denominator
to one if the numerator is zero, so that zero can be more conveniently
stored as zero over zero.  Also, the default return value is always
zero (over one).

\input{[[uni_chrrng_num_t]].tex} % C

Since no changes are made to the range portion of the structure, the
[[uni_cmprng]] function can be used for [[qsort]] and [[bsearch]].  An
optimized lookup function is provided as well:

% uni_chrrng_val prototype

\subsection{Multilevel Tables}

Multi-level tables are bit arrays split into smaller chunks, with
pointers to those chunks in a table at the next level.  Each level
except the top is also split into smaller chunks with the next level
being pointers to that level.  Duplicates in lower levels are removed
by having only one copy with multiple pointers to that copy.  All-zero
data and all-one data are also stored with special pointer values,
making sparse and repetitive bit arrays fairly storage-efficient.

The tables are stored as an opaque sequence of 32-bit words which are
interpreted dynamically.  When reading such a data structure, only the
pointer to the first word is needed.  When creating such a structure,
both the sequence of words and the total length are returned.

Use the following function to convert a plain bit array to a
multi-level table.  It may take a long time to run, so generally it
should be used to pre-generate tables.  The bit array is described by
[[bits]], which is an array of [[len]] bytes.  The [[low]] and
[[high]] numbers are the valid bit numbers; it is up to the user to
ensure that [[high]] - [[low]] $<$ [[len]] times 8.  The data need
not be a bit array; arbitrary data structures are supported.  The
[[minwidth]] parameter is the number of bytes in the data structure;
this is ignored if less than 4.  It is possible to make values outside
of the [[low]] to [[high]] range be either all zeroes or all ones;
[[def]] should be one or zero to indicate which.  The return value,
[[*ml]], is always allocated using [[malloc]], and contains [[*ml_len]]
32-bit words.

% bits_to_multi prototype

Once a table [[dat]] is generated, the first byte of data for a code
point [[val]] can be found using [[multi_tab_lookup]].  For data
structures whose size is not one byte, the code point must be
multiplied by the number of bytes.  For bit arrays, this means
dividing by 8.  For bit arrays, the actual bit must then be obtained
using a bit mask.

The return value from the function is zero for all zeroes, [[~0]] for all
ones, and 1 for anything else.  [[ret]] is [[NULL]] for all zeroes or
all ones, and a pointer to the first byte of the result otherwise.  If
the code point is outside the range defined by the table ([[low]] and
[[high]] during creation), and [[def]] is zero, the default provided
during table creation is returned (i.e., all zeroes or all ones).  If
[[def]] is non-zero, [[def]] is returned by the function, but [[ret]]
is [[NULL]].  Note that if [[def]] is [[~0]], there is no way to tell
the difference between out-of-range and all ones.

% multi_tab_lookup prototype

The above lookup function returns a pointer to raw data.  Since they
are most common, bit array tables can be queried using a simpler
function, which takes the unscaled code point and returns a boolean
flag.

% is_uni_x prototype

To get a list of all members of a multi-level table bit array, it can
be converted into a sorted range list.  The result ([[*ret]]) is always
allocated using [[malloc]].

% multi_bit_to_range prototype

The inverse operation is also possible, but it consumes a large amount
of time and memory.  Internally, the range list is first converted
into a plain bit array, which is then converted using
[[bits_to_multi]] above.  The return value is the multi-level table,
allocated using [[malloc]].  If [[ml_len]] is non-NULL, the storage
size of the table is returned as well.

% rng_to_multi_bit prototype

The multi-level tables can store more information, as well.  The
lookup function merely returns a pointer to a byte.  This byte can be
a collection of bits, or it can be a byte value.  For the latter case,
a separate generic lookup function is provided.  This takes a
byte-sized default value, and returns this if the byte is out-of-range
or zero.  Otherwise it returns the stored value, minus one.  This way,
the default value is stored most efficiently (as a zero), and the rest
only require minor adjustment on return.

% uni_x_of prototype

A function is provided to convert a [[uni_chrrng_dat_t]] sorted range
list to an offsetted multi-level byte array.  However, no inverse
function is provided.  This operation is not really intended for end
users, but for the static table generator.  Note that while the
default value must be provided, it is not stored, but instead is used
to remove the default value from the table by converting it to zero.

% rng_dat_to_multi prototype

By multiplying the value size with the lookup index, larger values can
be stored as well.  One implementation is to store the numerator and
denominator of a rational number as a 32-bit value.  To support
looking up these values, a generic function is provided.

% uni_x_val prototype

In addition, the [[uni_chrrng_num_t]] tables can be directly converted
to this format.  The inverse function is not provided.

% rng_num_to_multi prototype

For values where ranges are inappropriate, a function is provided to
convert a sorted code point array with 32-bit values (nominally
[[uni_cp_val_t]]) into a multi-level table.  No inverse function is
provided.  No generic lookup function is provided, either; remember to
multiply code point values by 4 when using [[multi_tab_lookup]].

% cp_val_to_multi prototype

\section{Boolean Properties}

The supported boolean properties are listed in table
\ref{tab:boolprop}.  For each supported boolean property \emph{BP},
the following symbols are defined:

\begin{itemize}
\item [[const uni_chrrng_t uni_]]\emph{BP}[[_rng[]]] is a sorted range
table with entries for that property.
\item [[uni_]]\emph{BP}[[_rng_len]] is the number of entries in that
array (a preprocessor symbol).
\item [[const uint32_t uni_]]\emph{BP}[[_mtab[]]] is a multi-level
table for that property.
\item [[int is_uni_]]\emph{BP}[[(uint32_t cp)]] is a boolean lookup
function for that property.  It is actually a preprocessor macro which
uses the multi-level table and [[is_uni_x]] internally.
\end{itemize}

For static linking, each property is in a separate object file, and
the sorted range table and multi-level table are stored separately, as
well.

\begin{table}
\begin{centering}
\begin{tabular}{llll}
AHex&Alpha&ASSIGNED&Bidi\_C\\
Bidi\_M&Cased&CE&CI\\
Comp\_Ex&CWCF&CWCM&CWKCF\\
CWL&CWT&CWU&Dash\\
Dep&DI&Dia&Ext\\
Gr\_Base&Gr\_Ext&Hex&IDC\\
Ideo&IDS&IDSB&IDST\\
Join\_C&LOE&Lower&Math\\
NChar&NFD\_QC&NFKD\_QC&Pat\_Syn\\
Pat\_WS&QMark&Radical&SD\\
STerm&Term&UIdeo&Upper\\
VS&WSpace&XIDC&XIDS\\
\end{tabular}
\caption{\label{tab:boolprop}Boolean properties (Unicode canonical short name)}
\end{centering}
\end{table}

\subsection{Bit Set Operations}

The following functions perform a generic bit operation (see table
\ref{tab:setop2} for a complete list of operations).

\input{op-table.tex} %%% doc
\caption{\label{tab:setop2}Set Operations}
\end{table}

Bit arrays consisting of a single unsigned integer can be operated on
using the following macro:

% BIT_SET_OP prototype

The [[op]] argument is intended to be one of the [[uni_setop_t]]
enumeration constants.

\input{[[uni_setop_t]].tex} % C

Arbitrary bit arrays of standard C unsigned integer types can be
operated on using the following functions.  The inputs are bit arrays
[[a]] and [[b]], which start at [[a_low]] and [[b_low]], respectively,
and contain [[a_len]] and [[b_len]] elements of the particular bit
size, respectively.  Although the endianness of each element of an
array is irrelevant, the array itself is assumed to be in
little-endian order if the arrays are not of identical size and
offset.  The result ([[res]]) is allocated from new memory using
[[malloc]] if the pointed-to pointer is NULL, or resized using
[[realloc]] if non-NULL and the reported [[res_max]] is too small. 
Its low bit is [[res_low]].  The number of relevant elements of the
bit size is returned in [[res_len]], and the actual number of elements
allocated is returned in [[res_max]].  All return pointers must be
non-[[NULL]].

% setop_bit64 prototype
% setop_bit32 prototype
% setop_bit16 prototype
% setop_bit8 prototype

Since sorted code point arrays are generally inefficient for storing
Unicode boolean properties (sets), no set operations are provided for
this data type.  However, the following sample code may be used if
needed:

\input{Perform set ops on cp array.tex} % C

For sorted code point range arrays, a single function is provided.  It
always allocates its results using [[malloc]].

% uni_chrrng_setop prototype

No set operations are provided for multi-level tables at this time.
Implementation would be insanely complex given the current
implementation, and should not be performed frequently, anyway.  The
only way to perform these operations right now is to convert the
multi-level tables to range arrays first, and then convert the result
back.  Alternately, the lookup function can simply be called for each
table, and the lookup results combined instead, using [[BIT_SET_OP]].


\section{Enumerated Properties}

For lists of names, there are both the canonical names and aliases.
These are stored in an array of [[uni_alias_t]] structures.  Any
undefined names are [[NULL]].

\input{[[uni_alias_t]].tex} % C

For reverse lookups, the [[uni_valueof_t]] structure's [[val]] field
is the index of that name in the alias array.

\input{[[uni_valueof_t]].tex} % C

An array of [[uni_valueof_t]] structures can be searched or sorted
using the generic comparison function.

% cmp_valueof prototype

The list of supported enumeration properties is in table
\ref{tab:enumprop}.  For each enumerated property \emph{EP}, the
following symbols are defined:

\begin{itemize}
\item [[uni_]]\emph{EP}[[_t]] is a C enumeration type whose literals
correspond to the property's values.
\item For each canonical enumeration value name \emph{EN}, the symbol
[[U_]]\emph{EP}[[_]]\emph{EN} is defined, which is an enumeration
literal of type [[uni_]]\emph{EP}[[_t]].  Enumeration literals
corresponding to canonical aliases are defined as well, as long as they
do not contain a minus sign or a decimal point.
\item [[U_NUM_]]\emph{EP} is another enumeration literal of type
[[uni_]]\emph{EP}[[_t]], which corresponds to the number of unique
values.  Note that where canonical aliases are present, this number
is smaller than the actual number of enumeration symbols.
\item [[const uni_alias_t uni_]]\emph{EP}[[_nameof[]]] is an array of
names corresponding with the enumeration literals.  The index is
simply a valid enumeration literal value.
\item [[const uni_valueof_t uni_]]\emph{EP}[[_valueof[]]] is a sorted
array of all unique names (including aliases) for this property; the
[[val]] field in each entry is the associated enumeration literal.
\item [[uni_]]\emph{EP}[[_valueof_len]] is the length of the
[[valueof]] array (a preprocessor symbol).
\item [[const uni_chrrng_dat_t uni_]]\emph{EP}[[_rng[]]] is a sorted
array of ranges.  The default value is not provided here.
\item [[uni_]]\emph{EP}[[_rng_len]] is the length of the range
array (a preprocessor symbol).
\item [[uint32_t *uni_]]\emph{EP}[[_mtab]] is a multi-level table whose
byte-length values correspond to the enumeration literal values.
\item [[int uni_]]\emph{EP}[[_of]] is a preprocessor macro which
returns the enumeration value for any code point.  The canonical
default is returned when the table has no entry.  Internally, the
multi-level table and [[uni_x_of]] are used for lookup.
\end{itemize}

For static linking, each property is in a separate object file.  In
addition, the range tables and multi-level tables are stored
separately.  The two name tables are stored together in order to share
constant name strings, but are stored separately from the range and
multi-level tables.

\begin{table}
\begin{centering}
\begin{tabular}{llll}
age&bc&blk&ccc\\
dt&ea&gc&GCB\\
hst&jg&jt&lb\\
NFC\_QC&NFKC\_QC&nt&sc\\
SB&WB&IDNA\_Status&ID\_Restrict\_Status\\
ID\_Restrict\_Type&&&\\
\end{tabular}
\caption{\label{tab:enumprop}Enumerated properties (Unicode canonical short name)}
\end{centering}
\end{table}

The IDNA\_Status property will not be available if the IDNA data files
were not present when this library was compiled.  The
ID\_Restricted\_Status and ID\_Restricted\_Type fields will not be
available if the security data files were not available when this
library was compiled.  The ID\_Restrict\_Type enumeration literals
change minus signs to underscores; this is the only current
enumeration that supports enumeration names with minus signs.

In addition, the gc property defines a symbol to support aliases which
cover more than one value.  For example, the enumeration literal
[[U_gc_Z]] is actually an alias for [[U_gc_Zl]], [[U_gc_Zp]], and
[[U_gc_Zs]].  The [[const uint64_t uni_gc_trans[]]] table takes an
enumeration literal as index, and returns a bit mask with all
appropriate base classes included with this literal set.  The base
classes themselves only have one bit set; to check if a class is a
base class, simply check if the bit is set for itself.

There are two additional support macros for the gc property.
[[uni_is_nl]] checks if a character is any valid newline-type character,
and [[uni_is_bs]] checks if a character is any valid backslash-type
character.  [[is_bs]] is not necessary for text already processed
using NFKC\_CF.

% uni_is_nl prototype
% uni_is_bs prototype

\section{Numeric Properties}

Each numeric property \emph{NP} is supported by the following symbols:

\begin{itemize}
\item [[const uni_chrrng_num_t uni_]]\emph{NP}[[_rng[]]] is a sorted range
table whose value is a rational representation of the numeric value.
\item [[int uni_]]\emph{NP}[[_rng_len]] is the length of the sorted
range table.
\item [[const uint32_t *uni_]]\emph{NP}[[_mtab[]]] is a multi-level
table with the same data.
\item [[void uni_]]\emph{NP}[[_val(uint32_t cp, const uint32_t *tab, int32_t *num, uint8_t *denom)]]
is a simple wrapper lookup function that uses the multi-level table to
retrieve the numerator and denominator.
\end{itemize}

The age property is officially numeric, but not supported as such. 
Instead, it is only supported as an enumerated property.  The ccc
property is similarly only enumerated, but the value of the
enumeration literal is equal to the numeric value of the property as
well.

The slc, suc, stc and scf properties are technically string properties,
not numeric properties.  However, since they always map to exactly one
character (or nothing), they can be considered numeric properties
whose value is the translated code point (or zero if nothing).

In addition, the value for the cjkRSUnicode field is interpreted as a
series of dotted numbers, with optional exclamation points.  This is
all encoded into the numerator and denominator fields, and can be
extracted using decoder macros.  The [[l]] parameter is set to the
number before the decimal point, and the [[r]] is set to the number
after the decimal point.  If both are zero, the number is not present.
If [[r]] is negative, then an exclamation point precedes the decimal
point, and the right side is the absolute value of [[r]].  When there
is more than one value, the [[val2]] macro will return a non-zero value.

% uni_kRSUnicode_val1 prototype
% uni_kRSUnicode_val2 prototype

\begin{table}
\begin{centering}
\begin{tabular}{llll}
nv&cjkRSUnicode&slc&suc\\
stc&scf&&\\
\end{tabular}
\caption{\label{tab:numprop}Numeric properties (Unicode canonical short name)}
\end{centering}
\end{table}

\section{String Properties}

Properties with string values are stored in two parts: a constant
string table, and tables which have offsets and lengths of strings
within that table.  Multiple properties may use the same string table.
The offset and length tables are either sorted code point arrays of
type [[uni_str_rng_t]], or multi-level tables of 32-bit values, which
are interpreted as (and can be cast into) [[uni_str_rng_val_t]].  The
[[flags]] fields in these structures are sometimes used for
property-specific purposes.

\input{[[uni_str_rng_t]].tex} % C
\input{[[uni_str_rng_val_t]].tex} % C

Note that the [[uni_str_rng_t]] data structure can also be cast to the
[[uni_cp_val_t]] type, whose [[val]] element can then be cast to
the [[uni_str_rng_val_t]] type.

The supported string properties are listed in table
\ref{tab:stringprop}.  Unless otherwise stated in the subsequent
sections, for each string property \emph{SP}, the following symbols
are defined:

\begin{itemize}
\item [[const uint32_t uni_]]\emph{ST}[[_strs[]]] is a string table
containing all string values.  \emph{ST} may or may not be the same as
\emph{SP}.
\item [[const uni_str_rng_t uni_]]\emph{SP}[[_rng[]]] is a sorted list
of code points with their associated string offsets and lengths.
\item [[uni_]]\emph{SP}[[_rng_len]] is the length of the above array
(a preprocessor symbol).
\item [[const uint32_t uni_]]\emph{SP}[[_mtab[]]] is a multi-level
table of 32-bit [[uni_str_rng_val_t]] values.
\end{itemize}

For static linking, the string table, sorted code point table, and
multi-level table are all in separate object files.

\begin{table}
\begin{centering}
\begin{tabular}{llll}
dm&canon\_decomp&compat\_decomp&canon\_comp\\
lc&uc&tc&cf\\
tcf&NFKC\_CF&bmg&cjkTraditionalVariant\\
cjkSimplifiedVariant&cjkCompatibilityVariant&IDNA\_Mapping&DUCET\\
DUCET\_CLDR&scx&&\\
\end{tabular}

\small{The following properties are not standard Unicode properties:
canon\_decomp, compat\_decomp, canon\_comp, tcf, DUCET, DUCET\_CLDR.}
\caption{\label{tab:stringprop}String Properties (Unicode canonical short name)}
\end{centering}
\end{table}

\subsection{Normalization}

Normalization data includes the decomposition, composition, full case
folding, and character combining class information.  This is all
combined into convenient functions for normalization.  Each
Unicode-defined normalization method has its own function:  canonical
decomposition (NFD), canonical composition (NFC), compatibility
decomposition (NFKD), compatibility composition (NFKC), and
compatibility decomposition with case folding (NFKC\_CF).  Each
function takes a buffer ([[buf]]) with a given number of input
characters ([[blen]]), and returns the updated number of characters.
Care must be taken to enusre enough space for the result:  generally
18 times the length of the input.  All of these assume that the innput
buffer is complete; that is, there are no more possible characters for
input into the normalization process.

% uni_NFD prototype
% uni_NFC prototype
% uni_NFKD prototype
% uni_NFKC prototype
% uni_NFKC_CF prototype

To support the normalization macros above, canonical ordering must be
performed.  The following function takes a buffer ([[buf]]) of a given
length ([[blen]]), and sorts the characters in-place.  It will never
need to expand the length of the array.  However, it may need more
characters to fully sort the input.  The [[last]] parameter indicates
that the function must assume there are no more characters available.
Otherwise, it will return the full length of the input if no more
characters are needed, or less than the full length if more are
needed.  If more are needed, the return value indicates the number of
characters which have already been sorted; these can safely be removed
before the next pass.

% uni_Canon_Order prototype

\subsection{Decomposition}

The raw decomposition data is not meant to be used directly.  Instead,
the following wrapper functions are provided.  They perform just the
decomposition step for the decomposition normal forms; canonical
ordering needs to be performed manually afterwards.  Like the full
normalization functions, they modify buffers in-place, so enough room
for the output (maximum 18 times the size of the input) needs to be
provided.  The return value is the length after transformation.

% uni_NFD_dec prototype
% uni_NFKD_dec prototype

Raw decomposition data is stored in [[uni_dm_full_strs]] (i.e.,
\emph{ST} is dm\_full).  This includes the raw dm property, as well as
two pseudo properties.  The canon\_decomp property contains only
full (recursive) canonical decompositions, and the compat\_decomp
property contains only full (recursive) compatibility decompositions
when they differ from the canon\_decomp result.  The raw dm property
is not recursively/fully decomposed.  The only enhancement of the dm
property is that no dt lookup is required to determine if a
decomposition is canonical or compatibility; the flag field is
non-zero for compatibility decompositions.

Lookup with the multi-level tables is simplified with some
preprocessor macros, which return the associated string's offset and
length.  For the compatibility lookups, an optional pointer to a flag
([[compat]]) can be used to check if the returned decomposition was
not canonical.

% uni_find_canon_decomp prototype
% uni_find_compat_decomp prototype
% uni_find_dm prototype

These macros use the internal-use only [[uni_x_dec]] function to do
their work.  For Hangul syllable strings, the [[h]] flag determines
behavior.  If positive, it acts as the [[full]] flag below, and the
returned offset is always $-1$, but the length is correct.  If
negative, the returned offset is always 1, and the length is zero.

% uni_x_dec prototype

For the Hangul syllables, a separate function must be use to
decompose.  A Hangul syllable string in the form LVT or LV can be
singularly or completely decomposed (dependent on the [[full]] flag)
using the following function.  The function will return nothing (-1)
if the input is not a decomposable Hangul syllable string, so it is
safe to call whenver no decompositionmapping is found in the tables.
Otherwise, it fills in the return buffer ([[res]]) and returns the
number of filled-in values (always 2 or 3).  The return buffer
must contain space for at least 2 characters if the [[full]] flag is
not set, or 3 if it is set.

% hangul_syllable_decomp prototype

To make this easier, a wrapper macro is provided.  This assumes that
at least [[len]] words are available in the passed-in buffer
([[buf]]), and copies out the result.  This is meant to be called
after [[uni_find_]]\emph{xxx}, using its results combined with the
[[hangul_syllable_decomp]] results.

% uni_get_decomp prototype

\subsection{Composition}

The raw composition data is not meant to be used directly.  Instead,
the following wrapper functions are provided.  They perform just the
composition step for the composition normal forms; canonical
decomposition needs to be performed manually beforehand.  Like the
full normalization functions, they modify buffers in-place.  However,
composition never extends the length of the input, so no extra space
needs to be available in the buffer.  However, more characters may be
needed to determine if composition is possible.  if [[nok]] is NULL,
the input buffer is assumed to be complete.  Otherwise, if the
returned [[*nok]] is less than the returned (updated) buffer length,
then the first [[*nok]] characters have been composed successfully,
but the remainder needs additional input before completion.  In any
case, the return value is the updated length of the buffer after
composition.

% uni_NFC_comp prototype

The raw composition tables, derived from the dm and Comp\_Ex
properties, are designed for multi-step lookup as well. In addition to
having a gap for the Hangul syllable strings, composition takes two
code points for input rather than just one.  The latter problem is
solved by looking up a ``string'' using the first input, which is
actually a sub-table that can be searched using the second input.  The
format of the sub-table is like a [[uni_cp_val_t]] structure; the
value is the composed character.  These sub-tables are stored in the
string table [[uni_canon_comp_strs]] (i.e., \emph{ST} is canon\_comp).
Only one pseudo-property is defined for this: canon\_comp.

The following macro takes the first element of the composition pair
and returns the offset and length of the sub-table by using
[[uni_x_dec]] and the multi-level table.

% uni_find_canon_comp prototype

To look up values in the sub-table, a separate search function is
provided.  Again, a hand-coded binary search is used instead of
[[bsearch]] for speed.

% uni_lookup_compent prototype

To look up Hangul syllable compositions, another separate function is
provided.  It returns zero if there is no valid composition; otherwise
it returns the result of composition.

% hangul_syllable_comp prototype

Combining the above two steps can be done using a wrapper macro.  Thus
the preferred method of looking up compositions is to call
[[uni_find_canon_comp]] for a code point, and then to use the results
for all candidates for the second input to composition with
[[uni_canon_comp]].

% uni_canon_comp prototype

\subsection{Collation Tables}

The Default Unicode Collation Element Table is available as the DUCET
pseudo-property.  The CLDR version of the DUCET is available as the
DUCET\_CLDR pseudo property.  Both of these properties also define the
variable uni\_\emph{property}\_var\_top, which is the default variable
top parameter for that table.  As with the normalization tables, raw
lookup is not encouraged.

To find the Unicode Collation Algorithm key string for a Unicode
string [[str]] of length [[slen]], the following function is provided.
It is meant to be called on normalized (NFD) strings, although there
are special exceptions where full normalization is not needed.  See
the standard for details.  The [[opts]] parameter may be [[NULL]], or
it may point to a structure which is zeroed out except for selected
parameter settings.  The return value is a zero-terminated array of
32-bit words, which is allocated using [[malloc]].

Note that the key consists of 32-bit words.  Keys cannot be compared
using [[memcmp]] unless the underlying architecture is big-endian.  If
a key is required to be byte-comparable, either use [[htonl]] on each
word, or use one of the various proposals in the UCA standard.  Note
also that zero bytes are likely present within the key, so [[strcmp]]
is not even a candidate.

% uni_str_uca_key prototype

The [[tab]] and [[strs]] parameters must be set together, and specify
the static data for the DUCET.  By default, the standard Unicode DUCET
is used.  Normally, the [[vartop]] parameter would be set at the same
time; by default it is the standard Unicode DUCET's variable top.

The [[level]] parameter is the maximum significant key level; by
default, it is 3.  If it is larger than 4, [[do_literal]] is implied.
The [[do_literal]] parameter, if non-zero, appends the literal input
string to the key.  Note that although the non-literal portions of the
key will never contain a zero word, no check is made that the input
string has the same property.  In order to retain the zero termination
property of the sort key, zeroes in the input string should be
converted to non-zeroes.  For example, both zeroes and ones could be
converted to ones, followed by one or two depending on the original
value.  This will not affect the sort order, as zeroes, ones, and twos
are all fully ignorable.

The behavior of variable weights is controlled by the [[var_mode]]
parameter, whose values correspond to the standard methods.

\input{[[uni_uca_opts_t]].tex} % C

\input{DUCET variable modes.tex} % C

To perform a straight string comparison without fully generating keys,
use the following function.  Once again, the [[opts]] parameter may be
NULL or a set of collation options.

% uni_uca_strcmp prototype

To perform raw DUCET lookups for strings, use the following function.
The return value is an array allocated using [[malloc]] of length
[[*rlen]].  If [[llen]] is non-NULL, it must be an array of length 4,
and the number of non-zero values at each of the four levels is stored
into that array, in order.  This performs all steps of the UCA except
for ordering the values by level, then code point rather than
vice-versa.  The format of the returned array is either one or two
32-bit words per DUCET entry.  If the level is greater than four, it
uses two words; otherwise, it uses one.  The first word is a
combination of the first three levels, using masks and shifts as
defined below.  The second word, if present, is the fourth level.

% uni_str_ducet prototype

\input{DUCET lookup format defs.tex} % C

Raw DUCET lookups for characters without the variable weight and level
processing can be performed using the following function.  The
[[lev123]] parameter is formatted just like the first word of each
pair returned by [[uni_str_ducet]].  Since there are DUCET entries for
strings of more than one character, the lookup requires string input.
This is done one character at a time, using a state structure to keep
track of progress.  Pass in the next character in [[c]], unless at end
of input, in which case pass in [[UNI_DUCET_LOOKUP_END]].  The return
value is one of [[UNI_DUCET_LOOKUP_AGAIN]], [[UNI_DUCET_LOOKUP_OK]],
or [[UNI_DUCET_LOOKUP_NONE]].  If [[NONE]], [[lev123]] and [[lev4]]
must be ignored.  Otherwise, they contain the next DUCET sort key
element.  If the passed-in character was not [[END]], or the return
value was [[AGAIN]], the function must always be called again to
finish the lookup.  The [[c]] parameter should be the next character
(or [[END]]) for the next call, unless the return value was [[AGAIN]],
in which case [[c]] is ignored.

The [[state]] parameter is a pointer to a pointer to an opaque state
structure.  It must initially point to [[NULL]].  After the last
lookup returns ([[c]] is [[UNI_DUCET_LOOKUP_END]] and the return value
was not [[UNI_DUCET_LOOKUP_AGAIN]]), the structure is automatically freed
and reset to [[NULL]].  It should never be freed manually.

% uni_DUCET_lookup prototype

The initial state can also be used to set one parameter:  the table to
use for lookups.  To do this, call this function first, with a
non-[[NULL]] [[tab]] and [[strs]].  This will allocate a new state for
use with the lookup function.

% uni_DUCET_lookup_tab prototype

\subsection{Case Conversion}

The slc, suc, and stc properties are provided as if they were numeric
properties; the numerator result is the single character to which the
input maps.  A lookup failure in the stc table has the result of a suc
lookup as its default.  A lookup failure in either the slc or suc
tables has the input code point itself as its default.

The lc, uc, and sc properties, however, provide not only a mapping,
but also a condition under which the mapping takes place.  The
properties are each stored in separate string tables, so their
\emph{ST} name is the same as their \emph{SP} name.  All currently
possible conditions are mapped to a flag:

\input{Special casing conditions.tex} % C

A flag in [[UNI_SC_FL_LOCALE]] indicates that the translation only
applies in specific locales.  The others ([[UNI_SC_FL_CONTEXT]])
depend on context: generally the entire containing grapheme cluster,
and possibly the next character must be known before deciding on a
translation.

Since each input code point may map to multiple translations
(depending on the condition),  the encoding of the string is to
provide all possible translations, each preceeded by their length.  The
condition flags for that interpretation are then or-ed into the length.

The actual lc, uc, and tc properties combine the slc, suc, and stc
lookups with the contents of the lc, uc, and tc string tables.
To do a lookup, the [[uni_case_convert]] function takes the result
buffer ([[buf]] of length [[buf_len]]) and the input code point
([[cp]]), the associated tables, and the currently known conditions
([[cond]]), and returns the number of returnable characters and fills
the buffer as much as possible.  A [[cond]] of [[~0]] indicates that
the caller does not know the current conditions.  The return is the
number of characters in the result (perhaps even zero), or -1 if no
change needs to be made, or -2 if the condition is currently unknown,
but may affect the result.  For a return value greater than
[[buf_len]], the function may be called again with more space.  For a
return value of -2, the function may be called again with a valid set
of condition flags. Note that the passed-in condition flags should
never set [[UNI_SC_FL_NOT]]; just leave a flag unset if it is not in
effect.

% uni_case_convert prototype

To make this a little easier, wrapper macros are provided which select
the correct set of needed tables.

% uni_lc prototype
% uni_uc prototype
% uni_tc prototype

Case folding converts to a canonical desired case, usually lower-case.
Like the plain case conversion properties, there are two cases:
simple and full.  In addition, some of the full folding is
conditional.  In this case, though, the only condition is whether or
not the tr or az locale is in use.  Encoding is identical to the case
conversion properties.  The scf property is numeric, and the cf
property can be derived using the [[uni_case_convert]] function, and
its associated wrapper macro:

% uni_cf prototype

The pseudo-property tcf, which includes the Turkic language exceptions,
can be obtained using a wrapper macro as well:

% uni_tcf prototype

The only other case conversion string property is NFKC\_CF, which is a
plain string property.  However, for convenience, a lookup function
similar to the decomposition functions is provided as well.

% uni_NFKC_Casefold prototype

\subsection{Other String Properties}

The scx property is officially not a string property, but it is stored
internally as a string property whose strings are sequences of
enumeration literal values corresponding to the sc property.  For
example, the code point U+0363 has the scx property ``Arab Syrc'',
which is stored as string of length two.  The first character of the
string is [[U_sc_Arab]], and the second is [[U_sc_Syrc]].

\section{Name Tables}

The following symbols are for internal use only.  They provide
information about ranges within the Unicode.txt data file.  These
ranges have code points algorithmically generated from the range's
base name.

[[const uni_chrrng_dat_t uni_range_rng[]]] \\
[[uni_range_rng_len]] \\
[[const uint32_t uni_range_mtab[]]] \\
[[uni_range_invalid]]

% End-doc Users-Guide

\end{document}
