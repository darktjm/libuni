% -*- mode: Noweb; noweb-code-mode: c-mode; -*-
% Build with noweb:
%  notangle -t8 build.nw > makefile
%  make
\documentclass[twoside,english]{report}
\usepackage[letterpaper,rmargin=1.5in,bmargin=1in]{geometry}
%%% latex preamble
\RCS $Id$
\RCS $Revision$
\RCS $Date$

%%% requires build

\begin{document}

\title{Unicode Support Routines}
\author{Thomas J. Moore}
\date{Version 1.0\\Revision \RCSRevision\\\RCSDate}
\maketitle

\begin{rawhtml}
<!-->
\end{rawhtml}
\iffalse
<<Sources>>=
$Id$
@

<<Version Strings>>=
"$Id$\n"
@

<<Common NoWeb Warning>>=
# $Id$
@
\fi
\begin{rawhtml}
<-->
\end{rawhtml}

\begin{abstract}

This document describes and implements a library which exports the
Unicode Character Database in a convenient manner.  Libraries already
exist that do this:  GLib, ICU, and others.  However, this library
does things my way, which allows me to e.g. build a regular expression
library and a compiler using these routines.

\end{abstract}

\tableofcontents

\chapter{Introduction}

I do a lot of simple memory allocations, and it's easy to forget to
include the sizeof(element) scale factor.  To make this easier, here
are a few macros.

<<mallocdef.h>>=
<<Common C Warning>>
#ifndef MALLOCDEF_H
#define MALLCDEF_H
#define resize(buf, len) do { \
    buf = realloc(buf, (len) * sizeof(*(buf))); \
    if(!(buf)) { \
        perror(#buf); \
        exit(1); \
    } \
} while(0)
#define inisize(buf, len) do { \
    buf = malloc((len) * sizeof(*(buf))); \
    if(!(buf)) { \
        perror(#buf); \
        exit(1); \
    } \
} while(0)
#define clearbuf(buf, len) memset(buf, 0, (len) * sizeof(*(buf)))
#endif
@

<<Common C Includes>>=
#include "mallocdef.h"
@

I also prefer a little bit higher warning level than the default.

<<makefile.vars>>=
CC=gcc
CFLAGS=-O2
EXTRA_CFLAGS += -Wall -Wmissing-prototypes -Wno-unused-result -Wshadow \
                -Wmissing-prototypes
EXTRA_CXXFLAGS += -Wall -Wmissing-prototypes -Wno-unused-result -Wshadow
@

\chapter{File Input}

Input is in the form of normalized Unicode.  First, the raw input
needs to be converted into integer code points.  Output may need to be
produced in different formats as well.

<<unistuff.h>>=
<<Common C Warning>>
#ifndef UNISTUFF_H
#define UNISTUFF_H

<<Library [[uni]] headers>>

#endif
@

<<Library [[uni]] Members>>=
uni_io.o
@

<<uni_io.c>>=
<<Common C Header>>
#include "uni_io.h"

<<Unicode I/O local definitions>>

<<Unicode I/O functions>>
@

<<Library [[uni]] headers>>=
#include "uni_io.h"
@

<<uni_io.h>>=
<<Common C Warning>>
#ifndef UNI_IO_H
#define UNI_IO_H

<<Unicode I/O Exports>>
#endif
@

\section{Basic Unicode I/O}

Unicode defines several file formats:  UTF-8, UTF-16, and UTF-32.  No
special conversion needs to be made from UTF-32 to integer code
points, other than to filter out invalid values and possibly byte-swap
the code point.

<<Common C Includes>>=
#include <stdint.h>
/* FIXME: this is glibc-specific */
/* BSD apparently uses <sys/endian.h> */
/* OpenBSD additionally uses different fn names */
/* othes may not even have any equivalent functions */
#include <endian.h>
@

\lstset{language=make}
<<makefile.vars>>=
EXTRA_CFLAGS += -D_BSD_SOURCE
@

\lstset{language=C}
<<Unicode I/O Exports>>=
int utf32_encode(uint32_t *buf, int cp, int bige);
@

<<Unicode I/O functions>>=
int utf32_encode(uint32_t *buf, int cp, int bige)
{
  if(cp > 0x10ffff || (cp >= 0xd800 && cp < 0xe000))
    return 0;
  *buf = bige ? htobe32(cp) : htole32(cp);
  return 1;
}
@

<<Unicode I/O Exports>>=
int utf32_decode(const uint32_t *s, int bige);
@

<<Unicode I/O functions>>=
int utf32_decode(const uint32_t *s, int bige)
{
  uint32_t c = bige ? be32toh(*s) : le32toh(*s);
  if(c > 0x10ffff || (c >= 0xd800 && c < 0xe000))
    return -1;
  return c;
}
@

<<Unicode I/O Exports>>=
int utf32_putc(int c, FILE *f, int bige);
@

<<Unicode I/O functions>>=
int utf32_putc(int c, FILE *f, int bige)
{
  uint32_t w;

  if(!utf32_encode(&w, c, bige))
    return 0;
  return fwrite(&w, 4, 1, f);
}
@

<<Unicode I/O Exports>>=
int utf32_getc(FILE *f, int bige); /* always reads 4 bytes */
@

<<Unicode I/O functions>>=
int utf32_getc(FILE *f, int bige)
{
  uint32_t w;
  int ret;

  if((ret = fread(&w, 1, 4, f)) != 4)
    return ret ? -2 : -1;
  ret = utf32_decode(&w, bige);
  return ret < 0 ? -3 : ret;
}
@

For UTF-16, a slightly more complex scheme is used, involving the
surrogate code points (D800 through DFFF) in pairs.  The first half of
the surrogate code points (D800 through DBFF) is always used for the
first member of the pair, and the second half is used for the other.
The first member gives the first 5 bits, and the second member gives
the remainder. Since this is for representing code points 10000 and
up, an extra bit can be gained giving the full range through 10FFFF.

<<Unicode I/O Exports>>=
/* make sure at least 2 words avail in buf */
/* returns # of words written */
int utf16_encode(uint16_t *buf, int cp, int bige);
@

<<Unicode I/O functions>>=
int utf16_encode(uint16_t *buf, int cp, int bige)
{
  if((cp >= 0xD800 && cp < 0xE000) || cp > 0x10FFFF)
    return 0;
  if(cp < 0x10000) {
    *buf = bige ? htobe16(cp) : htole16(cp);
    return 1;
  }
  int u = (cp & 0x1f0000) >> 16;
  cp &= 0xffff;
  uint16_t w;
  w = 0xd800 + ((u - 1) @<< 10) + (cp @>> 10);
  *buf++ = bige ? htobe16(w) : htole16(w);
  w = 0xdc00 + (cp > 0x3fff);
  *buf = bige ? htobe16(w) : htole16(w);
  return 2;
}
@

<<Unicode I/O Exports>>=
/* this function returns -1 on errors */
/* if the buffer may be invalid, ensure at least 2 words available */
int utf16_decode(const uint16_t *s, int *nread, int bige);
@

<<Unicode I/O functions>>=
int utf16_decode(const uint16_t *s, int *nread, int bige)
{
  uint16_t w = bige ? be16toh(*s) : le16toh(*s);
  if(nread)
    *nread = 1;
  if(w < 0xd800 || w >= 0xe000)
    return w;
  if(w >= 0xdc00)
    return -1;
  uint16_t w2 = bige ? be16toh(s[1]) : le16toh(s[1]);
  if(w2 >= 0xe000 || w2 < 0xdc00)
    return -1;
  if(nread)
    *nread = 2;
  int c = w2 & 0x3ff;
  c += (w & 0x3f) << 10;
  w = (w >> 10) & 0xf;
  c += (w + 1) << 16;
  return c;
}
@

<<Unicode I/O Exports>>=
int utf16_putc(int c, FILE *f, int bige);
@

<<Unicode I/O functions>>=
int utf16_putc(int c, FILE *f, int bige)
{
  unsigned char buf[4];
  uint16_t enc[2], encl;
  
  encl = utf16_encode(enc, c, bige);
  if(!encl)
    return 0;
  if(bige) {
    buf[0] = enc[0] >> 8;
    buf[1] = enc[0];
    if(encl > 1) {
      buf[2] = enc[1] >> 8;
      buf[3] = enc[1];
    }
  } else {
    buf[1] = enc[0] >> 8;
    buf[0] = enc[0];
    if(encl > 1) {
      buf[3] = enc[1] >> 8;
      buf[2] = enc[1];
    }
  }
  return fwrite(buf, 2, encl, f);
}
@

The input function needs to read ahead to get the second member of a
pair.  If that character is not what was expected, the correct
behavior is to undo the readahead and return the current code point as
an error.  However, it is not possible in C standard I/O to push more
than one character back into the stream.  For now, this is going to
have to be erroneous behavior.

<<Unicode I/O Exports>>=
int utf16_getc(FILE *f, int bige, int *nread);  /* nread == # of bytes read */
@

<<Unicode I/O functions>>=
int utf16_getc(FILE *f, int bige, int *nread)
{
  int c = fgetc(f), c2;
  if(c == EOF) {
    if(nread)
      *nread = 0;
    return -1;
  }
  c2 = fgetc(f);
  if(c == EOF) {
    if(nread)
      *nread = 1;
    return -2;
  }
  if(bige)
    c = c2 + (c << 8);
  else
    c += c2 << 8;
  if(c < 0xd800 || c >= 0xE000) {
    if(nread)
      *nread = 1;
    return c;
  }
  if(c >= 0xdc00) {
    if(nread)
      *nread = 2;
    return -3;
  }
  int res = (c & 0x3f) << 10;
  int w = (c >> 10) & 0xf;
  res += (w + 1) << 16;
  if((c = fgetc(f)) == EOF) {
    if(nread)
      *nread = 2;
    return -2;
  }
  if((c2 = fgetc(f)) == EOF) {
    if(nread)
      *nread = 3;
    return -2;
  }
  if(bige)
    c = c2 + (c << 8);
  else
    c += c2 << 8;
  if(c < 0xdc00 || c >= 0xe000) {
#if 0 /* FIXME: this does not work; ungetc() can't be called twice */
    ungetc(c2, f);
    ungetc(bige ? c >> 8 : c & 0xff, f);
    if(nread)
      *nread = 2;
#else
    if(nread)
      *nread = 4;
#endif
    return -3;
  }
  if(nread)
    *nread = 4;
  res += c & 0x3ff;
  return res + (c & 0x3ff);
}
@

For UTF-8, an even more complex encoding scheme is used.  Again, a
standalone memory codec is provided.  All code points over 007F are
encoded using a multi-byte sequence.  All characters in a multi-byte
sequence have their high bit set; the first non-zero bit determines
the byte's role.  All but the first byte have only one high bit set,
and encode 6 bits.  The first byte determines how many trailing bytes
there are, and also encode 3--5 bits.  The number of high bits set in
the first byte is the total number of bytes in the sequence.

<<Unicode I/O Exports>>=
/* make sure at least 4 bytes avail in buf */
/* returns # of bytes written */
int utf8_encode(char *buf, int cp);
@

<<Unicode I/O functions>>=
int utf8_encode(char *buf, int cp)
{
    if((cp >= 0xD800 && cp < 0xE000) || cp > 0x10FFFF)
        return 0;
    if(cp < 128) {
        *buf = cp;
        return 1;
    } else if(cp < 0x800) {
        *buf = 0xc0 + (cp >> 6);
        buf[1] = 0x80 + (cp & 0x3f);
        return 2;
    } else if(cp < 0x10000) {
        *buf = 0xe0 + (cp >> 12);
        buf[1] = 0x80 + ((cp >> 6) & 0x3f);
        buf[2] = 0x80 + (cp & 0x3f);
        return 3;
    } else {
        *buf = 0xf0 + (cp >> 18);
        buf[1] = 0x80 + ((cp >> 12) & 0x3f);
        buf[2] = 0x80 + ((cp >> 6) & 0x3f);
        buf[3] = 0x80 + (cp & 0x3f);
        return 4;
    }
}
@

<<Unicode I/O Exports>>=
/* this function returns -1 on errors */
/* if the buffer may be invalid, ensure at least 4 chars available */
int utf8_decode(const char *buf, int *nread);
@

<<Unicode I/O functions>>=
int utf8_decode(const char *buf, int *nread)
{
    if(*buf < 0x7f) {
        *nread = 1;
        return (unsigned char)*buf;
    }
    int c = (unsigned char)*buf++, ec;
    if((c & 0xf8) == 0xf0) {
        c &= 0x07;
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 1;
            return -1;
        }
        c = (c << 6) + (ec & 0x3f);
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 2;
            return -1;
        }
        c = (c << 6) + (ec & 0x3f);
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 3;
            return -1;
        }
        *nread = 4;
        c = (c << 6) + (ec & 0x3f);
        if(c < 0x10000 || c > 0x10FFFF)
            return -1;
        return c;
    }
    if((c & 0xf0) == 0xe0) {
        c &= 0x0f;
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 1;
            return -1;
        }
        c = (c << 6) + (ec & 0x3f);
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 2;
            return -1;
        }
        *nread = 3;
        c = (c << 6) + (ec & 0x3f);
        if(c < 0x800 || (c >= 0xD800 && c < 0xE000))
            return -1;
        return c;
    }
    if((c & 0xe0) == 0xc0) {
        c &= 0x1f;
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 1;
            return -1;
        }
        *nread = 2;
        c = (c << 6) + (ec & 0x3f);
        if(c < 0x80)
            return -1;
        return c;
    }
    *nread = 1;
    return -1;
}
@

<<Unicode I/O Exports>>=
int utf8_putc(int c, FILE *f);
@

<<Unicode I/O functions>>=
int utf8_putc(int c, FILE *f)
{
    char obuf[4];
    int nout = utf8_encode(obuf, c);
    return fwrite(obuf, 1, nout, f);
}
@

UTF-8 output is common enough to warrant a string version of the
output function as well.

<<Unicode I/O Exports>>=
int utf8_fputs(int *buf, int len, FILE *f);
@

<<Unicode I/O functions>>=
int utf8_fputs(int *buf, int len, FILE *f)
{
  int ret = 0;

  while(len-- > 0)
    ret += utf8_putc(*buf++, f);
  return ret;
}
@

Like UTF-16 input, UTF-8 input requires some readahead.  For anything
more than one character, C once again disallows returning the
characters to the stream.  So, once again, bad input causes erroneous
behavior.

<<Unicode I/O Exports>>=
int utf8_getc(FILE *f, int *nread);  /* nread == # of bytes read */
@

<<Unicode I/O functions>>=
int utf8_getc(FILE *f, int *nread)
{
  int c;
  int chrread = 1;

  if((c = getc(f)) == EOF) {
    c = -1;
    chrread = 0;
  } else if(c >= 0x80) {
    int ec;
    chrread = 1;
    if((c & 0xf8) == 0xf0) {
      <<Read 4-char utf-8>>
      if(c < 0x10000 || c > 0x10ffff)
        c = -3;
    } else if((c & 0xf0) == 0xe0) {
      <<Read 3-char utf-8>>
      if(c < 0x800 || (c >= 0xD800 && c < 0xE000))
        c = -3;
    } else if((c & 0xe0) == 0xc0) {
      <<Read 2-char utf-8>>
      if(c < 0x80)
        c = -3;
    } else
      c = -3;
  }
  if(nread)
    *nread = chrread;
  return c;
}
@

<<Read 4-char utf-8>>=
c &= 0x07;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
  if((ec = getc(f)) == EOF)
    c = -2;
  else if((ec & 0xc0) != 0x80) {
    ungetc(ec, f);
    c = -3;
  } else {
    chrread = 3;
    c = (c << 6) + (ec & 0x3f);
    if((ec = getc(f)) == EOF)
      c = -2;
    else if((ec & 0xc0) != 0x80) {
      ungetc(ec, f);
      c = -3;
    } else {
      chrread = 4;
      c = (c << 6) + (ec & 0x3f);
    }
  }
}
@

<<Read 3-char utf-8>>=
c &= 0x0f;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
  if((ec = getc(f)) == EOF)
    c = -2;
  else if((ec & 0xc0) != 0x80) {
    ungetc(ec, f);
    c = -3;
  } else {
    chrread = 3;
    c = (c << 6) + (ec & 0x3f);
  }
}
@

<<Read 2-char utf-8>>=
c &= 0x1f;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
}
@

\section{General File Input}

For arbitrary input formats other than ISO-Latin-1 and ASCII (which
map directly to Unicode), the iconv library should be used.

<<Unicode I/O local definitions>>=
#include <langinfo.h>
#include <locale.h>
#include <iconv.h>
@

A more general UTF reader would have to scan for a byte order mark and
remember what it said.  This requires retaining state.

<<Unicode I/O Exports>>=
typedef struct {
  FILE *f;
  char *name;
  <<Unicode file buffer state>>
} unifile_t;
@


Rather than automatically attaching state to a file on first access
and never really knowing when to free it, explicit routines are
provided to create and remove the state.   

<<Unicode I/O Exports>>=
/* encoding = NULL for automatic detection */
/* automatically detects utf16/utf32 if encoding == UTF-8 */
unifile_t *uni_fopen(const char *name, const char *encoding);
void uni_fclose(unifile_t *uf);
@

<<Unicode I/O functions>>=
unifile_t *uni_fopen(const char *name, const char *encoding)
{
  unifile_t *uf;

  inisize(uf, 1);
  clearbuf(uf, 1);
  uf->name = strdup(name);
  if(!uf->name) {
    free(uf);
    return NULL;
  }
  uf->f = fopen(name, "rb");
  if(!uf->f) {
    free(uf->name);
    free(uf);
    return NULL;
  }
  <<Initialize unicode file buffer state>>
  return uf;
}
@

<<Unicode I/O functions>>=
void uni_fclose(unifile_t *uf)
{
  free(uf->name);
  fclose(uf->f);
  <<Free unicode file buffer state>>
  free(uf);
}
@

One limitation of standard I/O is that only one character may be
pushed back into the stream.  To correct this, a 4-byte buffer is
kept, along with a count.  In order to avoid reading from the file
after the buffered characters have been removed, an end-of-file flag
is kept as well.  In order to allow error messages to display the
correct file offset, that is also kept.

<<Unicode file buffer state>>=
size_t fpos;
char readahead[4];
char raptr;
unsigned char eof;
@

The first thing to do is to decide the file's encoding.  ASCII and
ISO-Latin-1 can be read raw.  UTF-8 is taken to mean automatic Unicode
detection.  Anything else is passed to iconv.

<<Unicode file buffer state>>=
unsigned char enctype;
/*
 * 0 = ASCII/Latin-1
 * 1 = iconv
 * 8/16/32 = utf-8/16/32
 */
void *enchelp;
@

<<Unicode I/O local definitions>>=
#define ICBUF_SIZE 1024
typedef struct {
  iconv_t ic;
  char ibuf[ICBUF_SIZE];
  int ilen;
  uint32_t obuf[ICBUF_SIZE];
  int optr, olen;
} iconv_supt_t;
@

<<Initialize unicode file buffer state>>=
if(!encoding) {
  setlocale(LC_CTYPE, "");
  encoding = nl_langinfo(CODESET);
}
if(!encoding)
  encoding = "UTF-8";
if(encoding && (!strcmp(encoding, "ISO-8859-1") ||
                !strcmp(encoding, "ANSI_X3.4-1968"))) /* ASCII */
  return uf; /* enctype == 0 */
else if(encoding && strcmp(encoding, "UTF-8")) {
  iconv_supt_t *ics;
  inisize(ics, 1);
#if __BYTE_ORDER == __LITTLE_ENDIAN
#define bo "LE"
#else
#define bo "BE"
#endif
  ics->ic = iconv_open("UTF-32" bo, encoding);
  if(ics->ic == (iconv_t)-1) {
    perror(encoding);
    exit(1);
  }
  uf->enchelp = ics;
  uf->enctype = 1;
  return uf;
}
/* otherwise it's Unicode */
@

<<Free unicode file buffer state>>=
if(uf->enctype == 1 && uf->enchelp) {
  iconv_supt_t *ics = uf->enchelp;
  iconv_close(ics->ic);
  free(ics);
}
@

The first thing to do on opening a new Unicode file is to check for
the byte order mark (FEFF).  Flags are kept to indicate what was
found.

<<Unicode file buffer state>>=
unsigned char bige;
@

<<Initialize unicode file buffer state>>=
uf->enctype = 8;
/* Read and process BOM (FEFF) */
int c = getc(uf->f);
if(c == EOF)
  uf->eof = 1;
else if(c == 0xff) {
  c = getc(uf->f);
  if(c == 0xfe) {
    uf->bige = 1;
    uf->enctype = 16;
    uf->fpos = 2;
  } else {
    ungetc(c, uf->f);
    uf->readahead[0] = 0xff;
    uf->raptr = 1;
  }
} else if(c == 0xfe) {
  c = getc(uf->f);
  if(c == 0xff) {
    c = getc(uf->f);
    if(!c) {
      c = getc(uf->f);
      if(!c) {
        uf->enctype = 32;
	uf->fpos = 4;
      } else {
        ungetc(c, uf->f);
	uf->readahead[0] = 0;
	uf->raptr = 1;
        uf->enctype = 16;
	uf->fpos = 2;
      }
    } else {
      ungetc(c, uf->f);
      uf->enctype = 16;
      uf->fpos = 2;
    }
  } else {
    ungetc(c, uf->f);
    uf->readahead[0] = 0xfe;
    uf->raptr = 1;
  }
} else if(!c) {
  c = getc(uf->f);
  if(!c) {
    c = getc(uf->f);
    if(c == 0xfe) {
      c = getc(uf->f);
      if(c == 0xff) {
        uf->enctype = 32;
	uf->bige = 1;
	uf->fpos = 4;
      } else {
        ungetc(c, uf->f);
	uf->readahead[0] = 0;
	uf->readahead[1] = 0;
	uf->readahead[2] = 0xfe;
	uf->raptr = 3;
      }
    } else {
      ungetc(c, uf->f);
      uf->readahead[0] = 0;
      uf->readahead[1] = 0;
      uf->raptr = 2;
    }
  } else {
    ungetc(c, uf->f);
    uf->readahead[0] = 0;
    uf->raptr = 1;
  }
} else
  ungetc(c, uf->f);
@

The reader then uses the appropriate method to read a character based
on the [[enctype]].

<<Unicode I/O Exports>>=
int uni_fgetc(unifile_t *uf);
@

<<Unicode I/O functions>>=
int uni_fgetc(unifile_t *uf)
{
  if(!uf->enctype) {
    int c = getc(uf->f);
    if(c == EOF)
      uf->eof = 1;
    else
      uf->fpos++;
    return c;
  } else if(uf->enctype == 1) {
    <<Return a character using iconv>>
  }
  <<Return a Unicode character>>
}
@

Rather than using the potentially flawed direct I/O routines, the
Unicode reader always reads raw bytes and then calls the decoder
instead.  That way, the readahead issue disappears.

<<Return a Unicode character>>=
while(!uf->eof && uf->raptr < 4) {
  int nr = fread(uf->readahead + uf->raptr, 1, 4 - uf->raptr, uf->f);
  if(nr <= 0)
    uf->eof = 1;
  else
    uf->raptr += nr;
}
if(uf->eof && !uf->raptr)
  return -1;
int cp, l;
if(uf->enctype == 32) {
  cp = utf32_decode((uint32_t *)uf->readahead, uf->bige);
  l = 4;
} else if(uf->enctype == 16) {
  cp = utf16_decode((uint16_t *)uf->readahead, &l, uf->bige);
  l *= 2;
} else
  cp = utf8_decode(uf->readahead, &l);
if(l > uf->raptr) {
  fprintf(stderr, "Ill-formed character @ %lu\n", uf->fpos);
  uf->raptr = 0;
  return -2;
}
if(l < 4)
  memmove(uf->readahead, uf->readahead + l, 4 - l);
if(cp < 0)
  fprintf(stderr, "Ill-formed character @ %lu\n", uf->fpos);
uf->raptr -= l;
uf->fpos += l;
return cp < 0 ? -2 : cp;
@

For iconv:

<<Return a character using iconv>>=
iconv_supt_t *ics = uf->enchelp;
while(1) {
  if(ics->olen) {
    --ics->olen;
    return utf32_decode(&ics->obuf[ics->optr++], __BYTE_ORDER == __BIG_ENDIAN);
  }
  char *ibuf = ics->ibuf;
  size_t ilen = ics->ilen;
  char *obuf = (char *)ics->obuf;
  size_t olen = ICBUF_SIZE * 4;
  while(!uf->eof && ilen < ICBUF_SIZE) {
    int nr = fread(ibuf, 1, ICBUF_SIZE - ilen, uf->f);
    if(nr <= 0)
      uf->eof = 1;
    else
      ilen += nr;
  }
  while(1) {
    iconv(ics->ic, &ibuf, &ilen, &obuf, &olen);
    if(ibuf != ics->ibuf)
      break;
    /* ignore bad chars */
    if(!--ilen)
      break;
    memmove(ics->ibuf, ics->ibuf + 1, ilen);
  }
  if(ilen)
    memmove(ics->ibuf, ibuf, ilen);
  else if(uf->eof)
    iconv(ics->ic, NULL, NULL, &obuf, &olen);
  ics->olen = olen / 4;
  ics->optr = 0;
  ics->ilen = ilen;
}
@

\section{Unicode Normalization}

After reading raw Unicode, it needs to be normalized.

<<Library [[uni]] Members>>=
uninorm.o
@

<<Library [[uni]] headers>>=
#include "uninorm.h"
@

<<uninorm.h>>=
<<Common C Warning>>
#ifndef UNINORM_H
#define UNINORM_H

<<Unicode normalization support exports>>
#endif
@

<<uninorm.c>>=
<<Common C Header>>
#include "uninorm.h"

<<Unicode normalization support local definitions>>

<<Unicode normalization support functions>>
@

The UCD (Unicode Character
Database)\footnote{\url{http://www.unicode.org/Public/6.0.0/ucd}}
contains files which can be processed to obtain the tables needed for
normalization.  In particular, UnicodeData.txt, field 6 contains the
decomposition information, and field 4 contains the canonical ordering
information.  Case folding information is scattered, but combined into
the DerivedNormalizationProps.txt file.  The latter file also contains
information regarding valid compositions.

\lstset{language=make}
<<makefile.config>>=
# The location of the Unicode Character Database
#UCD_LOC = /usr/share/unicode-data
UCD_LOC = /home/darktjm/unicode/6.2/ucd
@

<<makefile.vars>>=
UNIDATA = $(UCD_LOC)/UnicodeData.txt
UNINORM = $(UCD_LOC)/DerivedNormalizationProps.txt
UNIVALALIAS = $(UCD_LOC)/PropertyValueAliases.txt
@

Rather than parse these files at run time, they are read in by a
helper program, and the results are spit out as C code.  The canonical
ordering information is a table of all reorderable code points, along
with their canonical combining class (the number indicating its sort
order), sorted by code point for binary searching ([[ccctab]]).

\lstset{language=C}
<<Unicode normalization support local definitions>>=
typedef struct {
  int cp, ccc;
} uni_ccc_t;
@

The decomposition information is a table of decomposable source code
points and their destination strings, sorted by source code point for
binary searching.  The canonical composition information consists of
all pairs of composable code points along with their composed form,
sorted by source code points for binary searching.

The decompose and compose tables are long, and the decompose
destination strings can vary from zero to over thirty characters.  In
order to keep the tables from being both long and wide, an auxiliary
table ([[deccp]]) is used to store the actual decomposition strings.
The decomposition table then just stores the offset into the string
table of the first character, and the length of the string.  Using
short integers for the length and offset further reduces the width of
the composition table by 4 bytes.

As another space-saving measure, the canonical and compatibility
decomposition tables are shared ([[decomp]]).  This is because
compatibility decomposition is a superset of canonical decomposition.
A negative length indicates compatibility decomposition.  However, the
canonical composition table only operates on pairs, and only one flag
can be contained in a sign bit anyway, so the canonical composition
table ([[cancomp]]) is separate.  Case folding is too different  as
well, so it is also separate ([[nfkc_cf]]).

<<Unicode normalization support exports>>=
/* Raw info */

/* for canonical/compatibility decomposition: */
/*  off is offset in deccp */
/*  |len| is length in deccp; len is negative if compatibility */
typedef struct {
    int cp;
    uint16_t off;
    int16_t len;
} uni_decent_t;
/* The decomposition strings */
extern const int deccp[];
@

As a convention, the generated file is the same as the C file which
uses it, but with a [[.gen]] extension.  The generated code creates
static variables where possible, to keep the information contained to
the C file that uses it.

<<Unicode normalization support local definitions>>=
#include "uninorm.gen"
@

\lstset{language=make}
<<makefile.rules>>=
uninorm.o: uninorm.gen
@

<<Plain Built Files>>=
*.gen \
@

Functions are then provided to look up this information.  To look up
the canonical or compatibility decomposition, the [[decomp]] table is
binary searched, and the sign of the length is taken into account.
Rather than hide the [[deccp]] table and make the function call more
complicated, the [[decent]] is returned raw.  The same is true of any
other function that returns variable-length information from [[deccp]].

\lstset{language=C}
<<Unicode normalization support exports>>=
const uni_decent_t *find_decomp(int cp, int canon);
@

<<Unicode normalization support local definitions>>=
static int cmp_de(const void *a, const void *b)
{
    return ((uni_decent_t *)a)->cp - ((uni_decent_t *)b)->cp;
}
@

<<Unicode normalization support functions>>=
const uni_decent_t *find_decomp(int cp, int canon)
{
    uni_decent_t de = {cp}, *ret;
    ret = (uni_decent_t *)bsearch(&de, decomp, sizeof(decomp)/sizeof(*decomp),
                                   sizeof(*decomp), cmp_de);
    if(!ret)
        return NULL;
    if(canon && ret > decomp && ret[-1].cp == cp)
        return ret - 1;
    if(!canon && ret < decomp + sizeof(decomp) / sizeof(*decomp) - 1 &&
       ret[1].cp == cp)
        return ret + 1;
    if(canon && ret->len < 0)
        return NULL;
    return ret;
}
@

The case folding table is independent, so it's just a simple binary
search.

<<Unicode normalization support exports>>=
/* same as above, but only returns nfkc_cf, and len is always positive */
const uni_decent_t *find_nfkc_cf(int cp);
@

<<Unicode normalization support functions>>=
const uni_decent_t *find_nfkc_cf(int cp)
{
    uni_decent_t de = {cp};
    return (uni_decent_t *)bsearch(&de, nfkc_cf, sizeof(nfkc_cf)/sizeof(*nfkc_cf),
                                    sizeof(*nfkc_cf), cmp_de);
}
@

The canonical combining class table only holds one value, so the
structure is private and the function returns the actuall ccc.  The
ccc is zero for anything not in the table.

<<Unicode normalization support exports>>=
/* the ccc for canonical ordering and composition */
int find_ccc(int cp);
@

<<Unicode normalization support local definitions>>=
static int cmp_ccc(const void *a, const void *b)
{
    return ((uni_ccc_t *)a)->cp - ((uni_ccc_t *)b)->cp;
}
@

<<Unicode normalization support functions>>=
int find_ccc(int cp)
{
    uni_ccc_t ccc = {cp}, *ret;
    ret =  (uni_ccc_t *)bsearch(&ccc, ccctab, sizeof(ccctab)/sizeof(*ccctab),
                                 sizeof(*ccctab), cmp_ccc);
    if(ret)
        return ret->ccc;
    else
        return 0;
}
@

Canonical composition is a lookup on a pair.  For convenience, you can
look up just one character to see if a pair might be coming.  Since
the table is independent, the function is pretty simple.

<<Unicode normalization support exports>>=
/* given a pair, give canonical composition */
int find_cancomp(int cp, int cp2);
/* see if cp might be first member of a composition pair */
int is_cancomp1(int cp);
@

<<Unicode normalization support local definitions>>=
static int cmp_comp(const void *a, const void *b)
{
    int *_a = (int *)a;
    uni_decent_t *_b = (uni_decent_t *)b;
    int acp1, acp2, bcp1, bcp2;

    acp1 = _a[0];
    acp2 = _a[1];
    bcp1 = deccp[_b->off];
    bcp2 = deccp[_b->off + 1];
    if(acp1 == bcp1)
        return acp2 - bcp2;
    else
        return acp1 - bcp1;
}
@

<<Unicode normalization support functions>>=
int find_cancomp(int cp, int cp2)
{
    int de[2] = {cp, cp2};
    uni_decent_t *res;
    res = bsearch(&de, cancomp, sizeof(cancomp)/sizeof(*cancomp),
                  sizeof(*cancomp), cmp_comp);
    if(res)
        return res->cp;
    else
        return -1;
}
@

<<Unicode normalization support local definitions>>=
static int cmp_comp1(const void *a, const void *b)
{
    uni_decent_t *_a = (uni_decent_t *)a, *_b = (uni_decent_t *)b;
    int acp1, bcp1;

    acp1 = _a->cp;
    bcp1 = deccp[_b->off];
    return acp1 - bcp1;
}
@

<<Unicode normalization support functions>>=
int is_cancomp1(int cp)
{
    uni_decent_t de = {cp};
    return bsearch(&de, cancomp, sizeof(cancomp)/sizeof(*cancomp),
                   sizeof(*cancomp), cmp_comp1) ? 1 : 0;
}
@

The raw functions are not meant to be used directly.  Instead, here
are some more high-level functions.  The first step in any
normalization is to decompose.  The only difference in the three
functions is what table they use, and whether or not they support
negative lengths.

<<Unicode normalization support exports>>=
/* wrappers */

/* decomp functions can operate on any number of chars, including 1 */
/* decomp functions return final length */
int NFD_dec(int *buf, int blen);
int NFKD_dec(int *buf, int blen);
/* NFKC_Casefold assumes NFD has already been run */
int NFKC_Casefold(int *buf, int blen);
@

<<Unicode normalization support functions>>=
#define any_dec(n, fn, len) \
  int n(int *buf, int blen) \
  { \
    int i; \
    for(i = 0; i < blen; i++) { \
      const uni_decent_t *de = fn; \
      if(de) { \
        int l = len; \
        memmove(buf + i + l, buf + i + 1, (blen - i - 1)*sizeof(int)); \
        blen += l - 1; \
        memcpy(buf + i, deccp + de->off, l * sizeof(int)); \
        /* result is fully expanded, so skip it */ \
        i += l - 1; \
      } \
    } \
    return blen; \
  }
any_dec(NFKD_dec, find_decomp(buf[i], 0), de->len < 0 ? -de->len : de->len)
any_dec(NFD_dec, find_decomp(buf[i], 1), de->len)
any_dec(NFKC_Casefold, find_nfkc_cf(buf[i]), de->len)
@

Canonical ordering is the second step in any normalization.  This
requires that any consecutive characters with non-zero canonical
combining class be ordered by their canonial combining class.  The
procedure is described as a bubble sort, so the sort must be stable.
If the last character passed in has a non-zero ccc, there may be more,
so a continuation is requested.

<<Unicode normalization support exports>>=
/* canon order function may need more chars than passed in */
/* set last to 1 if no continuations possible */
/* returns less than blen if more chars needed */
/* can skip # of chars equal to return value */
int Canon_Order(int *buf, int blen, int last);
@

<<Unicode normalization support local definitions>>=
/* for stable sort */
struct ccs {
    int cp, ccc;
    int opos;
};

static int cmpcc(const void *a, const void *b)
{
    const struct ccs *p1 = (const struct ccs *)a;
    const struct ccs *p2 = (const struct ccs *)b;
    if(p1->ccc != p2->ccc)
        return p1->ccc - p2->ccc;
    return p1->opos - p2->opos;
}
@

<<Unicode normalization support functions>>=
/* pass in last=1 if string is whole */
/* otherwise, if return is < blen, needs more chars */
/* but can safely skip return-val chars */
int Canon_Order(int *buf, int blen, int last)
{
    int i;
    for(i = 0; i < blen; i++) {
        int cc = find_ccc(buf[i]);
        if(cc) {
            int j;
            for(j = i + 1; j < blen; j++) {
                int cc2 = find_ccc(buf[j]);
                if(!cc2)
                    break;
            }
            if(j == blen && !last)
                return i;
            struct ccs ccbuf[j-i];
            ccbuf[0].ccc = cc;
            ccbuf[0].cp = buf[i];
            ccbuf[0].opos = i;
            int k;
            for(k = i + 1; k < j; k++) {
                ccbuf[k-i].ccc = find_ccc(buf[k]);
                ccbuf[k-i].cp = buf[k];
                ccbuf[k-i].opos = k;
            }
            qsort(ccbuf, j - i, sizeof(*ccbuf), cmpcc);
            for(k = i; k < j; k++)
                buf[k] = ccbuf[k-i].cp;
        }
    }
    return blen;
}
@

The last step in any composition normalization is canonical
composition.  Two characers may be combined if the first character has
a canonical combining class of zero and all intervening characters
have a non-zero canonical combining class less than the canonical
combining class of the second character.

<<Unicode normalization support exports>>=
/* canon comp function may need more chars than passed in */
/* set nok to non-NULL if continuation possible */
/* returns nok < blen if more chars needed */
/* can skip nok chars */
/* return value is updated length */
int NFC_comp(int *buf, int blen, int *nok);
@

<<Unicode normalization support functions>>=
int NFC_comp(int *buf, int blen, int *nok)
{
  int i, last0 = -1;
  for(i = 0; i < blen; i++) {
    int ccc = find_ccc(buf[i]);
    /* first char must be ccc=0 */
    if(ccc)
      continue;
    /* could also skip if !is_cancomp1(buf[i]) */
    last0 = i;
    int lastccc = 0;
    int j;
    for(j = i + 1; j < blen; j++) {
      ccc = find_ccc(buf[j]);
      /* second char must have no intervening equal ccc */
      /* this assumes canonical ordering has been done */
      if(ccc && lastccc && ccc == lastccc)
        continue;
      /* or greater ccc; this only happens if back down to 0 */
      if(!ccc && lastccc) {
        i = j - 1;
        break;
      }
      lastccc = ccc;
      int ccp = find_cancomp(buf[i], buf[j]);
      if(ccp >= 0) {
        /* composition replaces char 1 and deletes char 2 */
        buf[i] = ccp;
        memmove(buf + j, buf + j + 1, (blen - j - 1)*sizeof(int));
        blen--;
	/* start over at first potential 2nd char */
        j = i;
        lastccc = 0;
        continue;
      }
      /* if no match and hit a ccc=0, try next 1st char */
      if(!ccc) {
        i = j - 1;
        break;
      }
    }
  }
  if(nok)
    /* need more if we got a potential start char */
    /* but don't need to look at anything before that char */
    *nok = last0 >= 0 && is_cancomp1(buf[last0]) ? last0 : blen;
  return blen;
}
@

To demonstrate proper normalization, here are some macros which do the
full procedure.

<<Unicode normalization support exports>>=
#define NFKD(buf, blen) Canon_Order(buf, NFKD_dec(buf, blen), 1)
#define NFD(buf, blen) Canon_Order(buf, NFD_dec(buf, blen), 1)
#define NFKC(buf, blen) NFC_comp(buf, NFKD(buf, blen), NULL)
#define NFC(buf, blen) NFC_comp(buf, NFD(buf, blen), NULL)
@

To actually generate the tables, a C program ([[mkuninorm]]) is used.  A
simple table generator could be done using shell and sed, but there
are some caveats that make the C program much better suited to the
task.  Rather than compile in the file locations, they are supplied on
the command line.

\lstset{language=make}
<<C Build Executables>>=
mkuninorm \
@

<<makefile.rules>>=
uninorm.gen: mkuninorm $(UNIDATA) $(UNINORM)
	$< $(UNIDATA) $(UNINORM) >$@

mkuninorm: mkuninorm.o
	$(CC) $(CFLAGS) $(LDFLAGS) -o $@ $^
@

\lstset{language=C}
<<mkuninorm.c>>=
<<Common C Header>>

<<[[mkuninorm]] globals>>

int main(int argc, const char **argv)
{
  if(argc != 3) {
    fprintf(stderr, "Usage: %s path/to/UnicodeData.txt"
                       " path/to/DerivedNormalizationProps.txt\n",
            argv[0]);
    exit(1);
  }
    const char *unif = argv[1];
    const char *normf = argv[2];
  <<Make [[uninorm.gen]]>>
  return 0;
}
@

In order to save a little space on the [[deccp]] table, all entries
are read in, sorted, and combined if possible.  Thus the information
is stored in an expandable master decomposition table ([[dec]]) while
being read in.  A [[type]] field indicates which table it eventually
goes to.

<<[[mkuninorm]] globals>>=
typedef struct {
    int cp;
    uint32_t off;
    unsigned char len;
    unsigned char type;
    /* 0 = composition */
    /* 1 = canonical decomposition */
    /* 2 = compatibility decomposition */
    /* 3 = NFKC_CF */
} uni_decent_t;
uni_decent_t *dec;
int ndec = 0, maxdec;
@

<<Make [[uninorm.gen]]>>=
maxdec = 128;
inisize(dec, maxdec);
@

[[mkuninorm]] also has its own private [[deccp]] equivalent expandable
table ([[decs]]).

<<[[mkuninorm]] globals>>=
int *decs;
int ndecs = 0, maxdecs;
@

<<Make [[uninorm.gen]]>>=
maxdecs = 128;
inisize(decs, maxdecs);
@

For a first pass, the information provided by UnicodeData.txt is read.
This includes the ccc (field 4) and the non-casefolding decomposition
(field 6).  In order to create the composition table at the same time,
the DerivedNormalizationInfo.txt is read at the same time, only to
extract the Full Composition Exclusion information.

<<[[mkuninorm]] globals>>=
/* longest line is actually 218 chars */
char lbuf[512];
@

<<Make [[uninorm.gen]]>>=
FILE *f, *f2;
if(!(f = fopen(unif, "r"))) {
  perror(unif);
  exit(1);
}
if(!(f2 = fopen(normf, "r"))) {
  perror(normf);
  exit(1);
}
<<Stuff to do before reading [[unif]]>>
while(fgets(lbuf, sizeof(lbuf), f)) {
  <<Process a line of [[unif]]>>
}
fclose(f);
fclose(f2);
@

There is one list that is not in any file: Hangul syllables.  These
are only described in the standard. A Hangul syllable consists of
three parts, called L, V, and T.  There are code points for each L, V,
and T, as well as every combination of LV and LVT.  While LVT can
technically be decomposed directly to L, V, and T, it is instead
decomposed to LV and T, so that canonical composition (which only
works on pairs) can be performed. Thus, all entries for Hangul
syllables are type 0 (canonical composition).  No check is made in
Full Composition Exception, since as of version 6, there are no
matches.  If a future version has matches, this will need to change.

<<Add Hangul syllables>>=
/* hangul syllables */
int L, V, T;
int cp = 0xac00;
for(L = 0; L < 19; L++)
  for(V = 0; V < 21; V++)
    for(T = 0; T < 28; T++, cp++) {
      if(ndec == maxdec) {
        maxdec *= 2;
        resize(dec, maxdec);
      }
      dec[ndec].cp = cp;
      dec[ndec].off = ndecs;
      dec[ndec].len = 2;
      /* assuming none of these will get added to F_C_E */
      dec[ndec].type = 0;
      if(ndecs + 2 > maxdecs) {
        maxdecs *= 2;
        resize(decs, maxdecs);
      }
      if(!T) {
        decs[ndecs++] = 0x1100 + L;
        decs[ndecs++] = 0x1161 + V;
      } else {
        decs[ndecs++] = 0xac00 + 28 * (L * 21 + V);
        decs[ndecs++] = 0x11a7 + T;
      }
      ndec++;
    }
@

UnicodeData.txt is in code point order, so spitting out the ccc as
soon as it's found generates the table in the right order.  All other
information is stored, so there is no need to avoid intermingling data.

<<Stuff to do before reading [[unif]]>>=
/* first spit out the ccc table, so no ccc info needs to be kept */
puts("static const uni_ccc_t ccctab[] = {");
const char *nxt = ""; /* , for prev. line */
@

<<Process a line of [[unif]]>>=
if(isspace(*lbuf) || *lbuf == '#') /* technically impossible */
  continue;
/* field 1: code point */
char *s = strchr(lbuf, ';');
if(!s)
  exit(1);
*s++ = 0;
/* field 2 */
s = strchr(s, ';');
if(!s)
  exit(1);
/* field 3 */
s = strchr(s + 1, ';');
if(!s)
  exit(1);
/* field 4: ccc */
char *ccc = s + 1;
s = strchr(ccc, ';');
if(!s)
  exit(1);
*s++ = 0;
/* ccc table entry */
if(strcmp(ccc, "0")) {
  printf("%s\t{0x%s, %s}", nxt, lbuf, ccc);
  nxt = ",\n";
}
/* field 5 */
@

<<Make [[uninorm.gen]]>>=
/* end of ccc tab */
puts("\n};");
@

Next, the decomposition mapping field is extracted and stored.
Compatibility decompositions are denoted by a reason in angle
brackets; all others are canonical decompositions.  In order to retain
numerical order, the Hangul syllables are added at the appropriate
point.

<<Process a line of [[unif]]>>=
s = strchr(s, ';');
if(!s)
   exit(1);
/* field 6: decomp */
char *deccp = s + 1;
s = strchr(deccp, ';');
if(!s)
  exit(1);
*s = 0;
if(!strcmp(lbuf, "AC00")) {
  <<Add Hangul syllables>>
}
/* store non-empty deccp for later processing */
while(isspace(*deccp))
  deccp++;
if(!*deccp)
  continue;
int iscomp = *deccp == '<'; /* compatibility */
if(iscomp) {
  /* skip compatibility reason */
  while(*++deccp && *deccp != '>');
  if(!*deccp)
    exit(1);
  while(isspace(*++deccp));
  if(!*deccp)
    exit(1);
}
/* count # of code points */
int len;
for(len = 0, s = deccp; *s; len++) {
  while(*s && !isspace(*s))
    s++;
  while(isspace(*s)) s++;
}
/* store a new decomp entry */
if(ndec == maxdec) {
  maxdec *= 2;
  resize(dec, maxdec);
}
dec[ndec].cp = strtol(lbuf, NULL, 16);
dec[ndec].off = ndecs;
dec[ndec].len = len;
dec[ndec].type = iscomp ? 2 : 1;
/* store decomp string */
if(ndecs + len > maxdecs) {
  maxdecs *= 2;
  resize(decs, maxdecs);
}
for(len = 0, s = deccp; *s && len < dec[ndec].len; len++) {
  decs[ndecs + len] = strtol(s, &s, 16);
  while(isspace(*s)) s++;
}
if(dec[ndec].len != len)
  exit(1);
ndecs += len;
@

Before leaving this new decomposition entry, it needs to be checked to
see if it is a canonical composition entry as well.  This requires
checking the Full Composition Exclusion flag.  This flag is listed
in code point order, as a set of ranges.  The most relevant range is
kept; once the range becomes irrelevant, a new range is read from the
file.

<<[[mkuninorm]] globals>>=
char lbuf2[512];
@

<<Stuff to do before reading [[unif]]>>=
int exl = -1, exh = -1;
@

<<Process a line of [[unif]]>>=
/* composition is always in pairs, so no point if len != 2 */
if(dec[ndec].type == 1 && len == 2) {
  /* load a new range if currently above range */
  while(dec[ndec].cp > exh) {
    exl = exh = 0x110000;
    while(fgets(lbuf2, sizeof(lbuf2), f2)) {
      if((s = strchr(lbuf2, '#')))
        *s = 0;
      if(!strstr(lbuf2, "; Full_Composition_Exclusion "))
       continue;
      exl = strtol(lbuf2, &s, 16);
      if(*s == '.' && s[1] == '.')
        exh = strtol(s + 2, NULL, 16);
      else
        exh = exl;
      break;
    }
  }
  if(dec[ndec].cp < exl)
    dec[ndec].type = 0;
}
@

<<Process a line of [[unif]]>>=
++ndec;
@

The case folding decomposition strings are fully expanded; that is,
any code point in the target string which might also be decomposed has
already been decomposed.  The entries pulled from UnicodeData.txt,
however, are not fully expanded.  Neither are the Hangul syllable
entries.  Having them fully expanded reduces processing at run-time,
so they will be expanded here.

Each entry is extracted, converted to a fully expanded form, and added
back.  The old entries are kept, in order, for further expansion, so
the old entry count is retained.

<<[[mkuninorm]] globals>>=
int decbuf[256];
@

<<Make [[uninorm.gen]]>>=
/* Fully exapnd entries added so far */
int ondec = ndec;
int i;
for(i = 0; i < ondec; i++) {
  int declen = dec[i].len;
  memcpy(decbuf, decs + dec[i].off, declen * sizeof(int));
  <<Fully expand [[dec[i]]]/[[decbuf]]>>
}
@

Type 0 expansions and type 1 expansions can be applied to type 0 and 1
expansions to form type 1 expansions.  Type 2 expansions can be
applied to type 0, 1, and 2 expansions to form type 2 expansions.
When a new expansion type is created, a new entry with duplicate code
point but new type is created.  The target type is stored in [[ntype]].

<<Fully expand [[dec[i]]]/[[decbuf]]>>=
int ntype = dec[i].type;
@

First, all type 0 and 1 expansions are applied.  The number of
canonical expansions applied is saved to indicate if anything was
done.  Since the initial segment of entries is in code point order,
expansions are searched on that list using a binary search.  Any
expansion changes the type to 1.

<<[[mkuninorm]] globals>>=
static int cmpdeccp(const void *a, const void *b)
{
    const uni_decent_t *d1 = a, *d2 = b;

    return d1->cp - d2->cp;
}
@

<<Fully expand [[dec[i]]]/[[decbuf]]>>=
int didex = 0;
/* apply canonical decomp only */
if(ntype < 2) {
  int j;

  for(j = 0; j < declen; j++) {
    int cp = decbuf[j];
    uni_decent_t de = {cp}, *mdec;
    if((mdec = bsearch(&de, dec, ondec, sizeof(*dec), cmpdeccp)) &&
       mdec->type < 2) {
      ++didex;
      ntype = 1;
      if(mdec->len != 1) {
        memmove(decbuf + j + mdec->len, decbuf + j + 1,
                (declen - (j + 1)) * sizeof(int));
        declen += mdec->len - 1;
      }
      if(mdec->len)
        memcpy(decbuf + j, decs + mdec->off, mdec->len * sizeof(int));
      /* redo newly added character(s) */
      j--;
    }
  }
}
int didex_canon = didex;
@

Next, any type of expansion is allowed.  This may perform additional
canonical expansion, but only if a compatibility expansion was done
first.  Thus, all exansions done in this phase are considered
compatibility expansions, and change the type to 2.  If a previous
expansion was already done, the result must be saved before changing
the type to 2.

<<Fully expand [[dec[i]]]/[[decbuf]]>>=
int j;

for(j = 0; j < declen; j++) {
  int cp = decbuf[j];
  uni_decent_t de = {cp}, *mdec;
  if((mdec = bsearch(&de, dec, ondec, sizeof(*dec), cmpdeccp))) {
    ++didex;
    if(didex_canon) {
      <<Add/replace fully expanded entry>>
       didex_canon = 0;
     }
     ntype = 2;
     if(mdec->len != 1) {
       memmove(decbuf + j + mdec->len, decbuf + j + 1,
               (declen - (j + 1)) * sizeof(int));
       declen += mdec->len - 1;
     }
     if(mdec->len)
       memcpy(decbuf + j, decs + mdec->off, mdec->len * sizeof(int));
      /* redo newly added character(s) */
     j--;
   }
}
@

Finally, the fully expanded entry replaces the old one (if types did
not change) or gets added (if types changed).

<<Fully expand [[dec[i]]]/[[decbuf]]>>=
if(didex) {
  <<Add/replace fully expanded entry>>
}
@

<<Add/replace fully expanded entry>>=
if(ntype == dec[i].type) {
  /* if type didn't change, just replace */
  dec[i].off = ndecs;
  dec[i].len = declen;
} else {
  /* otherwise, add a new, duplicate entry */
  if(ndec == maxdec) {
    maxdec *= 2;
    resize(dec, maxdec);
  }
  dec[ndec].off = ndecs;
  dec[ndec].len = declen;
  dec[ndec].cp = dec[i].cp;
  dec[ndec].type = ntype;
  ndec++;
}
if(ndecs + declen > maxdecs) {
  maxdecs *= 2;
  resize(decs, maxdecs);
}
memcpy(decs + ndecs, decbuf, declen * sizeof(int));
ndecs += declen;
@

Now all canonical and compatibility decomposition information has been
read in.  The only thing that remains is the case folding information.
Like the Full Composition Exclusion flag, the code points for case
folding are ranges.  The values are fully decomposed strings.

<<Make [[uninorm.gen]]>>=
/* NFKC_CF */
if(!(f = fopen(normf, "r"))) {
  perror(normf);
  exit(1);
}
while(fgets(lbuf, sizeof(lbuf), f)) {
  char *s;
  if(isspace(*lbuf) || *lbuf == '#' || !strstr(lbuf, "; NFKC_CF;"))
    continue;
  if((s = strchr(lbuf, '#')))
    *s = 0;
  /* field 1: code point(s) */
  if(!(s = strchr(lbuf, ';')))
    exit(1);
  *s++ = 0;
  /* field 2: flag name */
  if(!(s = strchr(s, ';')))
    exit(1);
  /* field 3: decomposition string */
  while(isspace(*++s));
  char *deccp = s;
  /* count # of code points */
  int len;
  for(len = 0, s = deccp; *s; len++) {
    while(*s && !isspace(*s))
      s++;
    while(isspace(*s))
      s++;
  }
  /* parse field 1 */
  int rlow = strtol(lbuf, &s, 16), rhigh = rlow;
  if(*s == '.' && s[1] == '.') {
    rhigh = strtol(s + 2, &s, 16);
    if(rhigh < rlow)
      exit(1);
  }
  /* add entry for each code point in field 1 */
  /* all sharing the same target string */
  for(;rlow <= rhigh; rlow++, ndec++) {
    if(ndec == maxdec) {
      maxdec *= 2;
      resize(dec, maxdec);
    }
    dec[ndec].cp = rlow;
    dec[ndec].off = ndecs;
    dec[ndec].len = len;
    dec[ndec].type = 3;
  }
  /* add target string */
  if(ndecs + len > maxdecs) {
    maxdecs *= 2;
    resize(decs, maxdecs);
  }
  for(len = 0, s = deccp; *s && len < dec[ndec-1].len; len++) {
    decs[ndecs + len] = strtol(s, &s, 16);
    while(isspace(*s)) s++;
  }
  if(dec[ndec-1].len != len)
    exit(1);
  ndecs += len;
}
fclose(f);
@

Now that everything has been read in, it is time to compress the
decomposition strings.  There are two easy compressions:  all equal
strings are shared, and a string may have the previous string as its
prefix.  It is also possible to have a string share a suffix of the
previous string as a prefix, but that is more difficult and probably
not worth the effort.  The easy compressions are made easy by first
sorting by decomposition string.

<<[[mkuninorm]] globals>>=
static int cmpdecs(const void *a, const void *b)
{
    const uni_decent_t *d1 = a, *d2 = b;
    int l = d1->len < d2->len ? d1->len : d2->len;
    int i, c;
    for(i = 0; i < l; i++) {
        c = decs[d1->off + i] - decs[d2->off + i];
        if(c)
            return c;
    }
    return (int)d1->len - (int)d2->len;
}
@

<<Make [[uninorm.gen]]>>=
/* Reorder/merge decs */
qsort(dec, ndec, sizeof(*dec), cmpdecs);
@

Then, a new string buffer is created, where the compressed strings
will be added.  It is initially as large as the old one, so there is
no need to check the size.

<<Make [[uninorm.gen]]>>=
int *decs2, ndecs2 = 0;
inisize(decs2, ndecs);
@

Next, just add strings if they are not identical to the previous
entry, or just add a suffix if they have a prefix equal to the
previous entry.  Since there are gaps, the savings due to compression
cannot be calculated simply, so instead every time a string is not
added, it is counted towards the saved space.

<<Make [[uninorm.gen]]>>=
int saved = 0;
for(i = 0; i < ndec; ) {
  int l = dec[i].len;
   if(i > 0) {
    /* share prefix if identical to previous entry */
    int l2 = dec[i-1].len;
    if(l2 < l && !memcmp(decs2 + dec[i-1].off, decs + dec[i].off,
                         l2 * sizeof(*decs))) {
      ndecs2 -= l2;
      saved += l2;
    }
  }
  memcpy(decs2 + ndecs2, decs+dec[i].off, l * sizeof(*decs));
  /* share decomp string if identical to previous entry */
  int j;
  for(j = i + 1; j < ndec; j++) {
    if(cmpdecs(&dec[i], &dec[j]))
      break;
    else
      saved += l;
  }
  for(; i < j; i++)
    dec[i].off = ndecs2;
  ndecs2 += l;
}
@

Now that the table has been compressed, the old table can be removed.
The string table statistics are printed for my amusement.

<<Make [[uninorm.gen]]>>=
free(decs);
fprintf(stderr, "Total string table length %d (%d saved)\n", ndecs2, saved);
@

In this program, string offsets were stored as integers.  However, in
the main program, string offsets are short integers.  Thus, if the
final, compressed table is large enough for offset overflow, this is a
good time to say it.  The size was already printed, so die without
comment.

<<Make [[uninorm.gen]]>>=
if(ndecs2 > 0x10000)
  exit(1);
@

Now that all the tables have been generated, it is time to print them.
Order doesn't really matter, so here's the string table.

<<Make [[uninorm.gen]]>>=
/* dump string table */
fputs("const int deccp[] = {", stdout);
for(i = 0; i < ndecs2 - 1; i++) {
  if(!(i%8))
    fputs("\n\t", stdout);
  else
    putchar(' ');
  printf("0x%04X,", decs2[i]);
}
if(!(i%8))
  fputs("\n\t", stdout);
else
  putchar(' ');
printf("0x%04X\n"
       "};\n", decs2[i]);
@

Since we already sorted by decomposition string, the canonical
composition table should be next.  This consists of all type 0
entries.  In order to assist debugging, the pair is displayed in a
comment after each entry.

<<Make [[uninorm.gen]]>>=
/* Print canonical composition table ordered by decomp */
puts("static const uni_decent_t cancomp[] = {");
nxt = "";
for(i = 0; i < ndec; i++)
  if(!dec[i].type) {
    printf("%s\t{0x%04X, %u, %u} /* %04X %04X */", nxt, dec[i].cp,
           (int)dec[i].off, (int)dec[i].len, decs2[dec[i].off],
           decs2[dec[i].off + 1]);
    nxt = ",\n";
  }
puts("\n};");
@

The decomposition tables need to be sorted by code point.  In
addition, the combined canonical/compatibility table should be sorted
by type for duplicate code points.

<<[[mkuninorm]] globals>>=
static int cmpdeccp_t(const void *a, const void *b)
{
  const uni_decent_t *d1 = a, *d2 = b;

  if(d1->cp == d2->cp)
    return d1->type - d2->type;
  else
    return d1->cp - d2->cp;
}
@

<<Make [[uninorm.gen]]>>=
/* next tables need ordering by cp again */
qsort(dec, ndec, sizeof(*dec), cmpdeccp_t);
@

The combined table for canonical and compatibility decomposition
consists of all type 1 and 2 entries, and all type 0 entries for which
there is not also a type 1 entry.  The sign of the length needs to be
negative for type 2 entries.

<<Make [[uninorm.gen]]>>=
/* combined table for canonical/compatible decomp */
puts("static const uni_decent_t decomp[] = {");
nxt = "";
for(i = 0; i < ndec; i++)
  if(dec[i].type < 3) {
    /* skip canonial comp if canonical decomp exists for it */
    if(!dec[i].type && i < ndec - 1 && dec[i + 1].cp == dec[i].cp &&
       dec[i + 1].type == 1)
      i++;
    printf("%s\t{0x%04X, %u, %s%u}", nxt, dec[i].cp,
           (int)dec[i].off, dec[i].type > 1 ? "-" : "", (int)dec[i].len);
    nxt = ",\n";
  }
puts("\n};");
@

Finally, the case folding table is just all type 3 entries.

<<Make [[uninorm.gen]]>>=
/* NFKC_CF table */
puts("static const uni_decent_t nfkc_cf[] = {");
nxt = "";
for(i = 0; i < ndec; i++)
  if(dec[i].type == 3) {
    printf("%s\t{0x%04X, %u, %u}", nxt, dec[i].cp,
           (int)dec[i].off, (int)dec[i].len);
    nxt = ",\n";
  }
puts("\n};");
@

\section{Generic Normalized Unicode Input}

To make things even easier, we can make a function which returns a
normalized character.  Any normalization type we have code for, we
support.

<<Unicode normalization support exports>>=
typedef enum {
  UN_NONE, UN_NFD, UN_NFKD, UN_NFC, UN_NFKC, UN_NFKC_CF
} uni_normtype_t;
@

<<Unicode normalization support exports>>=
#include "uni_io.h"

int uni_fgetc_norm(unifile_t *uf, uni_normtype_t nt);
@

<<Unicode normalization support functions>>=
int uni_fgetc_norm(unifile_t *uf, uni_normtype_t nt)
{
  <<Get normalized character>>
}
@

A simple function which returns a normalized character requires that
the current decomposition and composition state be kept.  It would be
ideal to extend the [[unifile_t]] structure with the extra information
rather than allocating yet another wrapper, but for now I would prefer
[[unifile_t]] to stay isolated to unnormalized I/O.

In order to make this transparent, the extra support structure is
automatically created and placed on a linked list.  It is assumed that
the file will not be closed and reopened without an intervening EOF.
It is also assumed that the function will not be called any more after
it returns EOF.

<<Unicode normalization support local definitions>>=
typedef struct unifile_norm {
  struct unifile_norm *next;
  unifile_t *uf;
  <<Normalized input buffer state>>
} unifile_norm_t;
static unifile_norm_t *norm_files = NULL;
@

<<Get normalized character>>=
unifile_norm_t *ufn = norm_files;

while(ufn && ufn->uf != uf)
  ufn = ufn->next;
if(!ufn) {
  inisize(ufn, 1);
  clearbuf(ufn, 1);
  ufn->next = norm_files;
  norm_files = ufn;
}

<<Return normalized character if not EOF>>

unifile_norm_t **bufp;
for(bufp = &norm_files; *bufp != ufn; bufp = &(*bufp)->next);
*bufp = ufn->next;
free(ufn);

return -1;
@

For support, we'll need to store at least the maximal decomposition of
a character.  Technically, it is possible to require an infinite
buffer to support canonical ordering and composition, but we'll only
support a finite number and hope for the best.  Some future revision
should make the buffer automatically expand when necessary.

<<Normalized input buffer state>>=
/* maximal decomposition of a single char is 18 chars */
/* leave room in buf for at least 2 */
int buf[128];
@

We'll also need to know how many characters are in the buffer, and how
many of those are good to go.

<<Normalized input buffer state>>=
int blen, oklen;
@

To return a single character, we'll need to normalize until at least
one character could not possibly be modified any more.

<<Return normalized character if not EOF>>=
int oklen = ufn->oklen;
int *buf = ufn->buf;
while(!oklen && !uf->eof) {
  /* read a char into end of buf, if needed & possible */
  <<Read and normalize a character>>
}
if(oklen) {
  /* now we have oklen chars in buf that are ready to return */
  int c = buf[0];
  memmove(buf, buf + 1, (--ufn->blen) * sizeof(*buf));
  ufn->oklen = oklen - 1;
  return c;
}
@

To add more characters, append a character to the buffer and
decompose it.  The Unicode standard states that case folding
requires an additional NFD step beforehand, but reading of the data
suggests that instead, it needs canonical ordering and composition
afterwards, just like the NFC and NFKC.  This makes case folding just
another decomposition method.

int nread;

<<Read and normalize a character>>=
int c;
int bp = ufn->blen;
c = buf[bp] = uni_fgetc(uf);
if(nt == UN_NONE) {
  oklen = ufn->blen = ++bp;
  break;
}
if(c != -1)
  switch(nt) {
    case UN_NONE: /* here to remove warning */
    case UN_NFD:
    case UN_NFC:
      bp += NFD_dec(buf + bp, 1);
      break;
    case UN_NFKD:
    case UN_NFKC:
      bp += NFKD_dec(buf + bp, 1);
      break;
    case UN_NFKC_CF:
      bp += NFKC_Casefold(buf + bp, 1);
  }
@

Then, the entire set of characters already read in can be ordered.  If
no characters can be obtained from canonical ordering, try to read
more right away.

<<Read and normalize a character>>=
int oblen = Canon_Order(buf, bp, uf->eof);
if(!oblen) {
  ufn->blen = bp;
  continue;
}
@

Then, all characters which have been ordered can be composed.  If
composition might need more characters, the next trip around the loop
will get them.

<<Read and normalize a character>>=
switch(nt) {
  case UN_NONE: /* here to remove warning */
  case UN_NFD:
  case UN_NFKD:
    /* no composition */
    oklen = oblen;
    break;
  case UN_NFC:
  case UN_NFKC:
  case UN_NFKC_CF: {
    int cblen = NFC_comp(buf, oblen, uf->eof ? NULL : &oklen);
    if(uf->eof)
      oklen = cblen;
    /* move unordered characters down if composition removed chars */
    if(cblen < oblen) {
      if(bp > oblen)
        memmove(buf + cblen, buf + oblen, (bp - oblen) * sizeof(*buf));
      bp -= oblen - cblen;
    }
    break;
  }
}
ufn->blen = bp;
@

\section{Character Classification}

Once the characters arrive, they need to be classified for lexical
analysis and other purposes.

<<Library [[uni]] Members>>=
chartypes.o
@

<<Library [[uni]] headers>>=
#include "chartypes.h"
@

<<chartypes.h>>=
<<Common C Warning>>
#ifndef CHARTYPES_H
#define CHARTYPES_H

<<Character type exports>>
#endif
@

<<chartypes.c>>=
<<Common C Header>>
#include "chartypes.h"

<<Character type local definitions>>

<<Character type functions>>
@

Unicode is a 32-bit encoding, leaving 4 billion possible values.
Filtering for valid code points still leaves over 1 million possible
values.  Rather than having huge lookup tables for character types as
would be done with only 256 code points, each flag is represented as
an array of ranges, in numerical order.  Binary searches can be used
to determine if the flag is set.

<<Character type exports>>=
typedef struct {
    int low, high;
} uni_chrrng_t;
@

<<Character type functions>>=
int cmprng(const void *a, const void *b)
{
    const uni_chrrng_t *_a = a, *_b = b;

    if(_a->high < _b->low)
        return -1;
    else if(_b->high < _a->low)
        return 1;
    else
        return 0;
}
@

<<Character type functions>>=
#define is_x(x) \
    int is_uni_##x(int cp) \
    { \
        uni_chrrng_t cr = {cp, cp}; \
        return bsearch(&cr, unirng_##x, sizeof(unirng_##x)/sizeof(uni_chrrng_t), \
                       sizeof(uni_chrrng_t), cmprng) ? 1 : 0; \
    }
@

For classification, the array of ranges is augmented by a value for
each range.  The value can be a plain integer or an enumeration.

<<Character type exports>>=
typedef struct {
    int low, high, dat;
} uni_chrrng_dat_t;
@

<<Character type functions>>=
int cmprng_dat(const void *a, const void *b)
{
    const uni_chrrng_dat_t *_a = a, *_b = b;

    if(_a->high < _b->low)
        return -1;
    else if(_b->high < _a->low)
        return 1;
    else
        return 0;
}
@

<<Character type local definitions>>=
/* prefix with return type */
#define range_ret(x, i) \
     uni_##x##_of(int cp) \
    { \
        uni_chrrng_dat_t cr = {cp, cp}, *res; \
        res = bsearch(&cr, unirng_##x, sizeof(unirng_##x)/sizeof(uni_chrrng_dat_t), \
                      sizeof(uni_chrrng_dat_t), cmprng_dat); \
        if(!res) \
            return i; \
        return res->dat; \
    }
/* enum return type */
#define x_of(x, i) uni_##x##_t range_ret(x, i)
@

Another possibility would be to use multi-level tables with sharing at
the lower levels.  The multi-level table approach requires fewer
lookups (just as many table lookups as there are levels, vs. a binary
search against a long range list that might take, say, 10 or more
table probes), but will likely still produce larger tables than the
range tables, and is difficult to create efficiently at run-time.  The
latter problem means that it is difficult to find the union or
intersection of multi-level tables.

There are probably very efficient algorithms to generate multi-level
tables, but I am not aware of them.  The optimal number of levels and
level size is difficult to find, but a pretty good number can be found
by taking each level in turn, and finding the size which produces the
minimum total length taking into account all uniques and the length of
the array at the next level.  While it is possible to store the
pointers in less memory, the next level is always assumed to store
32-bit pointers for this computation.

The [[split_data]] routine takes an array of bytes, and splits it into
blocks of at least 4 bytes (there is no point in having a 4-byte
pointer to 4 bytes of data).  It returns the size of each block, an
array of pointers into the byte array to be used as the next level and
the number of unique blocks, and may also modify the byte array to
place the unique blocks at the start.

<<Split data into multi-level table>>=
<<[[split_data]] prereq>>

static uint32_t split_data(uint8_t *data, uint32_t len, uint32_t **arr,
                           uint32_t *nent, int moveit)
{
    /* track minimum size in minwidth (the block size) */
    /* default is no blocking, indicated by block size 4 */
    uint32_t minsize = len, minlen = 4;
    /* NULL means no pointers (block size 4) */
    *arr = NULL;
    uint32_t curlen = 8; /* start at block larger than raw */
    <<Variables saved from [[split_data]] loop>>

    while(curlen < len) {
        uint32_t blks = (len + curlen - 1) / curlen;
        <<Create multi-level array of size [[curlen]]>>
	<<Save multi-level array only if smaller than [[minsize]]>>
	curlen *= 2;
    }
    if(moveit && minlen > 4) {
      <<Shift unique array blocks down>>
    }
    *nent = minsize / minlen;
    return minlen;
}
@

For each pass, a new array for the index return is created.  In
addition, a side array is created to store only the unique block
indices.  This alone prevents having to scan the entire return array
for block matches, speeding things up orders of magnitude with sparse
data.  Additionally, this array is kept sorted, allowing binary
searching to further reduce the number of comparisons needed.  Using a
hash table for this would require extra storage, and may speed things
up further.  In any case, each block is simply added to the return
array, checking first for duplicates.  Only unique blocks are added to
the total size.

<<Create multi-level array of size [[curlen]]>>=
uint32_t i, j;
uint32_t size = 0;
uint32_t *ret, *ublocks, nublocks = 0;

ret = malloc(blks * sizeof(*ret));
ublocks = malloc(blks * sizeof(*ublocks));
for(i = 0; i < blks; i++) {
  int h = nublocks - 1, l = 0;
  while(l <= h) {
    j = (l + h) / 2;
    int c = memcmp(data + curlen * i, data + curlen * ublocks[j], curlen);
    if(!c)
      break;
    if(c < 0)
      h = j - 1;
    else
      l = j + 1;
  }
  if(l > h) {
    if(++h == nublocks)
      ublocks[nublocks++] = i;
    else {
      memmove(ublocks + h + 1, ublocks + h,
              (nublocks - h) * sizeof(*ublocks));
      ++nublocks;
      ublocks[h] = i;
    }
    j = h;
    size += curlen;
  }
  ret[i] = ublocks[j];
}
@

If the array is to be saved, it is saved directly in the result
pointer.  Otherwise, it is freed.  The binary search array only needs
to be saved if blocks will be moved down.

<<Variables saved from [[split_data]] loop>>=
uint32_t *min_ublocks = NULL, min_nublocks = 0;
@

<<Save multi-level array only if smaller than [[minsize]]>>=
if(size + (len / curlen) * 4 < minsize + (len / minlen) * 4) {
  minsize = size;
  minlen = curlen;
  if(*arr)
    free(*arr);
  *arr = ret;
  if(moveit) {
    if(min_ublocks)
      free(min_ublocks);
    min_ublocks = ublocks;
    min_nublocks = nublocks;
  } else
    free(ublocks);
} else {
  free(ret);
  free(ublocks);
}
@

Finally, to actually shift the data down, we need to re-sort the
unique index array in numerical order, and then just copy them in that
order.

<<[[split_data]] prereq>>=
static int cmp_i32(const void *a, const void *b)
{
  return *(int32_t *)a - *(int32_t *)b;
}
@

<<Shift unique array blocks down>>=
uint32_t i;
qsort(min_ublocks, min_nublocks, sizeof(uint32_t), cmp_i32);
for(i = 0; i < min_nublocks; i++)
  if(min_ublocks[i] != i)
    memcpy(data + minlen * i, data + minlen * min_ublocks[i], minlen);
for(i = 0; i < (len + minlen - 1) / minlen; i++) {
  uint32_t *p = bsearch(*arr + i, min_ublocks, min_nublocks,
                        sizeof(uint32_t), cmp_i32);
  (*arr)[i] = p - min_ublocks;
}
@

To actually create a multi-level table, the [[split_data]] routine
must be called on each level of the table, until there are no benefits
to splitting any more.

<<Split data into multi-level table>>=
static void doit(const char *name, uint8_t *dat, uint32_t dlen,
                 uint32_t low, uint32_t high)
{
  /* 32 bits, minimum 3 bits per level -> max 11 levels */
  uint32_t *lev[11], levlen[11], levents[11];

  <<Split data into optimal tables>>
  /* now that the table has been built, spit it out */
  printf("const uint8_t unitab_%s[] = {\n", name);
  <<Print optimal multi-level table>>
  puts("};");
}
@

<<Split data into optimal tables>>=
int32_t i;
uint32_t nlev, levent;
uint32_t levsz;
uint8_t *levdat;

/* first, optimize the raw data */
for(nlev = 0, levent = levsz = dlen, levdat = dat; levdat; nlev++) {
  uint32_t nptr;
  levlen[nlev] = split_data(levdat, levsz, &lev[nlev], &levents[nlev], 1);
  /* then, optimize the level's pointers */
  levdat = (uint8_t *)lev[nlev];
  levent = (levent + levlen[nlev] - 1) / levlen[nlev];
  /* but first, compress to as few bytes as needed */
  if(lev[nlev]) {
    /* for simplicity, limit to 8/16/32, leaving out 24 */
    nptr = levents[nlev];
    if(nptr <= (1 << 8)) {
      uint32_t *sp = lev[nlev];
      uint8_t *dp = (uint8_t *)sp;

      for(i = 0; i < levent; i++)
        *dp++ = *sp++;
      levsz = levent;
    } else if(nptr <= (1 << 16)) {
      uint32_t *sp = lev[nlev];
      uint16_t *dp = (uint16_t *)sp;

      for(i = 0; i < nptr; i++)
        *dp++ = *sp++;
      levsz = levent * 2;
    } else
      levsz = levent * 4;
  } else {
    /* if this is the last one, enure numbers are correct */
    levents[nlev] = levsz;
    levlen[nlev] = 1;
  }
}
@

<<Print optimal multi-level table>>=
/* first, the low and high characters */
/* this is endian-specific, so don't expect to save to file */
puts("  /* low, high, # of entries in top-level table (32-bit) */");
printf("  %d, %d, %d, %d,", ((uint8_t *)&low)[0],
       ((uint8_t *)&low)[1], ((uint8_t *)&low)[2], ((uint8_t *)&low)[3]);
printf(" %d, %d, %d, %d,", ((uint8_t *)&high)[0],
       ((uint8_t *)&high)[1], ((uint8_t *)&high)[2], ((uint8_t *)&high)[3]);
/* next, the number of bytes in the first level, so it can be skipped */
low = levents[nlev - 1];
printf(" %d, %d, %d, %d,\n", ((uint8_t *)&low)[0],
       ((uint8_t *)&low)[1], ((uint8_t *)&low)[2], ((uint8_t *)&low)[3]);
/* now, print each table, in reverse order (i.e., lookup order) */
for(i = nlev - 1; i >= 0; i--) {
  printf("  /** level %d **/\n", nlev - 1 - i);
  <<Print optimal table level [[i]]>>
}
@

<<Print optimal table level [[i]]>>=
/* the first byte is the number of bits to mask off the bottom */
/* assuming byte addressing */
/* at lowest level, this is always 0 */
uint32_t mbits = 0;
uint32_t j;
for(j = 0; j < i; j++)
  mbits += lg2(levlen[j]);
puts("  /* header: shift bits [, pwidth, twidth, tlen] */");
printf("  %d,", mbits);
if(i) {
  /* for pointer levels, next is the pointer width. */
  uint32_t nptr = levents[i - 1];
  int nbytes = nptr > (1 << 16) ? 4 : nptr > (1 << 8) ? 2 : 1;
  printf("  %d,", nbytes);
  /* and the width of the target entries */
  printf(" %d,", lg2(levlen[i - 1]));
  /* and the first pointer, which is a pointer past the end of the next level */
  /* or 0 if the next level has as many entries as the pointer width */
  switch(nbytes) {
    case 1:
      printf(" %d,", nptr & 0xff);
      break;
    case 2: {
      uint16_t p = nptr;
      printf(" %d, %d,", *((uint8_t *)&p), ((uint8_t *)&p)[1]);
      break;
    }
    case 4: {
      uint32_t p = nptr;
      printf(" %d, %d, %d, %d,", *((uint8_t *)&p), ((uint8_t *)&p)[1],
             ((uint8_t *)&p)[2], ((uint8_t *)&p)[3]);
      break;
    }
      
  }
}
putchar('\n');
/* then, the raw data */
puts("  /* raw data */");
putchar(' ');
for(j = 0; j < levents[i] * levlen[i]; j++) {
  if(j) {
    putchar(',');
    if(j > 1 && !((j - 1) % 16)) {
      putchar('\n');
      putchar(' ');
    }
  }
  if(i)
    printf(" %d", (int)((uint8_t *)lev[i - 1])[j]);
  else
    printf(" %d", (int)dat[j]);
}
if(i)
  putchar(',');
putchar('\n');
@

<<[[split_data]] prereq>>=
static uint32_t bsz(uint32_t l)
{
    l |= l >> 1;
    l |= l >> 2;
    l |= l >> 4;
    l |= l >> 8;
    l |= l >> 16;
    return l + 1;
}

static uint32_t lg2(uint32_t l)
{
    l = bsz(l - 1) - 1;
    l = ((l & 0xaaaaaaaa) >> 1) + (l & 0x55555555);
    l = ((l & 0xcccccccc) >> 2) + (l & 0x33333333);
    l = ((l & 0xf0f0f0f0) >> 4) + (l & 0x0f0f0f0f);
    l = ((l & 0xff00ff00) >> 8) + (l & 0x00ff00ff);
    l = ((l & 0xffff0000) >> 16) + (l & 0x0000ffff);
    return l;
}
@

Consider the character type table: 2710 table entries in the range
table, for a total of 32520 bytes and 12 lookups maximum (11 average).
In comparison, the table produced by the above code, with pointers
reduced to the minimum required (e.g. 1 byte to address 256 or fewer
uniques), takes up 36747 bytes, but only requires 4 lookups. This is
actually a relative win for the table approach.  However, consider the
deprecated flag: 9 range entries for a total of 72 bytes and 4 lookups
maximum (3 average), versus 2615 bytes and 4 lookups every time.  In
fact, storage requirements are always greater for the table, unless
there are a large number of relatively short ranges.  While the number
of lookups is lower (unless there are only a few large ranges), the
chance for cache thrashing is higher.  It's hard to say which is best.

<<C Test Executables>>=
tsttab \
@

<<makefile.rules>>=
tsttab.o: chartypes.gen.h
@

<<tsttab.c>>=
<<Common C Header>>

#include "chartypes.h"
// static_proto

static uint8_t mtab_lookup(const uint8_t *dat, uint32_t val, uint8_t def)
{
  <<Look up an entry in optimal table>>
}

#include <sys/resource.h>
struct rusage ru;

static void tstart(void)
{
  getrusage(RUSAGE_SELF, &ru);
}

static unsigned long tend(void)
{
  struct rusage ru2;
  getrusage(RUSAGE_SELF, &ru2);
  return (ru2.ru_utime.tv_sec - ru.ru_utime.tv_sec) * 1000000 +
            (ru2.ru_utime.tv_usec - ru.ru_utime.tv_usec);
}

static void doit_dat(const char *name, const uni_chrrng_dat_t *rng, uint32_t nent,
                     const uint8_t *mtab, uint8_t def)
{
    uint32_t i;

    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      uni_chrrng_dat_t cr = {i, i}, *res;
      uint8_t b;
      res = bsearch(&cr, rng, nent, sizeof(uni_chrrng_dat_t), cmprng_dat);
      b = res ? res->dat : def;
      if(b != mtab_lookup(mtab, i, def)) {
        fprintf(stderr, "mismatch %s@%d\n", name, i);
	exit(1);
      }
    }
    /* check performance */
    int j;
    unsigned long tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
    for(i = 0; i < 0x110000; i++) {
      uni_chrrng_dat_t cr = {i, i}, *res;
      uint8_t b;
      res = bsearch(&cr, rng, nent, sizeof(uni_chrrng_dat_t), cmprng_dat);
      b = res ? res->dat : def;
    }
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
    for(i = 0; i < 0x110000; i++) {
      uni_chrrng_dat_t cr = {i, i}, *res;
      uint8_t b = mtab_lookup(mtab, i, def);
    }
    tt = tend();
    printf("%s r%ld t%ld\n", name, tr, tt);
}

static void doit_bool(const char *name, const uni_chrrng_t *rng, uint32_t nent,
                      const uint8_t *mtab)
{
    uint32_t i;
    
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      uni_chrrng_t cr = {i, i}, *res;
      uint8_t b;
      res = bsearch(&cr, rng, nent, sizeof(uni_chrrng_t), cmprng);
      b = mtab_lookup(mtab, i / 8, 0);
      if(!(b & (1 << (i & 7))) != !res) {
        fprintf(stderr, "mismatch %s@%d\n", name, i);
	exit(1);
      }
    }
    /* check performance */
    int j;
    unsigned long tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
    for(i = 0; i < 0x110000; i++) {
      uni_chrrng_t cr = {i, i}, *res;
      uint8_t b;
      res = bsearch(&cr, rng, nent, sizeof(uni_chrrng_t), cmprng);
    }
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
    for(i = 0; i < 0x110000; i++) {
      uint8_t b;
      b = mtab_lookup(mtab, i / 8, 0);
      b &= i & 7;
    }
    tt = tend();
    printf("%s r%ld t%ld\n", name, tr, tt);
}

int main(void)
{
#define dat(x, d) doit_dat(#x, unirng_##x, unirng_##x##_len, unitab_##x, d)
#define bool(x) doit_bool(#x, unirng_##x, unirng_##x##_len, unitab_##x)
  <<Convert ranges to tables>>
  return 0;
}
@

<<Look up an entry in optimal table>>=
uint32_t low = *(uint32_t *)dat, high, skip;
if(val < low)
  return def;
dat += 4;
high = *(uint32_t *)dat;
if(val > high)
  return def;
dat += 4;
val -= low;
skip = *(uint32_t *)dat;
dat += 4;
low = 0;
while(*dat) {
  uint8_t psz = dat[1];
  uint32_t nextlen = 1 << dat[2];
  uint32_t idx = val >> *dat;
  uint32_t nskip = 0;
  val -= idx << *dat;
  dat += 3;
  switch(psz) {
    case 1:
      nskip = *dat;
      if(!nskip)
        nskip = 0x100;
      dat += low + 1;
      idx = dat[idx];
      break;
    case 2:
      nskip = *(uint16_t *)dat;
      if(!nskip)
        nskip = 0x10000;
      dat += low + 2;
      idx = ((uint16_t *)dat)[idx];
      break;
    case 4:
      nskip = *(uint32_t *)dat;
      dat += low + 4;
      idx = ((uint32_t *)dat)[idx];
      break;
  }
  dat += skip - low;
  low = idx * nextlen;
  skip = nskip * nextlen;
}
/* yay!  at last level */
dat += low + 1; /* skip to correct block */
return dat[val];
@

The actual range tables can be automatically generated from the UCD.

<<Character type local definitions>>=
#include "chartypes.gen"
@

<<Character type exports>>=
<<[[chartypes.gen]] dependencies>>
#include "chartypes.gen.h"
@

\lstset{language=make}
<<makefile.rules>>=
chartypes.h: chartypes.gen.h
chartypes.o: chartypes.gen.h

chartypes.gen: <<Character types generation deps>>

	echo >$@
	<<Generate [[chartypes.gen]]>>
	<<Post-process [[chartypes.gen]]>>

chartypes.gen.h: chartypes.c chartypes.gen
	echo >$@
	<<Generate [[chartypes.gen.h]]>>
	cproto -E "$(CC) $(CFLAGS) $(EXTRA_CFLAGS) -E" -v -e -f 0 $< >>$@
@

First, although technically all code points from 0000 through 10FFFF
are valid, some do not have an entry in the UCD.  These should be
considered invalid as well.  To generate the table, UnicodeData.txt is
parsed.  The parsing is too confusing when done directly in the make
rule, so a script is used instead.

<<Character types generation deps>>=
$(UNIDATA) ud-ranges \
@

<<Generate [[chartypes.gen]]>>=
# filtering valid input
fgrep -v Surrogate $(UNIDATA) | ./ud-ranges valid >>$@
@

\lstset{language=C}
<<Character type exports>>=
int is_uni_valid(int cp);  /* valid (well-formed) code points */
@

<<Character type functions>>=
is_x(valid)
@

The UnicodeData.txt parser needs to merge consecutive code points and
correctly process the range psuedo-entries.  To do this, it simply
tracks the previous code point, and spits it out as the end of the
range when the current point does not follow.  It then spits out the
current point as the start of the next range.  It takes an optional
argument specifying the array name; without that, it just spits out
the array and not the full C definition, in case the array needs
further processing.

\lstset{language=sh}
<<Build Script Executables>>=
ud-ranges \
@

<<ud-ranges>>=
#!/bin/sh
<<Common NoWeb Warning>>
test -n "$1" && echo "const uni_chrrng_t unirng_$1[] = {"
cut -d\; -f1-2 | (
  prevcp=-2
  while IFS=\; read cp n; do
    # if this cp doesn't follow prevcp, start new range
    prevcp=$((prevcp+1))
    if test $prevcp -lt $((0x$cp)); then
      # end prev. range if it exists
      test $prevcp -gt 0 && printf '0x%06X},\n' $((prevcp-1))
      # start new range
      prevcp=$((0x$cp))
      printf '\t{0x%06X, ' $prevcp
    fi
    # for range start entries, read range end and jump to end
    case "$n" in *" First>") IFS=\; read cp n; prevcp=$((0x$cp));; esac
  done
  if [ $prevcp -ne -2 ]; then
    # finish off last range
    if [ -n "$1" ]; then
      printf '0x%06X}\n' $prevcp
    else
      printf '0x%06X},\n' $prevcp
    fi
  fi
)
test -n "$1" && echo "};"
test -n "$1" && echo "const uint32_t unirng_${1}_len = sizeof(unirng_$1)/sizeof(unirng_${1}[0]);"
@

The sort of post-processing the script supports is adding the values
and sorting.  For the Unicode General Category (gc), each one is
extracted and run through the script, tacking on the class to the
array.  Then, the arrays are all sorted to make one range array for
the character class.

\lstset{language=C}
<<Character type exports>>=
uni_gc_t uni_gc_of(int cp);
@

<<Character type functions>>=
x_of(gc, U_GC_Cn)
@

<<[[chartypes.gen]] dependencies>>=
typedef struct {
  const char const *shortname, *longname;
} uni_abbrev_name_t;
typedef struct {
  const char *name;
  uint32_t mask;
} uni_gc_desc_t;
@

\lstset{language=make}
<<Generate [[chartypes.gen.h]]>>=
# standard character classes
echo "typedef enum {" >>$@
cut -d\; -f3 $(UNIDATA) | sort -u | while read x; do \
  grep "^gc ; $$x " $(UNIVALALIAS) | while read d d d d l d; do \
    echo "  U_GC_$$x, U_GC_$$l = U_GC_$$x,"; \
  done; \
done >>$@
echo "  /* Cn means unknown, actually.  Must be > 0 to use as flag */" >>$@
echo " U_GC_Cn, U_GC_Unassigned = U_GC_Cn" >>$@
echo "} uni_gc_t;" >>$@
@

<<Generate [[chartypes.gen]]>>=
# standard character classes
echo "const uni_abbrev_name_t uni_gc_nameof[] = {" >>$@
cut -d\; -f3 $(UNIDATA) | sort -u | while read x; do \
  grep "^gc ; $$x " $(UNIVALALIAS) | while read d d d d l d; do \
    echo "  {\"$$x\", \"$$l\"},"; \
  done; \
done >>$@
echo '  {"Cn", "Unassigned"}' >>$@
echo "};" >>$@
echo "const uni_chrrng_dat_t unirng_gc[] = {" >>$@
for x in `cut -d\; -f3 $(UNIDATA) | sort -u`; do \
  cut -d\; -f1-3 $(UNIDATA) | grep ";$$x" | ./ud-ranges | \
      sed "s/}/, U_GC_$$x}/"; \
done | sort -t\{ -k 2 | sed '$$s/,$$//' >>$@
echo "};" >>$@
echo "const uint32_t unirng_gc_len = sizeof(unirng_gc)/sizeof(unirng_gc[0]);" >>$@
echo "const uni_gc_desc_t uni_gc_names[] = {" >>$@
grep '^gc ;' $(UNIVALALIAS) | while read -r d d s d l t1 l2 t2 l3; do \
  case "$$t1" in \
    ";") ;; \
    "#") l3="$$l2 $$t2 $$l3"; l2=; t2="#" ;; \
    *) l2=; t2=; ;; \
  esac; \
  if [ "#" = "$$t2" ]; then \
    fl=; \
    while :; do \
      fl="$$fl (1 << U_GC_$${l3%%|*})"; \
      case "$$l3" in \
        *"|"*) fl="$$fl |"; l3="$${l3#*| }" ;; \
	*) break ;; \
      esac; \
    done; \
  else \
    fl="1 << U_GC_$$s"; \
  fi; \
  echo "  { \"$$s\", $$fl },"; \
  echo "  { \"$$l\", $$fl },"; \
  test -n "$$l2" && echo "  { \"$$l2\", $$fl },"; \
done | LANG=C sort >>$@
echo "};" >>$@
echo "const uint32_t uni_gc_names_len = sizeof(uni_gc_names)/sizeof(uni_gc_names[0]);" >>$@
@

After creating all of the range structures, we can convert them to
multi-level tables as well.

<<C Build Executables>>=
rngtotab \
@

% FIXME: this won't work.  rngtotab needs chartypes.gen.h, which
%        requires chartypes.gen, which requires rngtotab
<<Character types generation deps>>=
rngtotab.c \
@

<<Post-process [[chartypes.gen]]>>=
$(CC) $(CFLAGS) $(EXTRA_CFLAGS) -o rngtotab rngtotab.c
./rngtotab >>$@
@

<<rngtotab.c>>=
<<Common C Header>>
#include "chartypes.h"
#include "chartypes.gen"
// static_proto

<<Split data into multi-level table>>

static void doit_dat(const char *name, const uni_chrrng_dat_t *tab,
                     uint32_t nent, uint8_t def)
{
    uint32_t i, j;
    uint32_t dlen = tab[nent - 1].high - tab[0].low + 1;
    uint8_t *dat = malloc(bsz(dlen));

    memset(dat, def, bsz(dlen));
    for(i = 0; i < nent; i++)
	for(j = tab[i].low; j <= tab[i].high; j++)
	    dat[j - tab[0].low] = tab[i].dat;
    printf("/* %s: %d/%d bytes, %d lookups */\n", name, nent * 12, nent * 9, lg2(nent));
    doit(name, dat, dlen, tab[0].low, tab[nent - 1].high);
}

static void doit_bool(const char *name, const uni_chrrng_t *tab, uint32_t nent)
{
    uint32_t i, j;
    uint32_t dbits = tab[nent - 1].high - tab[0].low + 1;
    uint32_t dlen = (dbits + 7) / 8;
    uint8_t *dat = malloc(bsz(dlen));
    uint32_t start = tab[0].low / 8;

    memset(dat, 0, bsz(dlen));
    for(i = 0; i < nent; i++)
	for(j = tab[i].low; j <= tab[i].high; j++)
	    dat[j / 8 - start] |= 1 << (j & 7);
    printf("/* %s: %d/%d bytes, %d lookups */\n", name, nent * 8, nent * 6, lg2(nent));
    doit(name, dat, dlen, start, tab[nent - 1].high / 8);
}

int main(void)
{
#define dat(x, d) doit_dat(#x, unirng_##x, sizeof(unirng_##x)/sizeof(unirng_##x[0]), d)
#define bool(x) doit_bool(#x, unirng_##x, sizeof(unirng_##x)/sizeof(unirng_##x[0]))
  <<Convert ranges to tables>>
  return 0;
}
@

<<Convert ranges to tables>>=
dat(gc, U_GC_Cn);
dat(sc, U_SC_Zzzz);
dat(scx, ~0);
dat(range, 0);
bool(valid);
bool(depr);
bool(ign);
@

Some other standard categories which might be useful are ignored and
deprecated code points.  These are extracted from PropList.txt and
DerivedCoreProperties.txt, which is in a different format, requiring a
different parser script.

<<makefile.vars>>=
UNIPROP = $(UCD_LOC)/PropList.txt
UNIDER = $(UCD_LOC)/DerivedCoreProperties.txt
@

<<Character types generation deps>>=
$(UNIDER) der-ranges \
@

<<Generate [[chartypes.gen]]>>=
# other features
fgrep '; Deprecated #' $(UNIPROP) | ./der-ranges depr >>$@
fgrep '; Default_Ignorable_Code_Point #' $(UNIDER) | ./der-ranges ign >>$@
@

\lstset{language=C}
<<Character type exports>>=
int is_uni_depr(int cp);
int is_uni_ign(int cp);
@

<<Character type functions>>=
is_x(depr)
is_x(ign)
@

The new script is almost identical to [[ud-ranges]], but the format
for ranges is different.

\lstset{language=sh}
<<Build Script Executables>>=
der-ranges \
@

<<der-ranges>>=
#!/bin/sh
<<Common NoWeb Warning>>
test -n "$1" && echo "const uni_chrrng_t unirng_$1[] = {"
cut -d\; -f1 | (
  prevcp=-2
  while read cpr; do
    # if this cp doesn't follow prevcp, start new range
    prevcp=$((prevcp+1))
    cp=${cpr%%..*}
    if test $prevcp -lt $((0x$cp)); then
      # end prev. range if it exists
      test $prevcp -gt 0 && printf '0x%06X},\n' $((prevcp-1))
      # start new range
      prevcp=$((0x$cp))
      printf '\t{0x%06X, ' $prevcp
    fi
    # for range entries, jump to end
    case "$cpr" in *..*) cp=${cpr#*..}; prevcp=$((0x$cp));; esac
  done
  if [ $prevcp -ne -2 ]; then
    # finish off last range
    if [ -n "$1" ]; then
      printf '0x%06X}\n' $prevcp
    else
      printf '0x%06X},\n' $prevcp
    fi
  fi
)
test -n "$1" && echo "};"
test -n "$1" && echo "const uint32_t unirng_${1}_len = sizeof(unirng_$1)/sizeof(unirng_${1}[0]);"
@

The Script (sc) property comes from Scripts.txt in a similar way.  It
is a one-to-one mapping, so the same technique can be used as with the
gc property.  On the other hand, it is convenient to store the
Script\_Extensions (scx) property at the same place, since sc is the
default value for scx.

\lstset{language=C}
<<Character type exports>>=
/* need to manually convert U_SC_Hrkt to U_SC_Hira+U_SC_Kana */
uni_sc_t uni_sc_of(int cp);
/* if -1, use uni_sc_of() instead */
/* otherwise, use uni_scx_exp[] */
int uni_scx_of(int cp);
@

<<Character type functions>>=
x_of(sc, U_SC_Zzzz)
int range_ret(scx, -1)
@

<<makefile.vars>>=
UNISCR = $(UCD_LOC)/Scripts.txt
UNISCRX = $(UCD_LOC)/ScriptExtensions.txt
@

<<Character types generation deps>>=
$(UNISCR) $(UNIVALALIAS) \
@

<<Generate [[chartypes.gen.h]]>>=
echo "typedef enum {" >>$@
sed -n 's/^sc *; \([^ ]*\) *; \([^ ]*\)[ ;]*\([^ ]*\)/\1 \2 \3/p' \
  < $(UNIVALALIAS) | while read s l s2; do \
  printf %s "  U_SC_$$s,"; \
  test "$$s" = "$$l" || printf %s " U_SC_$$l = U_SC_$$s,"; \
  test -n "$$s2" && printf %s " U_SC_$$s2 = U_SC_$$s,"; \
  echo; \
done | sed '$$s/,$$//' >>$@
echo "} uni_sc_t;" >>$@
@

<<[[chartypes.gen]] dependencies>>=
typedef struct {
  const char *name;
  /* uni_sc_t */int script;
} uni_sc_desc_t;
@

<<Generate [[chartypes.gen]]>>=
# scripts
# Zzzz not explicitly listed; covers anything not in any other
# Hrkt not explicitly listed; covers Kana+Hira
echo "const uni_chrrng_dat_t unirng_sc[] = {" >>$@
sed -n 's/^sc *; \([^ ]*\) *; \([^ ]*\)[ ;]*\([^ ]*\)/\1 \2 \3/p' \
  < $(UNIVALALIAS) | egrep -v Zzzz\|Hrkt | \
  while read s l s2; do \
    fgrep "; $$l #" $(UNISCR) | ./der-ranges | sed "s/}/, U_SC_$$s}/"; \
  done | sort -t\{ -k 2 | sed '$$s/,$$//' >>$@
echo "};" >>$@
echo "const uint32_t unirng_sc_len = sizeof(unirng_sc)/sizeof(unirng_sc[0]);" >>$@
# note: assumes at most 6 scripts; may change in future
# compiler should give an array overflow warning if so, though
echo "const uni_sc_t uni_scx_exp[][6] = {" >>$@
grep '^[0-9A-F]' $(UNISCRX) | cut -d\; -f2 | cut -d\# -f1 | uniq | \
    sed 's/ *$$//;s/ /, U_SC_/g;s/, //' | while read s; do \
  echo "  {$$s},"; \
done | sed '$$s/,$$//' >>$@
echo "};" >>$@
echo "const uni_chrrng_dat_t unirng_scx[] = {" >>$@
grep '^[0-9A-F]' $(UNISCRX) | cut -d\; -f2 | cut -d\# -f1 | uniq | ( \
      i=0; \
      while IFS= read -r s; do \
        fgrep ";$$s#" $(UNISCRX) | ./der-ranges | sed "s/}/, $$i}/"; \
	i=$$((i+1)); \
      done) | sort -t\{ -k 2 | sed '$$s/,$$//' >>$@
echo "};" >>$@
echo "const uint32_t unirng_scx_len = sizeof(unirng_scx)/sizeof(unirng_scx[0]);" >>$@
# Zzzz not explicitly listed; covers anything not in any other
# Hrkt not explicitly listed; covers Kana+Hira
sed -n 's/^sc *; \([^ ]*\) *; \([^ ]*\)[ ;]*\([^ ]*\)/\1 \2 \3/p' \
  < $(UNIVALALIAS) | egrep -v Zzzz\|Hrkt | ( \
  tt=; \
  while read s l s2; do \
    for n in $$l $$s $$s2; do \
      tt="$$tt { \"$$n\", U_SC_$$s },;"; \
    done; \
  done; \
  echo "const uni_sc_desc_t uni_sc_names[] = {" >>$@; \
  echo "$${tt%,;}" | tr \; \\n | LANG=C sort -u >>$@; \
  echo "};" >>$@; \
  echo "const uint32_t uni_sc_names_len = sizeof(uni_sc_names)/sizeof(uni_sc_names[0]);" >>$@ \
)
@

A few specific character classes that are useful for the compiler do
not warrant a range table.  Instead, they are implemented as macros
that compare directly to the limited set they contain.  These are:
characters which act as line terminators, and characters which may
initiate backslash-escapes.

\lstset{language=C}
<<Character type exports>>=
#define is_nl(x) ((x) == '\n' || (x) == '\r' || (x) == '\v' || (x) == '\f' || \
                  (x) == 0x0085 || (x) == 0x2028 || (x) == 0x2029)
/* just use == '\\' for nfkc_cf */
#define is_bs(x) (c == '\\' || c == 0xFE68 || c == 0xFF3C)
@

A mainline is provided for testing some of the mess above.  It also
makes it easy to figure out what the gaps are, and what to do about
them.

\lstset{language=make}
<<C Test Support Executables>>=
chartypes \
@

<<makefile.rules>>=
chartypes: chartypes.c chartypes.o uninorm.o uni_io.o
	$(CC) $(CFLAGS) -DRNG_MAIN $(LDFLAGS) -o $@ $< uninorm.o uni_io.o
@

\lstset{language=C}
<<Character type functions>>=
#ifdef RNG_MAIN
#include "uninorm.h"

int main(int argc, const char **argv)
{
  <<Test character type functions>>
  return 0;
}
#endif
@

First is to loop over all possible code points, and print the flags and
classes.

<<Test character type functions>>=
int i;
for(i = 0; i < 0x110000; i++) {
  printf("%06X ", i);
  <<Print flags and classes of cp [[i]]>>
  putchar('\n');
}
@

<<Print flags and classes of cp [[i]]>>=
/* standard character class */
fputs(uni_gc_nameof[uni_gc_of(i)].shortname, stdout);
putchar(' ');
@

<<Print flags and classes of cp [[i]]>>=
/* other flags */
if(find_ccc(i))
  putchar('-'); /* can be moved by canon_order */
if(!is_uni_valid(i))
  putchar('?');
if(is_uni_depr(i))
  putchar('D');
if(is_uni_ign(i))
  putchar('!');
@

\section{Character Names}

Another way to get input is to use backslash-escapes.  These require
knowledge of all Unicode character names.

First, let's collect the names and their values in comma-separated
value files.  We can generate all single-character Unicode names which
are not algorithmically generated directly from UnicodeData.txt.

\lstset{language=make}
<<Plain Built Files>>=
charnames.uni \
@

<<makefile.rules>>=
charnames.uni: $(UNIDATA)
	cut -d\; -f1-2 $(UNIDATA) | \
	  egrep -v '<|CJK COMPATIBILITY IDEOGRAPH' | \
	    while IFS=\; read cp n; do \
              echo "$$n,$$((0x$$cp))"; \
            done | sort >$@
@

Aliases come from NameAliases.txt, from the exact same feild numbers.

<<makefile.vars>>=
UNIALTNAM = $(UCD_LOC)/NameAliases.txt
@

<<Plain Built Files>>=
charnames.unia \
@

<<makefile.rules>>=
charnames.unia: $(UNIALTNAM)
	grep '^[0-9A-F]' $(UNIALTNAM) | cut -d\; -f1-2 | \
	  while IFS=\; read cp n; do \
            echo "$$n,$$((0x$$cp))"; \
          done | sort >$@
@

Similarly, all multi-character Unicode names can be generated directly
from NamedSequences.txt.

<<makefile.vars>>=
UNISEQ = $(UCD_LOC)/NamedSequences.txt
@

<<Plain Built Files>>=
charnames.uniseq \
@

<<makefile.rules>>=
charnames.uniseq: $(UNISEQ)
	grep '^[A-Z]' $< | while IFS=\; read n cp; do \
           echo -n "$$n"; \
           for y in $$cp; do \
              echo -n ,$$((0x$$y)); \
           done; \
           echo; \
         done > $@
@

So, what do we do with all of these names?  The obvious thing is to
create a hash table to look up the code point using the name, and
another hash table to look up the name using the code point.  Since
the const of comparing integers is small, a binary search might
suffice for the code point-to-name lookup.  For the hash table, we
could either use some random hash function and hash table
implementation, or we could generate a perfect hash function using
[[gperf]].\footnote{After I wrote this, a program called [[cmph]]
(\url{http://sourceforge.net/projects/cmph}) caught my attention. This
generates minimal perfect hash functions in linear time (sub second
time for the entire Unicode name set!).  Its main problems are
that it generates data files rather than code (requiring changes to
the library to support code generation instead), and that it does not
perform the final comparison step, essentially just returning the
integer index of a possible hashed string (easily correctable).} My
initial implementation used [[gperf]], but generating a perfect hash
for the full Unicode set took 2 hours on my machine, and an additional
1 hour to compile.  Instead, I decided to go with the first thing I
ever implemented in a compiler: a single hash table used for all
strings in the system, preloaded with known symbols.

\section{Persistent Strings}

My original compiler-wide string hash table had a variable-sized hash
table, and an array of all strings in the table.  The array index was
the \emph{string number}, which was the only thing remembered about
the string from that point on, unless the string needed to be printed.
Strings never needed to be compared lexically.  Only a single function
is needed to look up the string number; it will automatically add the
string if not present and [[autoadd]] is non-zero.  For printing, a
second function returns a pointer to the start of the string.  As a
convention, a zero-length string is always string number zero.  Zero
is also returned for a non-zero-length string if it is not present and
was not added.  For unique strings, it may also be desirable to add
data to store additional information.  In the Unicode name
application, for example, the Unicode code point could be stored.  Of
course it could also be stored in a separate table, but that would add
four bytes of overhead minimum per code point (i.e., the string
number) as well as longer lookup times (i.e., having to do a binary
search on the supplemental table).  To support storing information
directly in the string table, [[autoadd]] can be a negative number,
indicating the number of bytes to add before the string to store data.

<<Library [[uni]] Members>>=
strs.o
@

\lstset{language=C}
<<Common C Includes>>=
#include "strs.h"
@

<<Library [[uni]] headers>>=
#include "strs.h"
@

<<strs.h>>=
<<Common C Warning>>
#ifndef STRS_H
#define STRS_H

/* if 0-len string, use 0; don't call this function! */
/* if len == 0, function does strlen() for you */
/* 0 returned if len == 0 or !autoadd and not already there */
/* autoadd < 0 to make -autoadd bytes room before entry */
/* make sure the entry was actually added, though! */
uint32_t strno(const char *str, int len, int autoadd);

/* str is not 0-terminated!  pass non-NULL len if len desired */
const char *str_for(uint32_t no, int *len);

<<Master string table exports>>

#endif
@

<<strs.c>>=
<<Common C Header>>

<<Master string table definitions>>

uint32_t strno(const char *str, int len, int autoadd)
{
  if(!len)
    len = strlen(str);
  if(!len)
    return 0;
  <<Convert [[str]]/[[len]] to string number>>
}

const char *str_for(uint32_t no, int *len)
{
  if(!no) {
    if(len)
      *len = 0;
    <<Return 0-len string>>
  }
  <<Convert [[no]] to string/[[len]]>>
}
@

For this hash table, though, I'll use a fixed-sized table at an
arbitrary size.  A 128K-entry table occupies one megabyte in memory
for a 64-bit system.  Nothing will be stored in the data cache for
very long for such a table.  Of course the comparison only needs to be
made on receiving strings, so it probably does not matter much.

As with my original hash tables, I am using a binary modulus.  I have
found very little difference in hash table performance with actual
program identifiers, and on some processors, there is a significant
performance benefit to using binary moduli.  The hash function simply
xors the length and all characters in the string after rotating the
previous hash value 3 bits left (there is no C function for this, so
equivalent code is used in the hope that the compiler will be smart
enough to figure it out; the compiler I'm using at least seems to
understand).  An xor is used instead of addition to avoid having to
deal with overflow.  The final hash value wraps any bits above 17 down
as well.  This is not done in the loop because there is no instruction
to rotate 17 bits on most processors, leaving the loop fast and the
slower 17-bit operation for the finishing touch.

<<Master string table exports>>=
#define HTAB_SIZE (128*1024)

#ifdef __GNUC__
__attribute__((unused))
#endif
static uint32_t hashfun(const char *nam, int len)
{
  uint32_t hash = len;

  while(--len >= 0)
    /* @<< 3 + @>> 29 should translate to a single rotate instruction */
    hash = ((hash @<< 3) + (hash @>> 29)) ^ *(unsigned char *)nam++;

  /* 17 * 2 == 34, so only need to do this once */
  /* / HTAB_SIZE should translate to >> log2(HTAB_SIZE) */
  /* % HTAB_SIZE should translate to & ~(HTAB_SIZE - 1) */
  return (hash / HTAB_SIZE ^ hash) % HTAB_SIZE;
}
@

The master string hash table requires static storage for strings
(i.e., storage which will not move during the string's lifetime), the
hash table itself, and the array of strings.  The array can store
offsets into the string storage rather than pure pointers, allowing
the strings to move if desired, and removing the need for 64-bit
pointers on 64-bit systems.  In addition, the hash table entries
themselves can be an index into the string array, giving them the same
benefits, as well as removing the need for a 64-bit hash bucket pointer.

<<Master string table exports>>=
extern uint32_t strhash[HTAB_SIZE];

typedef struct {
  uint32_t off; /* offset into string data */
  uint32_t nextent;
} hashent_t;
@

There are actually two sources of hash table entries:  built-in and
dynamically added.  The built-in strings should be just one large
array of hash table entries along with one large string buffer.  The
most efficient representation would be as arrays (i.e. C [[[]]]), but
in order to allow sharing the same routines with the programs which
build the arrays, I also allow representation as pointers, which
requires an additional read before use rather than a static address.
Rather than introduce this inefficiency into the compiler, I'll use a
[[#define]] to select, and build an extra binary for the alternative.
Similarly, the size of the string can be gotten with [[sizeof]] or
with an external integer, with the former being more efficient.
Taking the size directly only works if the string is included directly
in [[strs.c]], though.

\lstset{language=make}
<<makefile.rules>>=
strs-dyn.c: strs.c cproto.h
	ln -sf $< $@
strs-dyn.o: EXTRA_CFLAGS+=-DDYN_BUILTIN_STR
strs.o: strs.gen
@

\lstset{language=C}
<<Master string table exports>>=
#ifdef DYN_BUILTIN_STR
extern const char *builtin_str;
extern int builtin_str_size;

extern const hashent_t *builtin_hashe;
extern int builtin_hashe_size;
#else
/* builtin_str and builtin_hashe are local to strs.c */
#define builtin_str_size (sizeof(builtin_str) - 1) /* remove trailing C 0 */
#define builtin_hashe_size (sizeof(builtin_hashe)/sizeof(hashent_t))
#endif
@

<<Master string table definitions>>=
#ifndef DYN_BUILTIN_STR
#include "strs.gen"
#endif
@

The strings themselves are actually arbitrary binary data, so they
cannot be terminated by a sentinel as in normal C strings.  Instead,
the length needs to be stored along with the string.  My old string
routines stored the length before the string, so that the pointer
always points to the first character rather than the length.  This
seems harmless, so I'll do it here as well.  The actual format of the
length is a variable-length integer, with the high bit set on all but
the last byte.  This is obviously just to save space, since most
strings will only require one byte of length.  Since the length is
stored before the string, any additional data stored with the string
must be stored ahead of the length.  To assist in both parsing the
string length and finding a pointer to the space before it,
[[num_before]] can be used.

<<Master string table exports>>=
/*
 * strtab format:
 *  [<parm>] <len> * <utf8-str>
 *  len is big-endian variable-len, with high bit set for all bytes but first
 *  parm is dependent on string #, and interpreted based on that
 *  * indicates position of string offset
 */
/* 1st strtab (built-in) is contiguous string of length BUILTIN_STR_LEN */

/* len of str_for(n) is num_before(str_for(n)). */
/* length of len is returned in len */
#ifdef __GNUC__
__attribute__((unused))
#endif
static uint32_t num_before(const char *s, int *len)
{
  const unsigned char *p = (const unsigned char *)s;
  uint32_t ret = *--p & 0x7f;
  uint32_t shift = 0;
  while(*p-- > 0x7f)
    ret += (*p & 0x7f) << (shift += 7);
  if(len)
    *len = (int)((const unsigned char *)s - p) - 1;
  return ret;
}
@

<<Return 0-len string>>=
return "" + 1; /* in case num_before called on it */
@

The use of an array index or string offset to identify strings and
hash table entries eliminates the need to keep them at fixed
addresses.  This allows the use of simple dynamically resized arrays
for both.  However, as they grow, the expense of potentially copying
every single byte added so far to a newly allocated entry, as well as
the memory fragmentation caused by reallocation, may be too great.
Instead, they are stored in arrays of buffers.  The only resizable
arrays are the buffer pointers, which will grow very slowly.

<<Master string table definitions>>=
#define BUFARRAY_MIN 16 /* # of initial elements in indirect arrays */

#define STRBUF_LEN 65536 /* bytes */
static struct strbuf {
    uint16_t len;
    char buf[STRBUF_LEN];
} **ostrings = NULL;
static int nbufs = 0;

#define HBUF_LEN 1024 /* * sizeof(hashent_t) -> * 8 */
static struct hashebuf {
    uint16_t len;
    hashent_t ent[HBUF_LEN];
} **hashe = NULL;
static int nhashe = 0;
@

The separate storage for built-in and dynamically added strings
requires extra calculation.  Given an index, simply use a builtin
entry if the index is low enough, or subtract the number of builtin
entries and then convert to an indirect index plus an offset into the
buffer.   The other way around simply requires adding the built-in
size to any index returned.  Only the former conversion is presented
here, for hash table entries.  The same method must be applied to
builtin string offsets as well, though.

<<Master string table definitions>>=
static const hashent_t *he_for(uint32_t hn)
{
    if(hn < builtin_hashe_size)
        return &builtin_hashe[hn];
    hn -= builtin_hashe_size;
    return hashe[hn / HBUF_LEN]->ent + hn % HBUF_LEN;
}
@

First, we'll take care of the easy case.  Converting a string number
to a string requires looking up the hash entry, and then looking up
the string specified by its offset.  This routine is not for general
use, so there is no need to check that the string number is within
range first.

<<Convert [[no]] to string/[[len]]>>=
const hashent_t *he = he_for(no - 1);
no = he->off;
const char *ret;
if(no < builtin_str_size)
  ret = builtin_str + no;
else {
  no -= builtin_str_size;
  ret = ostrings[no / STRBUF_LEN]->buf + no % STRBUF_LEN;
}
if(len)
  *len = num_before(ret, NULL);
return ret;
@

The easy case for returning the string number is to find it in the
hash table and just return the hash entry's array index.  This is what
is actually stored in the hash table and the hash bucket next
pointers.

One potential optimization would be to move the found entry to the top
of the hash bucket, on the assumption that it will be accessed again
soon.  This may be revisited in the future, but for now, a simpler
loop structure is used with no modification of built-in hash entries.

<<Convert [[str]]/[[len]] to string number>>=
uint32_t h = hashfun(str, len);
int hen;
/* traverse hash bucket */
for(hen = strhash[h]; hen; ) {
  const hashent_t *he = he_for(hen - 1);
  int helen;
  const char *s = str_for(hen, &helen);
  if(len == helen && !memcmp(str, s, len))
    return hen;
  hen = he->nextent;
}
@

If the string was not found, it is automatically added.

<<Convert [[str]]/[[len]] to string number>>=
/* couldn't find it, so add it */
if(!autoadd)
  return 0;
@

If no strings have been previously added, the expandable arrays are
initialized.  This could have been done in some master initialization
routine instead, but it is not too expensive to do it here.

<<Convert [[str]]/[[len]] to string number>>=
if(!ostrings) {
  inisize(ostrings, BUFARRAY_MIN);
  clearbuf(ostrings, BUFARRAY_MIN);
  nbufs = BUFARRAY_MIN;
  inisize(hashe, BUFARRAY_MIN);
  clearbuf(hashe, BUFARRAY_MIN);
  nhashe = BUFARRAY_MIN;
}
@

Then, we need to compute the amount of memory actually needed for the
string.  In addition to the length of the string, the length of the
length and any requested additional storage must be allocated.

<<Convert [[str]]/[[len]] to string number>>=
uint32_t fulllen = 0, tl;
for(tl = len; tl > 0; tl >>= 7)
   fulllen++;
fulllen += len;
if(autoadd < 0)
  fulllen += -autoadd;
@

To actually store the string, we find the first buffer with enough
room.  If no such buffer exists, we add one.  Just in case the string
is too large to fit in any string buffer, the newly added buffer may
be enlarged to fit the new string.

<<Convert [[str]]/[[len]] to string number>>=
int i;
for(i = 0; i < nbufs && ostrings[i]; i++)
  if(STRBUF_LEN - ostrings[i]->len >= fulllen)
    break;
if(i == nbufs) {
  resize(ostrings, nbufs * 2);
  clearbuf(ostrings + nbufs, nbufs);
  nbufs *= 2;
}
if(!ostrings[i]) {
  if(fulllen > STRBUF_LEN) {
    ostrings[i] = malloc(sizeof(**ostrings) + fulllen - STRBUF_LEN);
    if(!ostrings[i]) {
      perror("string buffers");
      exit(1);
    }
  } else
    inisize(ostrings[i], 1);
  ostrings[i]->len = 0;
}
@

Then, the string is appended to the string buffer, leaving room before
it as needed.  The length is written in as well.  The newly added
string is added to the length, being careful not to overflow the
unsigned short, while allowing the length to equal or exceed 64k.
Since no added string could be just one byte (the only one-byte string
is the empty string), we indicate the buffer is full by stopping at
64k - 1.

<<Convert [[str]]/[[len]] to string number>>=
struct strbuf *sb = ostrings[i];
int off = builtin_str_size + i * STRBUF_LEN + sb->len + (fulllen - len);
unsigned char *p = (unsigned char *)sb->buf + sb->len + (fulllen - len);
memcpy(p, str, len);
while(len > 0) {
  *--p = len & 0x7f;
  len >>= 7;
  if(len)
    *p |= 0x80;
}
if(fulllen + ostrings[i]->len < STRBUF_LEN)
  ostrings[i]->len += fulllen;
else
  ostrings[i]->len = STRBUF_LEN - 1;
@

Next, we add the hash table entry for the string.  Room needs to be
made in the hash entry array for one more entry.

<<Convert [[str]]/[[len]] to string number>>=
for(i = 0; i < nhashe && hashe[i]; i++);
if(i > 0 && hashe[i-1]->len < HBUF_LEN)
  i--;
if(i == nhashe) {
  resize(hashe, nhashe * 2);
  clearbuf(hashe + nhashe, nhashe);
  nhashe *= 2;
}
if(!hashe[i]) {
  inisize(hashe[i], 1);
  hashe[i]->len = 0;
}
hen = builtin_hashe_size + i * HBUF_LEN + hashe[i]->len;
hashent_t *he = hashe[i]->ent + hashe[i]->len;
++hashe[i]->len;
@

Finally, we initialize the entry, link it into the hash bucket, and
update the hash table to point to it.  The index (plus one) is
returned as the string number.

<<Convert [[str]]/[[len]] to string number>>=
he->off = off;
he->nextent = strhash[h];
strhash[h] = hen + 1;
return hen + 1;
@

\section{Character Names, Revisited}

Now that we have the ability to use a table of built-in strings, the
various name tables can be added.  A general string table generator is
needed, since the Unicode names are not the only built-in strings.

\lstset{language=make}
<<makefile.rules>>=
strs.gen: mkstrs <<Built-in string table components>>

	<<Generate built-in string table using [[mkstrs]]>>
	<<Post-process built-in string table>>
@

<<C Build Executables>>=
mkstrs \
@

\lstset{language=C}
<<mkstrs.c>>=
<<Common C Header>>

<<[[mkstrs]] definitions>>

int main(int argc, const char **argv)
{
  <<Generate built-in string table>>
}
@

One of the things useful for the character tables is to store the
integer code point along with the name.  This requires, of course,
that all names be unique, since only one number can be stored.  This
is already required by the Unicode standard, so that is not a problem.
In addition, as mentioned above, it might save space to store Unicode
names as individual words, rather than full strings.

\lstset{language=make}
<<makefile.rules>>=
mkstrs.o: EXTRA_CFLAGS+=-DDYN_BUILTIN_STR
mkstrs: mkstrs.o strs-dyn.o
	$(CC) $(LDFLAGS) -o $@ $^
@

\lstset{language=C}
<<[[mkstrs]] definitions>>=
const char *builtin_str;
int builtin_str_size;
const hashent_t *builtin_hashe;
int builtin_hashe_size = 0;

uint32_t strhash[HTAB_SIZE];
@

There are three outputs of the program.  First is the built-in strings
it reads from the input file(s), in the order given, as C string
constants, one per line (automatically concatenated into a single
string constant by the C compiler).  Next, rather than printing the
hash structure, each entry is printed as a pair of hash value and
string offset.  This allows the outputs to be produced on the fly, as
entries are read in.  A separate program can collect the pairs of
numbers into a hash table.  Finally, in order to use the string
constants in the program without having to look them up first,
[[#define]] statements can be generated for the strings.  The first
string's index and the last string's index can be generated, as well
as an individual index for each string, named after a safe version of
the string itself.  The output files, as well as the names of the
[[#define]]s are all set by command line options. The files can be
either created or appended to, in case of adding to an existing set of
built-in strings.

<<Generate built-in string table>>=
<<[[mkstrs]] option variables>>
int exit_loop = 0;
while(--argc > 0 && !exit_loop && *(*++argv) == '-' && (*argv)[1])
  switch((*argv)[1]) {
    <<Process [[mkstrs]] option>>
    case '-':
      exit_loop = 1;
      break;
  }
@

<<[[mkstrs]] option variables>>=
const char *print_def = NULL, *enum_prefix = NULL,
           *out_fname = "-", *out_fmode = "w",
           *def_fname = "-", *def_fmode = "w",
           *hash_fname = "-", *hash_fmode = "w";
@

<<Process [[mkstrs]] option>>=
case 'o':
  out_fname = *argv + 2;
  if(*out_fname == '+') {
    out_fmode = "a";
    out_fname++;
  }
  break;
case 'd':
  def_fname = *argv + 2;
  if(*def_fname == '+') {
    def_fmode = "a";
    def_fname++;
  }
  break;
case 'h':
  hash_fname = *argv + 2;
  if(*hash_fname == '+') {
    hash_fmode = "a";
    hash_fname++;
  }
  break;
case 'D':
  print_def = *argv + 2;
  break;
case 'a':
  enum_prefix = *argv + 2;
  break;
@

In addition to appending to existing files, multiple input sources are
supported by supplying an initial string and hash table entry number
offset.

<<[[mkstrs]] option variables>>=
uint32_t pos = 1, spos = 0;
@

<<Process [[mkstrs]] option>>=
case 'l':
  pos = strtol(*argv + 2, NULL, 0) + 1;
  break;
case 'L':
  spos = strtol(*argv + 2, NULL, 0);
  break;
@

The remainder of the command-line arguments specify input files.  If
none are specified, standard input is used.

<<Generate built-in string table>>=
int do_stdin = !argc;
@

When merging string tables, the old string table and hash table need
to be read first.  The option to merge tables is selected by using the
append mode for the output files, and specifying a hash entry offset
for validation.  The number of entries in both the string and hash
table files should be equal to the supplied number.  The string offset
need not be supplied, as it is harder to get at (the [[-D]] option
gives the hash entry offset).

<<Generate built-in string table>>=
/* if appending to strings & hash, read old strings */
/* when done, open strings output */
int read_strings = pos > 1 &&
    out_fmode[0] == 'a' && hash_fmode[0] == 'a' &&
    strcmp(out_fname, "-") && strcmp(hash_fname, "-") &&
    strcmp(out_fname, hash_fname) && strcmp(def_fname, hash_fname) &&
    strcmp(out_fname, def_fname);
FILE *in_f;
int lbuf_len = 1024;
char *lbuf;
inisize(lbuf, lbuf_len);
if(read_strings) {
  if(!(in_f = fopen(out_fname, "r"))) {
    perror(out_fname);
    exit(1);
  }
  /* builtin_str is const, so read into regular char * first */
  char *prevstr = NULL;
  /* also, track allocated length rather than actual length */
  int prevstr_size = 0;
  inisize(prevstr, (prevstr_size = 1024));
  /* and track current insertion location instead */
  char *d = prevstr;
  while(1) {
    <<Read a full line into [[lbuf]]>>
    <<Parse and append old builtin string>>
  }
  fclose(in_f);
  builtin_str = prevstr;
  spos = builtin_str_size = d - prevstr;
}
@

<<Read a full line into [[lbuf]]>>=
int off = 0;
while(1) {
  if(!fgets(lbuf + off, lbuf_len - off, in_f) || !*lbuf)
    break;
  off += strlen(lbuf + off);
  if(lbuf[off - 1] == '\n')
    break;
  lbuf_len *= 2;
  resize(lbuf, lbuf_len);
}
if(!off)
  break;
@

The format of each line is a string constant.  The constant may be
followed by a comment, such as the string number and offset.  Rather
than parse an arbitrary C string constant, only the output format is
supported.  The output format is plain ASCII characters unescaped,
backslash and quote characters backslash-escaped, and anything else
escaped as a 3-digit octal escape.

<<[[mkstrs]] definitions>>=
typedef unsigned char uchar;
static void put_str(const char *buf, int len, FILE *f)
{
  for(; len--; buf++) {
    if(*buf == '\\')
      fputs("\\\\", f);
    else if(*buf == '"')
      fputs("\\\"", f);
    else if(*buf < ' ' || *buf > '~')
      fprintf(f, "\\%03o", (uchar)*buf);
    else
      fputc(*buf, f);
  }
}
@

<<Parse and append old builtin string>>=
if(*lbuf != '"') /* lines that aren't strings are ignored */
  continue;
char *s = lbuf + 1;
while(1) {
  for( ; *s && *s != '"' && (d - prevstr) < prevstr_size; s++, d++) {
    if(*s == '\\') {
      if(s[1] == '\\' || s[1] == '"')
        *d = *++s;
      else if(s[1] && s[2] && s[3]) {
        char c = s[4];
        s[4] = 0;
        *d = strtol(s + 1, NULL, 8);
        s[4] = c;
        s += 3;
      } else {
        fprintf(stderr, "Invalid previous string file\n");
        exit(1);
      }
    } else
      *d = *s;
  }
  /* here, either at end of string or end of buffer space */
  if(*s == '"')
    break;
  if(d == prevstr + prevstr_size) {
    int l = d - prevstr;
    resize(prevstr, (prevstr_size *= 2));
    d = prevstr + l;
    continue;
  }
  /* or end of input... */
  fprintf(stderr, "Invalid previous string file\n");
  exit(1);
}
@

The old hash entries need to be read in as well, in order to determine
string locations at the very least.  The format of the file is a pair
of numbers per line, separated by a space.  The first is the hash
value of the string, and the second is its offset.

<<Generate built-in string table>>=
if(read_strings) {
  if(!(in_f = fopen(hash_fname, "r"))) {
    perror(hash_fname);
    exit(1);
  }
  hashent_t *hashe = NULL; /* writable version of builtin_hashe */
  int hashe_size = 1024; /* its size */
  inisize(hashe, hashe_size);
  unsigned long h, str;
  uint32_t hen = 0;
  while(fscanf(in_f, "%lu %lu\n", &h, &str) == 2) {
    if(hen == hashe_size) {
      hashe_size *= 2;
      resize(hashe, hashe_size);
    }
    hashent_t *he = &hashe[hen];
    he->off = str;
    h %= HTAB_SIZE;
    he->nextent = strhash[h];
    strhash[h] = ++hen;
  }
  fclose(in_f);
  if(pos != hen + 1) {
    fprintf(stderr, "Invalid hash file; expected %u entries but got %u\n",
            pos - 1, hen);
    exit(1);
  }
  builtin_hashe_size = hen;
  builtin_hashe = hashe;
}
@

Now that the old data has been read in, the output files can be opened
for writing.  Technically, they could have used "r+" to avoid a second
open, but this way is easier to write.  In case any file names are
repeated, the file is only opened once.  The only reason this would
probably happen is if debugging, though.

<<Generate built-in string table>>=
/* open strings output */
FILE *out_f;
if(out_fname && *out_fname && strcmp(out_fname, "-")) {
  out_f = fopen(out_fname, out_fmode);
  if(!out_f) {
    perror(out_fname);
    exit(1);
  }
} else
  out_f = stdout;
/* open definitions output */
FILE *def_f;
if(def_fname && *def_fname && strcmp(def_fname, "-")) {
  if(!strcmp(def_fname, out_fname))
    def_f = out_f;
  else {
    def_f = fopen(def_fname, def_fmode);
    if(!def_f) {
      perror(def_fname);
      exit(1);
    }
  }
} else
  def_f = stdout;
/* open hash output */
FILE *hash_f;
if(hash_fname && *hash_fname && strcmp(hash_fname, "-")) {
  if(!strcmp(hash_fname, out_fname))
    hash_f = out_f;
  else if(!strcmp(hash_fname, def_fname))
    hash_f = def_f;
  else {
    hash_f = fopen(hash_fname, hash_fmode);
    if(!hash_f) {
      perror(hash_fname);
      exit(1);
    }
  }
} else
  hash_f = stdout;
@

The format of the input file is one string per line.  In order to
specify a value to insert before the string, an integer (in C integer
format) may be specified as well.  It is separated from the string by
a custom character.

<<[[mkstrs]] option variables>>=
char sep = 0;
@

<<Process [[mkstrs]] option>>=
case 'v':
  sep = isdigit((*argv)[2]) ? atoi(*argv + 2) : (*argv)[2];
  break;
@

One difficulty is that the built-in strings are massive: nearly 700K
for the Unicode names alone.  How could this be reduced, though?  One
way would be to 5-bit-encode the Unicode names, saving 30\% without
much effort.  However, space on modern machines is not at such a
premium any more, so this probably just wastes time.  Similarly, the
Unicode names can be split out as words, saving nearly 40\% in string
storage space, at the expense of adding 10000 more entries into the
hash table (for a total savings of 25\%, actually).  Again, this is
probably not worth it.  However, I may change my mind some time, so
this program supports splitting Unicode names into words.

<<[[mkstrs]] option variables>>=
int do_words = 0;
@

<<Process [[mkstrs]] option>>=
case 'w':
  /* NOTE: this option is for Unicode character name separation only! */
  /*  - words are separated by spaces */
  /*  - it assumes strings are ASCII (no high bits set!) */
  /*  - the words it adds are not usable for anything else */
  /*    unless already added (so load order is important) */
  do_words = 1;
  break;
@

Before spitting out the first string, we can add a definition to the
file indicating the start of the range.  However, if splitting words,
there may be hash entries for the words themselves first, so only
print it when not doing words.  In addition, it might help debugging
to print a comment before the first string in the output file as well.

<<Generate built-in string table>>=
if(print_def && !do_words) {
  fprintf(def_f, "#define %s_FIRST %u\n", print_def, pos);
  fprintf(out_f, "/* %s_FIRST */\n", print_def);
}
@

Then, we need to loop over all strings read from the inputs.

<<Generate built-in string table>>=
/* now, loop through new strings */
<<Prepare to loop over new builtin strings>>
do { /* for every input file */
  /* open next input file (stdin if none) */
  if(do_stdin || !strcmp(*argv, "-"))
    in_f = stdin;
  else {
    in_f = fopen(*argv, "r");
    if(!in_f) {
      perror(*argv);
      exit(1);
    }
  }
  if(!do_stdin)
    argv++;
  while(1) {
    <<Read a full line into [[lbuf]]>>
    <<Process a new built-in string>>
  }
} while(!do_stdin && --argc);
@

The format is not actually as described before:  in order to allow any
strings to be added, backslash escapes are supported as well.  These
are a simple escaped backslash, an escaped value separator, or a
two-digit hexadecimal character code.  So, the first thing to do is to
parse the string, replacing backslash-escapes and locating the value,
if present.

<<Process a new built-in string>>=
char *s, *d;
uint32_t val = 0;
for(s = d = lbuf; *s; s++, d++) {
  if(*s == '\\') {
    if(*++s == '\\' || *s == sep)
      *d = *s;
    else {
      if(!isxdigit(s[1]) || !isxdigit(s[2])) {
        fprintf(stderr, "Bad line: %s\n", lbuf);
        exit(1);
      }
      char t = s[3];
      s[3] = 0;
      *d = strtol(s, &s, 16);
      *s = t;
    }
  } else if(*s == sep) {
    val = strtol(s + 1, NULL, 0);
    break;
  } else if(*s == '\r' || *s == '\n')
    break;
}
uint32_t len = (uint32_t)(d - lbuf);
/* now, lbuf/len is string, val is value */
@

Now, if not in word mode, we simply spit out the string and move on.
The value and length are of course output first, and the string position
based on how many characters were output for the prefix. Rather than
check for ASCII characters, the length and code number are output as
octal escapes only.

<<[[mkstrs]] definitions>>=
static int put_ln(uint32_t n, FILE *f)
{
  uint32_t vl = 0, v[5], ret;
  while(n > 0x7f) {
    v[vl++] = n & 0x7f;
    n >>= 7;
  }
  ret = vl + 1;
  fprintf(f, "\\%03o", n);
  while(vl > 0)
    fprintf(f, "\\%03o", v[--vl] + 128);
  return ret;
}
@

<<Process a new built-in string>>=
if(!do_words) {
  <<Spit out built-in string>>
  /* spos is updated by prefix above */
  spos += len;
  pos++;
  continue;
}
@

<<Spit out built-in string>>=
fputc('"', out_f);
if(sep)
  spos += put_ln(val, out_f);
spos += put_ln(len, out_f);
put_str(lbuf, len, out_f);
if(sep)
  fprintf(out_f, "\" /* %u/%u%c%u */\n", spos, pos, sep, val);
else
  fprintf(out_f, "\" /* %u/%u */\n", spos, pos);
@

In addition to the string file, the definitions file may need
updating.  The format of the definition name is the prefix, followed
by an underscore, followed by the characters of the string.  If a
character is a space, it is output as double-underscores.  Otherwise,
if it is not a valid C identifier character, it is printed as an
underscore, followed by a lower-case x, follwed by the 2-digit hex
code.  This does not guarantee uniqueness, but it does guarantee that
the identifier is valid C, and in combination with the prefix, can
give a good assurance of uniqueness.

<<[[mkstrs]] definitions>>=
static void pr_enc(const char *s, int len, FILE *f)
{
  while(--len >= 0) {
    if(*s == ' ')
      fputs("__", f);
    else if(*s < ' ' || *s > '~' || (!isalnum(*s) && *s != '_'))
      fprintf(f, "_x%02x", *s);
    else
      fputc(*s, f);
    ++s;
  }
}
@

<<Spit out built-in string>>=
if(enum_prefix) {
  fprintf(def_f, "#define %s_", enum_prefix);
  pr_enc(lbuf, (int)(d - lbuf), def_f);
  fprintf(def_f, " %u\n", pos);
}
@

Finally, the hash table entry needs to be printed.

<<Spit out built-in string>>=
fprintf(hash_f, "%u %u\n", hashfun(lbuf, len), spos);
@

For word mode, processing strings is a little more complicated.
Instead of spitting out words as they arrive, they are added to the
hash table.  The original string is then converted to an encoded form.
The encoding retains spaces, but converts other contiguous strings
into their encoded string number.  Since encoding a single-word
character this way could not possible make it shorter, these are left
unencoded.

<<Process a new built-in string>>=
/* from this point on, it's word mode */
char *we, *ws = lbuf;
for(ws = lbuf; ws < lbuf + len; ws = we + 1) {
  for(we = ws; we < lbuf + len; we++)
    if(*we == ' ')
      break;
  if(we == ws)
    continue;
  int wlen = (int)(we - ws);
  /* don't bother if entire string is one word */
  if(wlen == len)
    break;
  <<Add word to string table and encode>>
}
@

In fact, the encoding of the words requires at least two bytes per
word, so words less than three characters are pointless to encode.

<<Add word to string table and encode>>=
if(wlen <= 2)
  continue; /* too short to bother */
@

Since there is no guarantee that substrings of Unicode names are not
also Unicode names, space needs to be allocated in front of the newly
added word to allow for a value.  Rather than mess with efficient
coding, the value is placed as a 32-bit value, along with a flag
indicating presence.  All newly added words get this.  When writing
the strings out, the words without values will be printed before the
words with values.  This will cause the string numbers to change.  In
order to support this, the updated string number is stored before the
string as a 32-bit number as well.  This means that 9 bytes need to be
allocated before any newly added word.  The first byte is the value
flag; the next four are the value, and the rest are for the updated
string number.  For newly added words, there is never a value
(although it may be added later).

<<Prepare to loop over new builtin strings>>=
int cur_hen = pos, old_hen = pos;
@

<<Add word to string table and encode>>=
/* auto-add and leave 9 bytes of room before word */
int str = strno(ws, wlen, -9);
/* if auto-added, set "value" flag to 0 */
if(str == cur_hen) {
  s = (char *)str_for(str, NULL);
  int ll; /* skip strlen */
  num_before(s, &ll);
  s -= ll + 1;
  *s = 0;
  cur_hen++;
}
@

Since UTF-8 does not cover all 32-bit values, and in order to avoid
conflict with unencoded words and spaces, the encoding of the
string number is a little-endian representation, with 7 bits per byte
and the high bit set on each byte.  This should not increase
the length, but just in case, we'll compute the replacement into a
buffer, and shift the remainder of the string to make room.

<<Add word to string table and encode>>=
/* encode strno(word) and replace word w/ encoding */
/* FIXME: expand lbuf if needed (not needed for Unicode 6.0) */
char ebuf[5];
int eblen;
uint32_t strout = str;
for(eblen = 0; eblen < 5 && strout; eblen++, strout >>=7)
  ebuf[eblen] = 0x80 | (strout & 0x7f);
memmove(ws + eblen, we, len - (int)(we - lbuf));
memcpy(ws, ebuf, eblen);
len -= wlen - eblen;
we -= wlen - eblen;
@

Finally, the encoded form (or unencoded if there was just one word) is
added to the string table.  Again, 9 bytes need to be added if the
string is newly added.  If it isn't, it is assumed to match a word
that was previously added, and so it also has 9 bytes of space.  If
that assumption does not hold, there is a big problem, so print the
unencoded bad string and exit.  To print the unencoded string, the
encoded string numbers are extracted and expanded.

<<Process a new built-in string>>=
int str = strno(lbuf, len, -9);
if(str < old_hen) {
  pr_lw(lbuf, len, stderr);
  fprintf(stderr, ": short name conflict (already there)\n");
  exit(1);
}
@

<<[[mkstrs]] definitions>>=
static void pr_lw_mayenc(const char *buf, int len, FILE *f, int enc)
{
  const char *s;
  
  while(1) {
    for(s = buf; len && !(*s & 0x80); s++, len--);
    if(s != buf) {
      if(enc)
        pr_enc(buf, (int)(s - buf), f);
      else
        fwrite(buf, (int)(s - buf), 1, f);
    }
    if(!len)
      return;

    uint32_t str;

    str = *s & 0x7f;
    int shift = 7;
    while(--len && (*++s & 0x80)) {
      str += (uint32_t)(*s & 0x7f) << shift;
      shift += 7;
    }
    buf = s;
    int l;
    s = str_for(str, &l);
    if(enc)
      pr_enc(s, l, f);
    else
      fwrite(s, l, 1, f);
  }
}
#define pr_lw(a,b,c) pr_lw_mayenc(a,b,c,0)
#define pr_lw_enc(a,b,c) pr_lw_mayenc(a,b,c,1)
@

We can now modify the 9 prefix bytes regardless of whether or not this
was added, but we still need to keep track of how many have been added
in order to detect word additions.  The value flag is set depending on
whether or not there is a value.  Since this is for Unicode character
names, which always have values, this flag also indicates whether it
is a full name or just a word.

<<Process a new built-in string>>=
int ll;
s = (char *)str_for(str, NULL);
num_before(s, &ll);
s -= ll + 1;
if(str == cur_hen) {
  *s = 0;  
  cur_hen++;
}
if(*s) { /* there should not already be a value there */
  pr_lw(lbuf, len, stderr);
  fprintf(stderr, ": short name conflict (already processed)\n");
  exit(1);
}
*s = 1;
if(sep) {
  *--s = val & 0xff;
  val >>= 8;
  *--s = val & 0xff;
  val >>= 8;
  *--s = val & 0xff;
  val >>= 8;
  *--s = val & 0xff;
}
@

After adding all of the strings to string table, they need to be
printed.

<<Generate built-in string table>>=
if(do_words) {
  <<Print word mode builtin strings>>
}
@

The standalone words need to be printed separately from the real
words, so that the range of string numbers printed for the real
numbers is accurate.  While it may seem that it does not matter which
order these are printed in, the words must be printed first.  As they
are added, their string number is updated, and these updated numbers
must be known before an encoded string using that word can be printed.
They are stored in the four bytes beyond the value.

<<Print word mode builtin strings>>=
uint32_t i;
for(i = old_hen; i < cur_hen; i++) {
  const char *str = str_for(i, NULL);
  int ll, len = num_before(str, &ll);
  if(str[-ll - 1]) /* has value */
    continue;
  fputc('"', out_f);
  spos += put_ln(len, out_f);
  put_str(str, len, out_f);
  fprintf(out_f, "\" /* %u/%u>%u */\n", spos, i, pos);
  fprintf(hash_f, "%u %u\n", hashfun(str, len), spos);
  spos += len;
  uint32_t npos = pos++;
  char *s = (char *)str - ll - 5;
  *--s = npos & 0xff;
  npos >>= 8;
  *--s = npos & 0xff;
  npos >>= 8;
  *--s = npos & 0xff;
  npos >>= 8;
  *--s = npos & 0xff;
}
@

Now that the words are all printed, the first element of the range is
known.

<<Print word mode builtin strings>>=
if(print_def) {
  fprintf(def_f, "#define %s_FIRST %u\n", print_def, pos);
  fprintf(out_f, "/* %s_FIRST */\n", print_def);
}
@

All encoded strings are printed next.  However, they must be updated
to new word locations first.  Since some may not really be encoded,
and therefore may be used by other words, the updated string number is
stored in them as well.

<<Print word mode builtin strings>>=
for(i = old_hen; i < cur_hen; i++) {
  const char *str = str_for(i, NULL);
  int ll, len = num_before(str, &ll);
  if(!str[-ll - 1]) /* has no value */
    continue;
  fputc('"', out_f);
  uint32_t val = 0;
  if(sep) {
    val =
      (uint32_t)(uchar)str[-ll - 2] +
      ((uint32_t)(uchar)str[-ll - 3] << 8) +
      ((uint32_t)(uchar)str[-ll - 4] << 16) +
      ((uint32_t)(uchar)str[-ll - 5] << 24);
    spos += put_ln(val, out_f);
  }
  <<Reencode builtin string [[str]] into [[lbuf]]/[[len]]>>
  spos += put_ln(len, out_f);
  put_str(lbuf, len, out_f);
  if(sep)
    fprintf(out_f, "\" /* %u/%u%c%u ", spos, pos, sep, val);
  else
    fprintf(out_f, "\" /* %u/%u ", spos, pos);
  /* print unencoded string */
  pr_lw(str, num_before(str, NULL), out_f);
  fputs(" */\n", out_f);
  fprintf(hash_f, "%u %u\n", hashfun(str, len), spos);
  if(enum_prefix) {
    fprintf(def_f, "#define %s_", enum_prefix);
    pr_lw_enc(str, num_before(str, NULL), def_f);
    fprintf(def_f, " %u\n", pos);
  }
  spos += len;
  uint32_t npos = pos++;
  d = (char *)str - ll - 5;
  *--d = npos & 0xff;
  npos >>= 8;
  *--d = npos & 0xff;
  npos >>= 8;
  *--d = npos & 0xff;
  npos >>= 8;
  *--d = npos & 0xff;
  npos++;
}
@

To reencode the string, all substrings with the high bit set must be
extracted, converted to numbers, looked up, and replaced with the
number that was stored 6-9 bytes before the length.

<<Reencode builtin string [[str]] into [[lbuf]]/[[len]]>>=
const char *s, *e = str + len;
char *d = lbuf;
for(s = str; s < e; s++) {
  if(!(*s & 0x80))
    *d++ = *s;
  else {
    uint32_t os = *s & 0x7f;
    int shift = 7;
    while(++s < e && (*s & 0x80)) {
      os += (uint32_t)(*s & 0x7f) << shift;
      shift += 7;
    }
    s--;
    if(os >= old_hen) {
      int oll;
      const char *ostr = str_for(os, NULL);
      num_before(ostr, &oll);
      os = (uint32_t)(uchar)ostr[-oll - 6] +
           ((uint32_t)(uchar)ostr[-oll - 7] << 8) +
           ((uint32_t)(uchar)ostr[-oll - 8] << 16) +
           ((uint32_t)(uchar)ostr[-oll - 9] << 24);
    }
    while(os > 0) {
      *d++ = 0x80 | (os & 0x7f);
      os >>= 7;
    }
  }
}
len = d - lbuf;
@


Finally, the last string has been printed, so spit out its number if
required.

<<Generate built-in string table>>=
if(print_def) {
  fprintf(def_f, "#define %s_LAST %u\n", print_def, pos - 1);
  fprintf(out_f, "/* %s_LAST */\n", print_def);
}
return 0;
@

Now that the string buffer is generated, we need to generate a hash
table.  This generates both the initial hash entry array and the hash
table itself.

\lstset{language=make}
<<C Build Executables>>=
mkstrs-hash \
@

<<Built-in string table components>>=
mkstrs-hash \
@

<<makefile.rules>>=
mkstrs-hash: mkstrs-hash.o
	$(CC) $(LDFLAGS) -o $@ $^
@

\lstset{language=C}
<<mkstrs-hash.c>>=
<<Common C Header>>
@

In order to generate the hash table, it needs to be built in memory.
Because the buckets are built by modifying only the newly added entry,
there is no need to keep any entries once they are output.  The only
additional storage kept is a count of how many entries were added to
each slot, so that an estimate of table badness can be printed on
completion.

<<mkstrs-hash.c>>=
uint32_t strhash[HTAB_SIZE], hdepth[HTAB_SIZE];
@

The mainline prints a header for the hash entry array for the static
version (requiring sed to alter it for non-static versions).  It then
reads every line from standard input, assuming it is the hash output
from the previous stage, and prints the hash table entry.  When done
with that, it dumps the hash table itself, as well as any statistics
it accumulated.

<<mkstrs-hash.c>>=
int main(void)
{
  <<Print hash entries>>
  <<Print hash table>>
  <<Print hash statistics>>
  return 0;
}
@

<<Print hash entries>>=
puts("const hashent_t builtin_hashe[] = {");
int he;
unsigned long h, str;
for(he = 0; scanf("%lu %lu", &h, &str) == 2; he++) {
  h %= HTAB_SIZE;
  if(he)
    fputs(",\n", stdout);
  printf("\t{%lu, %u}", str, strhash[h]);
  strhash[h] = he + 1;
  ++hdepth[h];
}
puts("\n};");
@

<<Print hash table>>=
fputs("uint32_t strhash[HTAB_SIZE] = {", stdout);
int i;
for(i = 0; i < HTAB_SIZE - 1; i++) {
  if(!(i & 15))
    fputs("\n\t", stdout);
  else
    putchar(' ');
  printf("%d,", strhash[i]);
}
printf(" %d\n"
       "};\n", strhash[i]);
@

<<Print hash statistics>>=
int maxd = 1, nhid = 0, nfill = 0;
for(i = 0; i < HTAB_SIZE; i++) {
  if(maxd < hdepth[i])
    maxd = hdepth[i];
  if(hdepth[i]) {
    nhid += hdepth[i] - 1;
    ++nfill;
  }
}
printf("/* max depth %d num hidden %d of %d; full %d/%d (%.2f%%) */\n",
       maxd, nhid, he, nfill, HTAB_SIZE, (double)nfill/(double)HTAB_SIZE*100);
@

Now we can start generating string tables.

Multi-character entries will be extracted from another table.  In
order to support this, multi-character entries have their code point
list replaced with a sequence number (the table entry index plus the
max Unicode code point), and the code points are printed as well, in a
format that is easily distinguishable from normal entries (i.e., with
a dash in the name).

<<Prepare for XML filter>>=
int nmulti = 0;
@

<<Print XML character list entry>>=
l = strchr(++s, ',');
if(!l)
  printf("%s,%s", buf, s);
else {
  printf("%s,0x%04X\n", buf, nmulti+0x110000);
  printf("%04X-0x%04X,0x%04X\n", nmulti,
         (int)strtol(s, NULL, 0), 
         (int)strtol(l + 1, NULL, 0));
  ++nmulti;
}
@

The main string table consists of all Unicode character names.
Any multi-character sequences are converted to an offset from
the maximum code point so that the sequences can be stored in a
separate table.

\lstset{language=make}
<<Built-in string table components>>=
charnames.uni charnames.unia charnames.uniseq \
@

<<makefile.config>>=
# Set to -w to use word mode (needs more work)
#UNI_NAME_WORD=-w
@

<<Generate built-in string table using [[mkstrs]]>>=
( \
  cat -n charnames.uniseq | while read -r n s; do \
    echo "$${s%%,*},$$((n+0x10ffff))"; \
  done; \
  cat charnames.uni{,a}; \
) | tr 'A-Z\055' 'a-z ' | \
  ./mkstrs $(UNI_NAME_WORD) -v, -DUNI_CHARS -hstrs-hash.gen -ostrs-strs.gen \
           -dbuiltin-strings.h
@

<<Post-process built-in string table>>=
./mkstrs-hash <strs-hash.gen >$@
echo "const char builtin_str[] =" >>$@
echo '#include "strs-strs.gen"' >>$@
echo ";" >>$@
@

<<makefile.rules>>=
builtin-strings.h: strs.gen
	touch builtin-strings.h
strs-strs.gen: strs.gen
	touch strs-strs.gen
@

The automatically generated names were not included in the above
lists.  In order to make it easier to look them up in the name-to-code
direction, their prefixes are added to the name table.  In addition, a
range table is added to [[chartypes]] to support the reverse lookup.

First, we'll generate a listing of the ranges, similar to the other
listings.

<<Build Script Executables>>=
mkunirng \
@

<<Plain Built Files>>=
charnames.unirng \
@

<<makefile.rules>>=
charnames.unirng: $(UNIDATA) mkunirng
	./mkunirng $(UNIDATA) >$@
@

\lstset{language=sh}
<<mkunirng>>=
#!/bin/sh
<<Common NoWeb Warning>>

@

There are three groups of automatically generated character names:
the Hangul syllables, the other ranges in UnicodeData.txt, and CJK
COMPATIBILITY IDEOGRAPH groups.  The first has a slightly more
involved generation algorithm; the other two simply append their code
point in hex after a dash.  What sets the CJK COMPATIBILITY IDEOGRAPH
code points apart is that they are listed individually in
UnicodeData.txt rather than as ranges.

The routine grabs both types at once, and outputs a start token (the
prefix followed by a caret) at the start of any range, and an end
token (the prefix followed by a dollar sign) at the end.

<<mkunirng>>=
cut -d\; -f1-2 $1 | egrep '<[^c]|CJK COMPATIBILITY IDEOGRAPH' | \
  dd conv=ucase 2>/dev/null | (
    last=; lastb=
    while IFS=\;, read a b c; do
      an=$((0x$a))
      b="${b%-*}"
      if [ -n "$last" ]; then
        case "$c" in
          *">") echo "$lastb \$,$last"; last= ;;
          *) last=$((last+1))
             if [ $last -ne $an -o "$lastb" != "$b" ]; then
               echo "$lastb \$,$((last-1))"
               last=$an
               lastb="$b"
               echo "$b ^,$an"
             fi ;;
        esac
      fi
      case "$c" in
        " FIRST>") echo "${b#<} ^,0x$a" ;;
        *">") echo "${b#<} \$,0x$a" ;;
        *) if [ -z "$last" ]; then
             last=$an
             lastb="$b"
             echo "$b ^,$an"
           fi ;;
      esac;
    done;
  )
@

For the name lookup, the start name of the range is added.  Rather
than try to look up both ends from the symbol table, further
verification is done using ranges.  In fact, there is no point in
storing the code point at all.

<<Built-in string table components>>=
charnames.unirng \
@

<<Generate built-in string table using [[mkstrs]]>>=
tr 'A-Z-' 'a-z ' < charnames.unirng | fgrep '^' | cut -d, -f1 | sort -u | \
  ./mkstrs $(UNI_NAME_WORD) -DUNI_RANGES -aUNI_RANGE \
           -h+strs-hash.gen -o+strs-strs.gen -d+builtin-strings.h \
           -l`fgrep UNI_CHARS_LAST builtin-strings.h | cut -d\  -f3`
@

The range table is generated using yet a different script, setting the
auxiliary value to the name from the symbol table.  To verify that a
symbol lookup was correct, just ensure that [[uni_range_of]]'s return
value matches the symbol lookup after extracting the code point.

<<Character types generation deps>>=
charnames.unirng \
@

\lstset{language=make}
<<makefile.rules>>=
chartypes.o: builtin-strings.h
@

<<Generate [[chartypes.gen]]>>=
# Unicode range reverse lookup
echo '#include "builtin-strings.h"' >>$@
echo "const uni_chrrng_dat_t unirng_range[] = {" >>$@
tr 'A-Z' 'a-z' < charnames.unirng | sed 's/[ -]/__/g;s/\^/_x5e/' | \
 while IFS=, read n cp; do \
   printf '\t{%s, ' $$cp; \
   IFS=, read ign cp; \
   echo "$$cp, UNI_RANGE_$${n}},"; \
done | sed '$$s/,$$//' >>$@
echo "};" >>$@
echo "const uint32_t unirng_range_len = sizeof(unirng_range)/sizeof(unirng_range[0]);" >>$@
@

\lstset{language=C}
<<Character type exports>>=
/* returns string # of low-end of range */
int uni_range_of(int cp);
@

<<Character type functions>>=
int range_ret(range, 0)
@

Now enough pieces are together to create some routines for character
name lookup.

<<Library [[uni]] headers>>=
#include "charnames.h"
@

<<charnames.h>>=
<<Common C Warning>>
#ifndef CHARNAMES_H
#define CHARNAMES_H

<<Character name lookup exports>>

#endif
@

<<Library [[uni]] Members>>=
charnames.o
@

<<charnames.c>>=
<<Common C Header>>
#include "charnames.h"

<<Character name lookup local definitions>>

<<Character name lookup functions>>
@

To find the code point given a name, we need to pass in a string and
its length, and expect to get a code point in return.  For the
multi-character sequences, we instead receive an array.  The length of
the array is returned instead of a code point; the existence of the
array distinguishes between the two.  If nothing is found, a negative
value is returned (since zero is a valid code point, and negative
numbers aren't).

<<Character name lookup exports>>=
/* returns < 0 if no match */
/* returns array in arr and length of arr if > 1 char */
/* otherwise returns cp and sets arr to NULL */
int cp_of_uni(const char *name, int len, const int **arr);
@

<<Character name lookup functions>>=
int cp_of_uni(const char *name, int len, const int **arr)
{
  <<Look up code point of Unicode character [[name]]/[[len]]>>
}
@

Both lookup functions use the number stored before the length in the
string table.

<<Character name lookup local definitions>>=
static int cp_for(uint32_t str)
{
    int llen;
    const char *cns = str_for(str, NULL);
    num_before(cns, &llen);
    return num_before(cns - llen, NULL);
}
@

<<Character name lookup local definitions>>=
#include "builtin-strings.h"
@

<<Character name lookup local definitions>>=
<<Character name generated structures>>
#include "charnames.gen"
@

\lstset{language=make}
<<makefile.rules>>=
charnames.gen: <<Character name data generation dependencies>>

	echo >$@
	<<Generate character name data>>
charnames.o: charnames.gen
@

Looking up Unicode names is much more difficult.  On a lookup failure,
we need to downcase the entire string and remove dashes and
underscores.  This requires allocating a buffer.  For now, this is
done on the stack.  To avoid stack overflow, no character names larger
than 128 characters are accepted (the actual maximum is around 83).

If the transformed name still can't be looked up, it needs to be
looked up as a range.

<<Look up code point of Unicode character [[name]]/[[len]]>>=
if(len > 128)
  return -1;
int cn = strno(name, len, 0);
if(cn < UNI_CHARS_FIRST || cn > UNI_CHARS_LAST) {
  char buf[len + 1];
  int i;
  for(i = 0; i < len; i++)
    buf[i] = name[i] == '_' || name[i] == '-' ? ' ' :
             isupper(name[i]) ? tolower(name[i]) : name[i];
  cn = strno(buf, len, 0);
  if(cn < UNI_CHARS_FIRST || cn > UNI_CHARS_LAST) {
    <<Look up code point of Unicode range element [[buf]]>>
  }
}
int cp = cp_for(cn);
@

To look up a range entry, the last word needs to be replaced by a
caret.  If that lookup fails, it isn't a range entry, so return $-1$.

<<Look up code point of Unicode range element [[buf]]>>=
char *s;
for(s = buf + len - 1; --s > buf; )
  if(*s == ' ')
    break;
if(s <= buf)
  return -1;
char c = *++s;
*s = '^';
cn = strno(buf, (int)(s - buf + 1), 0);
if(cn < UNI_RANGES_FIRST || cn > UNI_RANGES_LAST)
  return -1;
*s = c;
@

Then, that last word needs to be validated.  For all but the Hangul
syllables, the last word is a hexadecimal code point of at least four
characters with no superfluous leading zeroes.  There are no
multi-character sequences matching these names.  The table of ranges
returns the low end of the range, which is what we looked up, so if
they match, we're good to go.

<<Character name lookup local definitions>>=
#include "chartypes.h"
@

<<Look up code point of Unicode range element [[buf]]>>=
buf[len] = 0;
if(cn != UNI_RANGE_hangul__syllable___x5e) {
  if(len - (s - buf) < 4)
    return -1;
  if(len - (s - buf) > 4 && *s == '0')
    return -1;
  char *es;
  int cp = strtol(s, &es, 16);
  if(*es || uni_range_of(cp) != cn)
    return -1;
  *arr = NULL;
  return cp;
}
@

Hangul is more difficult, requiring character-by-character parsing.
It might be possible to come up with a regular expression to do the
match, but that would not help with returning the correct code point.
Instead, lookup tables are used for all three parts.  The tables are
filled from the abbreviations listed in Jamo.txt.

\lstset{language=make}
<<makefile.vars>>=
UNIJAMO = $(UCD_LOC)/Jamo.txt
@

<<Character name data generation dependencies>>=
$(UNIJAMO) \
@

<<Generate character name data>>=
for y in CHO JUNG JONG; do \
  echo "static const struct abent $${y}_dec[] = {"; \
  fgrep $${y}SEONG $(UNIJAMO) | cut -d\# -f1 | fgrep \; | \
    dd conv=lcase 2>/dev/null | cut -d\; -f2 | cat -n | while read a b; do \
      echo " {\"$$b\", $$((a-1))},"; \
    done | sort | sed '$$s/,$$//'; \
  echo "};"; \
done >>$@
@

\lstset{language=C}
<<Character name generated structures>>=
struct abent {
    const char *name;
    uint32_t cp;
};
@

<<Character name lookup local definitions>>=
#define NUM_CHO (sizeof(CHO_dec)/sizeof(CHO_dec[0]))
#define NUM_JUNG (sizeof(JUNG_dec)/sizeof(JUNG_dec[0]))
#define NUM_JONG (sizeof(JONG_dec)/sizeof(JONG_dec[0]))
@

The L portion is always either one character or one doubled character
or blank.  The blank can be detected by the fact that V shares no
characters with L.  In order to search the table quickly, the first
character, and, if doubled, the second cahracter are joined into a
string and searched using plain binary search.

<<Look up code point of Unicode range element [[buf]]>>=
/* L is 0 char or 1 char or 1 doubled char */
if(*s == s[1]) {
  c = s[2];
  s[2] = 0;
} else {
  c = s[1];
  s[1] = 0;
}
/* look up in table */
/* could use bsearch() but seems silly */
int l, h, m, cmp = 0;
l = 0; h = NUM_CHO - 1;

while(l <= h) {
  m = (l + h) / 2;
  cmp = strcmp(s, CHO_dec[m].name);
  if(cmp < 0)
    h = m - 1;
  else if(cmp > 0)
    l = m + 1;
  else
    break;
}
/* if not found, assume L is blank */
if(cmp)
  m = 0;
int L = CHO_dec[m].cp;
s += strlen(s);
*s = c;
@

Binary search fails too easily due to possible T presence, and the V
table is short with short strings, so the search for V is linear.

<<Look up code point of Unicode range element [[buf]]>>=
/* find first match */
int ll = 0;
for(m = 0; m < NUM_JUNG; m++) {
  ll = strlen(JUNG_dec[m].name);
  if(!strncmp(s, JUNG_dec[m].name, ll))
    break;
}
/* there is no blank V, so not found == error */
if(m == NUM_JUNG)
  return -1;
/* find longest match */
int m2;
for(m2 = m + 1; m2 < NUM_JUNG; m2++) {
  int ll2 = strlen(JUNG_dec[m2].name);
  if(ll2 <= ll || memcmp(JUNG_dec[m2].name, JUNG_dec[m].name, ll))
    break;
  if(!strncmp(s, JUNG_dec[m2].name, ll2)) {
    ll = ll2;
    m = m2;
  }
}
int V = JUNG_dec[m].cp;
s += strlen(JUNG_dec[m].name);
@

Finally, the remaining characters are a full string again, so binary
search is the easiest.

<<Look up code point of Unicode range element [[buf]]>>=
/* last part can be binary searched again, if present */
int T = 0;
if(*s) {
  l = 0; h = NUM_JONG - 1;
  while(l <= h) {
    m = (l + h) / 2;
    cmp = strcmp(s, JONG_dec[m].name);
    if(cmp < 0)
      h = m - 1;
    else if(cmp > 0)
      l = m + 1;
    else
      break;
  }
  /* unrecognized suffix is error */
  if(cmp)
    return -1;
  T = JONG_dec[m].cp + 1;
}
*arr = NULL;
return 0xAC00 + T + (NUM_JONG + 1) * (V + NUM_JUNG * L);
@

None of the ranged characters are multi-character, but any of the
others might be.  The sequences are loaded into an array.  The
sequence length can vary from 2--4, so table entry is zero-terminated.

<<Look up code point of Unicode character [[name]]/[[len]]>>=
if(cp < 0x110000) {
  *arr = NULL;
  return cp;
}
const int *ccp = &charseq[cp-0x110000][0];
int i;
for(i = 0; i < 4; i++)
  if(!ccp[i])
    break;
*arr = ccp;
return i;
@

\lstset{language=make}
<<Generate character name data>>=
# unicode character sequences
echo "const int charseq[][4] = {" >>$@
cut -d, -f2- < charnames.uniseq | sed 's/.*/{&},/;$$s/,$$//' >>$@
echo "};" >>$@
@

To do reverse lookups, the static string should be returned.  However,
for Unicode, we have to generate a name in case the name is
algorithmically generated.

\lstset{language=C}
<<Character name lookup exports>>=
/* always returns target length, even if len is too small */
int uni_gen_name_for(int cp, char *buf, int len);
@

<<Character name lookup functions>>=
int uni_gen_name_for(int cp, char *buf, int len)
{
  <<Return Unicode name of [[cp]]>>
}
@

<<Character name generated structures>>=
struct chrnm {
    uint32_t name, cp;
};
@

<<Character name lookup local definitions>>=
static int nmcmp(const void *a, const void *b)
{
    return ((struct chrnm *)a)->cp - ((struct chrnm *)b)->cp;
}
@

\lstset{language=make}
<<Character name data generation dependencies>>=
strs.gen \
@

Unicode is more difficult, of course, and not just because the name
needs to be copied into an output buffer.  The first part is simple,
and the same as for XML, though:  look up the code point in the name
table.  The table is only filled with entries which are not
automatically generated, so this might fail even if there is a good
name.  Like with XML, the table is generated from the string data.

\lstset{language=C}
<<Return Unicode name of [[cp]]>>=
uint32_t str;
struct chrnm cps = { 0, cp }, *res;

if((res = bsearch(&cps, uni_nmord, sizeof(uni_nmord)/sizeof(*uni_nmord), sizeof(*uni_nmord), nmcmp)))
  str = res->name;
else
  str = 0;
@

\lstset{language=make}
<<Generate character name data>>=
# cp -> string reverse lookup
echo 'static const struct chrnm uni_nmord[] = {' >>$@
sed -n -e '/UNI_CHARS_FIRST/,/UNI_CHARS_LAST/{s%.*/\* [0-9]*/\([0-9]*,[0-9]*\) .*\*/$$%  {\1},%;T;p;}' \
  strs-strs.gen | sort -t, -k 2n | sed '$$s/,$$//' >>$@
echo '};' >>$@
@

For strings that are found, we are not done, though.  Unless of course
no characters are actually to be copied out, since the length is
indeed accurate.

\lstset{language=C}
<<Return Unicode name of [[cp]]>>=
if(str) {
  int nlen;
  const char *s;

  s = str_for(str, &nlen);
  if(!len)
    return nlen;
  <<Post-process regular Unicode character name>>
  return nlen;
}
@

The first thing to do is zero-terminate the return string, as required
by the API.  At this point there is at least one character available
in the output buffer, so setting it blindly is OK.

<<Post-process regular Unicode character name>>=
if(len > nlen + 1)
  len = nlen + 1;
buf[--len] = 0;
@

Next, we copy out the characters, converting them to upper-case.  They
are stored lower-case in the symbol table because NFKC\_CF converts
strings to lower-case for comparison.

<<Post-process regular Unicode character name>>=
int i;
for(i = 0; i < len; i++, s++)
   buf[i] = islower(*s) ? toupper(*s) : *s;
@

The other transformation which was done for easy comparison is to
strip the dashes out.  To fix this, a table of dash locations is used.
This table stores the offsets into the name of up to three dashes; no
name was found with more than three.  The offset actually starts at one
so that the array of three can be zero-terminated.  This actually
requires a fourth byte as well.

% FIXME: use signed offset instead; sign == negative for dash, pos for
% space; also, remove cp and store offset into this table with name
% string, and expand d to 12 chars, and end loop at 12

<<Character name generated structures>>=
struct dashloc {
    int cp;
    unsigned char d[4];
};
@

<<Post-process regular Unicode character name>>=
const struct dashloc dl = {cp}, *dash;

dash = bsearch(&dl, dashloc, sizeof(dashloc)/sizeof(*dashloc), sizeof(*dashloc),
               cmpdl);
if(dash) {
  const unsigned char *d = dash->d;
  while(*d && *d <= len)
    buf[*d++ - 1] = '-';
}
@

<<Character name lookup local definitions>>=
static int cmpdl(const void *a, const void *b)
{
    return ((struct dashloc *)a)->cp - ((struct dashloc *)b)->cp;
}
@

To generate the array, some shell code is used to find dashes and
count characters.

\lstset{language=make}
<<Generate character name data>>=
# dash locations (stripped out for stored name)
echo "const struct dashloc dashloc[] = {" >>$@
cut -d\; -f1,2 $(UNIDATA) | fgrep -v 'CJK COMPATIBILITY IDEOGRAPH' | \
  fgrep -- - | while IFS=\; read cp x; do \
    d="$${x%%-*}"; \
    n=$$(($${#d}+1)); \
    ns=$$n; \
    while :; do \
      x="$${x#*-}"; \
      case "$$x" in \
        *-*) d="$${x%%-*}"; \
	     n=$$((n+$${#d}+1)); \
             ns=$$ns,$$n ;; \
          *) break ;; \
      esac; \
    done; \
    echo "    {0x$$cp,{$$ns}},"; \
  done | sed '$$s/,$$//' >>$@
echo "};" >>$@
@

Once again, the ranges add more effort.  At least if it is not one of
the range code points, we can return immediately.  Otherwise, we may
as well copy out as much of the name as we already have.  There are no
dashes in any of these names, except just before the hexadecimal code
point on the ones which have this.

\lstset{language=C}
<<Return Unicode name of [[cp]]>>=
int low = uni_range_of(cp);
if(!low)
  return 0;
int nlen;
const char *s;
s = str_for(low, &nlen);
int fulllen = nlen - 1; /* remove the ^ */
if(len > 1) {
  /* back up to last space */
  if(--nlen > len - 1)
    nlen = len - 1;
  else
    --nlen;
  len -= nlen;
  while(--nlen >= 0) {
    if(islower(*s))
      *buf++ = toupper(*s++);
    else
      *buf++ = *s++;
  }
}
@

Speaking of which, those now require appending the hexadecimal code
point, and a dash.  Precomputing the length is pretty easy, since
there are either 4, 5, or 6 digits depending on the code point range.
That means we can finally return a value if no output buffer was
given.  Otherwise, we write the suffix, followed by a 0, and return
the computed length.

<<Return Unicode name of [[cp]]>>=
if(low != UNI_RANGE_hangul__syllable___x5e) {
  fulllen += 4;
  if(cp > 0xffff)
    fulllen++;
  if(cp > 0xfffff)
    fulllen++;
  if(!len)
    return fulllen;
  if(len > 1) {
    *buf++ = '-';
    --len;
  }
  if(--len)
    snprintf(buf, len + 1, "%04X", cp);
  else
    *buf = 0;
  return fulllen;
}
@

For Hangul, the computation of the letters to append is pretty simple,
but the table already generated does not have the letters in the right
order, so they are added again, but this time in all caps and in the
right order, and without any additional data.

<<Return Unicode name of [[cp]]>>=
int t = (cp - 0xac00) % 28;
int v = (cp - 0xac00) / 28;
int l = v / 21;
v %= 21;
const char *ls = CHO_abbrev[l];
const char *vs = JUNG_abbrev[v];
const char *ts = t ? JONG_abbrev[t - 1] : "";
@

\lstset{language=make}
<<Generate character name data>>=
# hangul syllable lookup table
for y in CHO JUNG JONG; do \
  echo "static const char *$${y}_abbrev[] = {"; \
  fgrep $${y}SEONG $(UNIJAMO) | cut -d\# -f1 | fgrep \; | \
    while read a b; do \
      echo " \"$$b\","; \
    done | sed '$$s/,$$//'; \
  echo "};"; \
done >>$@
@

Having the letters makes computation of the full length trivial.

\lstset{language=C}
<<Return Unicode name of [[cp]]>>=
int ll = strlen(ls);
int vl = strlen(vs);
int tl = strlen(ts);
fulllen += ll + vl + tl;
if(!len)
  return fulllen;
@

Now to tack on what fits:  a space, followed by the L abbreviation,
followed by the V abbreviation, followed by the T abbreviation.  Then
we can return the computed length and it's over.

<<Return Unicode name of [[cp]]>>=
if(len > 1) {
  *buf++ = ' ';
  --len;
}
/* append L */
if(ll > len)
  ll = len;
len -= ll;
while(--ll >= 0)
  *buf++ = *ls++;
/* append V */
if(vl > len)
  vl = len;
len -= vl;
while(--vl >= 0)
  *buf++ = *vs++;
/* append T */
if(tl > len)
  tl = len;
while(--tl >= 0)
  *buf++ = *ts++;
/* terminate and return */
*buf = 0;
return fulllen;
@

\section{Testing}

Here is a master driver to call some of the above code for testing.

<<C Test Support Executables>>=
tst \
@

<<tst.c>>=
<<Common C Header>>
#include "unistuff.h"

@

For I/O, I'll just use a decent size fixed buffer, and in fact use it
as both a UCS-32 buffer and a character buffer, allowing both at the
same time by marking the divider with [[blen]].

<<tst.c>>=
#define BUF_SIZE 1024
int buf[BUF_SIZE];
int blen;

#define cbuf ((char *)(buf+blen))
#define cblen ((BUF_SIZE - blen) * sizeof(int))
@

The purpose of this program is to take Unicode characters as input,
and do things with them.

<<tst.c>>=
int main(int argc, const char **argv)
{
  <<Get and show characters for [[tst]]>>
  <<Manipulate characters for [[tst]]>>
  return 0;
}
@

For this program, input means command line arguments.  Characters are
specified by either a hexadecimal code point or a Unicode character
name.  They are all appended to [[buf]].

<<Get and show characters for [[tst]]>>=
int cp;

while(--argc > 0) {
  ++argv;
  int l = strlen(*argv);
  const int *mcp;
  cp = cp_of_uni(*argv, l, &mcp);
  if(cp >= 0) {
    /* valid name */
    if(!mcp)
      buf[blen++] = cp;
    else {
      int i;
      for(i = 0; i < cp; i++)
        buf[blen++] = mcp[i];
      cp = mcp[0];
    }
  } else {
    const char *s;
    cp = strtol(*argv, (char **)&s, 16);
    if(!*s) {
      /* valid hex code point */
      buf[blen++] = cp;
    } else {
      /* not valid anything */
      fprintf(stderr, "Can't parse argument %s\n", *argv);
      exit(1);
    }
  }
}
@

Next, we'll print some more information about each character.  This
includes characters artificially added by multi-character sequences.

<<Get and show characters for [[tst]]>>=
int i;
for(i = 0; i < blen; i++) {
  cp = buf[i];
  <<Show character for [[tst]]>>
}
@

First, I'd like to see the Unicode names, as well as the hexadecimal
code points.

<<Show character for [[tst]]>>=
printf("Character %04X\n", cp);
if(uni_gen_name_for(cp, cbuf, cblen))
  printf("Unicode: %s\n", cbuf);
@

Then, maybe some of the character classifications.  These are all
printed out by the [[chartypes]] mainline, but there is no harm in
repeating it.

<<Show character for [[tst]]>>=
fputs("Character class: ", stdout);
#define i cp
<<Print flags and classes of cp [[i]]>>
#undef i
putchar('\n');
@

Next, we'll try some normalization.  But first, we'll print the entire
buffer out before starting, and afer each step.

<<Manipulate characters for [[tst]]>>=
#define prbuf(s) do { \
  fputs(s":", stdout); \
  for(i = 0; i < blen; i++) \
    printf(" %04X", buf[i]); \
  putchar('\n'); \
  putchar('\''); \
  utf8_fputs(buf, blen, stdout); \
  putchar('\''); \
  putchar('\n'); \
} while(0)
prbuf("Start string");
@

Of course these are all destructive.  Another test program should
probably do normalization on copies so that the original can be reused
for more tests.

<<Manipulate characters for [[tst]]>>=
blen = NFD_dec(buf, blen);
prbuf("After decomp");
Canon_Order(buf, blen, 1);
prbuf("After canon-order (NFD)");
blen = NFC_comp(buf, blen, NULL);
prbuf("After canon-comp (NFC)");
blen = NFKD_dec(buf, blen);
prbuf("After compat-decomp");
Canon_Order(buf, blen, 1);
prbuf("After canon-order (NFKD)");
blen = NFC_comp(buf, blen, NULL);
prbuf("After canon-comp (NFKC)");
blen = NFKC_Casefold(buf, blen);
prbuf("After case folding (NFKC_CF)");
@

Since the normalization test above is inadequate, here is the full
normalization test suite from the UCD.

\lstset{language=make}
<<C Test Support Executables>>=
tstnorm \
@

<<makefile.vars>>=
UNINORM_TEST = $(UCD_LOC)/NormalizationTest.txt
@

<<Additional Tests>>=
./tstnorm <$(UNINORM_TEST)
@

\lstset{language=C}
<<tstnorm.c>>=
<<Common C Header>>
#include "unistuff.h"

/* longest line == 587 chars */
char lbuf[1024];

/* longest string == 18 chars */
int ibuf[5][128], ilen[5];
int obuf[128];

int main(void)
{
  int ret = 0;
  <<Read and process NormalizationTest.txt>>
  return ret;
}
@

First, we need to track when we're in part 1, and mark every code
point encountered there as having been processed.  According to the
test, after exiting part 1, we can run all four standard
normalizations on any code point not encountered and get no effect.

In general, when entering a new section, print the section name.
Otherwise, print a dot after every 50 tests.

<<Read and process NormalizationTest.txt>>=
char *didnorm;
inisize(didnorm, 0x110000);
clearbuf(didnorm, 0x110000);
int inpart1 = 0;
int ntests = 0;
while(fgets(lbuf, sizeof(lbuf), stdin)) {
  if(*lbuf == '#')
    continue;
  if(*lbuf == '@') {
    if(ntests)
      putchar('\n');
    if(lbuf[5] == '1')
      inpart1 = 1;
    else if(inpart1) {
      inpart1 = 0;
      int i;
      for(i = 0; i < 0x110000; i++)
        if(!didnorm[i]) {
	  <<Test normalization does not affect [[i]]>>
	}
    }
    fputs(lbuf, stdout);
    continue;
  }
  if(inpart1) {
    int cp = strtol(lbuf, NULL, 16);
    didnorm[cp] = 1;
  }
  <<Run normalization test for [[lbuf]]>>
  if(!(++ntests % 50)) {
    putchar('.');
    fflush(stdout);
  }
}
putchar('\n');
@

First, let's parse a line into the input buffers.  Each line has five
semicolon-separated fields, terminated by a semicolon.  Each field has
space-separated hexadecimal numbers.

<<Run normalization test for [[lbuf]]>>=
int i;
char *s = lbuf;
for(i = 0; i < 5; i++) {
  int l = 0;
  while(1) {
    ibuf[i][l++] = strtol(s, &s, 16);
    while(isspace(*s))
      s++;
    if(*s == ';')
      break;
  }
  ilen[i] = l;
  if(*s == ';')
    s++;
  else {
    fprintf(stderr, "bad line %s\n", lbuf);
    exit(1);
  }
}
@

Then, run the conformance tests.

<<Run normalization test for [[lbuf]]>>=
#define dotst(t, i0, il, r) do { \
  for(i = i0; i < il; i++) { \
    memcpy(obuf, ibuf[i], ilen[i] * sizeof(obuf[0])); \
    int l = t(obuf, ilen[i]); \
    if(l != ilen[r] || memcmp(obuf, ibuf[r], l * sizeof(obuf[0]))) { \
      ret = 1; \
      fprintf(stderr, "Failed "#t" test %d/%d on %s\nGot: ", r + 1, i + 1, lbuf); \
      int j; \
      for(j = 0; j < l; j++) \
        fprintf(stderr, " %04X", obuf[j]); \
      fputs("\nExpected: ", stderr); \
      for(j = 0; j < ilen[r]; j++) \
        fprintf(stderr, " %04X", ibuf[r][j]); \
      fputc('\n', stderr); \
      continue; \
    } \
  } \
} while(0)
@

<<Run normalization test for [[lbuf]]>>=
dotst(NFC, 0, 3, 1);
dotst(NFC, 3, 5, 3);
@

<<Run normalization test for [[lbuf]]>>=
dotst(NFD, 0, 3, 2);
dotst(NFD, 3, 5, 4);
@

<<Run normalization test for [[lbuf]]>>=
dotst(NFKC, 0, 5, 3);
@

<<Run normalization test for [[lbuf]]>>=
dotst(NFKD, 0, 5, 4);
@

<<Test normalization does not affect [[i]]>>=
obuf[0] = i;
if(NFC(obuf, 1) != 1 || obuf[0] != i ||
   NFD(obuf, 1) != 1 || obuf[0] != i ||
   NFKC(obuf, 1) != 1 || obuf[0] != i ||
   NFKD(obuf, 1) != 1 || obuf[0] != i) {
  fprintf(stderr, "Failed Part1 end check at %04x\n", i);
  ret = 1;
}
@

As a more thorough character name test, we can run every single known
character through the [[tst]] program.  This does not guarantee that
the lists themselves are OK, or that bad names will not trigger false
positives, but it's better than nothing.

<<Test Scripts>>=
tstchars \
@

\lstset{language=sh}
<<tstchars>>=
#!/bin/sh
<<Common NoWeb Warning>>
@

First, we'll iterate through all of the Unicode names mapping to one
character, both forward and reverse.

<<tstchars>>=
cat charnames.{asc,uni} | while IFS=, read a b; do
  b=$(($b))
  hb=`printf %04X $b`
  tr=$(./tst "$a" | head -n 1)
  tr="${tr#* }"
  test $hb = "$tr" || echo "$a,$b $hb"
  tr="`./tst $hb | grep -a '^Unicode:'`"
  tr="${tr#* }"
  test "$a" = "$tr" || echo "$hb $a $tr"
done
@

Next, all of the multi-character Unicoded characters.  They can only
be tested for name interpretation, since reverse lookups on
multi-character sequences are impossible.

<<tstchars>>=
cat charnames.uniseq | while IFS=, read a b; do
  tr=
  for x in $(./tst "$a" | grep -a '^Character [0-9A-F]' | cut -d\  -f2); do
    tr=$tr,$((0x$x))
  done
  test ,$b = "$tr" || echo "$a,$b $tr"
done
@

Another useful check is to ensure that the string table's strings are
all actually unique.  This is done by traversing each hash bucket to
see if there are any duplicates.

<<makefile.rules>>=
# explicit strs.gen dep should not be necessary, but gmake flakes on it
hashcheck: strs.o strs.gen
	$(CC) -o $@ $(CFLAGS) $(LDFLAGS) $(EXTRA_CFLAGS) -DHASHCHECK strs.c
@

<<C Test Executables>>=
hashcheck \
@

\lstset{language=C}
<<strs.c>>=
#ifdef HASHCHECK
int main(void)
{
    int i;
    int err = 0;
    for(i = 0; i < HTAB_SIZE; i++) {
        if(!strhash[i])
            continue;
        int j, k;
        for(j = strhash[i]; builtin_hashe[j - 1].nextent; j = builtin_hashe[j - 1].nextent) {
            const char *js = builtin_hashe[j - 1].off + builtin_str;
            int jl = num_before(js, NULL);
            for(k = builtin_hashe[j - 1].nextent; k; k = builtin_hashe[k - 1].nextent) {
                const char *ks = builtin_hashe[k - 1].off + builtin_str;
                if(num_before(ks, NULL) == jl && !memcmp(js, ks, jl)) {
                    fprintf(stderr, "duplicate string %d %d %.*s\n",
                            builtin_hashe[j - 1].off, builtin_hashe[k - 1].off,
                            jl, js);
                    err = 1;
                }
            }
        }
    }
    printf("%lu string space; %lu hash entries\n",
           (unsigned long)builtin_str_size,
           (unsigned long)builtin_hashe_size);
    return err;
}
#endif
@

\chapter{Unorganized crap}
\lstset{language=sh}
\section{mkhang}
<<mkhang>>=
#!/bin/sh

# 28 T
T="`fgrep JONGSEONG \"\$1\" | cut -d\\# -f1 | cut -d\\; -f2 | sed 's/^ */:/'`"
# 21 V
V="`fgrep JUNGSEONG \"\$1\" | cut -d\\# -f1 | cut -d\\; -f2 | sed 's/^ */:/'`"
vskip=28
# 19 L
L="`fgrep CHOSEONG \"\$1\" | cut -d\\# -f1 | cut -d\\; -f2 | sed 's/^ */:/'`"
lskip=$((21*28))

cp=$((0xac00))
for l in $L; do
  for v in $V; do
    lv="${l#:}${v#:}"
    echo "HANGUL SYLLABLE $lv,$cp"
    cp=$((cp+1))
    for t in $T; do
      echo "HANGUL SYLLABLE $lv${t#:},$cp"
      cp=$((cp+1))
    done
  done
done
@

\lstset{language=C}
\section{tok.c}
<<tok.c>>=
<<Common C Header>>
#include "unistuff.h"
#include "tok.h"

enum escstate {
    ES_NORM, ES_BS_NORM, ES_NORM_CBESC, ES_STR, ES_RAW_STR, ES_BS_STR,
    ES_STR_CBESC, ES_ML_START, ES_ML_RAW_START, ES_ML_STR_TOK,
    ES_ML_RAW_STR_TOK, ES_ML_STR, ES_ML_RAW_STR, ES_BS_ML_STR,
    ES_ML_STR_CBESC
};

void pr_loc(struct loc *l)
{
    fprintf(stderr, "%u,%u (%lu): ", l->lineno, l->colno, l->fpos);
}

typedef struct {
    unifile *next;
    FILE *f;
    struct loc loc;
#define DEC_LEN 512
    int decbuf[DEC_LEN];
    struct loc decloc[DEC_LEN];
    int decpos, declen;
    int ign_nl;
} unifile_t;
unifile_t *open_files;

static void tokb_append(int c, int **buf, uint32_t *len,
                        uint32_t *maxlen)
{
    if(!*maxlen) {
        *maxlen = 128;
        inisize(*buf, *maxlen);
    } else if(*len == *maxlen) {
        *maxlen *= 2;
        resize(*buf, *maxlen);
    }
    (*buf)[(*len)++] = c;
}

int buf_to_utf8(int *buf, int len)
{
    int i;
    char *s = (char *)buf;

    for(i = 0; i < len; i++)
        s += utf8_encode(s, buf[i]);
    return (int)(s - (char *)buf);
}

/* raw token text */
int *curtok;
uint32_t toklen = 0;
static uint32_t maxtok = 0;
#define tok_append(c) tokb_append(c, &curtok, &toklen, &maxtok)
struct loc tokloc;
static int lenesc = 0;

/* token processing state */
enum {
    TS_NONE, TS_ID, TS_ID_SEP, TS_NUM, TS_NUM_BASE, TS_NUM_DEC, TS_NUM_E,
    TS_NUM_ES, TS_NUM_EXP, TS_STRING, TS_RSTRING
} tokst;

/* processed token text */
uint32_t *numval;
uint32_t numlen;
uint32_t *nexp;
uint32_t explen;
static uint32_t maxexp = 0, maxnum = 0;
int nbase; /* negative for negative exponent */
int isflt; /* needed because positive exponent could still have dp */

static int *mltok;
static uint32_t mltokmax = 0, mltoklen = 0;
static int mltokmatch = 0;
#define mltok_append(c) tokb_append(c, &mltok, &mltoklen, &mltokmax)

static struct loc bsloc;
static int *bsbuf = NULL;
static uint32_t bslen = 0, nbs = 0;
#define bsbuf_append(c) tokb_append(c, &bsbuf, &nbs, &bslen)

static int add_decompc(unifile_t *uf, uni_normtype_t dect, int c,
                       struct loc fpos)
{
    /* decompose into output buffer */
    const uni_decent_t *de;
    switch(dect) {
      case UN_NFC:
        de = find_decomp(c, 1);
        break;
      case UN_NFKC:
        de = find_decomp(c, 0);
        break;
      case UN_NFKC_CF:
        de = find_nfkc_cf(c);
        break;
      case UN_NONE:
        de = NULL;
        break;
      default:
        fprintf(stderr, "??? norm type\n");
        exit(1);
    }
    int opos = (uf->decpos + uf->declen)%DEC_LEN;
    int l = !de ? 1 : de->len > 0 ? de->len : -de->len;
    /* dec buf should never overflow */
    if(uf->declen + l > DEC_LEN) {
        fprintf(stderr, "Incomprehensible Unicode at %lu\n",
                uf->loc.fpos - 1);
        return 0;
    }
    if(!de)
        uf->decbuf[opos] = c;
    else if(!l)
        return 1;
    else {
        if(opos + l > DEC_LEN) {
            memcpy(uf->decbuf + opos, deccp + de->off, DEC_LEN - opos);
            memcpy(uf->decbuf, deccp + de->off + DEC_LEN - opos,
                   l - (DEC_LEN - opos));
        } else
            memcpy(uf->decbuf + opos, deccp + de->off, l);
    }
    uf->declen += l;
    while(--l >= 0)
        uf->decloc[(l + opos)%DEC_LEN] = fpos;
    return 1;
}

static int readc(unifile_t *uf, uni_normtype_t dect)
{
    int c;
    int nread;

    while((c = utf8_getc(uf->f, &nread)) != -1) {
        if(c < 0) {
            c = 0xfffd;
            pr_loc(&uf->loc);
            fprintf(stderr, "Invalid UTF8 code seqence%s\n",
                    c == -2 ? " (premature EOF)" : "");
            if(c == -2)
                break;
            continue;
        }
        if(uf->ign_nl) {
            int ign_nl = uf->ign_nl;
            uf->ign_nl = 0;
            if(c == ign_nl)
                continue;
        }
        if(is_nl(c)) {
            if(c == '\n')
                uf->ign_nl = '\r';
            else if(c == '\r')
                uf->ign_nl = '\n';
            uf->loc.lineno++;
            uf->loc.colno = 0;
        } else
            uf->loc.colno++;
        c = add_decompc(uf, dect, c, uf->loc);
        uf->loc.fpos += nread;
        return c;
    }
    return 0;
}

static int process_tok(void)
{
    switch(tokst) {
      case TS_STRING: {
          /* convert \l to 0 to avoid canonicalization */
          /* save original 0s first, of course */
          int nlen, n, nz = 0;
          if(lenesc)
              for(n = 0; n < toklen; n++)
                  if(!curtok[n])
                      nz++;
          for(nlen = 0, n = lenesc; n; nlen++, n = curtok[n - 1]);
          int savepos[nlen + nz];
          if(lenesc) {
              for(n = 0, nz = 0; n < toklen; n++)
                  if(!curtok[n])
                      savepos[nz++] = n;
              for(n = lenesc, nlen = 0; n; nlen++) {
                  int pn = n - 1;
                  savepos[nlen + nz] = pn;
                  n = curtok[pn];
                  curtok[pn] = 0;
              }
              /* can't save actual locations since they may change */
              /* instead, save order in which they appear */
              int zp = 0, lp = nz;
              for(n = 0; n < nlen + zp - 1; n++) {
                  if(zp < nz && (lp - nz < nlen || savepos[zp] < savepos[lp]))
                      savepos[zp++] = n;
                  else if(zp < nz && savepos[zp] == savepos[lp]) {
                      savepos[zp++] = savepos[lp++] = n;
                  } else
                      savepos[lp++] = n;
              }
          }
          Canon_Order(curtok, toklen, 1);
          toklen = NFC_comp(curtok, toklen, NULL);
          if(lenesc) {
              int zp = 0, lp = nz, i;
              for(i = 0; i < toklen; i++) {
                  if(!curtok[i]) {
                      if(zp < nz && (lp - nz >= nlen || savepos[zp] < savepos[lp]))
                          zp++;
                      else if(zp < nz && savepos[zp] == savepos[lp]) {
                          zp++;
                          lp++;
                          curtok[i] = toklen - 1;
                      } else {
                          lp++;
                          curtok[i] = toklen - 1;
                      }
                  }
              }
          }
          return TOK_STRING;
      }
      case TS_RSTRING:
        Canon_Order(curtok, toklen, 1);
        toklen = NFC_comp(curtok, toklen, NULL);
        return TOK_STRING;
      default:
        return TOK_EOF;
    }
}

int gettok(unifile_t *uf)
{
    enum escstate escst = ES_NORM;
    int ebase = 0;

    uni_normtype_t nt = UN_NFKC_CF;
    int c;
    struct loc *loc;
#define readchar(nt) while(!uf->declen && readc(uf, nt))
#define nextchar() do { \
    c = uf->decbuf[uf->decpos]; \
    loc = &uf->decloc[uf->decpos]; \
    if(++uf->decpos == DEC_LEN) \
        uf->decpos = 0; \
    --uf->declen; \
} while(0)
#define backup() do { \
    if(!uf->decpos) \
        uf->decpos = DEC_LEN - 1; \
    else \
        --uf->decpos; \
    ++uf->declen; \
} while(0)
#define re_decomp(nt) do { \
    int l = uf->declen, i; \
    for(i = 0; i < l; i++) { \
        nextchar(); \
        tok_append(c); \
    } \
    toklen -= l; \
    for(i = 0; i < l; i++) \
        add_decompc(uf, nt, curtok[toklen + i], *loc); \
} while(0)

    while(1) {
        /* canonical ordering only affects final string values and */
        /* identifiers (i.e., they are all CC_WORDSEP) */
        /* both ordering and comp only affect CC_WORDSEP chars */
        /* comp can affect CC_LETTER, though, turning it to CC_ID */
        /* comp only affects CC_WORDSEP *after* CC_ID, CC_LETTER, CC_OPMID */
        readchar(nt);
        if(!uf->declen)
            break;
        nextchar();
        /* process backslashes and strings */
        /* also switch (de)normalization as needed */
        switch(escst) {
          case ES_NORM:
            if(c == '\\') {
                bsloc = *loc;
                escst = ES_BS_NORM;
            } else if(c == '"') {
                if(toklen > 0) {
                    backup();
                    return process_tok();
                }
                escst = ES_STR;
                nt = UN_NFC;
                tokloc = *loc;
            }
            break;
          case ES_STR:
            if(is_bs(c)) {
                escst = ES_BS_STR;
                nt = UN_NFKC_CF;
                break;
            }
            /* fall through */
          case ES_RAW_STR:
            if(c == '"') {
                readchar(nt);
                if(uf->declen > 0) {
                    nextchar();
                    if(c != '"') {
                        nt = UN_NFKC_CF;
                        /* may have split more, so can't just reinsert */
                        backup();
                        re_decomp(nt);
                        tokst = escst == ES_STR ? TS_STRING : TS_RSTRING;
                        escst = ES_NORM;
                        return process_tok();
                    }
                }
            } else if(is_nl(c)) {
                pr_loc(&tokloc);
                fputs("unterminated string detected (eol)\n", stderr);
                escst = ES_NORM;
                tokst = TS_STRING;
                return process_tok();
            }
            tok_append(c);
            break;
          case ES_BS_NORM:
            if(c == 'm') {
                escst = ES_ML_START;
                tokloc = bsloc;
                break;
            }
            if(c == '"') {
                escst = ES_RAW_STR;
                tokloc = bsloc;
                nt = UN_NFC;
                break;
            }
            if(c != '{') {
                pr_loc(&bsloc);
                fputs("bad bs escape (expected {)\n", stderr);
                escst = ES_NORM;
                continue;
            } else {
                escst = ES_NORM_CBESC;
                nt = UN_NFKC; /* for XML escapes, need to preserve case */
                nbs = 0;
            }
            break;
          case ES_BS_STR:
          case ES_BS_ML_STR:
            switch(c) {
              case 'a':
                tok_append(7);
                break;
              case 'b':
                tok_append(8);
                break;
              case 't':
                tok_append(9);
                break;
              case 'n':
                tok_append(10);
                break;
              case 'v':
                tok_append(11);
                break;
              case 'f':
                tok_append(12);
                break;
              case 'r':
                tok_append(13);
                break;
              case 'e':
                tok_append(27);
                break;
              case '"':
                tok_append('"');
                break;
              case '\\':
                tok_append('\\');
                break;
              case 'o':
                ebase = 8;
                break;
              case 'x':
                ebase = 16;
                break;
              case 'd':
                ebase = 10;
                break;
              case 'l':
                tok_append(lenesc);
                lenesc = toklen;
                break;
              case '{':
                escst = escst == ES_BS_STR ? ES_STR_CBESC : ES_ML_STR_CBESC;
                nbs = 0;
                nt = UN_NFKC;
                break;
            }
            if(ebase) {
                int nc = 0;
                readchar(nt);
                if(!uf->declen)
                    break;
                nextchar();
                if(c == '{') {
                    while(1) {
                        readchar(nt);
                        if(!uf->declen)
                            break;
                        nextchar();
                        if(c == '}')
                            break;
                        if(c >= '0' && c <= '9')
                            c -= '0';
                        else if(c >= 'a' && c <= 'f')
                            c -= 'a' - 10;
                        else
                            c = 16;
                        if(c >= ebase) {
                            pr_loc(loc);
                            fprintf(stderr, "Bad bs escape; expected digit\n");
                            backup();
                            escst = escst == ES_BS_STR ? ES_STR : ES_ML_STR;
                            nt = UN_NFC;
                            break;
                        }
                        /* FIXME: error if overflow */
                        nc = nc * ebase + c;
                    }
                } else {
                    nt = UN_NFC;
                    if(c >= '0' && c <= '9')
                        c -= '0';
                    else if(c >= 'a' && c <= 'f')
                        c -= 'a' - 10;
                    else
                        c = 16;
                    if(c >= ebase) {
                        pr_loc(loc);
                        fprintf(stderr, "Bad bs escape; expected digit\n");
                        backup();
                        escst = escst == ES_BS_STR ? ES_STR : ES_ML_STR;
                        break;
                    }
                    nc = nc * ebase + c;
                    /* process characters already there due to decomp */
                    while(uf->declen > 0) {
                        if(c >= '0' && c <= '9')
                            c -= '0';
                        else if(c >= 'a' && c <= 'f')
                            c -= 'a' - 10;
                        else
                            c = 16;
                        if(c >= ebase) {
                            backup();
                            break;
                        }
                        /* FIXME: error if overflow (highly unlikely) */
                        nc = nc * ebase + c;
                    }
                    /* pull in more chars if possible */
                    if(!uf->declen) {
                        while(1) {
                            readchar(nt);
                            nextchar();
                            if(c >= '0' && c <= '9')
                                c -= '0';
                            else if(c >= 'a' && c <= 'f')
                                c -= 'a' - 10;
                            else if(c >= 'A' && c <= 'F')
                                c -= 'A' - 10;
                            else
                                c = 16;
                            if(c >= ebase) {
                                backup();
                                break;
                            }
                            /* FIXME: error if overflow */
                            nc = nc * ebase + c;
                        }
                    }
                }
                tok_append(nc);
            }
            if(escst == ES_BS_STR) {
                escst = ES_STR;
                nt = UN_NFC;
            } else if(escst == ES_BS_ML_STR) {
                escst = ES_ML_STR;
                nt = UN_NFC;
            }
            break;
          case ES_NORM_CBESC:
          case ES_STR_CBESC:
          case ES_ML_STR_CBESC:
            if(is_nl(c)) {
                escst = escst == ES_NORM_CBESC ? ES_NORM : escst == ES_STR_CBESC ? ES_STR : ES_ML_STR;
                pr_loc(&bsloc);
                nbs = buf_to_utf8(bsbuf, nbs);
                fprintf(stderr, "bad bs escape (end of line detected): \\{%.*s\n",
                        nbs, (char *)bsbuf);
            }
            if(c == '}') {
                escst = escst == ES_NORM_CBESC ? ES_NORM : escst == ES_STR_CBESC ? ES_STR : ES_ML_STR;
                if(escst == ES_STR || escst == ES_ML_STR)
                    nt = UN_NFC;
                if(!nbs) {
                    pr_loc(&bsloc);
                    fputs("bad bs escape (empty)\n", stderr);
                    continue;
                }
                if(nbs > 4 && (bsbuf[0] == 'x' || bsbuf[0] == 'X')) {
                    int i;
                    for(i = 1; i < nbs; i++)
                        if(!isxdigit(bsbuf[i]))
                            break;
                    if(i == nbs) {
                        c = 0;
                        for(i = 1; i < nbs; i++) {
                            c <<= 4;
                            if(isdigit(bsbuf[i]))
                                c += bsbuf[i] - '0';
                            else
                                c += (bsbuf[i] & 0xf) + 9;
                        }
                        if(c > 0x10ffff) {
                            pr_loc(&bsloc);
                            nbs = buf_to_utf8(bsbuf, nbs);
                            fprintf(stderr, "No such unicode code point: \\{%.*s}\n",
                                    nbs, (char *)bsbuf);
                        } else {
                            char uname[128];
                            int ul = uni_gen_name_for(c, uname, sizeof(uname));
                            if(ul) {
				nbs = buf_to_utf8(bsbuf, nbs);
				pr_loc(&bsloc);
				fprintf(stderr, "warning: \\{%s} is more informative than \\{%.*s}\n",
				        uname, nbs, (char *)bsbuf);
                            }
                        }
                        break;
                    }
                }
                nbs = buf_to_utf8(bsbuf, nbs);
                const int *mcp;
		c = cp_of_uni((char *)bsbuf, nbs, &mcp);
                if(c < 0) {
                    pr_loc(&bsloc);
                    nbs = buf_to_utf8(bsbuf, nbs);
                    fprintf(stderr, "Unknown backslash-escape \\{%.*s}\n",
                            nbs, (char *)bsbuf);
                    continue;
                }
                if(mcp) {
                    int i;
                    for(i = 0; i < c; i++)
                        add_decompc(uf, nt, mcp[i], bsloc);
                } else
                    add_decompc(uf, nt, c, bsloc);
                if(escst == ES_STR || escst == ES_ML_STR) {
                    while(uf->declen) {
                        nextchar();
                        tok_append(c);
                    }
                }
                continue;
            }
            bsbuf_append(c);
            break;
          case ES_ML_START:
            if(c == '\\')
                escst = ES_ML_RAW_START;
            else if(c == '"') {
                escst = ES_ML_STR_TOK;
                mltoklen = 0;
            } else {
                pr_loc(&bsloc);
                fprintf(stderr, "Invalid multi-line string; expected \"\n");
                escst = ES_NORM;
            }
            break;
          case ES_ML_RAW_START:
            if(c == '"') {
                escst = ES_ML_RAW_STR_TOK;
                mltoklen = 0;
            } else {
                pr_loc(&bsloc);
                fprintf(stderr, "Invalid multi-line string; expected \"\n");
                escst = ES_NORM;
            }
          case ES_ML_STR_TOK:
          case ES_ML_RAW_STR_TOK:
            if(comp_chartype_of(c) == CC_SPACE) {
                if(!mltoklen) {
                    pr_loc(&bsloc);
                    fprintf(stderr, "Invalid multi-line end token\n");
                    /* no recovery */
                    exit(1);
                }
                if(is_nl(c)) {
                    if(mltok[mltoklen - 1] == ' ')
                        --mltoklen;
                    mltok_append(0);
                    escst = escst == ES_ML_STR_TOK ? ES_ML_STR : ES_ML_RAW_STR;
                    nt = escst == ES_ML_STR ? UN_NFC : UN_NONE;
                    mltokmatch = 0;
                } else if(mltok[mltoklen - 1] != ' ')
                        mltok_append(' ');
            } else if(mltoklen > 0 && mltok[mltoklen - 1] == ' ') {
                pr_loc(&bsloc);
                fprintf(stderr, "Invalid multi-line end token\n");
                /* no recovery */
                exit(1);
            } else
                mltok_append(c);
            break;
          case ES_ML_STR:
          case ES_ML_RAW_STR:
            /* FIXME: ES_ML_STR ignores initial WS unless backslash-cr prev. */
            /* FIXME: ES_ML_STR supports backslash-escapes */
            if(mltokmatch >= 0) {
                if(!mltokmatch && escst == ES_ML_STR &&
                   comp_chartype_of(c) == CC_SPACE)
                    continue;
                if((mltokmatch == mltoklen && c == '"')) {
                    if(escst == ES_ML_STR) {
                        escst = ES_NORM;
                        nt = UN_NFKC_CF;
                        continue;
                    } else {
                        mltokmatch++;
                        break;
                    }
                }
                if(mltokmatch > mltoklen) { /* only if end " of raw str */
                    if(comp_chartype_of(c) != CC_SPACE) {
                        /* place contents of mltok back into string */
                        /* mltok[0 .. mltoklen - 1] + '"' + */
                        /*  mltok[mltoklen + 1 .. mltokmatch - 1] */
                        mltokmatch = -1;
                    } else if(is_nl(c)) {
                        escst = ES_NORM;
                        nt = UN_NFKC_CF;
                        continue;
                    }
                }
                int pl = toklen;
                tok_append(c);
                pl = toklen - pl;
                if(mltokmatch >= 0 && mltokmatch + pl <= mltoklen &&
                   !memcmp(mltok + mltokmatch, curtok + toklen - pl, pl))
                    mltokmatch += pl;
                else
                    mltokmatch = -1;
            }
            if(is_nl(c)) {
                c = '\n';
                mltokmatch = 0;
            }
            break;
        }
        if(escst != ES_NORM)
            continue;
        /* got a character for a token */
        /* append to raw token; do canon comp later */
        tok_append(c);
    }
    switch(escst) {
      case ES_NORM:
        return process_tok();
      case ES_BS_NORM:
      case ES_NORM_CBESC:
        pr_loc(&bsloc);
        nbs = buf_to_utf8(bsbuf, nbs);
        fprintf(stderr, "bad bs escape (end of file detected): \\{%.*s\n",
                nbs, (char *)bsbuf);
        return TOK_EOF;
      case ES_STR:
      case ES_RAW_STR:
      case ES_BS_STR:
      case ES_STR_CBESC:
      case ES_ML_START:
      case ES_ML_RAW_START:
      case ES_ML_STR_TOK:
      case ES_ML_RAW_STR_TOK:
      case ES_ML_STR:
      case ES_ML_RAW_STR:
      case ES_BS_ML_STR:
      case ES_ML_STR_CBESC:
        pr_loc(&tokloc);
        fputs("unterminated string detected (eof)\n", stderr);
        escst = ES_NORM;
        tokst = TS_STRING;
        return process_tok();
    }
    return TOK_EOF;
}

@

\section{tok.h}
<<tok.h>>=
<<Common C Warning>>
#ifndef TOK_H
#define TOK_H

struct loc {
    uint32_t lineno, colno;
    size_t fpos;
};

void pr_loc(struct loc *l);

unifile_t *uni_fopen(const char *n, FILE *f);

/* raw token text */
extern int *curtok;
extern uint32_t toklen;
extern struct loc tokloc;
int buf_to_utf8(int *buf, int len);

/* token type */
enum {
    TOK_EOF, TOK_STRING, TOK_INT, TOK_FLOAT, TOK_ID, TOK_OP, TOK_PREDOC,
    TOK_POSTDOC, TOK_COMMA, TOK_SEMI,
    /* these should be handled invisibly */
    TOK_CDIR_START, TOK_CDIR_CONT, TOK_CDIR_END
};

/* processed token text */
uint32_t *numval;
uint32_t numlen;
uint32_t *nexp;
extern uint32_t explen;
extern int nbase; /* negative for negative exponent */
extern int isflt; /* needed because positive exponent could still have dp */

int gettok(unifile_t *uf);
#endif
@

\section{tststream.c}
<<tststream.c>>=
<<Common C Header>>
#include "unistuff.h"

int main(void)
{
    int c;
    int bs = 0;
    char *bsbuf = NULL;
    int bslen = 0, nbs = 0;
    while((c = utf_getc_norm(stdin, UN_NFC)) >= 0) {
        switch(bs) {
          case 0: /* no backslashes */
            if(c == '\\')
                bs = 1;
            break;
          case 1: /* got a backslash */
            if(c == 'm' || c == 'M') {
                bs = 3;
                break;
            }
            if(c == '"') {
                bs = 4;
                break;
            }
            if(c != '{') {
                fprintf(stderr, "bad bs escape (expected {)\n");
                bs = 0;
                continue;
            } else {
                bs = 2;
                nbs = 0;
            }
            break;
          case 2: /* {} escape */
            if(c == 0x0a || c == 0x0d || c == 0x0b || c == 0x0c || c == 0x85 || c == 0x2028 || c == 0x2029) {
                bs = 0;
                fprintf(stderr, "bad bs escape (end of line detected): \\{%.*s\n",
                        nbs, bsbuf);
            }
            if(c == '}') {
                bs = 0;
                if(!nbs) {
                    fprintf(stderr, "bad bs escape (empty)\n");
                    continue;
                }
                if(nbs > 4 && (bsbuf[0] == 'x' || bsbuf[0] == 'X')) {
                    int i;
                    for(i = 1; i < nbs; i++)
                        if(!isxdigit(bsbuf[i]))
                            break;
                    if(i == nbs) {
                        c = 0;
                        for(i = 1; i < nbs; i++) {
                            c <<= 4;
                            if(isdigit(bsbuf[i]))
                                c += bsbuf[i] - '0';
                            else
                                c += (bsbuf[i] & 0xf) + 9;
                        }
                        if(c > 0x10ffff) {
                            fprintf(stderr, "No such unicode code point: \\{%.*s}\n",
                                    nbs, bsbuf);
                        } else {
                            char uname[128];
                            int ul = uni_gen_name_for(c, uname, sizeof(uname));
                            if(ul)
			      fprintf(stderr, "warning: \\{%s} is more informative than \\{%.*s}\n",
			              uname, nbs, bsbuf);
                        }
                        break;
                    }
                }
                const int *mcp;
		c = cp_of_uni(bsbuf, nbs, &mcp);
                if(c < 0) {
                    fprintf(stderr, "Unknown backslash-escape \\{%.*s}\n",
                            nbs, bsbuf);
                    continue;
                }
                if(mcp) {
                    int i;
                    for(i = 0; i < c - 1; i++)
                        ascii_putc(mcp[i], stdout);
                    c = mcp[i];
                }
                break;
            }
            if(!bslen) {
                bslen = 128;
                inisize(bsbuf, bslen);
            } else if(nbs == bslen) {
                bslen *= 2;
                resize(bsbuf, bslen);
            }
            if(c > 0x7f) {
                fprintf(stderr, "Invalid character in backslash escape: \\{%.*s",
                        nbs, bsbuf);
                utf8_putc(c, stderr);
                bs = 0;
            }
            bsbuf[nbs++] = c;
            break;
          case 3: /* multi-line string */
            break;
          case 4: /* raw single-line string */
            break;
        }
        if(!bs)
            ascii_putc(c, stdout);
    }
    return 0;
}
@

\section{tsttok.c}
<<tsttok.c>>=
<<Common C Header>>
#include "unistuff.h"
#include "tok.h"

int main(int argc, const char **argv)
{
    unifile_t *uf = uni_fopen("stdin", stdin);
    int tok;
    while((tok = gettok(uf)) > 0) {
        printf("%d\n", tok);
    }
    uni_fclose(uf);
    return 0;
}
@

\chapter{Code Index}
\nowebchunks

\end{document}
