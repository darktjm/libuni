% -*- mode: Noweb; noweb-code-mode: c-mode; -*-
% Build with noweb:
%  notangle -t8 build.nw > makefile
%  make
\documentclass[twoside,english]{report}
\usepackage[letterpaper,rmargin=1.5in,bmargin=1in]{geometry}
%%% latex preamble
% Begin-doc uni-rcs
\RCS $Id$
\RCS $Revision$
\RCS $Date$
% End-doc uni-rcs

%%% requires tjm-ext

% Begin-doc unifonts
% shut l2h up about the font changes
% l2h ignore newfontfamily A[{
% l2h ignore unimono

% There are very few monospace Unicode fonts, and even fewer
% that support U+2620 SKULL AND CROSSBONES.
% GNU Unifont isn't pretty, but supports it: http://unifoundry.com/
\newfontfamily\unimonox[Path = /usr/share/fonts/unifont/]{unifont.ttf}
% DejaVu Sans Mono looks OK: http://dejavu-fonts.org/
% It's installed by texlive by fontsextra, so it doesn't need a path.
\newfontfamily\unimono[Scale = 0.8]{DejaVuSansMono}

% You could just replace LMMono with unimono globally for consistency:
%\let\Tt\unimono

% or even replace all 3:
%\setmainfont{DejaVuSerif}
%\setsansfont{DejaVuSans}
%\setmonofont{DejaVuSansMono}
% End-doc unifonts

\begin{document}

\title{Unicode Support Routines}
\author{Thomas J. Moore}
% Begin-doc uni-version
\date{Version 0.\RCSRevision\\\RCSDate}
% End-doc uni-version
\maketitle

\begin{rawhtml}
<!-->
\end{rawhtml}
\iffalse
<<Sources>>=
$Id$
@

<<Version Strings>>=
"$Id$\n"
@

<<Common noweb Warning>>=
# $Id$
@

\fi
\begin{rawhtml}
<-->
\end{rawhtml}

\begin{abstract}

This document describes and implements a library which provides
basic Unicode (\url{http://www.unicode.org}) support in a convenient
manner.  Libraries already exist that do this:  GLib, ICU, and others.
However, this library does things my way, which allows me to e.g.
build a regular expression library and a compiler using these
routines.  A large number of properties can be queried, and selected
properties have additional support routines.  All algorithms listed in
Table 3-1, Named Unicode Algorithms, in version 14.0 of the Unicode
standard are implemented except for the Bidirectional Algorithm (UBA)
and the Standard Compression Scheme for Unicode (SCSU).  Several of
these have additional modifications for localization using the Common
Locale Data Repository (CLDR).

\vspace{0.75in}

This document, its contents and all prior versions published by the
same author are granted to the Public Domain in 2021 by Thomas J.
Moore.

\vspace{0.25in}

This document was generated from the following sources, all of which are
attached to the original electronic forms of this document:
\input{Sources.tex} % txt

\end{abstract}

% Begin-doc Introduction
\tableofcontents
\listoftables

\chapter{Introduction}

The Unicode standard is a freely available standard describing a large
character set, and the features of those characters.  It also
describes several common transformations and processing methods for
those characters.  The ISO 10646 standard is also freely available at
this time, but like most ISO standards, this was not always so.  The
ISO standard describes the same character set, but has a much smaller
scope overall.  The Unicode Consortium also provides an electronically
readable form of its attribute information: the Unicode Character
Database.%
\footnote{\url{http://www.unicode.org/Public/zipped}}
The purpose of this library is to provide convenient access to that
information in C programs.  The ISO 10646 standard also provides such
information, but again, it is of more limited scope.  To the most
part, the ISO 10646 standard is completely ignored from this point on.

The Unicode consortium also provides a set of rules for interpreting
characters in different locales.  ISO has standards to this effect, as
well.  Both sets of standards and their data files are freely
available.  However, once again, the ISO data is ignored in favor of
the Unicode data.  The Unicode locale information is available in
electronically processable form in the Unicode Common Locale Data
Repository (CLDR).%
\footnote{\url{http://cldr.unicode.org},
\url{http://unicode.org/Public/cldr/}}
In addition to the regular UCD data, this library also provides access
to selected parts of the CLDR data to C programs.

% End-doc Introduction
<<uni_all.h>>=
<<Common C Warning>>
#ifndef UNI_ALL_H
#define UNI_ALL_H
/** \file uni_all.h Includes all libuni-related headers */

<<Library [[uni]] headers>>

#endif /* UNI_ALL_H */
@

<<Headers to Install>>=
uni/uni_all.h \
@

% Begin-doc Introduction
This document does not contain the UCD or CLDR itself; it is expected
that the user download them manually and place them where they can be
found.  Some Linux distributions may already provide at least the UCD
as a package, although the CLDR is probably less common.  This package
was tested with version 14.0.0 of the UCD, but the file formats are
relatively stable and it should work with later versions as well,
other than algorithm changes.  There is little reason to not use the
latest available version.%
\footnote{Other than the data growing rapidly with each release, but
there's no fighting that.  And, of course, related breakages, such as
the blk property exceeding 8 bits as of 9.0.0, and the cjkRSUnicode
property switching files in 7.0.0.}
This package was tested against CLDR version 39; I recommend always
getting the latest version available.  In particular, care must be
taken that the CLDR and UCD versions are compatible (in fact, CLDR 39
is for Unicode 13, so it fails some UCA tests that CLDR 40 Alpha 3
doesn't).  Some CLDR changes require modification of algorithms, so
you may need to wait for an update in this library before full support
for a later version is present.

Originally, I didn't care what version of the UCD was used, but 6.3.0
introduced some incompatible algorithm changes.  My original solution
was to support every algorithm change based on the UCD version used,
but instead, I will now only support one.  There are simply too many
algorithm changes in every major Unicode revision (and the major
revisions come too often).  I won't even abort if the UCD version is
too low; it is your responsibility to use the correct, latest
available version.  It is possible that the library will work with
older versions of the UCD (but probably not, as new symbols are often
introduced as well, and I even removed the pre-6.3 compatibility
symbols), using algorithms from later versions of Unicode, but I will
no longer put effort into ensuring support.  One of the advantages of
freely available data files is that I can force you to use the one I
want.

% End-doc Introduction

\lstset{language=make}
<<makefile.config>>=
# The location of the Unicode Character Database (unzipped)
# http://www.unicode.org/Public/zipped/<version>/UCD.zip
UCD_LOC = /usr/share/unicode-data

# The location of the Unicode Han Database (unzipped)
# http://www.unicode.org/Public/zipped/<version>/Unihan.zip
UNIHAN_LOC = $(UCD_LOC)

# The location of the Unicode Common Locale Data Repository (unzipped)
# http://www.unicode.org/Public/cldr/latest/core.zip
CLDR_LOC = /usr/share/unicode-data/cldr

@

% Begin-doc Introduction
Speaking of names, the Unicode character names are rather verbose.%
\footnote{It is recommended in some places that common names be
permitted in a less verbose manner, but that is locale-dependent and
will not be supported by this library.}
The XML standard, on the other hand, has fairly short names.  This
library can use either, or even both (where they do not conflict).  To
do this, the XML entity database is needed.  This is defined by the
W3C XML Entity Definitions for Characters.%
\footnote{\url{http://www.w3.org/TR/xml-entity-names/}}
This was tested with version 01 April 2010.%
\footnote{\url{http://www.w3.org/TR/2010/REC-xml-entity-names-20100401/}}
The only data file needed from that standard is unicode.xml,%
\footnote{\url{http://www.w3.org/2003/entities/2007xml/unicode.xml}}
which is linked from the standard.

% End-doc Introduction
<<makefile.config>>=
# The XML entity database http://www.w3.org/2003/entities/2007xml/unicode.xml
XMLUNI = /usr/share/unicode-data/unicode.xml

@

% Begin-doc Introduction
Other libraries exist which provide similar functionality; I have
looked at GLib\footnote{\url{http://www.gtk.org/}} and
ICU\footnote{\url{http://www.icu-project.org/}} in particular.  The
GLib library provides a number of utility functions for Unicode
character strings.  It provides normalization, classification, and
general string utilities. Of these, this library does not provide
general utf-8 string utilities.  It might have been useful to make
this a GLib extension instead, providing the missing properties (in
particular, the string properties such as character names).  However,
providing all functionality in a consistent way is easier than making
a GLib extension.

The ICU library is primarily C++, with a C wrapper.  While there is
nothing inherently wrong with this, I prefer C with a C++ wrapper over
C++ with a C wrapper in general.  The ICU does everything this library
does, and more.  Unfortunately, it is not very well documented.  Like
many modern library projects, it simply provides a basic API document
auto-generated from source.  This sort of documentation is necessary,
but insufficient.  Another part of the reason for not just using ICU
is that some operations I want to do in other code requires too much
overhead in ICU.

A lesser reason for not using either of these libraries is that I want
to use this library to support my own regular expression library, and
it seems redundant when both provide their own regular expression
implementation as well.  In addition, this library can be trimmed to
the minimal necessary code with static linking, and does not include
heavy wrapper code and additional utilities that the other libraries
include.

% End-doc Introduction
\chapter{Unicode Character Processing}

% Begin-doc unitypes
The supported character types (encoding forms) are UTF-8, UTF-16, and
UTF-32.  Technically, a UTF-32 character can be stored in 21 bits.
The other two require an unsigned storage type of the given width.
Instead of providing aliases for standard types, the standard sized C
types from \texttt{stdint.h} are used: [[uint8_t]], [[uint16_t]], and
[[uint32_t]], respectively.  All functions which take or return a
single character use the UTF-32 type.  If an error could be returned
for UTF-32 returning functions, a signed integer ([[int32_t]]) may be
used instead.

The stream encodings (encoding schemes) for these types are named
similarly.  The 8-bit encoding is UTF-8.  The 16-bit big-endian
encoding is UTF-16BE, and the little-endian encoding is UTF-16LE.  The
32-bit big-endian encoding is UTF-32BE, and the 32-bit little-endian
encoding is UTF-32LE.  UTF-16 and UTF-32 character streams must either
agree on a byte order ahead of time, or begin the stream with a Byte
Order Mark (U+FEFF).

% End-doc unitypes

\section{File I/O}

Basic I/O support is provided in the form of UTF decoders and
encoders.  More advanced I/O is provided in the form of arbitrary
encoding input.

<<Library [[uni]] Members>>=
uni_io.o
@

<<Libraries to Install>>=
uni \
@

\lstset{language=C}
<<uni_io.c>>=
<<Common C Header>>
#include "uni_io.h"

<<Unicode I/O local definitions>>

<<Unicode I/O functions>>
@

<<Library [[uni]] headers>>=
#include "uni_io.h"
@

<<uni_io.h>>=
<<Common C Warning>>
#ifndef UNI_IO_H
#define UNI_IO_H
/** \file uni_io.h UTF I/O and parsing */

#include <stdint.h> /* for basic data types */
#include <stdio.h> /* it's I/O, right? */

/** \addtogroup uni_io Unicode Encodings
    @{ */
<<Unicode I/O Exports>>
/** @} */
#endif
@

<<Headers to Install>>=
uni/uni_io.h \
@

\section{Basic Unicode I/O}

Unicode defines several file formats:  UTF-8, UTF-16, and UTF-32.  No
special conversion needs to be made from UTF-32 to integer code
points, other than to filter out invalid values and possibly byte-swap
the code point.

<<Common C Includes>>=
#include <stdint.h>
/* FIXME: this is glibc-specific */
/* BSD apparently uses <sys/endian.h> */
/* OpenBSD additionally uses different fn names */
/* others may not even have any equivalent functions */
/* at least non-glibc users should get compilation errors that point here */
#include <endian.h>
@

\lstset{language=make}
<<makefile.vars>>=
# for endian (was _BSD_SOURCE, but recent GNU libc hates that)
EXTRA_CFLAGS += -D_DEFAULT_SOURCE
@

\lstset{language=C}
<<Unicode I/O Exports>>=
/** Returns UTF-32 encoding length of \p cp.
  * Note that this is a preprocessor macro */
int uni_utf32_enclen(uint32_t cp);
#define uni_utf32_enclen(cp) 1
/** Encodes Unicode code point \p cp into \p buf.
 *  Returns number of 32-bit words generated.  Encoding in \p buf is
 *  UTF-32BE if \p bige is true, and UTF-32LE otherwise.   Invalid
 *  Unicode code points will not be encoded */
int uni_utf32_encode(uint32_t *buf, uint32_t cp, int bige);
@

<<Unicode I/O functions>>=
int uni_utf32_encode(uint32_t *buf, uint32_t cp, int bige)
{
  if(cp > 0x10ffff || (cp >= 0xd800 && cp < 0xe000))
    return 0;
  *buf = bige ? htobe32(cp) : htole32(cp);
  return 1;
}
@

<<Unicode I/O Exports>>=
/** Returns decoded Unicode code point at \p s.
 *  The encoding at \p s is UTF-32BE if \p bige is true, and UTF-32LE
 *  otherwise.  Returns -1 if \p s does not point to valid UTF-32 */
int32_t uni_utf32_decode(const uint32_t *s, int bige);
@

<<Unicode I/O functions>>=
int32_t uni_utf32_decode(const uint32_t *s, int bige)
{
  uint32_t c = bige ? be32toh(*s) : le32toh(*s);
  if(c > 0x10ffff || (c >= 0xd800 && c < 0xe000))
    return -1;
  return c;
}
@

<<Unicode I/O Exports>>=
/** Outputs Unicode code point \p c to file \p f.
 *  The enconding is UTF-32BE if \p bige is true, and UTF-32LE otherwise.
 *  Invalid Unicode code points will not be written out.  Returns number
 *  of 32-bit words written */
int uni_utf32_putc(uint32_t c, FILE *f, int bige);
@

<<Unicode I/O functions>>=
int uni_utf32_putc(uint32_t c, FILE *f, int bige)
{
  uint32_t w;

  if(!uni_utf32_encode(&w, c, bige))
    return 0;
  return fwrite(&w, 4, 1, f);
}
@

<<Unicode I/O Exports>>=
/** Returns Unicode code point read from UTF-32 file \p f.
 *  The encoding is UTF-32BE if \p bige is true, and UTF-32LE otherwise.
 *  This function always reads 4 bytes, and returns less than zero on
 *  errors:  -1 if no characters could be read, -2 on short read, and
 *  -3 on decoding failure.  Zero is returned on success */
int32_t uni_utf32_getc(FILE *f, int bige); /* always reads 4 bytes */
@

<<Unicode I/O functions>>=
int32_t uni_utf32_getc(FILE *f, int bige)
{
  uint32_t w;
  int32_t ret;

  if((ret = fread(&w, 1, 4, f)) != 4)
    return ret ? -2 : -1;
  ret = uni_utf32_decode(&w, bige);
  return ret < 0 ? -3 : ret;
}
@

For UTF-16, a slightly more complex scheme is used, involving the
surrogate code points (D800 through DFFF) in pairs.  The first half of
the surrogate code points (D800 through DBFF) is always used for the
first member of the pair, and the second half is used for the other.
Each member gives 10 bits.  The maximum code point is 10FFFF,
requiring 21 bits, but since the extensions start at 10000, the
maximum etension is 10FFFF$-$10000 = FFFFF, requiring 20 bits.

<<Unicode I/O Exports>>=
/** Returns UTF-16 encoding length of \p cp */
int uni_utf16_enclen(uint32_t cp);
#define uni_utf16_enclen(cp) (1 + ((cp) >= 0x10000))
/** Encodes Unicode code point \p cp into \p buf.
  * Returns number of 16-bit words generated.  Encoding in \p buf is
  * UTF-16BE if \p bige is true, and UTF-16LE otherwise.  Ensure that at
  * least 2 words are available in \p buf, or use \ref uni_utf16_enclen to
  * size the buffer first.  Invalid code points will not be encoded */
int uni_utf16_encode(uint16_t *buf, uint32_t cp, int bige);
@

<<Unicode I/O functions>>=
int uni_utf16_encode(uint16_t *buf, uint32_t cp, int bige)
{
  if((cp >= 0xD800 && cp < 0xE000) || cp > 0x10FFFF)
    return 0;
  if(cp < 0x10000) {
    *buf = bige ? htobe16(cp) : htole16(cp);
    return 1;
  }
  cp -= 0x10000;
  uint16_t w = 0xD800 + (cp >> 10);
  *buf++ = bige ? htobe16(w) : htole16(w);
  w = 0xDC00 + (cp & 0x3ff);
  *buf = bige ? htobe16(w) : htole16(w);
  return 2;
}
@

<<Unicode I/O Exports>>=
/** Returns decoded Unicode code point at \p s.
 *  The encoding at \p s is UTF-16BE if \p bige is true, and UTF-16LE
 *  otherwise.  At most \p len 16-bit words will be read, and the actual
 *  number of words read are returned in \p nread, if non-NULL.
 *  Returns -1 if \p s does not point to valid UTF-16, or \p len is
 *  too short */ 
int32_t uni_utf16_decode(const uint16_t *s, unsigned int len,
                         unsigned int *nread, int bige);
@

<<Unicode I/O functions>>=
int32_t uni_utf16_decode(const uint16_t *s, unsigned int len,
                         unsigned int *nread, int bige)
{
  if(!len) {
    if(nread)
      *nread = 0;
    return -1;
  }
  uint16_t w = bige ? be16toh(*s) : le16toh(*s);
  if(nread)
    *nread = 1;
  if(w < 0xd800 || w >= 0xe000)
    return w;
  if(w >= 0xdc00)
    return -1;
  if(len < 2) {
    if(nread)
      *nread = 0;
    return -1;
  }
  uint16_t w2 = bige ? be16toh(s[1]) : le16toh(s[1]);
  if(w2 >= 0xe000 || w2 < 0xdc00)
    return -1;
  if(nread)
    *nread = 2;
  return (w2 & 0x3ff) + ((uint32_t)(w & 0x3ff) << 10) + 0x10000;
}
@

<<Unicode I/O Exports>>=
/** Outputs Unicode code point \p c to file \p f.
 *  The enconding is UTF-16BE if \p bige is true, and UTF-16LE otherwise.
 *  Invalid Unicode code points will not be written out.  Returns number
 *  of 16-bit words written on success, and the negative of the number
 *  of 16-bit words written (or zero) on failure.  */
int uni_utf16_putc(uint32_t c, FILE *f, int bige);
@

<<Unicode I/O functions>>=
int uni_utf16_putc(uint32_t c, FILE *f, int bige)
{
  unsigned char buf[4];
  uint16_t enc[2], encl;
  int wlen;
  
  encl = uni_utf16_encode(enc, c, bige);
  if(!encl)
    return 0;
  if(bige) {
    buf[0] = enc[0] >> 8;
    buf[1] = enc[0];
    if(encl > 1) {
      buf[2] = enc[1] >> 8;
      buf[3] = enc[1];
    }
  } else {
    buf[1] = enc[0] >> 8;
    buf[0] = enc[0];
    if(encl > 1) {
      buf[3] = enc[1] >> 8;
      buf[2] = enc[1];
    }
  }
  wlen = fwrite(buf, 2, encl, f);
  return wlen == encl ? wlen : -wlen;
}
@

The input function needs to read ahead to get the second member of a
pair.  If that character is not what was expected, the correct
behavior is to undo the readahead and return the current code point as
an error.  However, it is not possible in C standard I/O to push more
than one character back into the stream.  For now, this is going to
have to be erroneous behavior.

<<Unicode I/O Exports>>=
/** Returns Unicode code point read from UTF-16 file \p f.
 *  The encoding is UTF-16BE if \p bige is true, and UTF-16LE otherwise.
 *  The number of 16-bit words actually read from the file are returned in
 *  \p nread, if non-NULL.  Zero is returned on success, and less than zero
 *  on errors:  -1 if no characters could be read, -2 on short read,
 *  -3 on decoding failure */
int32_t uni_utf16_getc(FILE *f, int bige, unsigned int *nread);
@

<<Unicode I/O functions>>=
int32_t uni_utf16_getc(FILE *f, int bige, unsigned int *nread)
{
  int c = fgetc(f), c2;
  if(c == EOF) {
    if(nread)
      *nread = 0;
    return -1;
  }
  c2 = fgetc(f);
  if(c == EOF) {
    if(nread)
      *nread = 1;
    return -2;
  }
  if(bige)
    c = c2 + (c << 8);
  else
    c += c2 << 8;
  if(c < 0xd800 || c >= 0xE000) {
    if(nread)
      *nread = 2;
    return c;
  }
  if(c >= 0xdc00) {
    if(nread)
      *nread = 2;
    return -3;
  }
  uint32_t res = 0x10000 + ((c & 0x3ff) << 10);
  if((c = fgetc(f)) == EOF) {
    if(nread)
      *nread = 2;
    return -2;
  }
  if((c2 = fgetc(f)) == EOF) {
    if(nread)
      *nread = 3;
    return -2;
  }
  if(bige)
    c = c2 + (c << 8);
  else
    c += c2 << 8;
  if(c < 0xdc00 || c >= 0xe000) {
    ungetc(bige ? c >> 8 : c & 0xff, f);
    /* POSIX only guarantees 1 ungetc */
    if(ungetc(c2, f) == EOF) {
      c2 = getc(f);
      if(nread)
        *nread = 4;
    } else if(nread)
      *nread = 2;
    return -3;
  }
  if(nread)
    *nread = 4;
  return res + (c & 0x3ff);
}
@

For UTF-8, an even more complex encoding scheme is used.  Again, a
standalone memory codec is provided.  All code points over 007F are
encoded using a multi-byte sequence.  All characters in a multi-byte
sequence have their high bit set; the first non-zero bit determines
the byte's role.  All but the first byte have only one high bit set,
and encode 6 bits.  The first byte determines how many trailing bytes
there are, and also encodes 3--5 bits.  The number of high bits set in
the first byte is the total number of bytes in the sequence.

<<Unicode I/O Exports>>=
/** Returns UTF-8 encoding length of \p cp.
  * Note that this is a preprocessor macro, and may evaluate \p cp more than
  * once */
int uni_utf8_enclen(uint32_t cp);
#define uni_utf8_enclen(cp) \
  (1 + ((cp) >= 0x80) + ((cp) >= 0x800) + ((cp) >= 0x10000))
/** Encodes Unicode code point \p cp into \p buf using UTF-8.
  * Returns number of bytes generated.  Ensure that at least 4 bytes are
  * available in \p buf, or use \ref uni_utf8_enclen to size the buffer
  * first.  Invalid Unicode code points will not be encoded */
int uni_utf8_encode(uint8_t *buf, uint32_t cp);
@

<<Unicode I/O functions>>=
<<Encode code point as UTF-8>>
@

<<Encode code point as UTF-8>>=
int uni_utf8_encode(uint8_t *buf, uint32_t cp)
{
    if((cp >= 0xD800 && cp < 0xE000) || cp > 0x10FFFF)
        return 0;
    if(cp < 128) {
        *buf = cp;
        return 1;
    } else if(cp < 0x800) {
        *buf = 0xc0 + (cp >> 6);
        buf[1] = 0x80 + (cp & 0x3f);
        return 2;
    } else if(cp < 0x10000) {
        *buf = 0xe0 + (cp >> 12);
        buf[1] = 0x80 + ((cp >> 6) & 0x3f);
        buf[2] = 0x80 + (cp & 0x3f);
        return 3;
    } else {
        *buf = 0xf0 + (cp >> 18);
        buf[1] = 0x80 + ((cp >> 12) & 0x3f);
        buf[2] = 0x80 + ((cp >> 6) & 0x3f);
        buf[3] = 0x80 + (cp & 0x3f);
        return 4;
    }
}
@

<<Unicode I/O Exports>>=
/** Returns decoded UTF-8 Unicode code point at \p s.
  * At most \p len bytes will be read, and the actual number of bytes
  * read are returned in \p nread, if non-NULL.  Returns -1 if \p s does
  * not point to valid UTF-8, or if \p len is too short */
int32_t uni_utf8_decode(const uint8_t *s, unsigned int len, unsigned int *nread);
@

<<Unicode I/O functions>>=
int32_t uni_utf8_decode(const uint8_t *s, unsigned int len, unsigned int *nread)
{
  if(!len) {
    if(nread)
      *nread = 0;
    return -1;
  }
  if(*s < 0x80) {
    if(nread)
      *nread = 1;
    return *s;
  }
  uint32_t c = *s++, ec;
  if((c & 0xf8) == 0xf0) {
    if(len < 4) {
      if(nread)
        *nread = 0;
      return -1;
    }
    c &= 0x07;
    ec = *s++;
    if((ec & 0xc0) != 0x80) {
      if(nread)
        *nread = 1;
      return -1;
    }
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if((ec & 0xc0) != 0x80) {
      if(nread)
        *nread = 2;
      return -1;
    }
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if((ec & 0xc0) != 0x80) {
      if(nread)
        *nread = 3;
      return -1;
    }
    if(nread)
      *nread = 4;
    c = (c << 6) + (ec & 0x3f);
    if(c < 0x10000 || c > 0x10FFFF)
      return -1;
    return c;
  } else if((c & 0xf0) == 0xe0) {
    if(len < 3) {
      if(nread)
        *nread = 0;
      return -1;
    }
    c &= 0x0f;
    ec = *s++;
    if((ec & 0xc0) != 0x80) {
      if(nread)
        *nread = 1;
      return -1;
    }
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if((ec & 0xc0) != 0x80) {
      if(nread)
        *nread = 2;
      return -1;
    }
    if(nread)
      *nread = 3;
    c = (c << 6) + (ec & 0x3f);
    if(c < 0x800 || (c >= 0xD800 && c < 0xE000))
      return -1;
    return c;
  } else if((c & 0xe0) == 0xc0) {
    if(len < 2) {
      if(nread)
        *nread = 0;
      return -1;
    }
    c &= 0x1f;
    ec = *s++;
    if((ec & 0xc0) != 0x80) {
      if(nread)
        *nread = 1;
      return -1;
    }
    if(nread)
      *nread = 2;
    c = (c << 6) + (ec & 0x3f);
    if(c < 0x80)
      return -1;
    return c;
  } else {
    if(nread)
      *nread = 1;
    return -1;
  }
}
@

<<Unicode I/O Exports>>=
/** Outputs Unicode code point \p c to file \p f using UTF-8 encoding.
 *  Invalid Unicode code points will not be written out.  Returns number
 *  of bytes written on success, and the negative of the number
 *  of bytes written (or zero) on failure.  */
int uni_utf8_putc(uint32_t c, FILE *f);
@

<<Unicode I/O functions>>=
int uni_utf8_putc(uint32_t c, FILE *f)
{
    uint8_t obuf[4];
    int nout = uni_utf8_encode(obuf, c);
    int nwr = fwrite(obuf, 1, nout, f);
    return nout == nwr ? nwr : -nwr;
}
@

UTF-8 output is common enough to warrant a string version of the
output function as well.

<<Unicode I/O Exports>>=
/** Outputs string of Unicode code points \p buf to file \p f using UTF-8
 ** encoding.
 *  The number of code points written is \p len (i.e., \p buf may contain
 *  zeroes).  Invalid Unicode code points will not be written out.
 *  Returns number of bytes written on success, and the negative of
 *  the number of bytes written (or zero) on failure.  */
int uni_utf8_fputs(uint32_t *buf, int len, FILE *f);
@

<<Unicode I/O functions>>=
int uni_utf8_fputs(uint32_t *buf, int len, FILE *f)
{
  int ret = 0;

  while(len-- > 0) {
    int ret1 = uni_utf8_putc(*buf++, f);
    if(ret1 <= 0)
      return -ret + ret1;
    ret += ret1;
  }
  return ret;
}
@

Like UTF-16 input, UTF-8 input requires some readahead.  For anything
more than one character, C once again disallows returning the
characters to the stream.  However, it is relatively easy to abort
input as soon as an invalid character occurs.  The price to pay is the
extremely poor performance of [[getc]], but using the direct
decoder function above can be used to get around that.

<<Unicode I/O Exports>>=
/** Returns Unicode code point read from UTF-8 file \p f.
 *  The number of bytes actually read from the file are returned in
 *  \p nread, if non-NULL.  Zero is returned on success, and less than zero
 *  on errors:  -1 if no characters could be read, -2 on short read,
 *  -3 on decoding failure */
int32_t uni_utf8_getc(FILE *f, unsigned int *nread);
@

<<Unicode I/O functions>>=
int32_t uni_utf8_getc(FILE *f, unsigned int *nread)
{
  int c;
  int chrread = 1;

  if((c = getc(f)) == EOF) {
    c = -1;
    chrread = 0;
  } else if(c >= 0x80) {
    int ec;
    chrread = 1;
    if((c & 0xf8) == 0xf0) {
      <<Read 4-char utf-8>>
      if(c < 0x10000 || c > 0x10ffff)
        c = -3;
    } else if((c & 0xf0) == 0xe0) {
      <<Read 3-char utf-8>>
      if(c < 0x800 || (c >= 0xD800 && c < 0xE000))
        c = -3;
    } else if((c & 0xe0) == 0xc0) {
      <<Read 2-char utf-8>>
      if(c < 0x80)
        c = -3;
    } else
      c = -3;
  }
  if(nread)
    *nread = chrread;
  return c;
}
@

<<Read 4-char utf-8>>=
c &= 0x07;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
  if((ec = getc(f)) == EOF)
    c = -2;
  else if((ec & 0xc0) != 0x80) {
    ungetc(ec, f);
    c = -3;
  } else {
    chrread = 3;
    c = (c << 6) + (ec & 0x3f);
    if((ec = getc(f)) == EOF)
      c = -2;
    else if((ec & 0xc0) != 0x80) {
      ungetc(ec, f);
      c = -3;
    } else {
      chrread = 4;
      c = (c << 6) + (ec & 0x3f);
    }
  }
}
@

<<Read 3-char utf-8>>=
c &= 0x0f;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
  if((ec = getc(f)) == EOF)
    c = -2;
  else if((ec & 0xc0) != 0x80) {
    ungetc(ec, f);
    c = -3;
  } else {
    chrread = 3;
    c = (c << 6) + (ec & 0x3f);
  }
}
@

<<Read 2-char utf-8>>=
c &= 0x1f;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
}
@

\section{General File Input}

A more general UTF reader would have to scan for a byte order mark and
remember what it said.  This requires retaining state.

<<Unicode I/O Exports>>=
/** Opaque type used for reading Unicode files */
typedef struct uni_file_t uni_file_t;
@

<<[[uni_file_t]]>>=
struct uni_file_t {
  FILE *f;
  char *name;
  <<Unicode file buffer state>>
};
@

<<Unicode I/O local definitions>>=
<<Unicode file buffer state deps>>
<<[[uni_file_t]]>>
@

<<Known Data Types>>=
uni_file_t,%
@

Rather than automatically attaching state to a file on first access
and never really knowing when to free it, explicit routines are
provided to create and remove the state.   

<<Unicode I/O Exports>>=
/** Open file \p name for reading Unicode code points.
  * The encoding is specified by the string \p encoding.  If the library
  * was compiled with iconv support, this supports all encodings supported
  * by iconv(3), and NULL attempts to use current locale's encoding.
  * Whether or not iconv support is included, some encodings are processed
  * internally.  The special encoding "UTF" defaults to UTF-8, but can
  * switch encoding based on an initial Byte-Order-Mark.  Similarly,
  * UTF-16 and UTF-32 can switch endianness based on a Byte-Order-Mark.
  * Note that the Byte-Order-Mark is returned as the first code point if
  * present.  The other non-iconv encodings are UTF-16LE, UTF-16BE,
  * UTF-32LE, UTF-32BE, and raw bytes (ISO-8859-1 and ANSI_X3.4-1968).
  * The return value is NULL on error.  */
uni_file_t *uni_fopen(const char *name, const char *encoding);
/** Close file \p uf opened by \ref uni_fopen */
void uni_fclose(uni_file_t *uf);
@

<<Unicode I/O functions>>=
uni_file_t *uni_fopen(const char *name, const char *encoding)
{
  uni_file_t *uf = calloc(sizeof(*uf), 1);

  if(!uf)
    return NULL;
  uf->name = strdup(name);
  if(!uf->name) {
    uni_fclose(uf);
    return NULL;
  }
  uf->f = fopen(name, "rb");
  if(!uf->f) {
    uni_fclose(uf);
    return NULL;
  }
  <<Initialize unicode file buffer state>>
  return uf;
}
@

<<Unicode I/O functions>>=
void uni_fclose(uni_file_t *uf)
{
  /* all frees are conditional so uni_fopen can uni_fclose partial progress */
  if(uf->name)
    free(uf->name);
  if(uf->f)
    fclose(uf->f);
  <<Free unicode file buffer state>>
  free(uf);
}
@

One limitation of standard I/O is that only one character may be
pushed back into the stream.  To correct this, a readahead buffer is
kept.  While C provides its own large buffers, preventing I/O overhead
on every read, the read itself can be expensive anyway if called too
often.  To avoid this, the buffer is much larger than needed for
simple UTF lookahead.

<<Unicode file buffer state deps>>=
#define UNIF_BUF_LEN 16384 /* about 4x my old usual buf size */
@

<<Unicode file buffer state>>=
char readahead[UNIF_BUF_LEN];
unsigned short raptr, ralen;
@

The first thing to do is to decide the file's encoding.  ASCII and
ISO-Latin-1 can be read raw, since they map directly to Unicode.
UTF is taken to mean automatic Unicode detection, with UTF-8 as a
default.  UTF-8, UTF-16, UTF-32, UTF-16LE, UTF-16BE, UTF-32LE, and
UTF-32BE are processed directly, with UTF-16 and UTF-32 using native
encoding if a byte order mark is not found.  For all other input
types (including the above-mentioned ones which are in other formats,
such as lower-case), the [[iconv]] function should be used.%
\footnote{The C99 library provides something similar in the
[[mbstowcs]] function, but the POSIX [[iconv]] function is commonly
available (perhaps even as a third party library), and could be
provided as a wrapper around [[mbstowcs]] if all else fails.  Unlike
[[mbstowcs]], the type of the output can always be set to Unicode
encodings, and the input type can be controlled more easily as well.}
This function should be in the standard C library, but may be provided
externally as well.  If so, [[CFLAGS]] and [[LDFLAGS]] may need to be
modified appropriately.  In order to accomodate the case where a
minimal C library has no working [[iconv]], or to otherwise avoid code
bloat, this can be disabled with a configuration variable.

\lstset{language=make}
<<makefile.config>>=
# Set to non-empty to enable iconv input support
#USE_ICONV=y

@

<<makefile.vars>>=
ifneq ($USE_ICONV,)
EXTRA_CFLAGS += -DUNI_USE_ICONV
endif
@

\lstset{language=C}
<<Unicode file buffer state deps>>=
#ifdef UNI_USE_ICONV
#include <langinfo.h>
#include <locale.h>
#include <iconv.h>
#endif
@

<<Unicode file buffer state>>=
/*
 * 0 = ASCII/Latin-1
 * 1 = iconv (if UNI_USE_ICONV)
 * 8/16/32 = utf-8/16/32
 */
unsigned char enctype;
unsigned char bige;
@

<<Initialize unicode file buffer state>>=
#ifdef UNI_USE_ICONV
if(!encoding) {
  setlocale(LC_CTYPE, "");
  encoding = nl_langinfo(CODESET);
}
#endif
if(encoding && (!strcmp(encoding, "ISO-8859-1") || /* Latin-1 */
                !strcmp(encoding, "ANSI_X3.4-1968"))) /* ASCII */
  return uf; /* enctype == 0 -> raw bytes */
if(encoding && !strncmp(encoding, "UTF", 3)) {
  if(!encoding[3])
    encoding = NULL;
  else if(encoding[3] == '-') {
    if(encoding[4] == '8' && !encoding[5]) {
      uf->enctype = 8;
      return uf;
    } else if(encoding[4] == '1' && encoding[5] == '6') {
      uf->enctype = 16;
      <<Check if [[encoding]] is has [[BE]] or [[LE]] suffix>>
    } else if(encoding[4] == '3' && encoding[5] == '2') {
      uf->enctype = 32;
      <<Check if [[encoding]] is has [[BE]] or [[LE]] suffix>>
    }
  }
}
@

<<Check if [[encoding]] is has [[BE]] or [[LE]] suffix>>=
if(encoding[6] == 'B' && encoding[7] == 'E' && !encoding[8]) {
  uf->bige = 1;
  return uf;
} else if(encoding[6] == 'L' && encoding[7] == 'E' && !encoding[8])
  return uf;
else if(!encoding[6]) {
  uf->bige = __BYTE_ORDER == __BIG_ENDIAN;
  encoding = NULL; /* need to determine endianness still */
} /* else use iconv */
@

<<Unicode file buffer state>>=
#ifdef UNI_USE_ICONV
iconv_t ic;
uint32_t *obuf; /* only allocated when needed */
uint16_t optr, olen;
#endif
@

<<Initialize unicode file buffer state>>=
#ifdef UNI_USE_ICONV
if(encoding) {
  uf->obuf = malloc(UNIF_BUF_LEN); /* max len == UNIF_BUF_LEN / 4 */
  if(!uf->obuf) {
    uni_fclose(uf);
    return NULL;
  }
#if __BYTE_ORDER == __BIG_ENDIAN
#define bo "BE"
#else
#define bo "LE"
#endif
  uf->ic = iconv_open("UTF-32" bo, encoding);
  if(uf->ic == (iconv_t)-1) {
    uni_fclose(uf);
    return NULL;
  }
  uf->enctype = 1;
  return uf;
}
#else
if(encoding) {
  errno = EINVAL;
  uni_fclose(uf);
  return NULL;
}
#endif
@

<<Free unicode file buffer state>>=
#ifdef UNI_USE_ICONV
if(uf->obuf)
  free(uf->obuf);
if(uf->enctype == 1)
  iconv_close(uf->ic);
#endif
@

The first thing to do on opening a new Unicode file is to check for
the byte order mark (U+FEFF).  Flags are kept to indicate what was
found.  Note that the byte order mark is still returned as the first
character, so all read characters must be backed out.

<<Initialize unicode file buffer state>>=
if(!uf->enctype)
  uf->enctype = 8; /* default is UTF-8 */
/* Read and process BOM (FEFF) */
int c;
<<Read and store a char from [[uf]]>>
if(c == 0xfe) {
  <<Read and store a char from [[uf]]>>
  if(c == 0xff && uf->enctype != 32) { /* FEFF UTF-16BE */
    uf->bige = 1;
    uf->enctype = 16;
  }
} else if(c == 0xff) {
  <<Read and store a char from [[uf]]>>
  if(c == 0xfe) { /* FFFE UTF-16LE or UTF-32LE (or native UTF-32) */
    if(uf->enctype == 16)
      uf->bige = 0;
    else {
      <<Read and store a char from [[uf]]>>
      if(!c) {
        <<Read and store a char from [[uf]]>>
        if(!c) { /* FFFE0000 UTF-32LE */
          uf->enctype = 32;
	  uf->bige = 0;
        } else if(!uf->enctype)
          uf->enctype = 16;
	/* else UTF-32 native-endian */
      } else if(!uf->enctype)
        uf->enctype = 16;
      /* else UTF-32 native-endian */
    }
  }
} else if(!c && uf->enctype != 16) {
  <<Read and store a char from [[uf]]>>
  if(!c) {
    <<Read and store a char from [[uf]]>>
    if(c == 0xfe) {
      <<Read and store a char from [[uf]]>>
      if(c == 0xff) { /* 0000FEFF UTF-32BE */
        uf->enctype = 32;
	uf->bige = 1;
      }
    }
  }
}
<<Unread all chars read from [[uf]]>>
@

<<Read and store a char from [[uf]]>>=
if(uf->raptr < uf->ralen)
  c = uf->readahead[uf->raptr++];
else if(!uf->ralen) {
  uf->ralen = fread(uf->readahead, 1, UNIF_BUF_LEN, uf->f);
  if(uf->raptr)
    c = uf->readahead[uf->raptr++];
  else
    c = EOF;
} else
  c = EOF;
@

<<Unread all chars read from [[uf]]>>=
uf->raptr = 0;
@

The reader then uses the appropriate method to read a character based
on the [[enctype]].

<<Unicode I/O Exports>>=
/** Returns next Unicode code point from file \p uf opened by \ref uni_fopen */
int32_t uni_fgetc(uni_file_t *uf);
@

<<Unicode I/O functions>>=
int32_t uni_fgetc(uni_file_t *uf)
{
  if(!uf->enctype) {
    return getc(uf->f);
#ifdef UNI_USE_ICONV
  } else if(uf->enctype == 1) {
    <<Return a character using iconv>>
#endif
  }
  <<Return a Unicode character>>
}
@

Rather than using the potentially flawed direct I/O routines, the
Unicode reader always reads raw bytes and then calls the decoder
instead.  That way, the readahead issue disappears.  The readahead
buffer is refilled whenever it has less than 4 characters, since all
encodings require at most that many.

<<Return a Unicode character>>=
if(uf->ralen < uf->raptr + 4 && (!uf->ralen || uf->ralen == UNIF_BUF_LEN)) {
  memcpy(uf->readahead, uf->readahead + uf->raptr, uf->ralen - uf->raptr);
  uf->ralen -= uf->raptr;
  uf->raptr = 0;
  uf->ralen += fread(uf->readahead + uf->ralen, 1, UNIF_BUF_LEN - uf->ralen, uf->f);
}
if(uf->raptr == uf->ralen)
  return -1;
int cp;
unsigned int l;
uint8_t *ra = (uint8_t *)uf->readahead + uf->raptr;
if(uf->enctype == 32) {
  cp = uni_utf32_decode((uint32_t *)ra, uf->bige);
  l = 4;
} else if(uf->enctype == 16) {
  cp = uni_utf16_decode((uint16_t *)ra, (uf->ralen - uf->raptr) / 2, &l, uf->bige);
  l *= 2;
} else
  cp = uni_utf8_decode(ra, uf->ralen - uf->raptr, &l);
if(l > uf->ralen - uf->raptr) {
  uf->ralen = uf->raptr = 0;
  return -2;
}
if(l < 4)
  movebuf(uf->readahead, uf->readahead + l, 4 - l);
uf->raptr += l;
return cp < 0 ? -2 : cp;
@

The [[iconv]] conversion proceeds much differently.  If a character is
availble in the output buffer, it validates and returns that character.
Otherwise, it fills up the input buffer from the file, and then
attempts to fill the output buffer using [[iconv]].

The [[iconv]] routine needs to be called one more time at end-of-file.
Detection of end-of-file is easy, but detection of whether or not this
last step has been done is not.  A flag is used to indicate this.

<<Unicode file buffer state>>=
#ifdef UNI_USE_ICONV
uint8_t did_ic_eof;
#endif
@

<<Return a character using iconv>>=
while(1) {
  if(uf->olen) {
    --uf->olen;
    uint32_t cp = uf->obuf[uf->optr++];
    if(cp > 0x10FFFF || (cp >= 0xD800 && cp < 0xE000))
      return -3;
    return cp;
  }
  /* if not at end of input, refill buffer */
  if(!uf->ralen || uf->ralen == UNIF_BUF_LEN) {
    /* if more than UNIF_BUF_LEN needed for a pass of iconv, this is screwed */
    if(uf->ralen && !uf->raptr) {
      errno = ENOSPC;
      return -2;
    }
    memmove(uf->readahead, uf->readahead + uf->raptr, uf->ralen - uf->raptr);
    uf->ralen += fread(uf->readahead + uf->ralen, 1, UNIF_BUF_LEN - uf->ralen, uf->f);
    if(!uf->ralen) /* len was 0, and 0 additional bytes read: EOF @start */
      return -1; /* assume iconv wouldn't return anything either */
    uf->ralen -= uf->raptr;
    uf->raptr = 0;
  }
  char *obuf = (char *)uf->obuf;
  size_t olen = UNIF_BUF_LEN;
  if(uf->ralen == uf->raptr) {
    if(uf->did_ic_eof)
      return -1;
    uf->did_ic_eof = 1;
    if(!uf->ralen)
      uf->ralen = uf->raptr = 1; /* avoid reading again */
    if(iconv(uf->ic, NULL, NULL, &obuf, &olen) < 0)
      return -2;
  } else {
    char *ibuf = uf->readahead + uf->raptr;
    size_t ilen = uf->ralen - uf->raptr;
    if(iconv(uf->ic, &ibuf, &ilen, &obuf, &olen) < 0 &&
       ibuf == uf->readahead + uf->raptr) {
      uf->raptr++;
      return -2;
    }
    uf->raptr = uf->ralen - ilen;
  }
  uf->optr = 0;
  uf->olen = UNIF_BUF_LEN - olen;
}
@

\section{Internal Storage}

The previous section assumed that internal storage is in 32-bit words,
or raw UTF-32.  However, this is not the best storage method for all
applications.  The other two encoding techniques have their advantages
as well.

UTF-32 is easy to index and compute the length.  However, it always
wastes at least 11 bits of storage per character.  UTF-16 and UTF-8
require a scan of the entire string to find an index or compute the
length.  However, they only waste 11 bits in the worst case, and more
commonly (especially in western scripts) waste much less.  In order to
compensate for the length problem, two separate lengths could be
stored for every string: the number of words stored, and the actual
string length.  In order to compensate for the index problem, if the
index is required frequently, conversion to UTF-32 is recommended. Use
of a side array storing indices would take just as much space, and
offer no real advantage other than slightly less cost in computing
each entry; that cost would likely be repaid every time the character
at a particular index is actually retrieved.  It would be possible to
save space in a side array by storing only a single bit per word
flagging all but the last word of each multi-word sequence, thus
reducing the scan of the entire string down to a scan of the bit
array, but again, just converting to UTF-32 is probably best:

<<Scan continuation array>>=
/* pseudo code */
int find_offset(s, n, flags)
{
  int m = 0, c = 0;
  while(1) {
    int b = count_bits(flags, m .. n - 1);
    if(!b)
      return n;
    m = n + 1;
    n += b;
  }
}

int offset_of(s, n, flags)
{
  return n - count_bits(flags, 0 .. n - 1);
}
@

Nonetheless, for raw storage, functions are provided to compute the
offset of the first word of a character, and the character offset of a
given word offset.  No checks are made to see if the string is valid
is intended for internally-generated strings.  The UTF-32 versions are
pointless, except when creating generic code that constructs function
names based on size.

<<Unicode I/O Exports>>=
/** Return the offset in \p buf, in raw words, of Unicode code point \p n.
  * The encoding of \p buf is UTF-32.  Note that this is a
  * preprocessor macro */
unsigned int uni_utf32_offset_of(const uint32_t *buf, unsigned int n);
#define uni_utf32_offset_of(b, n) (n)
/** Return the number of valid Unicode code points before \p buf[\p n].
  * The encoding of \p buf is UTF-32.  Note that this is a preprocessor
  * macro */
unsigned int uni_utf32_index_of(const uint32_t *buf, unsigned int n);
#define uni_utf32_index_of(b, n) (n)
/** Return the offset in \p buf, in raw words, of Unicode code point \p n.
  * The encoding of \p buf is native-endian UTF-16.  Results are
  * undefined if the first \p n - 1 code points in \p buf are not valid
  * UTF-16 */
unsigned int uni_utf16_offset_of(const uint16_t *buf, unsigned int n);
/** Return the number of valid Unicode code points before \p buf[\p n].
  * The encoding of \p buf is native-endian UTF-16.  Results are
  * undefined if the first \p n - 1 members of \p buf are not valid
  * UTF-16 */
unsigned int uni_utf16_index_of(const uint16_t *buf, unsigned int n);
/** Return the offset in \p buf, in raw words, of Unicode code point \p n.
  * The encoding of \p buf is UTF-8.  Results are undefined if the first
  * \p n - 1 code points in \p buf are not valid UTF-8 */
unsigned int uni_utf8_offset_of(const uint8_t *buf, unsigned int n);
/** Return the number of valid Unicode code points before \p buf[\p n].
  * The encoding of \p buf is native-endian UTF-16.  Results are
  * undefined if the first \p n - 1 members of \p buf are not valid
  * UTF-8 */
unsigned int uni_utf8_index_of(const uint8_t *buf, unsigned int n);
@

<<Unicode I/O functions>>=
unsigned int uni_utf16_offset_of(const uint16_t *buf, unsigned int n)
{
  unsigned int i, j;
  for(i = j = 0; i < n; i++, j++)
    if(buf[j] >= 0xd800 && buf[j] < 0xdc00)
      j++;
  return j;
}

unsigned int uni_utf16_index_of(const uint16_t *buf, unsigned int n)
{
  unsigned int i, j;
  for(i = j = 0; j < n; i++, j++)
    if(buf[j] >= 0xd800 && buf[j] < 0xdc00)
      j++;
  return i;
}

unsigned int uni_utf8_offset_of(const uint8_t *buf, unsigned int n)
{
  unsigned int i, j;
  for(i = j = 0; i < n; i++, j++)
    if(buf[j] >= 0x80)
      j += !(buf[j] & 0x20) ? 1 : !(buf[j] & 0x10) ? 2 : 3;
  return j;
}

unsigned int uni_utf8_index_of(const uint8_t *buf, unsigned int n)
{
  unsigned int i, j;
  for(i = j = 0; j < n; i++, j++)
    if(buf[j] >= 0x80)
      j += !(buf[j] & 0x20) ? 1 : !(buf[j] & 0x10) ? 2 : 3;
  return i;
}
@

A modified version of the above index functions can be used to compute
the length of a zero-terminated string.  Only the loop end condition
needs to change.  Unlike the above routines, a 32-bit version is
useful as well.

<<Unicode I/O Exports>>=
/** Compute the number of Unicode code points in zero-terminated \p buf */
unsigned int uni_utf32_strlen(const uint32_t *buf);
/** Compute the number of Unicode code points in zero-terminated \p buf.
  * Results are undefined if any code code points are not valid
  * native-endian UTF-16 */
unsigned int uni_utf16_strlen(const uint16_t *buf);
/** Compute the number of Unicode code points in zero-terminated \p buf.
  * Results are undefined if any code points are not valid UTF-8 */
unsigned int uni_utf8_strlen(const uint8_t *buf);
@

<<Unicode I/O functions>>=
unsigned int uni_utf32_strlen(const uint32_t *buf)
{
  unsigned int i;
  for(i = 0; buf[i]; i++);
  return i;
}

unsigned int uni_utf16_strlen(const uint16_t *buf)
{
  unsigned int i, j;
  for(i = j = 0; buf[j]; i++, j++)
    if(buf[j] >= 0xd800 && buf[j] < 0xdc00)
      j++;
  return i;
}

unsigned int uni_utf8_strlen(const uint8_t *buf)
{
  unsigned int i, j;
  for(i = j = 0; buf[j]; i++, j++)
    if(buf[j] >= 0x80)
      j += !(buf[j] & 0x20) ? 1 : !(buf[j] & 0x10) ? 2 : 3;
  return i;
}
@

To skip around in a string, simply avoid all continuation characters.
Macros are provided to make this as simple and efficient as possible. 
The UTF-16 macros are not much less efficient, if at all, if called
either at the start or the end of a multi-word character.  However,
the utf-8 macro for the possibility of a middle of a word is more
complex, and I prefer things to be orthogonal, so the next/prev macros
assume the argument points to the start of a character, and an extra
macro finds the start of a character.  Each takes a buffer pointer,
and returns the number to add to the buffer pointer to get to the
chosen destination.  For consistency, UTF-32 functions are provided as
well.

<<Unicode I/O Exports>>=
/** Find the offset to the start of a Unicode code point pointed to by \p buf.
  * The encoding of \p buf is UTF-32.  This is a preprocessor macro */
int uni_utf32_startc(const uint32_t *buf);
#define uni_utf32_startc(buf) 0
/** Find the offset to the next Unicode code point pointed to by \p buf.
  * The encoding of \p buf is UTF-32.  This is a preprocessor macro */
int uni_utf32_nextc(const uint32_t *buf);
#define uni_utf32_nextc(buf) 1
/** Find the offset to the Unicode code point prior to that pointed to by \p buf.
  * The encoding of \p buf is UTF-32.  This is a preprocessor macro */
int uni_utf32_prevc(const uint32_t *buf);
#define uni_utf32_prevc(buf) -1
/** Find the offset to the start of a Unicode code point pointed to by \p buf.
  * The encoding of \p buf is native-endian UTF-16.  This is a preprocessor macro;
  * \p buf may be evaluated more than once */
int uni_utf16_startc(const uint16_t *buf);
#define uni_utf16_startc(buf) (*(buf) >= 0xdc00 && *(buf) < 0xe000 ? -1 : 0)
/** Find the offset to the next Unicode code point pointed to by \p buf.
  * The encoding of \p buf is native-endian UTF-16.  This is a preprocessor macro;
  * \p buf may be evaluated more than once */
int uni_utf16_nextc(const uint16_t *buf);
#define uni_utf16_nextc(buf) (*(buf) < 0xd800 || *(buf) >= 0xdc00 ? 1 : 2)
/** Find the offset to the Unicode code point prior to that pointed to by \p buf.
  * The encoding of \p buf is native-endian UTF-16.  This is a preprocessor macro;
  * \p buf may be evaluated more than once */
int uni_utf16_prevc(const uint16_t *buf);
#define uni_utf16_prevc(buf) ((buf)[-1] < 0xd800 || (buf)[-1] >= 0xe000 ? -1 : -2)
/** Find the offset to the start of a Unicode code point pointed to by \p buf.
  * The encoding of \p buf is UTF-8.  This is a preprocessor macro;
  * \p buf may be evaluated more than once */
int uni_utf8_startc(const uint8_t *buf);
#define uni_utf8_startc(buf) (*(buf) < 0x80 || (*(buf) & 0x40) ? 0 : \
                               ((buf)[-1] & 0x40) ? -1 : \
			       ((buf)[-2] & 0x40) ? -2 : -3)
/** Find the offset to the next Unicode code point pointed to by \p buf.
  * The encoding of \p buf is UTF-8.  This is a preprocessor macro;
  * \p buf may be evaluated more than once */
int uni_utf8_nextc(const uint8_t *buf);
#define uni_utf8_nextc(buf) (*(buf) < 0x80 ? 1 : !(*(buf) & 0x20) ? 2 : \
                              !(*(buf) & 0x10) ? 3 : 4)
/** Find the offset to the Unicode code point prior to that pointed to by \p buf.
  * The encoding of \p buf is UTF-8.  This is a preprocessor macro;
  * \p buf may be evaluated more than once */
int uni_utf8_prevc(const uint8_t *buf);
#define uni_utf8_prevc(buf) (uni_utf8_startc((buf) - 1) - 1)
@

There is some performance benefit to decoding UTF-16 and UTF-8 without
error checking.  To emphasize that these are only for internally
generated, guaranteed valid strings, the functions to do this have
``valid'' in their name.  In addition, since errors are never
returned, the return value is unsigned.

<<Unicode I/O Exports>>=
/** Returns decoded Unicode code point at \p s.
 *  The encoding at \p s is UTF-16BE if \p bige is true, and UTF-16LE
 *  otherwise.  The number of words read are returned in \p nread, if non-NULL.
 *  Results are undefined if \p s does not point to valid UTF-16.
 *  Use \ref uni_utf16_decode if this is in question */
uint32_t uni_valid_utf16_decode(const uint16_t *s, unsigned int *nread, int bige);
/** Returns decoded Unicode code point at \p s.
 *  The encoding at \p s is UTF-8.  The number of bytes read are returned
 *  in \p nread, if non-NULL.  Results are undefined if \p s does not
 *  point to valid UTF-16. Use \ref uni_utf8_decode if this is in question */
uint32_t uni_valid_utf8_decode(const uint8_t *s, unsigned int *nread);
@

<<Unicode I/O functions>>=
uint32_t uni_valid_utf16_decode(const uint16_t *s, unsigned int *nread, int bige)
{
  uint16_t w = bige ? be16toh(*s) : le16toh(*s);
  if(w < 0xd800 || w >= 0xe000) {
    if(nread)
      *nread = 1;
    return w;
  }
  if(nread)
    *nread = 2;
  uint16_t w2 = bige ? be16toh(s[1]) : le16toh(s[1]);
  return (w2 & 0x3ff) + ((uint32_t)(w & 0x3ff) << 10) + 0x10000;
}

uint32_t uni_valid_utf8_decode(const uint8_t *s, unsigned int *nread)
{
  if(*s < 0x80) {
    if(nread)
      *nread = 1;
    return *s;
  }
  uint32_t c = *s++, ec;
  if((c & 0xf8) == 0xf0) {
    c &= 0x07;
    ec = *s++;
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if(nread)
      *nread = 4;
    c = (c << 6) + (ec & 0x3f);
    return c;
  } else if((c & 0xf0) == 0xe0) {
    c &= 0x0f;
    ec = *s++;
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if(nread)
      *nread = 3;
    c = (c << 6) + (ec & 0x3f);
    return c;
  } else /* if((c & 0xe0) == 0xc0) */ {
    c &= 0x1f;
    ec = *s++;
    if(nread)
      *nread = 2;
    c = (c << 6) + (ec & 0x3f);
    return c;
  }
}
@

Actually, since internal UTF-32 has the underlying architecture's
natural endianness, the UTF-16 internal routines should use this as
well.  These have ``int'' in the name to distinguish themselves.  For
consistency, UTF-8 and UTF-32 versions of these functions are
provided, as well.  Since all but the UTF-8 routines are simple, they
are all in-lined.  The UTF-16 versions do this by being a static
function.

<<Unicode I/O Exports>>=
/** Encodes Unicode code point \p cp in native-endian UTF-32 at \p buf.
 *  Returns number of words written.  This is a preprocessor macro */
int uni_int_utf32_encode(uint32_t *buf, uint32_t cp);
#define uni_int_utf32_encode(buf, cp) ((*(buf) = cp), 1)
#ifdef __GNUC__
__attribute__((unused))
#endif
static int uni_int_utf16_encode(uint16_t *buf, uint32_t cp)
{
  if(cp < 0x10000) {
    *buf = cp;
    return 1;
  }
  cp -= 0x10000;
  *buf++ = 0xd800 + (cp >> 10);
  *buf = 0xdc00 + (cp & 0x3ff);
  return 2;
}
/** Encodes Unicode code point \p cp in native-endian UTF-16 at \p buf.
  * Ensure that at least 2 words are available in \p buf, or use
  * \ref uni_utf16_enclen to size the buffer first.  Returns number
  * of words written */
int uni_int_utf16_encode(uint16_t *buf, uint32_t cp);
/** Encodes Unicode code point \p cp into \p buf using UTF-8.
  * Returns number of bytes generated.  Ensure that at least 4 bytes are
  * available in \p buf, or use \ref uni_utf8_enclen to size the buffer
  * first.  Invalid Unicode code points will not be encoded */
int uni_int_utf8_encode(uint8_t *buf, uint32_t cp);
#define uni_int_utf8_encode uni_utf8_encode
@

<<Unicode I/O Exports>>=
#ifdef __GNUC__
__attribute__((unused))
#endif
static uint32_t uni_int_utf16_decode(const uint16_t *s, unsigned int *nread)
{
  uint16_t w = *s;
  if(w < 0xd800 || w >= 0xe000) {
    if(nread)
      *nread = 1;
    return w;
  }
  if(nread)
    *nread = 2;
  return (s[1] & 0x3ff) + ((uint32_t)(w & 0x3ff) << 10) + 0x10000;
}
/** Returns decoded Unicode code point at \p s.
 *  The encoding at \p s is native-endian UTF-16.  Results are undefined
 *  if \p s does not point to valid UTF-16.  The number of words read
 *  are returned in \p nread if non-NULL */
uint32_t uni_int_utf16_decode(const uint16_t *s, unsigned int *nread);
/* gcc warns (-Waddress) if nr is constant */
/* #define uni_int_utf32_decode(buf, nr) ((nr) ? (*(int *)(nr) = 1), *(buf) : *(buf)) */
#ifdef __GNUC__
__attribute__((unused))
#endif
static uint32_t uni_int_utf32_decode(const uint32_t *s, unsigned int *nread)
{
  if(nread)
    *nread = 1;
  return *s;
}
/** Returns decoded Unicode code point at \p s.
 *  The encoding at \p s is native-endian UTF-32.  The number of words read
 *  are returned in \p nread if non-NULL */
uint32_t uni_int_utf32_decode(const uint32_t *s, unsigned int *nread);
/** Returns decoded Unicode code point at \p s.
 *  The encoding at \p s is UTF-8.  The number of bytes read are returned
 *  in \p nread, if non-NULL.  Results are undefined if \p s does not
 *  point to valid UTF-16. Use \ref uni_utf8_decode if this is in question */
uint32_t uni_int_utf8_decode(const uint8_t *s, unsigned int *nread);
#define uni_int_utf8_decode uni_valid_utf8_decode
@

<<C Prototypes>>=
/* Repeat, but necessary since build.nw doesn't skip statics */
int uni_int_utf16_encode(uint16_t *buf, uint32_t cp);
uint32_t uni_int_utf16_decode(const uint16_t *s, unsigned int *nread);
uint32_t uni_int_utf32_decode(const uint32_t *s, unsigned int *nread);
@

All functions developed from this point forward which take only one
character for input always take UTF-32; since they would need to do
the translation anyway, it is no burden to require the calling of one
of the above two functions to decode the other formats.  Functions
which take more than one character for input should also take
(guaranteed) valid UTF-16 and UTF-8.  Similarly, functions which
return just one code point always return UTF-32, but those which
return more than one can return UTF-16 or UTF-8 as well.  Returning
UTF-32 for single characters eliminates the need for output buffer
management.

There are at least three different ways to return the potentially
multi-character results: return a pointer to a constant string, fill
in a provided return buffer of fixed size (up to its length), or fill
in a provided buffer of variable size (expanding using [[realloc]] if
necessary).  Since constant strings are only one format, they can
really only be used internally.  The choice between the other two
depends largely on how much effort is required to compute the results
repeatedly:  the function must be called twice if the return buffer is
too small.  Of course if memory management makes the use of
[[realloc]] inappropriate, forcing the third method is not a good
idea, either.  So, for consistency, a hybrid of the latter two methods
is used.  Each output buffer is passed in using three parameters: a
pointer to a pointer to the buffer, an integer offset to where the
results should be placed (appended), and a pointer to the current
buffer length.  If the offset is less than zero, the buffer is never
expanded.  As a special shortcut, a fixed-length buffer of size zero
can be indicated with a buffer length pointer of [[NULL]], and the
zero-length buffer need not be a valid pointer, either.  Rather than
use my [[resize]] macro for this, the function sets the output buffer
to [[NULL]] on memory allocation errors.  The number of words which
would have been returned, had the buffer been large enough, is
returned. The following helpers are provided to implement this policy.

<<Buffer return parameters for UTF-(@sz)>>=
uint<<@sz>>_t **buf, int off, unsigned int *buf_len
@

<<[[uni_returnN_buf]](@sz) Prototype>>=
/** Helper function to return UTF-<<@csz>> data \p str/\p len into a UTF-<<@sz>> buffer.
  * If \p off is less than zero, \p buf points to a pointer to a buffer of length
  * \p *buf_len.  The buffer need not be long enough to hold the results.
  * If \p buf or \p buf_len are NULL, a zero-length buffer is assumed.
  * If \p off is greater than or equal to zero, \p buf points to a pointer
  * to a dynamically resizable buffer whose current size is \p *buf_len.
  * In the latter case, \p *buf will be made at least large enough to hold
  * the result (possibly larger), and the result will be placed at offset
  * \p off from the start of the buffer.  If there is insufficient memory,
  * \p *buf will be set to NULL (and freed if it was not already NULL).
  * This function returns the number of <<@sz>>-bit words it would have
  * returned had the buffer been large enough */
uint32_t uni_return<<@csz>>_buf<<@sz>>(const uint<<@csz>>_t *str, uint32_t len,
                                <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<[[uni_return]](@csz)[[_bufN]] Prototypes>>=
<<[[uni_returnN_buf]][[32]] Prototype>>;
<<[[uni_returnN_buf]][[16]] Prototype>>;
<<[[uni_returnN_buf]][[8]] Prototype>>;
@

<<Unicode I/O Exports>>=
<<[[uni_return]][[32]][[_bufN]] Prototypes>>
<<[[uni_return]][[16]][[_bufN]] Prototypes>>
<<[[uni_return]][[8]][[_bufN]] Prototypes>>
@

<<Unicode I/O functions>>=
<<[[uni_return]][[32]][[_bufN]]>>
<<[[uni_return]][[16]][[_bufN]]>>
<<[[uni_return]][[8]][[_bufN]]>>
@

<<[[uni_return]](@csz)[[_bufN]]>>=
<<[[uni_returnN_buf]][[32]]>>
<<[[uni_returnN_buf]][[16]]>>
<<[[uni_returnN_buf]][[8]]>>
@

<<[[uni_returnN_buf]](@sz)>>=
<<[[uni_returnN_buf]][[<<@sz>>]] Prototype>>
{
  unsigned int rlen;
  
  /* not very efficient, I guess: always calculate output len first */
  <<Calculate result length [[rlen]] for [[str]] of length [[len]]>>
  /* now, store into output buffer */
  if(off < 0) {
    <<Store result into constant-length [[*buf]] of length [[*buf_len]]>>
    return rlen;
  }
  <<Store result into resizable [[*buf]] of length [[*buf_len]] at [[off]]>>
  return rlen;
}
@

<<Calculate result length [[rlen]] for [[str]] of length [[len]]>>=
#if <<@csz>> == <<@sz>>
rlen = len;
#else
unsigned int i;
for(i = rlen = 0; i < len; ) {
  unsigned int clen;
#ifdef __GNUC__ /* shut up gcc warning for UTF-32 */
  __attribute__((unused))
#endif
  uint32_t c = uni_int_utf<<@csz>>_decode(str + i, &clen);
  i += clen;
  rlen += uni_utf<<@sz>>_enclen(c);
}
#endif
@

<<Store result into constant-length [[*buf]] of length [[*buf_len]]>>=
unsigned int blen = buf_len ? *buf_len : 0;
len = blen;
if(len > rlen)
  len = rlen;
if(!len)
  return rlen;
uint<<@sz>>_t *bptr = *buf;
#if <<@csz>> == <<@sz>>
cpybuf(bptr, str, len);
#else
while(len) {
  unsigned int clen;
  uint32_t c = uni_int_utf<<@csz>>_decode(str, &clen);
  str += clen;
  if(blen >= 4) {
    int csz = uni_int_utf<<@sz>>_encode(bptr, c);
    bptr += csz;
    len -= csz;
    blen -= csz;
  } else {
    uint<<@sz>>_t cbuf[4];
    int csz = uni_int_utf<<@sz>>_encode(cbuf, c);
    if(csz > len)
      csz = len;
    cpybuf(bptr, cbuf, csz);
    bptr += csz;
    len -= csz;
    blen -= csz;
  }
}
#endif
@

<<Store result into resizable [[*buf]] of length [[*buf_len]] at [[off]]>>=
/* allocate space if needed */
if(!*buf)
  *buf_len = 0; /* shouldn't be necessary */
else if(!*buf_len) { /* should never happen */
  free(*buf); /* will probably corrupt memory */
  *buf = NULL;
}
if(rlen + off > *buf_len) {
  if(!*buf_len) {
    unsigned int asize = off + rlen;
    if(asize < 5)
      asize = 5;
    *buf = malloc(asize * sizeof(**buf));
    if(!*buf)
      return rlen;
    *buf_len = asize;
  } else {
    while(*buf_len < rlen + off)
      *buf_len *= 2;
    uint<<@sz>>_t *nbuf = realloc(*buf, *buf_len * **buf);
    if(!nbuf) {
      free(*buf);
      *buf = NULL;
      return rlen;
    }
    *buf = nbuf;
  }
}
/* store result; sufficient space guaranteed */
uint<<@sz>>_t *bptr = *buf + off;
#if <<@sz>> == <<@csz>>
cpybuf(bptr, str, rlen);
#else
len = rlen;
while(len) {
  unsigned int clen;
  uint32_t c = uni_int_utf<<@csz>>_decode(str, &clen);
  str += clen;
  int csz = uni_int_utf<<@sz>>_encode(bptr, c);
  bptr += csz;
  len -= csz;
}
#endif
@

\chapter{Properties}

This library's primary purpose is to preparse the Unicode information
and query it efficiently.  The Unicode Character Database defines
numerous properties for characters.  Since there are many properties,
and any particular application may only need a few of them, an attempt
is made to keep each property in a separate object file.  Static
linking will only pull in the required properties, and there is little
penalty for shared libraries which include everything.  Some code is
shared by all properties, though; this is placed in a common file.

<<Library [[uni]] headers>>=
#include "uni_prop.h"
@

<<uni_prop.h>>=
<<Common C Warning>>
#ifndef UNI_PROP_H
#define UNI_PROP_H
/** \file uni_prop.h Unicode Properties */

#include "uni_io.h"
/** \addtogroup uni_prop Unicode Property Queries
    @{ */
<<Unicode property exports>>
/** @} */
#endif /* UNI_PROP_H */
@

<<Headers to Install>>=
uni/uni_prop.h \
@

\lstset{language=make}
<<makefile.rules>>=
uni_prop.h: uni_io.h
@

<<Library [[uni]] Members>>=
uni_prop.o
@

\lstset{language=C}
<<uni_prop.c>>=
<<Common C Header>>
#include "uni_prop.h"
// static_proto

<<Unicode property functions>>
@

Some Unicode property exports are generated, and some are hand-coded.
Some of the hand-coded exports may be useful for generation, so they
are in a separate code chunk, which can be included in the generator
without depending on the generated output. 

<<Unicode property exports>>=
<<Unicode property exports for generator>>
@

<<Unicode property functions>>=
<<Unicode property functions for generator>>
@

These properties come in a variety of formats, but for the purpose of
this library, they may be generally classified according to the type
of value:  Boolean, String, Enumerated, or Numeric.  Each class of
values has a particular set of desirable operations related to a
property of that class.

\section{Boolean Properties}

Boolean properties are either true or false.  They define a subset of
characters: those for which the value is true.  As with any set, the
desirable operations are:

% Begin-doc op-table
%% l2h substitution ominus &ominus;
% tidy doesn't understand &ominus
%% l2h substitution ominus &#8854;
% substitution replaces # with space, so use 0-arg macro instead
% l2h macro ominus 0 &##8854;
% l2h substitution oplus &oplus;
% l2h substitution wedge &and;
% l2h substitution vee &or;
% l2h substitution neg &not;
% l2h macro overline 1 <span style="text-decoration:overline">#1</span>
% l2h substitution emptyset &empty;
% l2h substitution alpha &alpha;
% l2h substitution in &isin;
% l2h substitution notin &notin;
% l2h substitution cap &cap;
% l2h substitution cup &cup;
% End-doc op-table

\begin{itemize}
\item Finding out if a character \emph{is} or is not an element of the
set.
\item Querying the \emph{members} of a set.
\item Applying set \emph{operators} on a pair of sets.  There are
16 possible set operations between two sets (each potential member is
either in or not in each of the 2 sets = $2^2$ membership
combinations; member is either in or not in result = $4^2$ total
combinations).  See table \ref{tab:setop} for a complete list of
operations.

% Begin-doc op-table
\begin{table}
\begin{quote}
\begin{tabular}{lllllcrll}
$x\in A$&false&false&true&true&&~&\\
$x\in B$&false&true&false&true&&&\\
\emph{op}&\multicolumn{4}{c}{$x\in A\textrm{ \emph{op} }B$}&\emph{when}&\multicolumn{2}{l}{\emph{\#}}&\emph{Op Name}\\
$\emptyset$&false&false&false&false&never&0&&NIL\\
$\alpha$&true&true&true&true&always&1&&ALL\\
$A$&false&false&true&true&$x\in A$&2&&A\\
$\overline A$&true&true&false&false&$x\notin A$&3&&INV\_A NOT\_A\\
$B$&false&true&false&true&$x\in B$&4&&B\\
$\overline B$&true&false&true&false&$x\notin B$&5&&INV\_B NOT\_B\\
$A\ominus B$&false&true&true&false&$(x\in A)\oplus (x\in B)$&6&&SYM\_DIFF XOR\\
$\overline{A\ominus B}$&true&false&false&true&$(x\in A)\overline\oplus (x\in B)$&7&&INV\_SYM\_DIFF XNOR\\
$A\cap B$&false&false&false&true&$(x\in A)\wedge (x\in B)$&8&&INTER AND\\
$\overline{A\cap B}$&true&true&true&false&$(x\in A)\overline\wedge (x\in B)$&9&&INV\_INTER NAND\\
$A-B$&false&false&true&false&$(x\in A)\wedge (x\notin B)$&10&&A\_MINUS\_B\\
$\overline{A-B}$&true&true&false&true&$(x\notin A)\vee (x\in B)$&11&&INV\_A\_MINUS\_B\\
$B-A$&false&true&false&false&$(x\notin A)\wedge (x\in B)$&12&&B\_MINUS\_A\\
$\overline{B-A}$&true&false&true&true&$(x\in A)\vee (x\notin B)$&13&&INV\_B\_MINUS\_A\\
$A\cup B$&false&true&true&true&$(x\in A)\vee (x\in B)$&14&&UNION OR\\
$\overline{A\cup B}$&true&false&false&false&$(x\in A)\overline\vee (x\in B)$&15&&INV\_UNION NOR\\
\end{tabular}

{\small Operation numbers are assigned using the formula $o_0 \oplus
(x \in A)\wedge o_1 \oplus (x \in B)\wedge o_2 \oplus (x \in A\cap
B)\wedge o_3$, where $o_n$ is true if bit $n$ is one in the operation
number.  Operation names are of the enumeration type [[uni_set_op_t]],
and are prefixed with [[UNI_SOP_]].  Where two operation names are
given, either will work.}
\end{quote}
% End-doc op-table

\caption{\label{tab:setop}Set Operations}
\end{table}
\end{itemize}

\subsection{Storage Methods}

All of the above require that every member be stored.  The \emph{is}
test can be stored in a single bit.  However, there are over a million
valid character values, so the total storage for a simple bit array
would still be over 128 kilobytes, even if the array is limited to the
valid code point range.  Some properties only apply to a limited range
of characters, so these might use a little less space.  Even so, with
numerous properties, this adds up quickly, and may even slow things
down by keeping everything out of the cache.  The set
\emph{operations} are easy to perform, but require manipulating up to
384 kilobytes of data.

<<Unicode property exports for generator>>=
#define UNI_MAX_CP 0x10ffff /**< Maximum valid Unicode code point */
/** \addtogroup uni_prop_bitarrays Generic Bit Array Access
    @{ */
/** Find bit array \p a's element index from bit number \p i */
#define UNI_BSET_ELT(a, i) ((i)/(sizeof(*(a))*8))
/** Find bit array \p a's element from bit number \p i.
  * This is an lval with an address */
#define UNI_BSET_AELT(a, i) (a)[UNI_BSET_ELT(a, i)]
/** Find bit array \p a's element's bit number from full bit number \p i */
#define UNI_BSET_BIT(a, i) ((i)%(sizeof(*(a))*8))
/** Find full bit number given bit array \p a's element index \p e and
 ** element bit number \p b */
#define UNI_BSET_ENTRY(a, e, b) ((e)*(sizeof(*(a))*8) + b)
/** Convert full bit number \p i into single-bit mask for bit array \p a's element */
#define UNI_BSET_MASK(a, i) (1ULL << UNI_BSET_BIT(a, i))
/** True if full bit number \p i is set in bit array \p a */
#define UNI_BSET_IS_SET(a, i) ((UNI_BSET_AELT(a, i) & UNI_BSET_MASK(a, i)) != 0)
/** Set bit number \p i in bit array \p a */
#define UNI_BSET_SET(a, i) UNI_BSET_AELT(a, i) |= UNI_BSET_MASK(a, i)
/** Clear bit number \p i in bit array \p a */
#define UNI_BSET_CLEAR(a, i) UNI_BSET_AELT(a, i) &= ~UNI_BSET_MASK(a, i)
@

<<C Prototypes>>=
uint_ei_type UNI_BSET_ELT(uint_e_type *a, uint_i_type i);
lval uint_e_type UNI_BSET_AELT(uint_e_type *a, uint_i_type i);
uint_b_type UNI_BSET_BIT(uint_e_type *a, uint_i_type i);
uint_i_type UNI_BSET_ENTRY(uint_e_type *a, uint_ei_type e, uint_b_type b);
uint_e_type UNI_BSET_MASK(uint_e_type *a, uint_i_type i);
int UNI_BSET_IS_SET(uint_e_type *a, uint_i_type i);
void UNI_BSET_SET(uint_e_type *a, uint_i_type i);
void UNI_BSET_CLEAR(uint_e_type *a, uint_i_type i);
@

<<[[uni_setop_t]]>>=
/** Logical operation names for \ref UNI_BIT_SET_OP */
typedef enum {
  UNI_SOP_NIL, /**< \f$\emptyset\f$ */ UNI_SOP_ALL, /**< \f$\alpha\f$ */
  UNI_SOP_A, /**< \f$A\f$ */ UNI_SOP_INV_A, /**< \f$\overline A\f$ */
  UNI_SOP_NOT_A = UNI_SOP_INV_A, /**< \f$\overline A\f$ */
  UNI_SOP_B, /**< \f$B\f$ */ UNI_SOP_INV_B, /**< \f$\overline B\f$ */
  UNI_SOP_NOT_B = UNI_SOP_INV_B, /**< \f$\overline B\f$ */
  UNI_SOP_SYM_DIFF, /**< \f$A\ominus B\f$ **/
  UNI_SOP_XOR = UNI_SOP_SYM_DIFF, /**< \f$A\ominus B\f$ **/
  UNI_SOP_INV_SYM_DIFF, /**< \f$\overline{A\ominus B}\f$ **/
  UNI_SOP_XNOR = UNI_SOP_INV_SYM_DIFF, /**< \f$\overline{A\ominus B}\f$ **/
  UNI_SOP_INTER, /**< \f$A\cap B\f$ */ UNI_SOP_AND = UNI_SOP_INTER, /**< \f$A\cap B\f$ */
  UNI_SOP_INV_INTER, /**< \f$\overline{A\cap B}\f$ */
  UNI_SOP_NAND = UNI_SOP_INV_INTER, /**< \f$\overline{A\cap B}\f$ */
  UNI_SOP_A_MINUS_B, /**< \f$A-B\f$ */
  UNI_SOP_INV_A_MINUS_B, /**< \f$\overline{A-B}\f$ */
  UNI_SOP_B_MINUS_A, /**< \f$B-A\f$ */
  UNI_SOP_INV_B_MINUS_A, /**< \f$\overline{B-A}\f$ */
  UNI_SOP_UNION, /**< \f$A\cup B\f$ */
  UNI_SOP_OR = UNI_SOP_UNION, /**< \f$A\cup B\f$ */
  UNI_SOP_INV_UNION, /**< \f$\overline{A\cup B}\f$ */
  UNI_SOP_NOR = UNI_SOP_INV_UNION /**< \f$\overline{A\cup B}\f$ */
} uni_set_op_t;
@

<<Unicode property exports for generator>>=
<<[[uni_setop_t]]>>
#define UNI_BIT_SET_OP_BIT_(op, n) (~((((op) >> n) & 1) - 1))
/** Perform a set operation between unsigned integers \p A and \p B.
  * The operations (\p op) are defined by \ref uni_set_op_t */
#define UNI_BIT_SET_OP(A, op, B) \
  (((A) & UNI_BIT_SET_OP_BIT_(op, 1)) ^ ((B) & UNI_BIT_SET_OP_BIT_(op, 2)) ^ \
   ((A) & (B) & UNI_BIT_SET_OP_BIT_(op, 3)) ^ UNI_BIT_SET_OP_BIT_(op, 0))
/** @} */
@

<<C Prototypes>>=
uint_t UNI_BIT_SET_OP(uint_t A, uni_set_op_t op, uint_t B);
@

<<Known Data Types>>=
uni_set_op_t,%
@

The set \emph{operations} must be performed on arrays of a particular
type.  Since C does not really support generics, and using the
preprocessor would be impractical, the noweb parameterized macro
facility is used.  Previous versions of this document wrote the
function definition to an include file, and used the preprocessor to
redefine types and names for four separate [[#include]]s.  That is
probably the only way to get debugable code using the preprocessor.

<<Unicode property exports for generator>>=
#include <stdint.h>
@

<<Unicode property exports>>=
/** \addtogroup uni_prop_bitarrays
    @{ */
/* uint64_t is not standard, but common */
<<[[uni_setop_bit]] Prototype for [[64]]-bit integers>>;
<<[[uni_setop_bit]] Prototype for [[32]]-bit integers>>;
<<[[uni_setop_bit]] Prototype for [[16]]-bit integers>>;
<<[[uni_setop_bit]] Prototype for [[8]]-bit integers>>;
/** @} */
@

<<[[uni_setop_bit]] Prototype for (@sz)-bit integers>>=
/** Perform set operations on sets described by bit arrays.
  * The bit arrays \p a and \p b map bit 0 to \p a_low and \p b_low,
  * respectively.  The array lengths are \p a_len and \p b_len,
  * respectively, making their bit lengths \f$<<@sz>>\cdot\texttt{a\_len}\f$
  * and  \f$<<@sz>>\cdot\texttt{b\_len}\f$, respectively.  In each set,
  * a one bit indicates membership, and a zero does not.  The result is
  * stored in dynamically resizable \p *res, whose currently allocted size
  * is \p *res_max.  The result's starting bit is stored in \p *res_low
  * and its array size, in <<@sz>>-bit words, is in \p *res_len */
void uni_setop_bit<<@sz>>(uint<<@sz>>_t *a, uint32_t a_low, uint32_t a_len,
                        uni_set_op_t op,
		        uint<<@sz>>_t *b, uint32_t b_low, uint32_t b_len,
		        uint<<@sz>>_t **res, uint32_t *res_low, uint32_t *res_len,
		        uint32_t *res_max)
@

<<Unicode property functions>>=
<<[[uni_setop_bit]] for [[64]]-bit integers>>
<<[[uni_setop_bit]] for [[32]]-bit integers>>
<<[[uni_setop_bit]] for [[16]]-bit integers>>
<<[[uni_setop_bit]] for [[8]]-bit integers>>
@

<<[[uni_setop_bit]] for (@sz)-bit integers>>=
<<[[uni_setop_bit]] Prototype for [[<<@sz>>]]-bit integers>>
{
  <<Perform set operation on bit strings>>
}
@

First, there are few degenerate cases which do not involve [[a]]
and/or [[b]].  While the general routine will return the same result,
the logic for the degenerate cases is much simpler, so a value can be
returned immediately.  The $\overline A$ and $\overline B$ operations
are degenerate cases by this definition, but they require a bit of
finesse that is provided by the main routine, anyway.  So, instead of
duplicating that code here, the unused input is simply zeroed to
improve its performance.

<<Perform set operation on bit strings>>=
if(op == UNI_SOP_INV_A)
  b_len = 0;
else if(op == UNI_SOP_INV_B)
  a_len = 0;
@

If both inputs are empty, the operation is determined by applying the
operator to two zeroes.  This will always match bit zero of the
operation number, and will be equivalent to applying $\emptyset$ or
$\alpha$.  If just one is empty, then that reduces the operations to
four: the above two, the non-empty set, or the inverse of the
non-empty set.  The operation number is adjusted to reflect this.

<<Perform set operation on bit strings>>=
int zz_op = /* UNI_BIT_SET_OP(0, op, 0) */ op & UNI_SOP_ALL;
if(!a_len && !b_len)
  op &= UNI_SOP_ALL;
else if(!a_len)
  op &= UNI_SOP_B | UNI_SOP_ALL;
else if(!b_len)
  op &= UNI_SOP_A | UNI_SOP_ALL;
@

The $\emptyset$ operation returns an empty result.  The $\alpha$
operation returns a result with all bits set.  The $A$ and $B$
operations just return copies of the requested input.

<<Perform set operation on bit strings>>=
if(op == UNI_SOP_NIL) {
  *res_len = 0;
  *res_low = 0;
  if(*res_max) {
    *res_max = 0;
    free(*res);
  }
  return;
}
if(op == UNI_SOP_ALL) {
  *res_len = UNI_MAX_CP / <<@sz>> + 1;
  *res_low = 0;
  check_size(*res, *res_max, *res_len);
  memset(*res, ~0, *res_len * <<@sz>> / 8);
  return;
}
if(op == UNI_SOP_A) {
  *res_len = a_len;
  *res_low = a_low;
  check_size(*res, *res_max, a_len);
  if(*res != a)
    cpybuf(*res, a, *res_len);
  return;
}
if(op == UNI_SOP_B) {
  *res_len = b_len;
  *res_low = b_low;
  check_size(*res, *res_max, b_len);
  if(*res != b)
    cpybuf(*res, b, *res_len);
  return;
}
@

The bit strings do not necessarily start at the same position.  For
convenience, [[a]] will be reassigned to be the set with the earlier
start position, or only start position if one of the inputs is empty.
If a swap occurs, the operation number must also be adjusted by
swapping the $A$ and $B$ bits.  In addition, if one input is empty, it
is moved past the end of the first so it is not involved in
calculations until the end.

<<Perform set operation on bit strings>>=
if(!a_len || b_low < a_low) {
  uint<<@sz>>_t *t = a;
  a = b;
  b = t;
  a_len ^= b_len;
  b_len ^= a_len;
  a_len ^= b_len;
  a_low ^= b_low;
  b_low ^= a_low;
  a_low ^= b_low;
  op = (op & ~(UNI_SOP_A|UNI_SOP_B)) | ((op & UNI_SOP_A) << 1) |
                                       ((op & UNI_SOP_B) >> 1);
}
if(!b_len)
  b_low = a_low + a_len * <<@sz>>;
@

The bit strings do not necessarily start at zero.  If neither does,
then the region before the bit strings is all zeroes, meaning that the
result will be all zeroes or all ones depending on [[zz_op]].  If it
is all zeroes, the start of the result can be moved up.  In fact, the
start of the result only needs to be set once ones start rolling in,
so a flag is kept to determine if a one has been encountered yet.

<<Perform set operation on bit strings>>=
uint32_t i;
int did_start = 0;

if(a_low > 0 && zz_op) {
  uint32_t need = a_low / <<@sz>>;
  did_start = 1;
  *res_low = 0;
  check_size(*res, *res_max, need);
  if(need > 0)
    memset(*res, ~0, need * sizeof(**res));
}
@

If the inputs do not start at the same place, there is an initial
region where only [[a]] has values other than zero.  Rather than
complicate the case where [[b]] is involved by checking for this case,
a separate loop is performed for this region.  If the result start was
set to zero above, the result may be misaligned with [[a]].  If so,
the bits from the previous operation are shifted into place.  Since
the above set all ones, the first previous value is also all ones.
While it is setting values, it also does not actually increase the
result size or set the value if it is zero.  Instead, it keeps track
of the last position that was set to zero after a non-zero set, so
that the result can be truncated with no excess zeroes.  Naturally,
when a new non-zero value comes along, the zeroes need to be filled in
after all.  The first non-zero value also sets the start position, if
it was not already set above.

<<Perform set operation on bit strings>>=
uint<<@sz>>_t cur_a = 0, prev_a, r;
uint32_t last_zero = 0;
int misalign_a = did_start ? (a_low - *res_low) % <<@sz>> : 0;
uint32_t end = a_low + a_len * <<@sz>>;
if(end < b_low)
  end = b_low;
for(; (i = a_low) < end - <<@sz>> + 1; a_low += <<@sz>>, a_len--, a++) {
  <<Obtain next aligned bits from source [[a]]>>
  r = UNI_BIT_SET_OP(prev_a, op, 0);
  <<Set res if non-zero or mark if zero>>
}
@

<<Obtain next aligned bits from source (@which)>>=
prev_<<@which>> = cur_<<@which>>;
cur_<<@which>> = *<<@which>>;
if(misalign_<<@which>>) {
  prev_<<@which>> |= cur_<<@which>> @<< misalign_<<@which>>;
  cur_<<@which>> >>= <<@sz>> - misalign_<<@which>>;
} else
  prev_<<@which>> = cur_<<@which>>;
@

<<Set res if non-zero or mark if zero>>=
if(!r) {
  if(!last_zero && did_start)
    last_zero = i - *res_low;
} else {
  if(!did_start) {
    did_start = 1;
    *res_low = i;
  }
  check_size(*res, *res_max, (i - *res_low) / <<@sz>>);
  if(last_zero) {
    memset(&UNI_BSET_AELT(*res, last_zero), 0, (i - *res_low - last_zero) / 8);
    last_zero = 0;
  }
  UNI_BSET_AELT(*res, i - *res_low) = r;
}
@

Now, either [[a]] is pointing past the end of the lower input, or it
is pointing at a word which contains the position [[b_low]].  In the
former case, there may be a(nother) region where both [[a]] and [[b]]
are not present.  If so, the region can be set to [[zz_op]] as before,
except for the first word if there was a misalignment.  In either
case, if there was a misalignment, [[cur_a]] contains some of the bits
to be applied to the next word.  Only full words which do not contain
[[b_low]] are processed here.

<<Perform set operation on bit strings>>=
if(misalign_a && i < b_low - <<@sz>> - 1) {
  r = UNI_BIT_SET_OP(cur_a, op, 0);
  <<Set res if non-zero or mark if zero>>
  i += <<@sz>>;
  cur_a = 0; /* a_len must be 0 */
}
if(!a_len) {
  misalign_a = 0; /* cur_a is aligned, and remaining 0s do not need align */
  if(did_start)
    i -= (i - *res_low) % <<@sz>>; /* align i with result instead of a */
}
if(i < b_low - <<@sz>> - 1) {
  uint32_t skip = (b_low - i) / <<@sz>>;
  skip *= <<@sz>>;
  if(!zz_op) {
    if(!last_zero && did_start)
      last_zero = i - *res_low;
    if(!did_start)
      i = b_low; /* if b causes a start, *res_low should be b_low */
    else
      i += skip;
  } else {
    <<Set gap to all ones>>
  }
}
@

<<Set gap to all ones>>=
if(!did_start) {
  did_start = 1;
  *res_low = i;
}
uint32_t tot = i - *res_low + skip;
check_size(*res, *res_max, tot / <<@sz>>);
if(last_zero) {
  memset(&UNI_BSET_AELT(*res, last_zero), 0, (i - *res_low - last_zero) / 8);
  last_zero = 0;
}
memset(&UNI_BSET_AELT(*res, i - *res_low), ~0, skip / 8);
i += skip;
@

Now the position is at the word containing [[b_low]], if [[b]] is
non-empty.  If [[a]] has not yet ended, or ended in the last word,
bits remain in [[cur_a]] for this word.  If it ended earlier than
that, [[cur_a]] is zero, so either way [[cur_a]] needs to be applied. 
Both [[b]] and [[a]] need to be aligned to the result before use.  If
the result has not yet started, it will start aligned with [[a]] if
[[a]] has not yet ended (i.e., [[i]] is an integral [[<<@sz>>]]
multiple following [[a_low]]); otherwise, due to the above code, it
will start aligned with [[b]] (i.e., [[i]] was set to [[b_low]]).
First any full words in common with both [[a]] and [[b]] are
processed; then whichever one has words left is processed.  Note that
it is not possible for [[a]] to have data left to process if [[b]] is
absent.

<<Perform set operation on bit strings>>=
if(b_len) {
  uint32_t misalign_b = did_start ? (b_low - *res_low) % <<@sz>> : 
                                    (b_low - i) % <<@sz>>;
  uint<<@sz>>_t cur_b = 0, prev_b;
  for(; a_len && b_len; b++, b_len--, b_low += <<@sz>>, i += <<@sz>>,
                        a++, a_len--, a_low += <<@sz>>) {
    <<Obtain next aligned bits from source [[b]]>>
    <<Obtain next aligned bits from source [[a]]>>
    r = UNI_BIT_SET_OP(prev_a, op, prev_b);
    <<Set res if non-zero or mark if zero>>
  }
  for(; b_len; b++, b_len--, b_low += <<@sz>>, i += <<@sz>>) {
    <<Obtain next aligned bits from source [[b]]>>
    r = UNI_BIT_SET_OP(cur_a, op, prev_b);
    <<Set res if non-zero or mark if zero>>
    cur_a = 0;
  }
  for(; a_len; a++, a_len--, a_low += <<@sz>>, i += <<@sz>>) {
    <<Obtain next aligned bits from source [[a]]>>
    r = UNI_BIT_SET_OP(prev_a, op, cur_b);
    <<Set res if non-zero or mark if zero>>
    cur_b = 0;
  }
  if(cur_a || cur_b) {
    r = UNI_BIT_SET_OP(cur_a, op, cur_b);
    <<Set res if non-zero or mark if zero>>
    i += <<@sz>>;
  }
}
@

Finally, if the upper set did not end at the last valid code point,
the upper region needs to be set the same way as the initial region.

<<Perform set operation on bit strings>>=
if(zz_op) {
  if(did_start)
    i -= (i - *res_low) % <<@sz>>; /* align i with result */
  if(i <= UNI_MAX_CP) {
    uint32_t skip = (UNI_MAX_CP - i) / <<@sz>> + 1;
    skip *= <<@sz>>;
    <<Set gap to all ones>>
  }
}
@

The only thing remaining is to update the length of the result.

<<Perform set operation on bit strings>>=
if(did_start)
  *res_len = ((last_zero ? last_zero : i) - *res_low) / <<@sz>>;
else
  *res_len = 0;
@

A plain bit array also makes it difficult to perform the
\emph{members} operation: every single potential member needs to be
tested.  Other storage strategies are needed.

<<Unicode property exports>>=
/* List members enumeration not directly supported */
@

<<Unicode property functions>>=
#if 0
  <<List bit array members>>
#endif
@

<<List bit array members>>=
int i; elt_t e;
for(i = 0; i < tab_size; i++) {
  e = tab[i];
  while(e) {
    /* strings.h usually selects optimal built-in implementation of ffs() */
    int b = ffs(e); /* or ffsl, or ffsll if glibc */
    add_ent(UNI_BSET_ENTRY(tab, i, b - 1));
    e &= ~(1 << (b - 1));
  }
}
@

One of the simplest strategies is to provide a sorted table of
members.  Membership is tested by binary search, so the speed of
lookup is proportional to $\log_2(N)$, where $N$ is the number of set
members.  Querying the \emph{members} of the set is trival: the table
is already the list.  Applying set \emph{operations} only requires
looking at the members already present, with the exception of the
odd-numbered ones:  inversion may be more expensive than with bit
arrays.  This does not save much space for properties which apply to
a large number of characters; in fact, it might even take more space.

<<Unicode property exports for generator>>=
/** Compare 32-bit signed integers pointed to by \p a and \p b.
  * Intended for use with qsort and bsearch */
int uni_cmp_cp(const void *a, const void *b);
@

<<Unicode property exports>>=
/** Returns true if Unicode code point \p c is in sorted array \p tab.
  * The length of \p tab is \p tab_len.  It is assumed that the
  * code point is contained in the first 32-bit signed integer in
  * \p tab's structure, and that \p tab is sorted by that element
  * as per \ref uni_cmp_cp */
#define uni_is_cp(c, tab, tab_len) \
  bsearch(&c, tab, tab_len, sizeof(tab[0]), uni_cmp_cp)

/* members are already in list */
/* set operations not directly supported */
@

<<C Prototypes>>=
int uni_is_cp(elt_t c, elt_t *tab, size_t tab_len);
@

<<Unicode property functions for generator>>=
int uni_cmp_cp(const void *a, const void *b)
{
  return *(int32_t *)a - *(int32_t *)b;
}
@

<<Unicode property functions>>=
#if 0
  <<Perform set ops on cp array>>
#endif
@

<<Perform set ops on cp array>>=
uint32_t i = 0;
uint32_t tab[max], tab_len = 0;
int invert = /* UNI_BIT_SET_OP(0, op, 0) */ op & 1;
uint32_t aptr, bptr;
for(aptr = bptr = 0; aptr < a_len || bptr < b_len; ) {
  int res;
  uint32_t cp;
  if(bptr == b_len || (aptr < a_len && a[aptr] < b[bptr])) {
    res = UNI_BIT_SET_OP(~0, op, 0);
    cp = a[aptr++];
  } else if(aptr == a_len || (bptr < b_len && a[aptr] > b[bptr])) {
    res = UNI_BIT_SET_OP(0, op, ~0);
    cp = b[bptr++];
  } else {
    res = UNI_BIT_SET_OP(~0, op, ~0);
    cp = a[aptr++];
    bptr++;
  }
  if(invert)
    while(i < cp)
      tab[tab_len++] = i++;
  if(res)
    tab[tab_len++] = cp;
}
if(invert)
  while(i < max)
    tab[tab_len++] = i++;
@

A possible improvement on that strategy is to store ranges instead of
individual code points.  This relies on the fact that most properties
apply to large consecutive ranges; if this is not true, then this is
actually worse than storing individual code points.  Whenever the
number of ranges is less than half of the number of individual code
points, this table will be smaller, requiring less time to search, and
less time to perform set \emph{operations}.  In particular, negation
is usually much faster with a range table than with an individual code
point table.  Not only can absent members be skipped, but present
members within a range can be skipped as well.

<<[[uni_chrrng_t]]>>=
/** A simple 64-bit range of 32-bit integers (Unicode code points) */
typedef struct {
    uint32_t low, /**< Low end of range */ high; /**< High end of range */
} uni_chrrng_t;
@

<<Unicode property exports for generator>>=
<<[[uni_chrrng_t]]>>
/** Compare \ref uni_chrrng_t code point ranges pointed to by \p a and \p b.
  * Intended for use with qsort and bsearch */
int uni_cmprng(const void *a, const void *b);
/** Returns true if \p cp is within one of the ranges in \p tab.
  * The \p tab array must be sorted as per \ref uni_cmprng.  Its length
  * is \p tab_len */
int uni_is_cp_chrrng(uint32_t cp, const uni_chrrng_t *tab, uint32_t tab_len);
@

<<Known Data Types>>=
uni_chrrng_t,%
@

<<Unicode property functions for generator>>=
int uni_cmprng(const void *a, const void *b)
{
    const uni_chrrng_t *_a = a, *_b = b;

    if(_a->high < _b->low)
        return -1;
    else if(_b->high < _a->low)
        return 1;
    else
        return 0;
}

int uni_is_cp_chrrng(uint32_t cp, const uni_chrrng_t *tab, uint32_t tab_len)
{
#if 0
  uni_chrrng_t cr = {cp, cp};
  return bsearch(&cr, tab, tab_len, sizeof(uni_chrrng_t), uni_cmprng) ? 1 : 0;
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > tab[j].high)
      l = j + 1;
    else
      return 1;
  }
  return 0;
#endif
}
@

<<Unicode property exports for generator>>=
/** Perform set operations on sets described by sorted code point range arrays.
  * Each array describes a set containing all integers within the array's
  * ranges.  Each array must be sorted as per \ref uni_cmprng.  The result
  * range is newly allocated if non-empty */
void uni_chrrng_setop(const uni_chrrng_t *a, uint32_t a_len, uni_set_op_t op,
                      const uni_chrrng_t *b, uint32_t b_len,
		      uni_chrrng_t **rtab, uint32_t *r_len);
@

While it would be possible to invert in place (and even use the same
array as the input), if there is enough room, it's easier to just
generate a new array.  At most one range is added (all the ranges
below the current ranges plus one above the top-most range), and at
most one range is removed (replaced by ranges below if top-most is at
top, except if one is already at the bottom).

<<Unicode property functions for generator>>=
static void uni_chrrng_invert(const uni_chrrng_t *a, uint32_t a_len,
		              uni_chrrng_t **rtab, uint32_t *r_len)
{
  uni_chrrng_t *tab;
  uint32_t tab_len = 0;
  inisize(tab, a_len + 1);
  uint32_t i;
  if(a_len && a[0].low) {
    tab[0].low = 0;
    tab[0].high = a[0].low - 1;
    tab_len++;
  }
  for(i = 0; i < a_len - 1; i++, tab_len++) {
    tab[tab_len].low = a[i].high + 1;
    tab[tab_len].high = a[i + 1].low - 1;
  }
  if(a_len && a[a_len - 1].high < UNI_MAX_CP) {
    tab[tab_len].low = a[a_len - 1].high + 1;
    tab[tab_len].high = UNI_MAX_CP;
    tab_len++;
  }
  *rtab = tab;
  *r_len = tab_len;
}
@

For the general operation, as with ordinary bit sets, a few degenerate
cases are handled immediately.  Again, rather than supporting a
previously allocated result structure, a new one is always allocated.
Unlike inversion, it is very likely that this is necessary in order to
avoid overwriting either one of the inputs when they are misaligned.

The generic routine operates by finding points where either input
changes state.  Then, the entire range where the the states where the
same get the result from a single operation call whose result is
almost staticaly calculated using constant inputs.

<<Unicode property functions for generator>>=
void uni_chrrng_setop(const uni_chrrng_t *a, uint32_t a_len, uni_set_op_t op,
                      const uni_chrrng_t *b, uint32_t b_len,
		      uni_chrrng_t **rtab, uint32_t *r_len)
{
  uni_chrrng_t *tab;
  uint32_t tab_len = 0, max_tab = 8;
  <<Shortcut all ops that don't involve both range table sets>>
  inisize(tab, max_tab);
  int invert = /* UNI_BIT_SET_OP(0, op, 0); */ op & 1;
  uint32_t aptr = 0, bptr = 0;
  uint32_t alow, ahigh, blow, bhigh;
  <<Set low and high for [[a]]>>
  <<Set low and high for [[b]]>>
  int32_t lasthigh = -1, curlow = -1, curhigh = -1;
  while(alow <= UNI_MAX_CP || blow <= UNI_MAX_CP) {
    uint32_t nextlow, nexthigh, res;
    <<Find next range where either [[a]] or [[b]] is different>>
    /* into nextlow-nexthigh and compute bit op as well (into res) */
    /* Invert the skipped range (where a=b=0) if necessary */
    if(invert && nextlow > lasthigh + 1) {
      if(curlow < 0)
        curlow = lasthigh + 1;
      curhigh = nextlow - 1;
    }
    /* store res into result */
    if(res) {
      /* extend or create current range */
      if(curlow < 0)
        curlow = nextlow;
      curhigh = nexthigh;
    } else if(curlow >= 0) {
      /* store & finish current range */
      check_size(tab, max_tab, tab_len + 1);
      tab[tab_len].low = curlow;
      tab[tab_len++].high = curhigh;
      curlow = -1;
    }
    lasthigh = nexthigh;
  }
  /* Invert the final range (where a=b=0) if necessary */
  if(invert && lasthigh < UNI_MAX_CP) {
    if(curlow < 0)
      curlow = lasthigh + 1;
    curhigh = UNI_MAX_CP;
  }
  /* store & finish the current range */
  if(curlow >= 0) {
    check_size(tab, max_tab, tab_len + 1);
    tab[tab_len].low = curlow;
    tab[tab_len++].high = curhigh;
  }
  *rtab = tab;
  *r_len = tab_len;
}
@

<<Set low and high for (@x)>>=
<<@x>>low = <<@x>>ptr < <<@x>>_len ? <<@x>>[0].low : UNI_MAX_CP + 1;
<<@x>>high = <<@x>>ptr < <<@x>>_len ? <<@x>>[0].high : UNI_MAX_CP + 1;
@

<<Find next range where either [[a]] or [[b]] is different>>=
if(alow < blow) {
  nextlow = alow;
  res = UNI_BIT_SET_OP(~0, op, 0);
  if(ahigh < blow) {
    nexthigh = ahigh;
    aptr++;
    <<Set low and high for [[a]]>>
  } else {
    nexthigh = blow - 1;
    alow = blow;
  }
} else if(alow > blow) {
  nextlow = blow;
  res = UNI_BIT_SET_OP(0, op, ~0);
  if(bhigh < alow) {
    nexthigh = bhigh;
    bptr++;
    <<Set low and high for [[b]]>>
  } else {
    nexthigh = alow - 1;
    blow = alow;
  }
} else { /* alow == blow */
  nextlow = alow;
  res = UNI_BIT_SET_OP(~0, op, ~0);
  nexthigh = ahigh;
  if(ahigh <= bhigh) {
    aptr++;
    <<Set low and high for [[a]]>>
  } else
    alow = bhigh + 1;
  if(bhigh <= nexthigh) {
    nexthigh = bhigh;
    bptr++;
    <<Set low and high for [[b]]>>
  } else
    blow = nexthigh + 1;
}
@

Any operations involving just one set are done immediately.  Although
it would be possible to shortcut operations against empty (or full)
sets as well, the slower generic routines are used instead of trying
to test for those cases.

<<Shortcut all ops that don't involve both range table sets>>=
if(op == UNI_SOP_NIL) {
  inisize(tab, 1); /* so free always works */
  *rtab = tab;
  *r_len = 0;
  return;
} else if(op == UNI_SOP_ALL) {
  inisize(tab, 1);
  tab[0].low = 0;
  tab[0].high = UNI_MAX_CP;
  *rtab = tab;
  *r_len = 1;
  return;
} else if(op == UNI_SOP_A) {
  inisize(tab, a_len);
  cpybuf(tab, a, a_len * sizeof(*a));
  *rtab = tab;
  *r_len = a_len;
  return;
} else if(op == UNI_SOP_INV_A) {
  uni_chrrng_invert(a, a_len, rtab, r_len);
  return;
} else if(op == UNI_SOP_B) {
  inisize(tab, b_len);
  cpybuf(tab, b, b_len);
  *rtab = tab;
  *r_len = b_len;
  return;
} else if(op == UNI_SOP_INV_B) {
  uni_chrrng_invert(b, b_len, rtab, r_len);
  return;
}
@

Another possible simple improvement is to store fixed ranges, for
example 32 characters at at time, with a bit array for that range.
Again, two words are required per entry (start and bit mask), so this
only saves storage space and time if each block has on average two or
more bits set.  This loses the range advantage of being able to skip
large contiguous groups of present members, but also partly loses the
range disadvantage of requiring contiguous members for efficiency.  It
also partly gains the disadvantage of requiring scanning of bit arrays
to produce the \emph{members} list.  No sample implementation is
provided here.

A more complex possible improvement is to split the original bit array
into a multi-level table, with duplicate subtables shared.  The bit
array is partitioned into chunks; these are stored at the lowest
level.  Higher levels are just tables of pointers to lower levels.
This increased indirection is less efficient than the raw bit array,
but having two pointers to the same subtable can significantly reduce
storage requirements.  Note that another name for this structure is an
optimized prefix tree (aka optimized trie) on the code point bit string.

There are other trie-style data structures.  For example, ICU uses
what they call a folding trie.  This structure uses two levels in a
way that optimizes characteristics of the Unicode character set
somewhat, but it does not appear to do a very good job of compressing
data.  On the other hand, with UTF-16 input, it can look up data with
only one table access for BMP code points, and two (one for each
UTF-16 character) for non-BMP code points, making lookup time almost
optimal.  However, even using 64k tables for each level is not very
space or cache efficient.  Another possibility is the Judy array.  It
might be worth testing it, but its inner structure is complex, and it
still doesn't do like node merging for less space usage.

The multi-level table approach requires fewer lookups than the code
point arrays (just as many table lookups as there are levels, vs. a
binary search against a long range list that might take, say, 10 or
more table probes), but even when using space-saving measures will
likely still produce larger tables than even unoptimized range tables
(range tables could be optimized for space by using only 24 bits per
code point, for example.) To share duplicate subtables, additional
management of a list of unique lower-level tables at each level, along
with their users, is required. Making a change of any kind is a
non-trivial operation.  This additional storage and computational
expense makes these more suited to static one-time generation.  On the
other hand, their straight lookup performance benefit over ranged
tables is three-fold or more for many Unicode tables, without a need
to do any special optimizations.

<<FIXME>>=
Make them more mergable.  Plan: align booleans' ranges to
multiples of 32 (so low-level chunks don't need shifting for merging),
store a checksum (CRC-32 or simple shifting sum) in a side table,
along with a duplicate count (maybe 24 bits of cksum plus 8 bits of
count).  Never bother resizing lowest level; use largest chunk size
instead.  Likewise for each level, maybe.  Provide single merging
function which takes 2 tables and a bitop.  Similar function also for
range tables, maybe.
@

There are probably very efficient algorithms to generate good
multi-level tables, but I am not aware of them.  The optimal number of
levels and level size is difficult to find, but a pretty good number
can be found by taking each level in turn, and finding the size which
produces the minimum total length taking into account all uniques and
the length of the array at the next level.  Of course the size of the
next level is determined by redundancy elimination as well, so this
requires a recursive search with backtracking.  In addition, having
too many levels eliminates some of the performance benefit, so the
search depth cannot be infinite.%
\footnote{Then again, a specific structure could be imposed,
eliminating the need for a search, but not always selecting the
optimal size.  For example, choosing a 3-level X/16/8 table might be
good enough for most cases.  However, in order to gain the space
savings, the routine would still need to perform a lot of block
comparisons to eliminate redundancy.  If it's going to take a lot of
time and memory anyway, why not go all the way?}
The initial bit array may be a subset of the desired values, in which
case an initial range can (actually, must) be specified as well.  When
a value lies outside of that range, it can be given a default;
currently only all zeroes and all ones are supported. The structure
may also store multiple bit arrays at once, in which case it may be
necessary to keep more than one byte together contiguously at the
lowest level; this may be specified using [[minwidth]].  By default,
[[minwidth]] can never be lower than 4, since 32-bit words are used.

<<Unicode property exports for generator>>=
/** Convert bit array \p bits (\p len bytes) into a multi-level table.
  * The first bit of \p bits is mapped to \p low * 8, and its last bit is
  * \p high * 8.  All bits outside of that range have the value zero if \p def
  * is zero, and one otherwise.  The lowest level of the table is broken
  * into chunks whose minimum number of bytes is \p minwidth (or 4 if
  * \p minwidth is too small).  The returned table is placed in \p ml,
  * and its size is \p ml_len words */
void uni_bits_to_multi(const uint8_t *bits, uint32_t len, uint32_t low,
                       uint32_t high, uint8_t def, uint32_t minwidth,
		       <<Multi-level table type>> **ml, uint32_t *ml_len);
/** Look up bit array segment corresponding to \p val in multi-level table
  * Returns 0 if segment is all zeroes, ~0 if all ones, and 1 if \p ret
  * points to a bit array segment.  Since the lowest granularity of return
  * is a byte, a bit must be looked up using the index divided by 8, followed
  * by a bit mask.  Note that \p ret is NULL for all zeroes and all ones
  * returns as well */
int uni_multi_tab_lookup(const <<Multi-level table type>> *dat, uint32_t val,
                         const uint8_t **ret, int def);
@

<<Unicode property functions for generator>>=
#define UNI_MAX_MULTILEV 3
void uni_bits_to_multi(const uint8_t *bits, uint32_t len, uint32_t low,
                       uint32_t high, uint8_t def, uint32_t minwidth,
		       <<Multi-level table type>> **ml, uint32_t *ml_len)
{
  <<Split [[bits]] into multi-level table [[*ml]]>>
}
@

<<Unicode property functions>>=
int uni_multi_tab_lookup(const <<Multi-level table type>>*dat, uint32_t val,
                         const uint8_t **ret, int def)
{
  <<Find multi-level table entry [[val]]>>
}
@

Recursion is actually accomplished using a backtracking stack.

<<Split [[bits]] into multi-level table [[*ml]]>>=
uint32_t i;
<<Saved multi-level table information>>
int curlev = 0;
/* The block size of the block currently being worked on */
uint32_t curblk[UNI_MAX_MULTILEV];

curblk[0] = 0;
while(1) {
  <<Unsaved multi-level table information>>

  <<Compute next [[curblk[curlev]]]>>
  if(!curblk[curlev]) {
    if(!curlev--)
      break;
    <<Free [[curlev]]>>
    continue;
  }
  <<Compute level [[curlev]]>>
  <<Create next level bit array>>
  ++curlev;
  curblk[curlev] = 0;
  <<Save or free [[curlev]]>>
  if(curlev == UNI_MAX_MULTILEV - 1) {
    curlev--;
    <<Free [[curlev]]>>
  }
}
@

The minimum block size is twice the pointer size at the next level.
Any smaller, and the next level could just as well be a copy.
Actually, any smaller than the architecture's memory alignment may
cause performance issues.  For now, this is set to a minimum of 4. 
The pointer size at the next level is 1, 2, or 4 depending on the
number of blocks at this level.  The maximum block size is one half
the total size; if the total size is not binary, the data is padded
with zeroes.

<<Saved multi-level table information>>=
/* raw data array for current set of levels */
uint8_t *lev[UNI_MAX_MULTILEV] = { (uint8_t *)bits };
uint32_t curlen[UNI_MAX_MULTILEV] = { len };
@

<<Unsaved multi-level table information>>=
uint32_t blks;
@

<<Compute next [[curblk[curlev]]]>>=
if(!curblk[curlev]) {
  if(curlen[curlev] <= 256 * 2)
#if 0 /* too small, really */
    curblk[curlev] = 2;
#else
    curblk[curlev] = 4;
#endif
  else if(curlen[curlev] <= 256 * 256 * 4)
    curblk[curlev] = 4;
  else
    curblk[curlev] = 8;
  if(!curlev && curblk[curlev] < minwidth)
    curblk[curlev] = minwidth;
} else {
  curblk[curlev] *= 2;
  if(curblk[curlev] >= curlen[curlev])
    curblk[curlev] = 0;
}
if(curblk[curlev])
  blks = (curlen[curlev] + curblk[curlev] - 1) / curblk[curlev];
@

For each pass, a new pointer array is created.  While this will
eventually be reduced to the pointer size, 32-bit pointers are used
during the search.  In addition, a side array is created to store only
the unique block indices.  This alone prevents having to scan the
entire return array for block matches, speeding things up orders of
magnitude with sparse data.  Additionally, this array is kept sorted,
allowing binary searching to further reduce the number of comparisons
needed.  Using a hash table for this would require extra storage, and
may speed things up further.  In any case, each block is simply added
to the pointer array, checking first for duplicates.  As another
special compensation for sparse (or dense) arrays, all-zero entries
are always encoded as the pointer zero, and all-one entries are always
encoded as the maximum pointer value (all ones).  Not only does this
save a tiny bit of space, but scanning through dense or sparse
sections of the array can be much faster.  Care must be taken during
the comparisons to not access data past the end of the actual bit
array, instead treating them as zeroes.

<<FIXME>>=
Maybe add hash to block before sort for dup detect; maybe also store
hash with mtab.  See note above about making this more mergable.
@

<<Unsaved multi-level table information>>=
uint32_t j;
uint32_t *ptr, *ublocks, nublocks = 0;
uint8_t *data = lev[curlev];
uint32_t blklen, shortlen;
@

<<Compute level [[curlev]]>>=
blklen = shortlen = curblk[curlev];
ptr = malloc(blks * sizeof(*ptr));
ublocks = malloc(blks * sizeof(*ublocks));
for(i = 0; i < blks; i++) {
  int h, l;
  /* first, check for 0 or 1 */
  int is0 = 1, is1 = 1;
  if(i == blks - 1) {
    shortlen = curlen[curlev] % blklen;
    if(!shortlen)
      shortlen = blklen;
  }
  for(l = 0; l < shortlen && (is0 || is1); l++) {
    if(data[blklen * i + l])
      is0 = 0;
    if(data[blklen * i + l] != (uint8_t)~0)
      is1 = 0;
  }
  if(is0 && (!def || shortlen == blklen)) {
    ptr[i] = 0;
    continue;
  }
  if(is1 && (def || shortlen == blklen)) {
    ptr[i] = ~0;
    continue;
  }
  h = nublocks - 1;
  l = 0;
  while(l <= h) {
    j = (l + h) / 2;
    /* comparing in reverse order to make shortlen cmp easier */
    int c = memcmp(data + blklen * ublocks[j], data + blklen * i, shortlen);
    if(!c) {
      uint32_t k;
      if(shortlen == blklen)
        break;
      /* make c > 0 if there is any non-0 element in ublock */
      for(k = shortlen; k < blklen && !c; k++)
        c = data[blklen * ublocks[j] + k];
      if(!c)
        break;
    }
    if(c > 0)
      h = j - 1;
    else
      l = j + 1;
  }
  if(l > h) {
    if(++h == nublocks)
      ublocks[nublocks++] = i;
    else {
      movebuf(ublocks + h + 1, ublocks + h, nublocks - h);
      ++nublocks;
      ublocks[h] = i;
    }
    j = h;
  }
  ptr[i] = ublocks[j] + 1;
}
@

Now that the search is complete, the pointers can be compressed to the
minimum word size required.  The data will eventually be shifted down,
but it is not necessary (or safe) yet.  However, some preparation must
be done here, so that the pointers will eventually point to the
shifted blocks.  The actual pointers need to be adjusted to use the
offset of that pointer in the unique blocks array.  This could not
have been done above, because the array was having members inserted in
the middle, invalidating all of the indices.  To look up the index
more quickly, the block array is stored by block number first (it was
sorted by block contents, instead).

<<Saved multi-level table information>>=
/* unique block pointers */
uint32_t *curublk[UNI_MAX_MULTILEV] = {NULL};
uint32_t curnublk[UNI_MAX_MULTILEV];
@

<<Create next level bit array>>=
curublk[curlev] = ublocks;
curnublk[curlev] = nublocks;
qsort(ublocks, nublocks, sizeof(uint32_t), uni_cmp_cp);
/* for comparison with bsearch, adjust to actual stored value */
for(i = 0; i < nublocks; i++)
  ++ublocks[i];
for(i = 0; i < blks; i++) {
  if(!ptr[i] || ptr[i] == (uint32_t)~0)
    continue;
  uint32_t *p = bsearch(ptr + i, ublocks, nublocks, sizeof(uint32_t), 
                        uni_cmp_cp);
  ptr[i] = p - ublocks + 1;
}
lev[curlev + 1] = (uint8_t *)ptr;
/* note: 0 and ~0 are reserved values, so space for 2 must be reserved */
if(nublocks <= 254) {
  uint8_t *p = (uint8_t *)ptr;
  for(i = 0; i < blks; i++)
    p[i] = ptr[i];
  curlen[curlev + 1] = blks;
} else if(nublocks <= 65534) {
  uint16_t *p = (uint16_t *)ptr;
  for(i = 0; i < blks; i++)
    p[i] = ptr[i];
  curlen[curlev + 1] = blks * 2;
} else
  curlen[curlev + 1] = blks * 4;
@

To retain the minimum-sized level, the current set of minima is
stored.  If the current level is either smaller or the same size, but
with fewer levels, it is saved.  Otherwise, it and all of its children
are freed.

<<Saved multi-level table information>>=
uint32_t cursize[UNI_MAX_MULTILEV];
uint8_t *minlev[UNI_MAX_MULTILEV] = { (uint8_t *)bits };
uint32_t *minublk[UNI_MAX_MULTILEV] = { NULL };
uint32_t minlen[UNI_MAX_MULTILEV] = { len }, minnublk[UNI_MAX_MULTILEV];
uint32_t minblk[UNI_MAX_MULTILEV] = { 0 }, minsize = len;
@

<<Create next level bit array>>=
cursize[curlev] = nublocks * curblk[curlev];
cursize[curlev + 1] = curlen[curlev + 1];
@

<<Save or free [[curlev]]>>=
uint32_t save_size = 0;
for(i = 0; curblk[i]; i++)
  save_size += cursize[i];
save_size += cursize[i];
int save_lev = save_size < minsize;
if(save_size == minsize) {
  for(j = 0; minblk[j]; j++);
  save_lev = i < j;
}
if(save_lev) {
  for(i = 0; i < UNI_MAX_MULTILEV; i++) {
    if(minlev[i] != lev[i] && minlev[i])
      free(minlev[i]);
    if(minublk[i] != curublk[i] && minublk[i])
      free(minublk[i]);
    minblk[i] = curblk[i];
    minlev[i] = lev[i];
    minlen[i] = curlen[i];
    minublk[i] = curublk[i];
    minnublk[i] = curnublk[i];
  }
  minsize = save_size;
}
@

<<Free [[curlev]]>>=
if(curublk[curlev] != minublk[curlev])
  free(curublk[curlev]);
curublk[curlev] = NULL;
if(lev[curlev + 1] != minlev[curlev + 1])
  free(lev[curlev + 1]);
lev[curlev + 1] = NULL;
@

Now, the data can be combined into a single buffer, which is parsed
for every lookup.  This buffer is an opaque buffer of 32-bit integers,
with each data chunk aligned to a 32-bit boundary.  That way, 16-bit
and 32-bit values can be read directly from the buffer without
alignment issues.  The buffer is read from top to bottom, traversing
pointer tables until the data is reached.


<<Multi-level table type>>=
uint32_t
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
#define lev_psz(l) \
  (!(l) ? 1 : minnublk[l - 1] <= 254 ? 1 : minnublk[l - 1] <= 65534 ? 2 : 4)
for(curlev = 0; curlev < UNI_MAX_MULTILEV - 1 && minlev[curlev + 1]; curlev++);
*ml_len = 0<<Multi-level table buffer length>>;
inisize(*ml, *ml_len);
uint32_t *mp = *ml;
@

To determine the size of the buffer, we need to know what the buffer
will contain.  I will approach this by developing the lookup function
at the same time.  This function returns either a pointer into the
lowest-level data arrays, or an integer indicating that this is a zero
or one entry.

<<Find multi-level table entry [[val]]>>=
*ret = NULL;
@

First, we need to filter out the main range.  If the value is out of
range, the default is returned. This can be stored in the next word.
Since it's only one bit, other information can be stored in that word
as well.

<<Find multi-level table entry [[val]]>>=
if(val < *dat || val > dat[1])
  return def ? def : dat[2] & 1 ? ~0 : 0;
val -= *dat;
dat += 3;
@

<<Multi-level table buffer length>>=
+3
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
*mp++ = low;
*mp++ = high;
*mp++ = def & 1;
@

There is no other rarely used information necessary for scanning the
table that can be stored along with the default bit.  However, for
some applications, it may be nice to know the size of the table.  So,
the remaining bits of that word are filled with that size (in words).

<<FIXME>>=
Provide function that returns mt[2] >> 1 ("storage_len")
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
mp[-1] |= *ml_len << 1;
@

After looking up the value at a particular level, the level needs to
be skipped.  The size of the table can be stored as a compressed
pointer beyond the end of the next level's data.  This is not possible
for the first level, though, so the length needs to be stored
separately.  There is no point in storing a compression length as
well, so this is stored as a 32-bit integer.

<<Find multi-level table entry [[val]]>>=
uint32_t skip = *dat++;
@

<<Multi-level table buffer length>>=
+1
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
*mp++ = minlen[curlev] / lev_psz(curlev);
@

Following this is a list of levels.  The first word of the last level
(i.e., the data level) is zero to terminate the list.

<<Find multi-level table entry [[val]]>>=
<<Prepare to scan multi-level table pointers>>
while(*dat) {
  <<Scan multi-level table pointers>>
}
@

<<Multi-level table buffer length>>=
+1;
for(i = 1; i <= curlev; i++)
  *ml_len += 0<<Multi-level table buffer pointer [[i]] length>>;
*ml_len += 0<<Multi-level table buffer data length>>
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
while(curlev > 0) {
  <<Dump multi-level table pointers>>
  --curlev;
}
*mp++ = 0;
@

Next, we need information regarding this level of the array.  First,
each array corresponds to a particular set of bits; all bits below
that are ignored.  The bit number below which the rest may be ignored
is stored next.  For the lowest level of the array, this is always
zero, so this corresponds with the loop terminator.  Rather than store
the number of bits in an entire 32-bit word, it is stored in a byte.
The other three bytes are available for other information that is only
needed for a pointer level.

<<Scan multi-level table pointers>>=
uint32_t desc = *dat++;
uint8_t shift = desc;
uint32_t idx = val >> shift;

val -= idx << shift;
@

<<Multi-level table buffer pointer [[i]] length>>=
+1
@

<<Dump multi-level table pointers>>=
uint32_t desc;
<<Compute pointer level descriptor>>
*mp++ = desc;
@

<<Compute pointer level descriptor>>=
for(i = 0, desc = 0; i < curlev; i++)
  desc += lg2(minblk[i] / lev_psz(i));
@

Now that the index into the subarray is known, to actually look
something up in the table requires the size of the table entries.
This is stored next.

<<Scan multi-level table pointers>>=
desc >>= 8;
uint8_t psz = desc;
@

<<Compute pointer level descriptor>>=
uint8_t psz = lev_psz(curlev);
desc |= psz << 8;
@

It also needs the offset of the subtable.  Since that is what we are
looking up, it can be obtained from the previous pass.  For the first
pass, the offset is always zero.

<<Prepare to scan multi-level table pointers>>=
uint32_t toff = 0;
@

Now the lookup can be performed.  First, the offset beyond the table
is read, and then the pointer itself.  Then, the data pointer is
advanced past the end of the entire level.

<<Scan multi-level table pointers>>=
switch(psz) {
  case 1: {
    const uint8_t *p = (const uint8_t *)dat;
    toff = p[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint8_t)~0)
      return ~0;
    dat += skip / 4 + 1; /* aka (skip + 1 + 3) / 4 */
    skip = *p;
    break;
  }
  case 2: {
    const uint16_t *p = (const uint16_t *)dat;
    toff = p[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint16_t)~0)
      return ~0;
    dat += skip / 2 + 1; /* aka (skip + 1 + 1) / 2 */
    skip = *p;
    break;
  }
  case 4:
    toff = dat[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint32_t)~0)
      return ~0;
    toff = *dat;
    dat += skip + 1;
    break;
}
@

<<Multi-level table buffer pointer [[i]] length>>=
+((i != curlev ? minnublk[i] * minblk[i] : minlen[i]) + lev_psz(i) + 3) / 4
@

<<Dump multi-level table pointers>>=
uint8_t *mpp = (uint8_t *)mp;
switch(psz) {
 case 1:
   *mpp++ = minnublk[curlev - 1];
   break;
 case 2:
   *(uint16_t *)mpp = minnublk[curlev - 1];
   mpp += 2;
   break;
 case 4:
   *(uint32_t *)mpp = minnublk[curlev - 1];
   mpp += 4;
   break;
}
<<Copy table data out>>
free(minlev[curlev]);
@

<<Copy table data out>>=
if(minblk[curlev]) {
  /* copy all but last blindly */
  for(i = 0; i < minnublk[curlev] - 1; i++) {
    cpybuf(mpp, minlev[curlev] + minblk[curlev] * (minublk[curlev][i] - 1),
           minblk[curlev]);
    mpp += minblk[curlev];
  }
  /* the last may be short */
  uint32_t shortlen;
  if(minublk[curlev][i] == (minlen[curlev] + minblk[curlev] - 1) / minblk[curlev]) {
    shortlen = minlen[curlev] % minblk[curlev];
    if(!shortlen)
      shortlen = minblk[curlev];
  } else
    shortlen = minblk[curlev];
  cpybuf(mpp, minlev[curlev] + minblk[curlev] * (minublk[curlev][i] - 1),
         shortlen);
  /* silence valgrind, even though random garbage is perfectly safe */
  if(shortlen < minblk[curlev])
    memset(mpp + shortlen, 0, minblk[curlev] - shortlen);
  mpp += minblk[curlev];
  free(minublk[curlev]);
} else {
  cpybuf(mpp, minlev[curlev], minlen[curlev]);
  mpp += minlen[curlev];
}
/* align mpp */
if((mpp - (uint8_t *)mp) & 3) {
  /* silence valgrind here as well */
  memset(mpp, 0, 4 - ((mpp - (uint8_t *)mp) & 3));
  mpp += 4 - ((mpp - (uint8_t *)mp) & 3);
}
mp = (uint32_t *)mpp;
@

The table offset just acquired is a block number (actually, plus one
to allow for zero).  To convert it to a word offset, it needs to be
multiplied by the next block size, in pointer size units.  This is
stored next in [[desc]].  Since there are two bytes left, it is stored
as a 16-bit integer.  If another byte is ever needed, it can be
obtained by changing this to a shift value (it is always a power of
2).

<<Scan multi-level table pointers>>=
desc >>= 8;
toff = (toff - 1) * desc;
skip *= desc;
@

<<Compute pointer level descriptor>>=
desc |= (minblk[curlev - 1] / lev_psz(curlev - 1)) << 16;
@

The final lookup simply finds the byte offset of the remaining value
in the selected subtable.  Note that only bytes may be addressed;
storing more than one byte requires shifting the value left first
before finding the first byte of the value.  Similarly, storing only
one bit requires that the value be shifted right first for addressing,
and then using the shifted out bits to create a bit mask.

<<Find multi-level table entry [[val]]>>=
*ret = (const uint8_t *)dat + 4 + toff + val;
return 1;
@

<<Multi-level table buffer data length>>=
+((curlev ? minnublk[0] * minblk[0] : len) + 3) / 4
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
uint8_t *mpp = (uint8_t *)mp;
<<Copy table data out>>
if(mp - *ml != *ml_len) {
  fprintf(stderr, "len mismatch: %d %d\n", (int)(mp - *ml), (int)*ml_len);
  exit(1);
}
@

Generally, even with the complexity I added by compressing pointers,
the table lookups are faster than equivalent range table lookups.  As
long as duplicates are removed, the space taken up is comparable to
ordinary range tables, and sometimes even better.  With the
suppression of zero and one blocks, it is faster to find the
\emph{members} list than with plain bit tables, but is still not very
fast.  On the other hand, making modifications, or creating new tables
from scratch, is very expensive. Even if we stick with a table's
current structure, there needs to be side storage that deals with the
shared arrays.  If a change is made to an array at any level, then all
pointers at the next level need to be checked: if they point to the
same place, the block needs to be duplicated first, and after updating
the block (duplicate or not), it needs to be compared against all
existing blocks at the same level for re-merging.  The last step is
not strictly necessary for run-time operations, but there is no way
with the current structure to detect shared blocks.  An additional bit
for each pointer could be used to indicate sharing, which would never
be turned off even if the last owner disappears.  There are many
possibilities, and I do not wish to dwell on it any more.  For now,
the multi-level tables are expected to be read-only and only provide
the \emph{in} function.

However, converting a multi-level table to a sorted list of code point
ranges adequately satisfies the [[list]] function, and is relatively
easy to implement.  First, since the entire table is to be scanned
sequentally, the structure of the table is read.  Next, a loop simply
advances counters at each level of the table to get to the data, and
parses the data (slowly) using the generic bit tester.  The only
shortcuts here are when reading all zeroes or all ones, which bypass
the lowest level scan.  The function might get a little faster if it
didn't re-read every level's pointer for every low-level block, but
doing it this way is simpler and probably not a big performance hit.
A bigger speed gain could probably be had by optimizing the bit tester
loop.

<<Unicode property exports>>=
/** Convert a multi-level bit array table to a sorted range array.
  * The multi-level table \p mt is assumed be a set defined by a
  * single-bit bit array, and is converted to a set defined by the
  * sorted range array \p *ret/\p *rlen */
void uni_multi_bit_to_range(const uint32_t *mt, uni_chrrng_t **ret,
                            uint32_t *rlen);
@

<<Unicode property functions>>=
void uni_multi_bit_to_range(const uint32_t *mt, uni_chrrng_t **ret,
                            uint32_t *rlen)
{
  <<Multi-level structure storage>>
  
  <<Gather multi-level structure>>
  <<Convert multi-level structure to range list>>
}
@

<<Multi-level structure storage>>=
const uint32_t *dat[UNI_MAX_MULTILEV + 1];
uint32_t sh[UNI_MAX_MULTILEV + 1], psz[UNI_MAX_MULTILEV + 1],
         bsz[UNI_MAX_MULTILEV + 1];
uint32_t low, high;
int inv;
uint32_t nl;
@

<<Gather multi-level structure>>=
low = mt[0] * 8;
high = mt[1] * 8 + 7;
inv = mt[2] & 1;
uint32_t desc;
uint32_t nskip = mt[3], skip;
uint32_t l;
mt += 4;
bsz[0] = nskip; /* top level is just one block */
for(l = 0; (desc = *mt++); l++) {
  dat[l] = mt;
  psz[l] = (desc >> 8) & 0xff;
  sh[l] = desc & 0xff;
  switch(psz[l]) {
    case 1:
      skip = nskip / 4 + 1;
      nskip = *(uint8_t *)mt;
      break;
    case 2:
      skip = nskip / 2 + 1;
      nskip = *(uint16_t *)mt;
      break;
    default:
      skip = nskip + 1;
      nskip = *mt;
  }
  nskip *= (bsz[l + 1] = desc >> 16);
  mt += skip;
}
dat[l] = mt;
nl = l;
@

<<Convert multi-level structure to range list>>=
uint32_t pno[UNI_MAX_MULTILEV];
uint32_t ret_max;
int32_t rno, in_r;
ret_max = 8;
inisize(*ret, ret_max);
rno = in_r = 0;
if(inv && low > 0) {
  (*ret)[0].low = 0;
  in_r = 1;
}
for(l = 0; l < nl; l++)
  pno[l] = 0;
while(1) {
  uint32_t ptr = 1;
  for(l = 0; l < nl; l++) {
    switch(psz[l]) {
      case 1:
        ptr = ((uint8_t *)dat[l] + (ptr - 1) * bsz[l])[pno[l] + 1];
	if(ptr == 0xff)
	  ptr = ~0;
	break;
      case 2:
        ptr = ((uint16_t *)dat[l] + (ptr - 1) * bsz[l])[pno[l] + 1];
	if(ptr == 0xffff)
	  ptr = ~0;
	break;
      default:
        ptr = (dat[l] + (ptr - 1) * bsz[l])[pno[l] + 1];
    }
    if(!ptr || ptr == (uint32_t)~0)
      break;
  }
  if(!ptr) {
    if(in_r) {
      (*ret)[rno++].high = low - 1;
      in_r = 0;
    }
    low += 8 << sh[l];
  } else if(ptr == (uint32_t)~0) {
    if(!in_r) {
      check_size(*ret, ret_max, rno + 1);
      (*ret)[rno].low = low;
      in_r = 1;
    }
    low += 8 << sh[l];
  } else {
    uint32_t i;
    uint8_t *rdat = (uint8_t *)dat[nl] + (ptr - 1) * bsz[nl];
    for(i = 0; i < bsz[nl] * 8 && low <= high; i++, low++)
      if(!UNI_BSET_IS_SET(rdat, i) != !in_r) {
	if(!in_r) {
	  check_size(*ret, ret_max, rno + 1);
	  (*ret)[rno].low = low;
	  in_r = 1;
	} else {
	  (*ret)[rno++].high = low - 1;
	  in_r = 0;
	}
      }
  }
  if(low > high) {
    low = high + 1;
    break;
  }
  if(l == nl)
    l--;
  while(++pno[l] == bsz[l])
    l--;
  for(l++; l < nl; l++)
    pno[l] = 0;
}
if(in_r) {
  if(!inv)
    (*ret)[rno++].high = low - 1;
  else
    (*ret)[rno++].high = UNI_MAX_CP; /* or ~0? */
} else if(inv && high < UNI_MAX_CP) { /* or ~0? */
  check_size(*ret, ret_max, rno + 1);
  (*ret)[rno].low = low;
  (*ret)[rno++].high = UNI_MAX_CP;  /* or ~0? */
}
*rlen = rno;
@

The inverse operation is not so simple, but is worth implementing for
use with the UCD parser.  The parser can simply read in a range table,
and convert it to a multi-level table.  Since the multi-level table
generation algorithm partitions the bit array, the range table
converter actually constructs the bit array first, and calls the
function already developed above to do the conversion.

This is fairly inelegant and inefficient, but it is not meant to be
called at run-time very often.  Tables are meant to be pre-generated.

<<Unicode property exports for generator>>=
/** Convert a sorted range array to a multi-level bit array table.
  * The sorted range array \p tab/\p tab_len is assumed to be a set
  * defined by inclusion in one of its ranges.  It is converted to
  * a multi-level bit array table.  The table itself is returned, and,
  * if \p ml_len is not NULL, its length is returned in \p *ml_len */
uint32_t *uni_rng_to_multi_bit(const uni_chrrng_t *tab, uint32_t tab_len,
                               uint32_t *ml_len);
@

<<Unicode property functions for generator>>=
uint32_t *uni_rng_to_multi_bit(const uni_chrrng_t *tab, uint32_t tab_len,
                               uint32_t *ml_len)
{
  uint32_t low, high, len, i;
  uint32_t *ml;
  uint8_t *bits, def;

  /* degenerate cases:  all 0, all 1 */
  if(!tab_len) {
    uni_bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  if(tab_len == 1 && !tab[0].low && tab[0].high == UNI_MAX_CP) {
    uni_bits_to_multi(NULL, 0, 1, 0, 1, 0, &ml, ml_len);
    return ml;
  }
  /* not all 0 or all 1 */
  low = tab[0].low;
  high = tab[tab_len - 1].high;
  /* if starts & ends with 1, make it an inverse array */
  /* should also do this if low == 0 && tab[0].high > UNI_MAX_CP - high */
  /* or vice-versa, but it complicates things a bit */
  def = !low && high == UNI_MAX_CP;
  if(def) {
    low = tab[0].high + 1;
    high = tab[tab_len - 1].low - 1;
  }
  /* align with byte boundary */
  low &= ~7;
  len = ((high - low + 1 + 7) / 8);
  inisize(bits, len);
  /* Optimize(maybe): only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  if(def)
    /* fill in ones if low not aligned with byte boundary */
    bits[0] = (1 << (tab[0].high + 1) % 8) - 1;
  for(i = def; i < tab_len - def; i++) {
    uint32_t l = tab[i].low - low, h = tab[i].high - low + 1;
    uint8_t hm = (1 << h % 8) - 1;
    uint8_t lm = ~((1 << l % 8) - 1);
    if(h / 8 == l / 8) {
      UNI_BSET_AELT(bits, l) |= lm & hm;
      continue;
    }
    if(lm != (uint8_t)~0) {
      UNI_BSET_AELT(bits, l) |= lm;
      l += 8 - (l & 7);
    }
    if(l < (h & ~7))
      memset(&UNI_BSET_AELT(bits, l), 0xff, h / 8 - l / 8);
    if(hm)
      UNI_BSET_AELT(bits, h) |= hm;
  }
  if(def && (tab[tab_len - 1].low - low) % 8)
    /* fill in ones if high + 1 not aligned with byte boundary */
    bits[(tab[tab_len - 1].low - low) / 8] |=
                         ~((1 << (tab[tab_len - 1].low - low) % 8) - 1);
  uni_bits_to_multi(bits, len, low / 8, high / 8, def, 0, &ml, ml_len);
  free(bits);
  return ml;
}
@

As mentioned earlier, I am not aware of any fast, efficient algorithms
for generating the multi-level table structure.  This makes general
bit operations difficult at best.  The general algorithm for unaligned
bit sets above would seem trivial in comparison to the tricks needed
to get unaligned multi-level tables together.  Right now, the only
supported method for set operations is to convert to a range table,
perform the operation, and convert back.  For less frequent
operations, simply performing the set operation on the results of
multiple lookups will do the trick.  This is extremely inefficient,
but at least it does not require creating a huge unmaintanable mess of
code for something that should rarely be done.

Another possibility would be to use more complex data structures.  I
doubt I could come up with a more space-efficient storage method than
the sorted range table, but some are much better at operations like
lookup, scanning, or modification.  An appropriately designed hash
table could be optimized off-line to provide even faster lookups, at
the cost of fragility due to the hash algorithm probably depending on
a particular Unicode release.  Prefix trees (tries, if you prefer)
have good theoretical performance, and in fact the multi-level table
is just a prefix tree of sorts.  It and many other faster tree
approaches achieve their performance by assuming a constant time
parent-to-child traversal with a large set of children, which once
again equates to large storage penalties.  I have reduced the penalty
somewhat in my own implementation by using smaller pointers, at the
expense of slowing the lookup down a bit.  For now, the multi-level
table, sorted range table, and sorted code point table are the only
supported data structures.

<<FIXME>>=
Add discussion of compressed tries:
  Liang/TeX: overlapping arrays
     build normally
     merge suffixes
     side bitmask of filled positions
     add node by computing bitmask
        find first fit pos in side mask
     each array ent stores index of child
     add node symbol itself
  ref by Liang: bitmask of structure
     e.g. 32-bit
@

\subsection{Testing}

In order to actually compare performance and size, a separate test
program is provided.

<<C Test Support Executables>>=
tsttab \
@

<<makefile.rules>>=
tsttab.o: uni_prop.h
@

<<tsttab.c>>=
<<Common C Header>>

#include "uni_prop.h"
// static_proto

<<POSIX timing support>>

<<Functions to help test generated tables>>

int main(void)
{
  <<Test generated tables>>
  return 0;
}
@

To test a boolean, a function is provided to run the entire test, and
a call to that function is done in the mainline.  The function is
passed the range table, the multi-level table, and the name of the
property.  These can all be generated from the property name with a
preprocessor macro.

<<Functions to help test generated tables>>=
volatile int32_t tres;

#define bool(x) doit_bool(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab)
static void print_mtab_info(const uint32_t *, uint32_t);
static void doit_bool(const char *name, const uni_chrrng_t *rng, uint32_t nent,
                      const uint32_t *mtab)
{
    uint32_t i;
    
    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      int rv = uni_is_cp_chrrng(i, rng, nent);
      int mv = <<range table result for [[i]]>>;
      if(!rv != !mv) {
        fprintf(stderr, "mismatch %s@%d %d %d\n", name, i, rv, mv);
	exit(1);
      }
    }
    /* check performance */
    int j;
    double tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = uni_is_cp_chrrng(i, rng, nent);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = <<range table result for [[i]]>>;
    tt = tend();
    printf("  r%.0f t%.0f %.2fx\n", tr, tt, tr / tt);
    /* check conversion */
    uni_chrrng_t *rng2;
    uint32_t nent2;
    uni_multi_bit_to_range(mtab, &rng2, &nent2);
    if(nent2 != nent || cmpbuf(rng, rng2, nent)) {
      for(j = 0; j < nent && j < nent2; j++)
        if(cmpbuf(&rng[j], &rng2[j], 1))
	  break;
      fprintf(stderr, "misconvert %d/%d @%d\n", (int)nent, (int)nent2, j);
      exit(1);
    }
}
@

<<Multi-level Table Info Vars>>=
uint32_t i = 4, l;
uint32_t skip, desc, nskip = mt[3], psz;
@

<<Functions to help test generated tables>>=
static void print_mtab_info(const uint32_t *mt, uint32_t rsize)
{
  <<Multi-level Table Info Vars>>

  fputs("  mtab: ", stdout);
  if(mt[2] & 1)
    fputs("*inv* ", stdout);
  printf(" %d", nskip);
  <<Multi-level Table Info Pointers Loop>>
    printf("/%d", desc >> 16);
    i += skip;
  <<Multi-level Table Info Pointers End of Loop>>
  i += skip;
  printf(" (%d bytes, %d lookups max) (%.2fx rng)\n", i * 4, l + 1,
         (double)(i * 4) / rsize);
}
@

<<Multi-level Table Info Pointers Loop>>=
for(l = 0; (desc = mt[i++]); l++) {
  psz = (desc >> 8) & 0xff;
  switch(psz) {
    case 1:
      skip = nskip / 4 + 1;
      nskip = *(uint8_t *)&mt[i];
      break;
    case 2:
      skip = nskip / 2 + 1;
      nskip = *(uint16_t *)&mt[i];
      break;
    default:
     skip = nskip + 1;
     nskip = mt[i];
  }
  nskip *= desc >> 16;
@

<<Multi-level Table Info Pointers End of Loop>>=
}
skip = (nskip + 3) / 4;
@

<<FIXME>>=
emoji/emoji-test.txt -> run it, somehow
@

\subsection{Parsing the UCD}

A parser program is used to generate these tables as static,
compilable C code from the UCD tables.  One of the advantages of the
range table method is that creating range tables is trival, and could
be done using a simple shell script.  However, during the stages of
this project where that was in use, the generation time began to
dominate compilation time, so this was converted to C.  Since it is in
C anyway, it may as well generate multi-level tables directly, as
well.  In fact, the logic outside of the scripts was becoming
cumbersome as well, so all of that is now incorporated into the C
program.

This program has a special build rule so it can be used to generate
other C code without depending on that C code itself (as with
[[cproto.h]]).  The code itself can't use the standard header macro,
either, since it needs to limit its include files.  One thing it does
need, though, is the ability to read lines of arbitrary length.  This
is provided in my support library, which is referenced rather than
built here, again to avoid building [[cproto.h]].  Since
[[uni_prop.h]] and [[uni_prop.c]] depend on the output of this
program, but this program also needs to use some of the functions
defined therein, the functions and definitions are reinserted directly
into [[parse-ucd.c]] without any dependencies on the generated data.

\lstset{language=make}
<<C Build Executables>>=
parse-ucd \
@

\lstset{language=C}
<<parse-ucd.c>>=
<<Common C Warning>>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <ctype.h>
#include <unistd.h>
#include <stdint.h>
<<Additional parse-ucd includes>>
<<Unicode property exports for generator>>

<<Unicode property functions for generator>>

<<UCD parser local definitions>>

<<UCD parser local functions>>

int main(void)
{
  <<UCD parser variables>>

  <<Parse character data files>>
  <<Post-process property data>>
  <<Dump character information as C code>>
  <<Clean up after parsing UCD files>>
  return 0;
}
@

\lstset{language=make}
<<makefile.vars>>=
PARSE_UCD_DEPS := parse-ucd.c $(SUPT_LIB_LOC)/libtjm-supt.a \
                  <<Additional parse-ucd C files>>

PARSER_CFLAGS :=
PARSER_LDFLAGS :=
@

<<makefile.config>>=
# The location of my support library (tjm-ext.nw)
SUPT_LIB_LOC := ../build

@

<<makefile.rules>>=
parse-ucd: $(PARSE_UCD_DEPS)
	$(CC) $(CFLAGS) $(EXTRA_CFLAGS) $(PARSER_CFLAGS) \
	   -o $@ parse-ucd.c -L$(SUPT_LIB_LOC) -ltjm-supt $(LDFLAGS) $(PARSER_LDFLAGS)
@

<<Additional parse-ucd C files>>=
mfgets.h \
@

\lstset{language=C}
<<Additional parse-ucd includes>>=
#include "mfgets.h"
@

All files will be parsed by the same program.  The only necessary
parameters are the locations and versions of the UCD files, which can
just be compiled in based on the configuration parameters.  For
convenience, the known (but not enforcably standardized) format of the
version string in a UCD file can be used to extract its version.
Similarly, the version can be extracted from the DTD of the CLDR; this
is less likely to change without notice, but it is still not
enforcably standardized.  Since it's too much trouble to keep the
version numbers up-to-date in the configuration file, I always attempt
to extract the version from the data.

\lstset{language=make}
<<makefile.vars>>=
PARSER_CFLAGS += -DUCD_LOC=\"$(UCD_LOC)\" -DUNIHAN_LOC=\"$(UNIHAN_LOC)\"
# $(shell ...) expands $(..._LOC) incorrectly, so use shell's $() instead
UCD_VER = $$( \
     sed -n -e '1{s/.*-//;s/\.txt.*//;s/\.//g;p;}' $(UCD_LOC)/PropList.txt)
# plain versions don't have .0 attached, so attach .0 and strip if wrong
CLDR_VER = $$( \
   sed -n -e '/cldrVersion/{ \
      s/[^"]*"//;s/".*//;s/$$/.0/;s/\(\..*\)\..*/\1/;s/\.//g;p;}' \
       $(CLDR_LOC)/common/dtd/ldml.dtd)
PARSER_CFLAGS += -DUCD_VER=$(UCD_VER) -DCLDR_VER=$(CLDR_VER)
@

Rather than construct file names using the provided paths, the program
just changes to the location directory before reading files, and all
files are read relative to the current directory.  In order to be able
to change back to the starting directory, once again my utility
library has a portable [[getcwd]] that does not use fixed buffer sizes.

\lstset{language=C}
<<UCD parser variables>>=
<<Parser common variables>>
char *cwd = getcwd_full();
@

<<Parser common variables>>=
FILE *f;
char *lbuf = NULL;
unsigned int lbuflen, llen;
#define open_f(fn) do { \
  if(!(f = fopen(fn, "r"))) { \
    perror(fn); \
    exit(1); \
  } \
} while(0)
#define force_chdir(d) do { \
  if(chdir(d)) { \
    perror(d); \
    exit(1); \
  } \
} while(0)
@

<<Parse character data files>>=
<<Initialize UCD files>>
<<Parse UCD files>>
<<Initialize Unihan files>>
<<Parse Unihan files>>
@

<<Initialize UCD files>>=
force_chdir(UCD_LOC);
@

<<Initialize Unihan files>>=
force_chdir(cwd); /* in case unihan_loc is relative */
force_chdir(UNIHAN_LOC);
@

<<Dump character information as C code>>=
force_chdir(cwd);
free(cwd);
@

UCD text files consist of semicolon-separated fields, with the code
point(s) in the first field.  Comments are introduced with the number
sign.  Blank lines and purely comment lines have no fields. 
Whitespace is stripped from the beginning and ending of each field.
Comments are stripped off, but kept on the side, since one particular
file stores relevant information in a line comment.  Rather than
process each field as it is encountered, like I would normally do,
each line is split into an array of fields first.  This makes the
parsing routines more readable and obvious as well.

<<Process a line of [[UnicodeData.txt]]>>=
split_line(lbuf);
if(num_fields < 15) { /* should never happen! */
  perror("UnicodeData.txt");
  exit(1);
}
@

<<UCD parser local definitions>>=
static char **fields;
static int num_fields, max_fields = 0;
static char *line_comment;
@

<<Additional parse-ucd C files>>=
mallocdef.h \
@

<<Additional parse-ucd includes>>=
#include "mallocdef.h"
@

<<UCD parser local functions>>=
static void split_line(char *buf)
{
  char fc = 0;

  if(!max_fields)
    inisize(fields, (max_fields = 16));
  num_fields = 0;
  line_comment = NULL;
  while(isspace(*buf)) buf++;
  if(!*buf || *buf == '#')
    return;
  while(1) {
    while(isspace(*buf))
      buf++;
    if(fc == '#')
      line_comment = buf;
    char *f = buf, *nf;
    for(nf = buf; *nf && (fc == '#' || (*nf != ';' && *nf != '#')); nf++);
    fc = *nf;
    buf = nf + 1;
    while(nf > f && isspace(nf[-1])) --nf;
    *nf = 0;
    if(line_comment)
      return;
    check_size(fields, max_fields, num_fields + 1);
    fields[num_fields++] = f;
    if(!fc)
      return;
  }
}
@

Since named properties are being parsed, and named variables will be
created, it is useful to have a database of all of the property names
and their aliases before starting.  For this reason, the first file to
process is the file which contains those names:  [[PropertyAliases.txt]].
This file is read in and stored in a local table, sorted by name.  The
sorting is done after the fact to reduce complexity.

<<[[uni_alias_t]]>>=
/** A name and its known aliases */
typedef struct {
  const char *short_name, /**< Primary name; usually short */
             *long_name, /**< Secondary name, usually longer;
	                  **  sometimes same as \ref short_name */
  *alt_name, /**< First alias, if non-NULL */
  *alt_name2; /**< Second alias, if non-NULL */
} uni_alias_t;
@

<<Unicode property exports for generator>>=
<<[[uni_alias_t]]>>
@

<<Known Data Types>>=
uni_alias_t,%
@

<<UCD parser local definitions>>=
static uni_alias_t *prop_aliases;
static int num_prop_aliases = 0, max_prop_aliases = 0;
@

<<Initialize UCD files>>=
open_f("PropertyAliases.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  split_line(lbuf);
  if(num_fields < 2)
    continue;
  check_size(prop_aliases, max_prop_aliases, num_prop_aliases + 1);
  prop_aliases[num_prop_aliases].short_name = strdup(fields[0]);
  prop_aliases[num_prop_aliases].long_name = strdup(fields[1]);
  /* there are no comments on alias lines, so field count is correct */
  prop_aliases[num_prop_aliases].alt_name =
                 num_fields > 2 ? strdup(fields[2]) : NULL;
  prop_aliases[num_prop_aliases].alt_name2 =
                 num_fields > 3 ? strdup(fields[3]) : NULL;
  if(num_fields > 4) {
    perror("PropertyAliases.txt");
    exit(1);
  }
  num_prop_aliases++;
}
fclose(f);
@

There are actually two fields to sort by: long name and short name.
The main table will be sorted by long name, and a side table with
pointers will be used to sort the table by short name as well.  That
way, lookups can use binary searching on either field.

<<UCD parser local functions>>=
static int cmp_longname(const void *a, const void *b)
{
  return strcmp(((uni_alias_t *)a)->long_name, ((uni_alias_t *)b)->long_name);
}
@

<<Initialize UCD files>>=
qsort(prop_aliases, num_prop_aliases, sizeof(uni_alias_t), cmp_longname);
@

<<UCD parser local definitions>>=
uni_alias_t **prop_aliases_short;
@

<<UCD parser local functions>>=
static int cmp_shortname(const void *a, const void *b)
{
  return strcmp((*(uni_alias_t **)a)->short_name,
                (*(uni_alias_t **)b)->short_name);
}
@

<<Initialize UCD files>>=
inisize(prop_aliases_short, num_prop_aliases);
for(i = 0; i < num_prop_aliases; i++)
  prop_aliases_short[i] = &prop_aliases[i];
qsort(prop_aliases_short, num_prop_aliases, sizeof(*prop_aliases_short),
      cmp_shortname);
@


Properties are added to a master property list, containing the name
and the parsed contents.  This is simply tacked on next to the
like-named property in the property name table, if applicable.  Some
properties are artificial, though, so they are appended to the end.  A
separate count is provided for this purpose.

<<UCD parser local definitions>>=
<<[[prop_t]] prerequisites>>
typedef struct {
  const char *name;
  <<Property parsed contents>>
} prop_t;
static prop_t *parsed_props;
static uint32_t nparsed, maxparsed = 0;
@

<<Known Data Types>>=
prop_t,%
@

<<UCD parser local functions>>=
static int add_prop(const char *name)
{
  uni_alias_t *pn, n;
  uint32_t i;
  static char **added_names = NULL;
  static int *added_names_p = NULL;
  static int num_added_names = 0, max_added_names = 0;

  if(!maxparsed) {
    inisize(parsed_props, (maxparsed = num_prop_aliases));
    nparsed = num_prop_aliases;
    clearbuf(parsed_props, nparsed);
  }
  n.long_name = name;
  pn = bsearch(&n, prop_aliases, num_prop_aliases, sizeof(*prop_aliases),
               cmp_longname);
  if(!pn) {
    uni_alias_t **ppn;
    pn = &n;
    n.short_name = name;
    ppn = bsearch(&pn, prop_aliases_short, num_prop_aliases,
                  sizeof(*prop_aliases_short), cmp_shortname);
    if(ppn)
      pn = *ppn;
    else
      pn = NULL;
  }
  if(pn)
    i = (int)(pn - prop_aliases);
  else {
    int h, l, m, c;
    for(l = 0, h = num_added_names - 1; l <= h; ) {
      m = (h + l) / 2;
      c = strcmp(name, added_names[m]);
      if(c < 0)
        h = m - 1;
      else if(c > 0)
        l = m + 1;
      else
        return added_names_p[m];
    }
    if(maxparsed == nparsed) {
      check_size(parsed_props, maxparsed, nparsed + 1);
      <<Resize prop-associated arrays>>
    }
    i = nparsed;
    clearbuf(&parsed_props[nparsed], 1);
    <<Clear prop-associated array[i]>>
    nparsed++;
    if(num_added_names == max_added_names) {
      check_size(added_names, max_added_names, num_added_names + 1);
      if(!added_names_p)
	inisize(added_names_p, max_added_names);
      else
	resize(added_names_p, max_added_names);
    }
    if(l < num_added_names) {
      movebuf(added_names + l + 1, added_names + l, num_added_names - l);
      movebuf(added_names_p + l + 1, added_names_p + l, num_added_names - l);
    }
    added_names[l] = strdup(name);
    added_names_p[l] = i;
    num_added_names++;
  }
  if(!parsed_props[i].name)
    parsed_props[i].name = strdup(name);
  return i;
}
@

The first file to parse for actual data is the main database file,
[[UnicodeData.txt]].

<<Parse UCD files>>=
open_f("UnicodeData.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Process a line of [[UnicodeData.txt]]>>
}
fclose(f);
@

The first field is a code point.  While it appears that each line only
deals with one code point, there are actually ranges as well.  These
are specified by two consecutive entries with names ending in \texttt{,
First>} and \texttt{, Last>}, respectively.

<<Parser common variables>>=
uint32_t low, high;
char *s;
@

<<Process a line of [[UnicodeData.txt]]>>=
low = high = strtol(fields[0], NULL, 16);
if(fields[1][0] == '<' && (s = strchr(fields[1], ',')) &&
   !strcasecmp(s, ", First>")) {
  if(!mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    perror("UnicodeData.txt");
    exit(1);
  }
  split_line(lbuf);
  if(num_fields < 15 ||
     fields[1][0] != '<' || !(s = strchr(fields[1], ',')) ||
     strcasecmp(s, ", Last>")) { /* should also never happen! */
    perror("UnicodeData.txt");
    exit(1);
  }
  high = strtol(fields[0], NULL, 16);
}
@

The parsed contents for boolean properties are the range table and the
associated multi-level table.  In order to build the range table in
place, the storage size of the range table is kept as well.

<<Property parsed contents>>=
uni_chrrng_t *rng;
uint32_t len, max_len;
uint32_t *mt;
@

<<Parser common variables>>=
uint32_t i, j;
@

<<UCD parser local functions>>=
static void add_bool_rng(prop_t *p, uint32_t low, uint32_t high)
{
  if(!p->max_len)
    inisize(p->rng, (p->max_len = 8));
  if(p->len && p->rng[p->len - 1].high == low - 1)
    p->rng[p->len - 1].high = high;
  else {
    check_size(p->rng, p->max_len, p->len + 1);
    p->rng[p->len].low = low;
    p->rng[p->len].high = high;
    ++p->len;
  }
}
@

<<UCD parser local definitions>>=
#define decl_bool(n) int prop_##n = -1
#define add_bool(n) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  add_bool_rng(&parsed_props[prop_##n], low, high); \
} while(0)
@

The only directly derived boolean properties in [[UnicodeData.txt]] are
ASSIGNED, which is true if the character is present, and
Bidi\_Mirrored, which is true if field 10 is Y%
\footnote{This property, like many boolean properties, has property
value aliases that indicate Yes, T, and True as possible truth values
as well.  For now, though, all files are parsed assuming Y, since that
is always the case in the current UCD.}%
.

<<UCD parser variables>>=
decl_bool(ASSIGNED);
decl_bool(Bidi_Mirrored);
@

<<Process a line of [[UnicodeData.txt]]>>=
add_bool(ASSIGNED);
if(fields[9][0] == 'Y')
  add_bool(Bidi_Mirrored);
@

The [[PropList.txt]] file has more boolean properties, though.  In
fact, all entries in this file specify boolean properties.

<<Parse UCD files>>=
open_f("PropList.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Process a line of [[PropList.txt]]>>
}
fclose(f);
@

Like most of the rest of the UCD files, this file specifies ranges
using double-dot-separated code points.  Also, there are numerous
blank lines and comments, which need to be skipped.

<<Process a line of [[PropList.txt]]>>=
<<Parse dotted cp>>
@

<<Parse dotted cp>>=
split_line(lbuf);
if(!num_fields || !isxdigit(*lbuf))
  continue;
low = high = strtol(lbuf, &s, 16);
if(s && *s == '.' && s[1] == '.')
  high = strtol(s + 2, &s, 16);
if(*s) { /* should never happen, but if it does: ignore */
  fprintf(stderr, "bad col 1: %s\n", lbuf);
  continue;
}
@

Each boolean property is true if field two is the name of that
property.  Since the [[add_prop]] routine finds the correct entry based
on the name (assuming the name exists), it can be called every time,
and the correct entry will be updated.  Hyphen is a deprecated
property, so it is explicitly excluded.  It would be nice if there
were some place where a list of deprecated propreties could be read.
At the moment, there's only a table in the human-readable UAX~\#44.

<<Process a line of [[PropList.txt]]>>=
if(!strcmp(fields[1], "Hyphen")) /* deprecated 6.0.0 */
  continue;
<<Just add field two as binary property>>
@

<<Just add field two as binary property>>=
add_bool_rng(&parsed_props[add_prop(fields[1])], low, high);
@

Speaking of [[DerivedCoreProperties.txt]], this file has entirely
boolean properties as well, including a deprecated one.

<<Parse UCD files>>=
open_f("DerivedCoreProperties.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  if(!strcmp(fields[1], "Grapheme_Link")) /* deprecated 5.0.0 */
    continue;
  <<Just add field two as binary property>>
}
fclose(f);
@

The Emoji property, introduced in 9.0.0, is listed in a separate file,
[[emoji-data.txt]].  This is distributed separately from the UCD prior
to 13.0.0, but I will only look for it in the UCD emoji subdirectory.

<<Parse UCD files>>=
open_f("emoji/emoji-data.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  <<Just add field two as binary property>>
}
fclose(f);
@

The [[CompositionExclusions.txt]] file is simply a commmented list of
code points, one per line, each of which has the
Composition\_Exclusion property.  Since comments are processed as
fields, and all lines have trailing comments, the number of fields is
sufficient to use the same parsing technique as for the other files.

<<UCD parser variables>>=
decl_bool(Composition_Exclusion);
@

<<Parse UCD files>>=
open_f("CompositionExclusions.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_bool(Composition_Exclusion);
}
fclose(f);
@

Unfortunately, this file does not have all entries in order. Instead,
it lists multiple groups, and each group is in order.  This requires
post-processing.  In fact, since this might happen with other files as
well, this processing may as well be done for all tables.

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng)
    fixup_rng(&parsed_props[i]);
@

<<UCD parser local functions>>=
static void fixup_rng(prop_t *p)
{
  uint32_t i;
  qsort(p->rng, p->len, sizeof(uni_chrrng_t), uni_cmprng);
  /* starting at top means only optimized entries are memmove'd */
  for(i = p->len - 1; i > 0; i--) {
    uint32_t j = i;
    while(i > 0 && p->rng[i - 1].high == p->rng[i].low - 1)
      i--;
    if(i == j)
      continue;
    p->rng[i].high = p->rng[j].high;
    if(j < p->len - 1)
        movebuf(p->rng + i + 1, p->rng + j + 1, p->len - (j + 1));
    p->len -= j - i;
    if(!i)
      break;
  }
}
@

[[DerivedNormalizationProps.txt]] contains the only remaining required
boolean UCD properties.  Unlike the others, there are some non-boolean
properties in this file as well.  Luckily, some boolean values can be
detected by simply counting the fields.  The Expands\_On\_* properties
are deprecated.  Technically, some of the quick check fields could be
booleans as well, but they will be taken care of as enumerations.

<<Parse UCD files>>=
open_f("DerivedNormalizationProps.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  <<Process a line of [[DerivedNormalizationProps.txt]]>>
}
fclose(f);
@

<<Process a line of [[DerivedNormalizationProps.txt]]>>=
if(num_fields == 2) {
  if(!strncmp(fields[1], "Expands_On_", 11)) /* deprecated 6.0.0 */
    continue;
  <<Just add field two as binary property>>
  continue;
}
@

There are no boolean fields exported from Unihan.

Now, to finish up post-processing, the multi-level table is generated.

<<Post-process property data>>=
uint32_t ml_len;
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng)
    parsed_props[i].mt = uni_rng_to_multi_bit(parsed_props[i].rng,
                                              parsed_props[i].len,
                                              &ml_len);
@

\subsection{Generating the Static Data}

The boolean properties can now be printed, using the canonical short
name.  Declarations are all printed to the same file:
[[uni_prop.gen.h]].  However, each boolean property is printed to a
different file, so that static linking will only pull in the desired
properties and representations.  They are then linked into a separate
library from the support routines.  The names of the files are
extracted from the generated header file, and as such, [[parse-ucd]]
is a prerequisite to generating the full makefile.  This complicates
things, and makes builds much slower.  In the future, this may be
changed to use a recursive make, so that only generating the library
is dependent on the generated code.

\lstset{language=make}
<<makefile.rules>>=
uni_prop.gen.h: parse-ucd
	./parse-ucd
@

<<Headers to Install>>=
uni/uni_prop.gen.h \
@

\lstset{language=C}
<<Unicode property exports>>=
/** \file uni_prop.gen.h Automatically generated supplement to uni_prop.h */
#include "uni_prop.gen.h"
@

<<makefile.rules>>=
uni_prop.h: uni_prop.gen.h
@

<<UCD parser local definitions>>=
#define open_wf(f, fn) \
  FILE *f; \
  if(!(f = fopen(fn, "w"))) { \
    perror(fn); \
    exit(1); \
  }
@

<<Dump character information as C code>>=
open_wf(gen_h, "uni_prop.gen.h");
fputs("/** \\addtogroup uni_prop */\n"
      "/** @{ */\n"
       "/** \\addtogroup uni_prop_dat Unicode Property Data */\n"
      "/** @{ */\n", gen_h);
@

<<Clean up after parsing UCD files>>=
fputs("/** @} */\n/** @} */\n", gen_h);
fclose(gen_h);
@

\lstset{language=make}
<<makefile.vars>>=
MAKEFILES+=makefile.unidat
@

<<makefile.rules>>=
makefile.unidat: uni_prop.gen.h
	printf 'RNGDAT_NAMES := ' >$@
	fgrep '[]' $^ | <<Filter generated file names>>
	       sed -e 's/\[.*//;s/.* //' | tr \\n ' ' >>$@
-include makefile.unidat
$(RNGDAT_NAMES:%=%.gen.c): uni_prop.gen.h
	@touch "$@"
$(RNGDAT_NAMES:%=%.gen.o): uni_prop.h
@

<<Library [[uni]] Members>>=
$(RNGDAT_NAMES:%=%.gen.o)
@

<<C Headers>>=
uni_prop.gen.h \
@

Converting these to lower-case would be more consistent, but once the
name is known, casing shouldn't really matter.  In addition, a simple
query function (actually, preprocessor macro wrapper) is printed for
the multi-level table lookup functions.  Finally, a list of boolean
properties needs to be converted into calls to [[bool()]] for the test
program.  This is generated directly into a fixed-named C file,
[[tsttab.tests.gen.c]].

<<Plain Built Files>>=
tsttab.tests.gen.c \
@

\lstset{language=C}
<<Test generated tables>>=
#include "tsttab.tests.gen.c"
@

<<Dump character information as C code>>=
open_wf(tstf, "tsttab.tests.gen.c");
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    const char *lname = i < num_prop_aliases ? prop_aliases[i].long_name :
                                               parsed_props[i].name;
    char nbuf[64];
    sprintf(nbuf, "uni_%s_rng.gen.c", name);
    open_wf(of, nbuf);
    fprintf(of, "#include \"uni_prop.h\"\n\n"
                "const uni_chrrng_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].len; j++)
      fprintf(of, "\t{ 0x%04X, 0x%04X }%s\n", parsed_props[i].rng[j].low,
                                              parsed_props[i].rng[j].high,
					      j < parsed_props[i].len - 1 ? "," : "");
    fputs("};\n", of);
    fclose(of);
    fprintf(gen_h, "/** Sorted range table for Unicode boolean %s property */\n"
                   "extern const uni_chrrng_t uni_%s_rng[];\n"
		   "/** Length of \\ref uni_%s_rng */\n"
		   "#define uni_%s_rng_len %d /* %d lookups max */\n"
		   "/** True if Unicode %s property is true for \\p x */\n"
                   "#define uni_is_%s(x) uni_is_x(x, uni_%s_mtab)\n",
		   lname, name, name, name, parsed_props[i].len,
		   lg2(parsed_props[i].len + 1), lname, name, name);
    print_mtab(name, lname, parsed_props[i].mt, gen_h);
    fprintf(tstf, "bool(%s);\n", name);
  }
}
@

<<Clean up after parsing UCD files>>=
fclose(tstf);
@

<<Unicode property exports>>=
/** Determine if \p cp is present in multi-level bit array \p tab */
int uni_is_x(uint32_t cp, const uint32_t *tab);
@

<<Unicode property functions>>=
int uni_is_x(uint32_t cp, const uint32_t *tab)
{
  const uint8_t *mr;
  uint8_t mv = uni_multi_tab_lookup(tab, cp / 8, &mr, 0);
  if(mr)
    mv = *mr;
  return mv & UNI_BSET_MASK(mr, cp) ? 1 : 0;
}
@

<<range table result for [[i]]>>=
uni_is_x(i, mtab)
@

\lstset{language=sh}
<<Clean temporary files>>=
rm -f uni_*.gen.[co]
@

When printing the multi-level table, comments are added to indicate
the struture.

\lstset{language=C}
<<UCD parser local functions>>=
static void print_mtab(const char *name, const char *lname, const uint32_t *mt,
                       FILE *gen_h)
{
  if(!mt)
    return; /* mtab may be inappropriate for some tables */
  char nbuf[64];
  sprintf(nbuf, "uni_%s_mtab.gen.c", name);
  open_wf(mf, nbuf);
  <<Multi-level Table Info Vars>>
  uint32_t j;
  fprintf(mf, "#include <stdint.h>\n\n"
              "const uint32_t uni_%s_mtab[] = {\n", name);
  fprintf(mf, "\t/* low, high, inv/fullen, l0len */\n"
              "\t0x%X, 0x%X, (%d << 1) + %d, %d,\n",
	      mt[0], mt[1], mt[2] >> 1, mt[2] & 1, nskip);
  fprintf(gen_h, "/** Multi-level table for Unicode %s property */\n"
                 "extern const uint32_t uni_%s_mtab[]; /* %d", lname, name, nskip);
  <<Multi-level Table Info Pointers Loop>>
    fprintf(mf, "\t/* level %d (pointers) */\n"
                "\t0x%X, /* shift=%d psz=%d nextblk=%d */\n",
                l, desc, desc & 0xff, psz, desc >> 16);
    fprintf(gen_h, "/%d", desc >> 16);
    fprintf(mf, "\t/* raw pointers; pointer[0] = l%dlen = %d */\n\t", l + 1,
            nskip / (desc >> 16));
    for(j = 0; j < skip - 1; j++, i++)
      fprintf(mf, "0x%X,%s", mt[i], !((j + 1) % 8) ? "\n\t" : " ");
    fprintf(mf, "0x%X,\n", mt[i++]);
  <<Multi-level Table Info Pointers End of Loop>>
  fprintf(mf, "\t/* level %d (raw data) */\n"
              "\t0%s", l, skip ? ",\n\t" : "\n");
  for(j = 0; j < skip; j++, i++)
    fprintf(mf, "0x%X%s", mt[i], j == skip - 1 ? "\n" :
                                                 !((j + 1) % 8) ? ",\n\t" :
						                  ", ");
  fputs("};\n", mf);
  fclose(mf);
  fprintf(gen_h, " (%d-level) */\n"
                 "/** Length (in words) of multi-level table for Unicode %s property */\n"
                 "#define uni_%s_mtab_len %d\n", l + 1, name, name, i);
}
@

\subsection{Additional Properties}

A few specific character classes that are useful for some applications
do not warrant a full set of tables.  Instead, they are implemented as
macros that compare directly to the limited set they contain.  These
are:  characters which act as line terminators, characters which
may initiate backslash-escapes, and UTF-16 surrogates.

<<Unicode property exports for generator>>=
/** Returns true if \p cp is a newline character */
int uni_is_nl(uint32_t cp);
#define uni_is_nl(x) ((x) == '\n' || (x) == '\r' || (x) == '\v' || (x) == '\f' || \
                  (x) == 0x0085 || (x) == 0x2028 || (x) == 0x2029)
/** Returns true if \p cp is a backslash character.
  * This is the equivalent to \p cp == '\\' when processing NFKC_CF text */
int uni_is_bs(uint32_t cp);
#define uni_is_bs(x) ((x) == '\\' || (x) == 0xFE68 || (x) == 0xFF3C)
/** Returns true if \p cp is a UTF-16 surrogate */
int uni_is_surrogate(uint32_t cp);
#define uni_is_surrogate(x) ((x) >= 0xD800 && (x) < 0xE000)
@

\section{Enumerated Properties}

Enumerated properties have a limited number of values, which have
names of their own.  These names may have aliases, as well.  Of course
all properties have a limited number of values, so it is a matter of
interpretation whether it qualifies as an enumeration.  The general
rule followed here is that this only includes properties with an
explicit list of aliases.  That includes one psuedo-property: the list
of properties itself.  An enumerated property can be split into sets,
each of which consists of the characters having one specific
enumerated value.  This means that each enumerated property also
defines a number of boolean properties, which are covered above.
Additional useful operations include:

\begin{itemize}
\item Find out the value \emph{of} the property for a character.  This
should be a numeric value indicating the enumeration literal index.
\item Given a value, find out its \emph{name} and primary \emph{alias}.
\item Given a name or alias, find out its \emph{value}.  This should
support Unicode sloppy matching:  case-insensitive, with hyphens and
underscores removed (except as noted).
\item Obtain a \emph{list} of all possible values and aliases, for more
general searching.
\end{itemize}

\subsection{Storage Methods}

Again, for the \emph{of} operation, the multi-level table seems
appropriate.  Rather than storing the actual string value, the
enumeration is simply converted into an integer from zero to the
number of elements, and that integer is stored.  For all enumerations
but the character name, this used to be just one byte.  As of Unicode
8, the [[blk]] property requires 2 bytes.  For the character name, a
different approach is required.  Since every code point has a
different name, there will never be any sharing of lower-level tables.
The only possible sharing would be in the algorithmically generated
code points.  Since this is a significant special case, it gets its
own section, later on.

To obtain a subset for boolean operations, the range list is provided
as well.  It is augmented by a data field.  No enumeration actually
requires 32 bits, so adding another separate field is wasteful.
Instead, an 8-bit bit field is added.  This does slow things down
slightly, but for performance, the multi-level table is better, anyway.

<<[[uni_chrrng_dat8_t]]>>=
/** A simple 64-bit range structure with room for 8-bit data */
typedef struct {
    uint32_t low, /**< Low end of range */ high:24, /**< High end of range */
             dat:8; /**< Value for all code points within this range */
} uni_chrrng_dat8_t;
#define uni_chrrng_dat8_val(x) (x).dat
#define uni_set_chrrng_dat8_val(x, v) (x).dat = v
#define uni_chrrng_high8(x) (x).high
@

<<Unicode property exports for generator>>=
<<[[uni_chrrng_dat8_t]]>>
<<[[uni_cmprng_dat]][[8]] Prototype>>;
<<[[uni_chrrng_dat]][[8]] Prototype>>;
@

<<[[uni_cmprng_dat]](@sz) Prototype>>=
/** Compare \ref uni_chrrng_dat<<@sz>>_t code point ranges pointed to by \p a and \p b.
  * Intended for use with qsort and bsearch */
int uni_cmprng_dat<<@sz>>(const void *a, const void *b)
@

<<[[uni_chrrng_dat]](@sz) Prototype>>=
/** Return the value associated with \p cp in a \ref uni_chrrng_dat<<@sz>>_t table.
  * If \p cp is not covered by any range, \p def is returned.  The 
  * table \p tab/\p tab_len must be sorted as per \ref uni_cmprng_dat<<@sz>>.
  * If there are overlapping ranges in the array, results are undefined */
uint<<@sz>>_t uni_chrrng_dat<<@sz>>(uint32_t cp, const uni_chrrng_dat<<@sz>>_t *tab,
                         uint32_t tab_len, uint<<@sz>>_t def)
@

<<Known Data Types>>=
uni_chrrng_dat8_t,%
@

<<Unicode property functions for generator>>=
<<[[uni_cmprng_dat]][[8]]>>
<<[[uni_chrrng_dat]][[8]]>>
@

<<[[uni_cmprng_dat]](@sz)>>=
<<[[uni_cmprng_dat]](@sz) Prototype>>
{
    const uni_chrrng_dat<<@sz>>_t *_a = a, *_b = b;
    uint32_t ahigh = uni_chrrng_high<<@sz>>(*_a),
             bhigh = uni_chrrng_high<<@sz>>(*_b);

    if(ahigh < _b->low)
        return -1;
    else if(bhigh < _a->low)
        return 1;
    else
        return 0;
}
@

<<[[uni_chrrng_dat]](@sz)>>=
<<[[uni_chrrng_dat]](@sz) Prototype>>
{
#if 0
  uni_chrrng_dat<<@sz>>_t cr = {cp, cp}, *tab_el;
  tab_el = bsearch(&cr, tab, tab_len, sizeof(*tab), uni_cmprng_dat<<@sz>>);
  return tab_el ? uni_chrrng_dat<<@sz>>(*tab_el) : def;
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > uni_chrrng_high<<@sz>>(tab[j]))
      l = j + 1;
    else
      return uni_chrrng_dat<<@sz>>_val(tab[j]);
  }
  return def;
#endif
}
@

Some enums, and other numeric types, require more storage.  For 16
bits, the [[low]] field can be split as well.  For 32 bits, if they
are unique, it is unlikely that long ranges (if any ranges at all)
exist.  For this reason, a low+length approach is used instead; this
splits the [[low]] field to make room for a [[len]] field (which is
the length of the range minus one).

<<[[uni_chrrng_dat16_t]]>>=
/** A simple 64-bit range structure with room for 16 bits of data */
typedef struct {
    uint32_t low:24, /**< Low end of range */
             datl:8, /**< Low half of 16-bit data assigned to range */
	     high:24, /**< High end of range */
	     dath:8; /**< High half of 16-bit data assigned to range */
} uni_chrrng_dat16_t;
#define uni_chrrng_dat16_val(x) ((x).datl + ((x).dath << 8))
#define uni_set_chrrng_dat16_val(x, v) do { \
  uint16_t _v = v; \
  (x).datl = _v & 0xff; \
  (x).dath = _v >> 8; \
} while(0)
#define uni_chrrng_high16(x) (x).high
@

<<[[uni_chrrng_dat32_t]]>>=
/** A simple 64-bit range structure with room for 32 bits of data */
typedef struct {
    uint32_t low:24, /**< Low end of range */
             len:8, /**< Size of range - 1 (high = low + len) */
	     dat; /* 32-bit data assigned to range */
} uni_chrrng_dat32_t;
#define uni_chrrng_dat32_val(x) (x).dat
#define uni_set_chrrng_dat32_val(x, v) (x).dat = v
#define uni_chrrng_high32(x) ((x).low + (x).len)
@

<<Unicode property exports for generator>>=
<<[[uni_chrrng_dat16_t]]>>
<<[[uni_cmprng_dat]][[16]] Prototype>>;
<<[[uni_chrrng_dat]][[16]] Prototype>>;
<<[[uni_chrrng_dat32_t]]>>
<<[[uni_cmprng_dat]][[32]] Prototype>>;
<<[[uni_chrrng_dat]][[32]] Prototype>>;
@

<<Known Data Types>>=
uni_chrrng_dat16_t,uni_chrrng_dat32_t,%
@

<<Unicode property functions for generator>>=
<<[[uni_cmprng_dat]][[16]]>>
<<[[uni_chrrng_dat]][[16]]>>
<<[[uni_cmprng_dat]][[32]]>>
<<[[uni_chrrng_dat]][[32]]>>
@

For the \emph{name} and \emph{alias} operations, a simple lookup table
can be provided, indexed on the enumeration value (again, except for
the name property).  The table fulfulls the \emph{list} requirement as
well.

For the \emph{value} operation, either a sorted table of names or some
other structure can be provided.  The advantage of the sorted table of
names is that it takes no effort to create, search, and provides the
\emph{list} operation (specifially for searching) as well.  A
statically generated hash function might be faster, though.  Since
applications that need to look up a lot of \emph{value}s are probably
rare, only the simple sorted table is provided, and the library user
is expected to convert that to a hash table, prefix tree, or some
other structure as needed.

\subsection{Name Tables}

The first thing to read in is the list of aliases.  These are keyed on
property short name.  Since there is already a list of property names,
the array of aliases may as well correspond.  As the list of aliases
is read, the comment may be stored as an alias as well.  The group
names for the gc property are stored in this file, and the comment for
the definition line for these names indicates the fundamental values
that are combined into this group.

<<UCD parser local definitions>>=
uni_alias_t **val_aliases;
int *num_val_aliases, *max_val_aliases;
@

<<Initialize UCD files>>=
inisize(val_aliases, num_prop_aliases);
clearbuf(val_aliases, num_prop_aliases);
inisize(num_val_aliases, num_prop_aliases);
clearbuf(num_val_aliases, num_prop_aliases);
inisize(max_val_aliases, num_prop_aliases);
clearbuf(max_val_aliases, num_prop_aliases);
@

<<Resize prop-associated arrays>>=
resize(val_aliases, maxparsed);
resize(num_val_aliases, maxparsed);
resize(max_val_aliases, maxparsed);
@

<<Clear prop-associated array[i]>>=
val_aliases[i] = NULL;
num_val_aliases[i] = max_val_aliases[i] = 0;
@

Then, the alias file can be read and stored in the array.  While it
would be more efficient to only look up the destination when things
change, it's easier to just go ahead and put it in place.

<<Initialize UCD files>>=
open_f("PropertyValueAliases.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  split_line(lbuf);
  if(num_fields < 3)
    continue;
  uni_alias_t me, *mep = &me;
  me.short_name = fields[0];
  uni_alias_t **ret = bsearch(&mep, prop_aliases_short, num_prop_aliases,
                              sizeof(*prop_aliases_short), cmp_shortname);
  if(!ret) {
    perror("PropertyValueAliases.txt");
    exit(1);
  }
  int idx = (int)(*ret - prop_aliases);
  check_size(val_aliases[idx], max_val_aliases[idx], num_val_aliases[idx] + 1);
  val_aliases[idx][num_val_aliases[idx]].short_name = strdup(fields[1]);
  val_aliases[idx][num_val_aliases[idx]].long_name = strdup(fields[2]);
  /* there are no comments on alias lines, so field count is correct */
  val_aliases[idx][num_val_aliases[idx]].alt_name =
                 num_fields > 3 ? strdup(fields[3]) : NULL;
  val_aliases[idx][num_val_aliases[idx]].alt_name2 =
                 num_fields > 4 ? strdup(fields[4]) : NULL;
  if(num_fields > 5) {
    perror("PropertyValueAliases.txt");
    exit(1);
  }
  if(line_comment && !strcmp(fields[0], "gc"))
    val_aliases[idx][num_val_aliases[idx]].alt_name2 = strdup(line_comment);
  num_val_aliases[idx]++;
}
fclose(f);
@

Since we're about to scan files with the enumerations in text form,
the next logical step is to build a search mechanism.  In this case,
just a simple sorted array, using the standard loose matching.  This
is necessary because some fields do not match any of the literal
values we have gathered so far.  This table will be dumped as well,
for the \emph{value} function.

<<[[uni_valueof_t]]>>=
/** A simple structure associating a name with an integer */
typedef struct {
  const char *name; /**< The name */
  int val; /**< The value */
} uni_valueof_t;
@

<<Unicode property exports for generator>>=
<<[[uni_valueof_t]]>>
@

<<Known Data Types>>=
uni_valueof_t,%
@

<<UCD parser local definitions>>=
uni_valueof_t **enum_vals;
uint32_t *enum_vals_len;
@

<<Unicode property exports for generator>>=
/** Compare strings pointed to indirectly by \p a and \p b.
  * This is simple comparison using strcmp.  This function is intended
  * for use with bsearch and qsort for arrays of \ref uni_valueof_t, but
  * can be used with any array of structures whose first member is a
  * string pointer */
int uni_cmp_valueof(const void *a, const void *b);
@

<<Unicode property functions for generator>>=
int uni_cmp_valueof(const void *a, const void *b)
{
  return strcmp(((uni_valueof_t *)a)->name, ((uni_valueof_t *)b)->name);
}
@

<<Initialize UCD files>>=
inisize(enum_vals, num_prop_aliases);
clearbuf(enum_vals, num_prop_aliases);
inisize(enum_vals_len, num_prop_aliases);
clearbuf(enum_vals_len, num_prop_aliases);
for(i = 0; i < num_prop_aliases; i++) {
  if(!val_aliases[i])
    continue;
  /* ignore boolean properties */
  if(num_val_aliases[i] == 2 && !strcmp(val_aliases[i][0].short_name, "N") &&
     !strcmp(val_aliases[i][1].short_name, "Y"))
    continue;
  uni_valueof_t *valueof;
  int num_valueof = 0;
  inisize(valueof, num_val_aliases[i] * 4);
  for(j = 0; j < num_val_aliases[i]; j++) {
    const uni_alias_t *va = &val_aliases[i][j];
    valueof[num_valueof].name = strdup(va->short_name);
    valueof[num_valueof++].val = j;
    valueof[num_valueof].name = strdup(va->long_name);
    valueof[num_valueof++].val = j;
    if(va->alt_name) {
      valueof[num_valueof].name = strdup(va->alt_name);
      valueof[num_valueof++].val = j;
    }
    /* need to skip comments stored in alt_name2; all contain | */
    if(va->alt_name2 && !strchr(va->alt_name2, '|')) {
      valueof[num_valueof].name = strdup(va->alt_name2);
      valueof[num_valueof++].val = j;
    }
  }
  for(j = 0; j < num_valueof; j++) {
    char *n = (char *)valueof[j].name, *d = n;
    for(; *n; n++) {
      if(isupper(*n))
        *d++ = tolower(*n);
      else if(*n != '_' && *n != '-' && !isspace(*n))
        *d++ = *n;
    }
    *d = 0;
  }
  qsort(valueof, num_valueof, sizeof(*valueof), uni_cmp_valueof);
  /* remove duplicates now that they are always contiguous */
  for(j = 1; j < num_valueof; j++)
    if(!strcmp(valueof[j].name, valueof[j - 1].name)) {
      free((char *)valueof[j].name);
      if(valueof[j].val != valueof[j - 1].val) {
        perror(valueof[j - 1].name);
	exit(1);
      }
      movebuf(valueof + j, valueof + j + 1, num_valueof - (j + 1));
      --j;
      --num_valueof;
    }
  enum_vals[i] = valueof;
  enum_vals_len[i] = num_valueof;
}
@

<<Resize prop-associated arrays>>=
resize(enum_vals, maxparsed);
resize(enum_vals_len, maxparsed);
@

<<Clear prop-associated array[i]>>=
enum_vals[i] = NULL;
enum_vals_len[i] = 0;
@

For each value alias, an enumeration needs to be printed to the header
file.  Both the short and long name should be supported, but there is
little value in supporting the secondary aliases.  This excludes
binary values, which have pointless yes/no aliases, and, if possible,
deprecated values.  Special care must be taken with the version
aliases, as the numeric values have decimal points.  The CCC aliases
are numeric as well, but in this case, the short name should be
ignored and used as the value directly instead.

In order to support the \emph{name} and \emph{value} functions, tables
need to be dumped as well.  First, a table similar to the locally
stored table is printed; this does the \emph{name} operation.  Next,
value table built just previously is dumped for the \emph{value}
operation.  Unlike the enumerations, no values may be skipped when
generating the string tables.  The string tables are stored in the
same file, because both files use the same set of string constants,
and most compilers share storage for like string constants.

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  const char *pn = i < num_prop_aliases ? prop_aliases[i].short_name :
                                          parsed_props[i].name;
  const char *lname = i < num_prop_aliases ? prop_aliases[i].long_name :
                                             parsed_props[i].name;
  if(!enum_vals[i])
    continue;
  /* ignore boolean properties */
  if(parsed_props[i].rng)
    continue;
  <<Ignore unimplemented enums>>
  <<Generate C code for enumeration constants>>
}
@

<<Generate C code for enumeration constants>>=
fprintf(gen_h, "/** Enumeration constant names for property %s.\n"
               "  * Indexed by \\ref uni_%s_t enumeration literal;\n"
	       "  * length is UNI_NUM_%s */\n"
	       "extern const uni_alias_t uni_%s_nameof[];\n", lname, pn, pn, pn);
char nbuf[64];
sprintf(nbuf, "uni_%s_nameof.gen.c", pn);
open_wf(nf, nbuf);
fprintf(nf, "#include \"uni_prop.h\"\n\n"
	    "const uni_alias_t uni_%s_nameof[] = {\n", pn);
/* generating enum on gen_h */
fprintf(gen_h, "/** Enumeration constants for enumerated property %s */\n"
	       "typedef enum {\n", lname);
int last_nameof = -1;
for(j = 0; j < num_val_aliases[i]; j++) {
  const uni_alias_t *va = &val_aliases[i][j];
  /* print nameof table entry */
  char *e_nameof;
  int cur_nameof = strtol(va->short_name, &e_nameof, 10);
  if(!*e_nameof && strcmp(va->short_name, va->long_name))
    while(++last_nameof != cur_nameof)
      fputs("\t{ NULL },\n", nf);
  fputs("\t{ ", nf);
  put_string(nf, va->short_name);
  if(strcmp(va->long_name, va->short_name)) {
    fputs(", ", nf);
    put_string(nf, va->long_name);
  }
  if(va->alt_name && strcmp(va->alt_name, va->long_name)) {
    fputs(", ", nf);
    put_string(nf, va->alt_name);
  }
  /* again, | is to skip comments */
  if(va->alt_name2 && !strchr(va->alt_name2, '|') &&
     strcmp(va->alt_name, va->alt_name2)) {
    fputs(", ", nf);
    put_string(nf, va->alt_name2);
  }
  fputs(" }", nf);
  if(j < num_val_aliases[i] - 1)
    putc(',', nf);
  putc('\n', nf);
  /* filter out bad enum values */
  /* skip version short name */
  if(strchr(va->short_name, '.')) {
    fprintf(gen_h, "\tUNI_%s_%s, /**< %s */\n", pn, va->long_name, va->short_name);
    continue;
  }
  /* use numeric values directly instead of making into enum */
  int sn_bad = 0;
  if(!isdigit(va->short_name[0]) || !strcmp(va->long_name, va->short_name)) {
    const char *np;
    /* slow, but necessary with e.g. ID_Restrict_Type */
    fprintf(gen_h, "\tUNI_%s_", pn);
    for(np = va->short_name; *np; np++) {
      if(isalnum(*np))
        putc(*np, gen_h);
      else if(*np == ' ' || *np == '-' || *np == '_')
        putc('_', gen_h);
      else {
        fprintf(gen_h, "_%02X", (int)(uint8_t)*np);
	sn_bad = 1;
      }
    }
    fprintf(gen_h, ", /**< %s */", sn_bad ? "*" : va->long_name);
  }
  if(strcmp(va->long_name, va->short_name)) {
    fprintf(gen_h, "%cUNI_%s_%s = ", isdigit(va->short_name[0]) ? '\t' : ' ',
                    pn, va->long_name);
    if(!isdigit(va->short_name[0]))
      fprintf(gen_h, "UNI_%s_%s,", pn, va->short_name);
    else
      fprintf(gen_h, "%s,", va->short_name);
    fprintf(gen_h, " /**< %s */", sn_bad ? "*" : va->short_name);
  }
  /* filter out alt names with hyphens */
  if(va->alt_name && !strchr(va->alt_name, '-') &&
     strcmp(va->alt_name, va->long_name))
    fprintf(gen_h, " UNI_%s_%s = UNI_%s_%s, /**< %s */",
                   pn, va->alt_name, pn, va->long_name, va->long_name);
  /* again, | is to skip comments */
  if(va->alt_name2 && !strchr(va->alt_name2, '|') && !strchr(va->alt_name2, '-') &&
     strcmp(va->alt_name, va->long_name) && strcmp(va->alt_name, va->alt_name2))
    fprintf(gen_h, " UNI_%s_%s = UNI_%s_%s, /**< %s */",
                   pn, va->alt_name2, pn, va->long_name, va->long_name);
  fputc('\n', gen_h);
}
fputs("};\n", nf);
fprintf(gen_h, "\tUNI_NUM_%s /**< Number of enumeration values for %s */\n"
               "} uni_%s_t;\n", pn, lname, pn);
fprintf(gen_h, "/** Sorted list of all names for values of %s property\n"
               "  * Names are in their canonical form, and are associated\n"
	       "  * with the \\ref uni_%s_t enumeration literal for that\n"
	       "  * name */\n"
	       "extern const uni_valueof_t uni_%s_valueof[];\n", lname, pn, pn);
fprintf(nf, "const uni_valueof_t uni_%s_valueof[] = {\n", pn);
for(j = 0; j < enum_vals_len[i]; j++) {
  const char *name = val_aliases[i][enum_vals[i][j].val].short_name;
  if(isdigit(*name))
    name = val_aliases[i][enum_vals[i][j].val].long_name;
  fputs("\t{ ", nf);
  put_string(nf, enum_vals[i][j].name);
  fprintf(nf, ", UNI_%s_", pn);
  /* slow, but necessary with e.g. ID_Restrict_Type */
  /* also, the CLDR ones have /, +, space, etc. */
  for(; *name; name++) {
    if(isalnum(*name))
      putc(*name, nf);
    else if(*name == ' ' || *name == '-' || *name == '_')
      putc('_', nf);
    else
      fprintf(nf, "_%02X", (int)(uint8_t)*name);
  }
  fprintf(nf, " }%s\n", j < enum_vals_len[i] - 1 ? "," : "");
}
fputs("};\n", nf);
fclose(nf);
fprintf(gen_h, "/** Length of \\ref uni_%s_valueof */\n"
	       "#define uni_%s_valueof_len %d /* %d lookups max */\n",
               pn, pn, enum_vals_len[i], lg2(enum_vals_len[i] + 1));
@

<<UCD parser local functions>>=
static void put_string(FILE *f, const char *s)
{
  if(!s)
    fputs("NULL", f);
  else {
    putc('"', f);
    while(*s) {
      if(*s == '\\' || *s == '"')
        putc('\\', f);
      if(isprint(*s))
        putc(*s, f);
      else
        fprintf(f, "\\%03o", (int)(uint8_t)*s);
      s++;
    }
    putc('"', f);
  }
}
@

Since the two arrays are stored in the same file, the logic to
generate file names needs to skip the valueof array.

<<Filter generated file names>>=
fgrep -v _valueof\[ | \
@

Another useful array to print is for approximate name matching.  In
this case, approximate is defined by UAX44-LM3: case-insensitive,
optionally without any prefix of is, and without any spaces,
underscores, and hyphens.  A function to perform most of this
transformation in-place is provided as well.  If additional
transformations are required, such as compatibility decomposition, they
must be done before calling this function.  All enumerations are
ASCII-only, so no decomposition is needed, and they are simply stored
lower-case with hyphens, spaces, and underscores removed.  Initial
``is'' is left in; that should only be removed from user input.  To
compare using this function, strip, find a match, and if none is found,
search again with initial ``is'' removed if present.  A sample search
function is provided.

<<Unicode property exports for generator>>=
/** Prepare name for loose enumeration name matching.
  * Removes all underscores, spaces, and dashes, and converts upper-case
  * characters to lower-case in-place in \p n.  Does not perform normalization */
void uni_enum_name_strip(char *n);
@

<<Unicode property functions for generator>>=
void uni_enum_name_strip(char *n)
{
  char *s, *d;
  for(s = d = n; *s; s++)
    if(*s != '-' && *s != '_' && *s != ' ')
      *d++ = tolower(*s);
  *d = 0;
}
@

<<Unicode property exports>>=
/** Look up \p n in stripped name table \p tab/\p len loosely.
  * This assumes that \p tab is fully stripped as per \ref uni_enum_name_strip,
  * and then sorted using simple string compares (e.g. \ref uni_cmp_valueof).
  * The name \p n is itself stripped, and if lookup fails, and \p n is
  * prefixed with "is", that is stripped as well and lookup is repeated.
  * If lookup succeeds, the integer value is returned, reduced to 8 bits.
  * Otherwise, \p def is returned.  Note that since this routine always
  * duplicates and strips \p n, this may not be the most efficient way to
  * perform such lookups. */
uint32_t uni_x_valueof_approx(const char *n, const uni_valueof_t *tab, int len,
                              uint32_t def);
@

<<Unicode property functions>>=
uint32_t uni_x_valueof_approx(const char *n, const uni_valueof_t *tab, int len,
                              uint32_t def)
{
  /* yuck - strdup on every call */
  char *s = strdup(n);
  uni_valueof_t v, *f;
  uni_enum_name_strip(s);
  v.name = s;
  /* more yuck - bsearch is 1/2 as fast as hand-coded binary search */
  f = bsearch(&v, tab, len, sizeof(*tab), uni_cmp_valueof);
  if(!f && !strncmp(s, "is", 2) && s[2]) {
    v.name = s + 2;
    /* more yuck - bsearch is 1/2 as fast as hand-coded binary search */
    f = bsearch(&v, tab, len, sizeof(*tab), uni_cmp_valueof);
  }
  free(s);
  return f ? f->val : def;
}
@

<<Generate C code for enumeration constants>>=
sprintf(nbuf, "uni_%s_valueof_approx.gen.c", pn);
open_wf(anf, nbuf);
fprintf(gen_h, "/** Sorted list of all names for values of %s property for\n"
	       "  * approximate matching.  Names have been stripped using\n"
	       "  * \\ref uni_enum_name_strip, and are associated with the\n"
	       "  * \\ref uni_%s_t enumeration literal for that name.  The\n"
	       "  * length of this array is also \\ref uni_%s_valueof_len */\n"
	       "extern const uni_valueof_t uni_%s_valueof_approx[];\n",
	       lname, pn, pn, pn);
fprintf(anf, "#include \"uni_prop.h\"\n\n"
             "const uni_valueof_t uni_%s_valueof_approx[] = {\n", pn);
uni_valueof_t *approx_array;
inisize(approx_array, enum_vals_len[i]);
for(j = 0; j < enum_vals_len[i]; j++) {
  approx_array[j].name = strdup(enum_vals[i][j].name);
  approx_array[j].val = enum_vals[i][j].val;
  uni_enum_name_strip((char *)approx_array[j].name);
}
qsort(approx_array, enum_vals_len[i], sizeof(*approx_array), uni_cmp_valueof);
for(j = 0; j < enum_vals_len[i]; j++) {
  const char *name = val_aliases[i][approx_array[j].val].short_name;
  if(isdigit(*name))
    name = val_aliases[i][approx_array[j].val].long_name;
  fputs("\t{ ", anf);
  put_string(anf, approx_array[j].name);
  fprintf(anf, ", UNI_%s_", pn);
  /* slow, but necessary with e.g. ID_Restrict_Type */
  /* also, the CLDR ones have /, +, space, etc. */
  for(; *name; name++) {
    if(isalnum(*name))
      putc(*name, anf);
    else if(*name == ' ' || *name == '-' || *name == '_')
      putc('_', anf);
    else
      fprintf(anf, "_%02X", (int)(uint8_t)*name);
  }
  fprintf(anf, " }%s\n", j < enum_vals_len[i] - 1 ? "," : "");
  free((char *)approx_array[j].name);
}
fputs("};\n", anf);
fclose(anf);
free(approx_array);
fprintf(gen_h, "/** Look up \\ref uni_%s_t value for name \\p x using loose\n"
	       "  * matching.  See \\ref uni_x_valueof_approx for details.\n"
	       "  * Lookup failure returns ~0 */\n"
	       "#define uni_%s_lookup(n) "
                   "uni_x_valueof_approx(n, uni_%s_valueof_approx, %d, ~0)\n",
	       pn, pn, pn, enum_vals_len[i]);
@

There is still one nagging issue:  the gc aliases include some that are
actually combinations of the others.  Since each [[uni_valueof_t]] can only 
hold one value, this is not going to work.  To support this, an extra
table is generated which translates a value into a 64-bit mask that
includes all base values.  For the base values themselves, only their
own bit will be set.  The pseudo values are recognized by a comment in
their last field, which is stored in one of the aliases.  This is the
reason the vertical bar was filtered out above:  all of the aliases
have vertical bars in their last field.

<<Generate C code for enumeration constants>>=
if(!strcmp(pn, "gc")) {
  fputs("/** Convert aggregate \\ref uni_gc_t values to constituent codes.\n"
	"  * Each element of this array is a bit mask with a bit set for each\n"
	"  * base code associated with the \\ref uni_gc_t index.  For example,\n"
	"  * uni_gc_trans[uni_gc_M] has bits uni_gc_Mc, uni_gc_Me and\n"
	"  * uni_gc_Mn set.  A base code is identified by having its own\n"
	"  * bit set in the result */\n"
        "extern const uint64_t uni_gc_trans[];\n", gen_h);
  open_wf(tf, "uni_gc_trans.gen.c");
  fputs("#include \"uni_prop.h\"\n\n"
	"const uint64_t uni_gc_trans[] = {\n", tf);
  for(j = 0; j < num_val_aliases[i]; j++) {
    const char *desc;
    fprintf(tf, "\t/* %2s */ ", val_aliases[i][j].short_name);
    desc = val_aliases[i][j].alt_name2;
    if(desc && !strchr(desc, '|'))
      desc = NULL;
    if(!desc)
      fprintf(tf, "1ULL << UNI_gc_%s", val_aliases[i][j].short_name);
    else {
      const char *desce;
      while(1) {
        desce = strchr(desc, ' ');
	if(!desce)
	  desce = desc + strlen(desc);
	fprintf(tf, "(1ULL << UNI_gc_%.*s)%s", (int)(desce - desc), desc,
	                                     *desce ? " | " : "");
	if(!*desce)
	  break;
	desc = desce + 3; /* past ' | ' */
      }
    }
    if(j < num_val_aliases[i] - 1)
      putc(',', tf);
    putc('\n', tf);
  }
  fputs("};\n", tf);
  fclose(tf);
}
@

One other set of aliases has already been read, and is treated in some
contexts almost like enumeration properties: the property names
themselves.  While it defeats the purpose of placing properties in
separate files for statically linking only needed properties, a master
table of all properties and their exported symbol values will make
other projects' data file generation much easier.

<<[[uni_propdesc_t]]>>=
/** A property supported by libuni. */
typedef struct {
  uni_alias_t name; /**< The property's name and aliases */
  const uint32_t *mtab;  /**< The property's multi-level lookup table */
  uint32_t mtab_len; /**< The property's multi-level lookup table's length */
  const void *tab; /**< The property's range table; type is determined by \ref type */
  uint32_t tab_len; /**< The property's range table length */
  uni_prop_type_t type; /**< The property type; see \ref uni_prop_type_t */
  <<Additional property structure members>>
} uni_propdesc_t;
@

<<Unicode property exports for generator>>=
/** The type of a property in \ref uni_propdesc_t */
typedef enum {
  UNI_PROP_TYPE_NONE,  /**< Dummy array entry; never happens */
  <<Additional property type names>>
  /** Boolean; range table is \ref uni_chrrng_t and multi-level table is
   ** 1 bit per code point */
  UNI_PROP_TYPE_BOOL,
  /** Enumeration; range table is \ref uni_chrrng_dat8_t and multi-level
   ** is 1 byte per code point */
  UNI_PROP_TYPE_ENUM,
  /** Enumeration; range table is \ref uni_chrrng_dat16_t and multi-level
   ** is 2 bytes per code point */
  UNI_PROP_TYPE_ENUM2
} uni_prop_type_t;
<<[[uni_propdesc_t]]>>
@

<<Known Data Types>>=
uni_propdesc_t,uni_prop_type_t,%
@

<<UCD parser local definitions>>=
uni_valueof_t *prop_ptrs;
@

<<Dump character information as C code>>=
fprintf(gen_h, "/** A table containing a property descriptor for all generated properties */\n"
	       "extern const uni_propdesc_t uni_propdesc[];\n");
open_wf(pnf, "uni_propdesc.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_propdesc_t uni_propdesc[] = {\n", pnf);
uint32_t num_prop_ptrs = 0, max_prop_ptrs;
inisize(prop_ptrs, max_prop_ptrs = 20);
int index = 0;
for(i = 0; i < nparsed; i++) {
  uni_prop_type_t t = UNI_PROP_TYPE_NONE;
  const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                            parsed_props[i].name;
  if(!parsed_props[i].mt)
    continue;
  if(parsed_props[i].rng)
    t = UNI_PROP_TYPE_BOOL;
  <<Set prop type for export>>
  if(t == UNI_PROP_TYPE_NONE)
    continue;
  if(index)
    fputs(",\n", pnf);
  check_size(prop_ptrs, max_prop_ptrs, num_prop_ptrs + 4);
  fprintf(pnf, "\t{{\"%s\"", name);
  prop_ptrs[num_prop_ptrs].name = strdup(name);
  prop_ptrs[num_prop_ptrs++].val = index;
  if(i < num_prop_aliases) {
    fprintf(pnf, ", \"%s\"", prop_aliases[i].long_name);
    if(strcmp(prop_aliases[i].long_name, name)) {
      prop_ptrs[num_prop_ptrs].name = strdup(prop_aliases[i].long_name);
      prop_ptrs[num_prop_ptrs++].val = index;
    }
    if(prop_aliases[i].alt_name && strcmp(prop_aliases[i].alt_name, name) &&
       strcmp(prop_aliases[i].long_name, prop_aliases[i].alt_name)) {
      fprintf(pnf, ", \"%s\"", prop_aliases[i].alt_name);
      prop_ptrs[num_prop_ptrs].name = strdup(prop_aliases[i].alt_name);
      prop_ptrs[num_prop_ptrs++].val = index;
    }
    if(prop_aliases[i].alt_name2 && strcmp(prop_aliases[i].alt_name, name) &&
       strcmp(prop_aliases[i].long_name, prop_aliases[i].alt_name) &&
       strcmp(prop_aliases[i].alt_name, prop_aliases[i].alt_name2)) {
      fprintf(pnf, ", \"%s\"", prop_aliases[i].alt_name2);
      prop_ptrs[num_prop_ptrs].name = strdup(prop_aliases[i].alt_name);
      prop_ptrs[num_prop_ptrs++].val = index;
    }
  }
  fprintf(pnf, "},\n"
               "\t\tuni_%s_mtab, uni_%s_mtab_len, uni_%s_%s, uni_%s_%s_len,\n",
               name, name, name, <<Plain table name>> "rng", name,
	       <<Plain table name>> "rng");
  fprintf(pnf, "\t\t%d", (int)t);
  <<Print additional property structure members>>
  fputc('}', pnf);
  index++;
}
fprintf(gen_h, "/** Length of \\ref uni_propdesc array */\n"
	       "#define uni_propdesc_len %d\n"
	       "/** Sorted list of all generated properties' names and aliases.\n"
               "  * Names are in their canonical form, and are associated\n"
	       "  * with the index into \\ref uni_propdesc of its descriptor */\n"
               "extern const uni_valueof_t uni_propdesc_valueof[];\n",
               index);
fputs("\n};\n\n"
      "const uni_valueof_t uni_propdesc_valueof[] = {\n", pnf);
qsort(prop_ptrs, num_prop_ptrs, sizeof(*prop_ptrs), uni_cmp_valueof);
for(i = num_prop_ptrs - 1; i > 0; i--) {
  if(!strcmp(prop_ptrs[i -1].name, prop_ptrs[i].name)) {
    if(prop_ptrs[i - 1].val != prop_ptrs[i].val) {
      fprintf(stderr, "Prop name conflict %s\n", prop_ptrs[i].name);
      exit(1);
    }
    free((char *)prop_ptrs[i].name);
    num_prop_ptrs--;
    movebuf(prop_ptrs + i, prop_ptrs + i + 1, num_prop_ptrs - i);
  }
}
for(i = 0; i < num_prop_ptrs; i++)
  fprintf(pnf, "\t{\"%s\", %d}%s\n", prop_ptrs[i].name, prop_ptrs[i].val,
                                   i < num_prop_ptrs - 1 ? "," : "");
fputs("};\n", pnf);
fclose(pnf);
fprintf(gen_h, "/** Length of \\ref uni_propdesc_valueof */\n"
	       "#define uni_propdesc_valueof_len %d /* %d lookups max */\n"
	       "/** Sorted list of all names for generated properties for\n"
	       "  * approximate matching.  Names have been stripped using\n"
	       "  * \\ref uni_enum_name_strip, and are associated with the\n"
	       "  * \\ref uni_propdesc array index of that property's\n"
	       "  * descriptor.  The length of this array is also\n"
	       "  * \\ref uni_propdesc_valueof_len */\n"
               "extern const uni_valueof_t uni_propdesc_valueof_approx[];\n",
	       num_prop_ptrs, lg2(num_prop_ptrs + 1));
open_wf(paf, "uni_propdesc_valueof_approx.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_valueof_t uni_propdesc_valueof_approx[] = {\n", paf);
for(i = 0; i < num_prop_ptrs; i++)
  uni_enum_name_strip((char *)prop_ptrs[i].name);
qsort(prop_ptrs, num_prop_ptrs, sizeof(*prop_ptrs), uni_cmp_valueof);
for(i = num_prop_ptrs - 1; i > 0; i--) {
  if(!strcmp(prop_ptrs[i -1].name, prop_ptrs[i].name)) {
    if(prop_ptrs[i - 1].val != prop_ptrs[i].val) {
      fprintf(stderr, "Prop name loose conflict %s\n", prop_ptrs[i].name);
      exit(1);
    }
    num_prop_ptrs--;
    free((char *)prop_ptrs[i].name);
    movebuf(prop_ptrs + i, prop_ptrs + i + 1, num_prop_ptrs - i);
  }
}
for(i = 0; i < num_prop_ptrs; i++)
  fprintf(paf, "\t{\"%s\", %d}%s\n", prop_ptrs[i].name, prop_ptrs[i].val,
                                   i < num_prop_ptrs - 1 ? "," : "");
fputs("};\n", paf);
fclose(paf);
fprintf(gen_h, "/** Look up \\ref uni_propdesc array index for name \\p n\n"
	       "  * using loose matching.  See \\ref uni_x_valueof_approx\n"
	       "  * for details.  If lookup fails, ~0 is returned */\n"
	       "#define uni_propdesc_lookup(n) uni_x_valueof_approx(n, "
                      "uni_propdesc_valueof_approx, %d, ~0)\n",
	       num_prop_ptrs);
@

Now that the aliases have been taken care of, it's time to read in the
enumeration properties themselves.  As with the boolean types, a range
table will be built first, and then converted to a multi-level table
when finished.

<<Property parsed contents>>=
uni_chrrng_dat8_t *rng_dat8;
uni_chrrng_dat16_t *rng_dat16;
uint32_t def;
@

<<UCD parser local functions>>=
<<[[add_enum_rng]][[8]]>>
<<[[add_enum_rng]][[16]]>>
#define add_enum_rng(p, l, h, v) do { \
  if(num_val_aliases[p] >= 256) \
    add_enum_rng16(&parsed_props[p], l, h, v); \
  else \
    add_enum_rng8(&parsed_props[p], l, h, v); \
} while(0)
@

<<[[add_enum_rng]](@sz)>>=
static void add_enum_rng<<@sz>>(prop_t *p, uint32_t low, uint32_t high, uint<<@sz>>_t val)
{
  if(p->def == val)
    return;
  if(!p->max_len)
    inisize(p->rng_dat<<@sz>>, (p->max_len = 8));
  if(p->len && p->rng_dat<<@sz>>[p->len - 1].high == low - 1 &&
     uni_chrrng_dat<<@sz>>_val(p->rng_dat<<@sz>>[p->len - 1]) == val)
    p->rng_dat<<@sz>>[p->len - 1].high = high;
  else {
    check_size(p->rng_dat<<@sz>>, p->max_len, p->len + 1);
    p->rng_dat<<@sz>>[p->len].low = low;
    p->rng_dat<<@sz>>[p->len].high = high;
    uni_set_chrrng_dat<<@sz>>_val(p->rng_dat<<@sz>>[p->len], val);
    ++p->len;
  }
}
@

<<Set prop type for export>>=
if(parsed_props[i].rng_dat8)
  t = UNI_PROP_TYPE_ENUM;
if(parsed_props[i].rng_dat16)
  t = UNI_PROP_TYPE_ENUM2;
@

<<Additional property structure members>>=
uint32_t def; /**< The default value for enumeration types */
const uni_alias_t *nameof; /**< The names and aliases for enumeration constants */
/** Name-to-value lookup tables for enumeration constants */
const uni_valueof_t *valueof, *valueof_approx;
uint32_t nameof_len, valueof_len;  /**< The length of the above three tables */
@

<<Print additional property structure members>>=
fprintf(pnf, ", %d", (int)parsed_props[i].def);
if((t == UNI_PROP_TYPE_ENUM || t == UNI_PROP_TYPE_ENUM2) && enum_vals[i]) {
  fprintf(pnf, ", uni_%s_nameof, uni_%s_valueof, uni_%s_valueof_approx,\n"
               "\t\tUNI_NUM_%s, uni_%s_valueof_len",
               name, name, name, name, name);
} else
  fputs(", NULL, NULL, NULL, 0, 0", pnf);
@

<<UCD parser local functions>>=
static uint32_t enum_val(int pno, const char *v)
{
  char *s = strdup(v), *t, *d;
  uni_valueof_t me, *vp;
  me.name = s;

  d = t = s;
  for(; *t; t++) {
    if(isupper(*t))
      *d++ = tolower(*t);
    else if(*t != '_' && *t != '-' && !isspace(*t))
      *d++ = *t;
  }
  *d = 0;
  vp = bsearch(&me, enum_vals[pno], enum_vals_len[pno], sizeof(me),
               uni_cmp_valueof);
  /* permit excess prefix of "is" */
  if(!vp && s[0] == 'i' && s[1] == 's' && s[2]) {
    me.name = s + 2;
    vp = bsearch(&me, enum_vals[pno], enum_vals_len[pno], sizeof(me),
                 uni_cmp_valueof);
  }
  free(s);
  if(!vp) {
    perror(v);
    exit(1);
  }
  return vp->val;
}
@

<<UCD parser local definitions>>=
#define decl_enum(n, d) \
  int prop_##n = -1; \
  const char *def_##n = d
#define add_enum(n, v) do { \
  if(prop_##n < 0) { \
    prop_##n = add_prop(#n); \
    parsed_props[prop_##n].def = def_##n ? enum_val(prop_##n, def_##n) : 0; \
  } \
  if(*v) \
    add_enum_rng(prop_##n, low, high, enum_val(prop_##n, v)); \
} while(0)
#define add_int(n, val) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(isdigit(*val)) \
    add_enum_rng(prop_##n, low, high, strtol(val, NULL, 0)); \
  else \
    add_enum(n, val); \
} while(0)
@

\subsection{Testing}

To test the table implementations, a different, but nearly identical,
set of routines is used compared to the ones one for booleans.

<<Functions to help test generated tables>>=
<<[[dat]][[8]] test>>
@

<<[[dat]](@sz) test>>=
#define dat<<@sz>>(x, d) doit_dat<<@sz>>(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab, d)

static void doit_dat<<@sz>>(const char *name, const uni_chrrng_dat<<@sz>>_t *rng, uint32_t nent,
                       const uint32_t *mtab, uint<<@sz>>_t def)
{
    uint32_t i;

    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      uint<<@sz>>_t rv = uni_chrrng_dat<<@sz>>(i, rng, nent, def);
      uint<<@sz>>_t mv = <<range table data for [[i]]>>;
      if(rv != mv) {
        fprintf(stderr, "mismatch %s@%d %d %d\n", name, i, (int)rv, (int)mv);
	exit(1);
      }
    }
    /* check performance */
    int j;
    double tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = uni_chrrng_dat<<@sz>>(i, rng, nent, def);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = <<range table data for [[i]]>>;
    tt = tend();
    printf("  r%.0f t%.0f %.2fx\n", tr, tt, tr / tt);
}
@

\subsection{Parsing the UCD}

First, [[UnicodeData.txt]] has a few fields.  Field 3 is gc.  Field 4
is ccc.  Field 5 provides bc, but its default value is complicated and
more easily obtained from [[extracted/DerivedBidiClass.txt]]%
\footnote{\label{fn:extracted}Technically, extracted files are
informative only, and are not guaranteed to be generated correctly.
However, I will trust that they are.  Generating the correct data from
scratch is trivial, but an extra complication to the code I'd rather
avoid.}%
. Field 6 provides
dt, albeit indirectly. Fields 7 through 9 provide nt, even more
indirectly%
\footnote{[[extracted/DerivedNumericType.txt]] might be a better
source for nt, as it includes Unihan data.}%
.

<<Initialize UCD files>>=
decl_enum(gc, "Cn");
decl_enum(ccc, 0);
decl_enum(bc, "L");
decl_enum(dt, "None");
decl_enum(nt, "None");
@

<<Process a line of [[UnicodeData.txt]]>>=
add_enum(gc, fields[2]);
add_int(ccc, fields[3]);
/* add_enum(bc, fields[4]); */
if(fields[5][0] == '<') {
  char *eval = fields[5] + 1;
  while(*eval && *eval != '>')
    eval++;
  if(!*eval) {
    perror("dt");
    exit(1);
  }
  *eval = 0;
  add_enum(dt, fields[5] + 1);
  *eval = '>';
} else if(fields[5][0])
  add_enum(dt, "can");
if(fields[6][0])
  add_enum(nt, "decimal");
else if(fields[7][0])
  add_enum(nt, "digit");
else if(fields[8][0])
  add_enum(nt, "numeric");
@

<<Parse UCD files>>=
open_f("extracted/DerivedBidiClass.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(bc, fields[1]);
}
fclose(f);
@

The other file that has mixed field types is
[[DerivedNormalizationProps.txt]].  The Boolean parser simply looked
at the number of fields, and added if the number of fields was too low
to be anything but a boolean.  This parser can't really do the same,
without first loading the expected default values.  There are only
four properties, so it's safer to just do them by hand.  In fact, two
of them can only have two values: Yes and No.  This makes them boolean
in my view, so that they will become.  There is one other caveat:  the
default value for those two is Yes, and the listed value is always No,
so they need to be inverted when finished.

<<Initialize UCD files>>=
decl_enum(NFC_QC, "Y");
decl_bool(NFD_QC);
decl_enum(NFKC_QC, "Y");
decl_bool(NFKD_QC);
@

<<Process a line of [[DerivedNormalizationProps.txt]]>>=
if(num_fields == 3) {
  if(!strcmp(fields[1], "NFC_QC"))
    add_enum(NFC_QC, fields[2]);
  else if(!strcmp(fields[1], "NFD_QC"))
    add_bool(NFD_QC);
  else if(!strcmp(fields[1], "NFKC_QC"))
    add_enum(NFKC_QC, fields[2]);
  else if(!strcmp(fields[1], "NFKD_QC"))
    add_bool(NFKD_QC);
}
@

<<Parse UCD files>>=
uni_chrrng_t *new;
uint32_t new_len;
uni_chrrng_setop(parsed_props[prop_NFD_QC].rng,
                 parsed_props[prop_NFD_QC].len, UNI_SOP_INV_A,
		 NULL, 0, &new, &new_len);
free(parsed_props[prop_NFD_QC].rng);
parsed_props[prop_NFD_QC].rng = new;
parsed_props[prop_NFD_QC].len = new_len;
uni_chrrng_setop(parsed_props[prop_NFKD_QC].rng,
                 parsed_props[prop_NFKD_QC].len, UNI_SOP_INV_A,
		 NULL, 0, &new, &new_len);
free(parsed_props[prop_NFKD_QC].rng);
parsed_props[prop_NFKD_QC].rng = new;
parsed_props[prop_NFKD_QC].len = new_len;
@

Then there are a number of files that describe just one enumerated
property.  Many of these follow here.

<<Initialize UCD files>>=
decl_enum(sc, "Zzzz");
decl_enum(blk, "No_Block");
decl_enum(hst, "NA");
decl_enum(lb, "XX");
decl_enum(GCB, "XX");
decl_enum(SB, "XX");
decl_enum(WB, "XX");
decl_enum(ea, "N");
decl_enum(age, "unassigned");
@

<<Parse UCD files>>=
open_f("Scripts.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  /* need to manually convert UNI_SC_Hrkt to UNI_SC_Hira+UNI_SC_Kana */
  add_enum(sc, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("Blocks.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(blk, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("HangulSyllableType.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(hst, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("LineBreak.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(lb, fields[1]);
}
fclose(f);
@

Note that the E\_Base, E\_Modifier, Glue\_After\_Zwj and E\_Base\_GAZ
Grapheme Break properties introduced in 9.0.0 are artifiicial, and are
based on Emoji properites.  There is no convenient file from which to
pull them.  They are also removed in 11.0.0, so no attempt will be
made to support them.

<<Parse UCD files>>=
open_f("auxiliary/GraphemeBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
   add_enum(GCB, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("auxiliary/SentenceBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(SB, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("auxiliary/WordBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(WB, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
/* EastAsianWidth.txt has a complex default */
open_f("extracted/DerivedEastAsianWidth.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(ea, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("DerivedAge.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(age, fields[1]);
}
fclose(f);
@

And there is one file that has two properties per line:
[[ArabicShaping.txt]]%
\footnote{Actually, [[ArabicShaping.txt]] has three values per line,
but field 2 (descriptive name) is informative and will not be read in
as a property unless I find a use.}%
.  However, like the bc and ea properties, the jt
property's default value is not a single value.  For that reason, it
is read from the derived file instead%
\footnote{see footnote \ref{fn:extracted} on page \pageref{fn:extracted}.}%
.

<<Initialize UCD files>>=
decl_enum(jt, "U");
decl_enum(jg, "No_Joining_Group");
@

<<Parse UCD files>>=
open_f("ArabicShaping.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  /* add_enum(jt, fields[2]); */
  add_enum(jg, fields[3]);
}
fclose(f);
open_f("extracted/DerivedJoiningType.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(jt, fields[1]);
}
fclose(f);
@

There are several more enumerated properties that I don't use, but
it's easier to add and ignore than to implement later.

<<Initialize UCD files>>=
decl_enum(InSC, "Other");
decl_enum(InPC, "NA"); // was InMC before 8.0.0
decl_enum(vo, "R");
@

<<Parse UCD files>>=
open_f("IndicSyllabicCategory.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(InSC, fields[1]);
}
fclose(f);
open_f("IndicPositionalCategory.txt"); // was IndicMatraCategory.txt before 8
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(InPC, fields[1]);
}
fclose(f);
open_f("VerticalOrientation.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(vo, fields[1]);
}
fclose(f);
@

\lstset{language=txt}
<<FIXME>>=
Unihan_IRGSources.txt: (cj)kIICore - no propvalaliases
@

\subsection{Generating the Static Data}

Once finished, the tables can be dumped.  Just as with boolean
properties, only range tables have been built, so it's time to
generate the multi-level table, assuming that the range data is
correct.  Since the way that the automatic enumerations are generated
does not guarantee that zero is the default value, and zero is stored
more efficiently than any other value, all values below the default
are incremented by one and the default value is aliased to zero as
well.  The default value itself is never stored in the table; it is
supplied in the lookup function as well.  The actual multi-level table
data is just the adjusted enumeration value, one byte per entry.  For
larger values, it is the adjusted value in raw native-endian form.

\lstset{language=C}
<<Unicode property exports for generator>>=
<<[[uni_rng_dat]][[8]][[to_multi]] Prototype>>;
<<[[uni_rng_dat]][[16]][[to_multi]] Prototype>>;
<<[[uni_rng_dat]][[32]][[to_multi]] Prototype>>;
@

<<[[uni_rng_dat]](@sz)[[to_multi]] Prototype>>=
/** Convert a range table with <<@sz>>-bit data to a multi-level table.
  * The range table \p tab/\p len must be sorted as per \ref uni_cmprng_dat<<@sz>>,
  * with no duplicate ranges.  All values outside of the range, as well as
  * values matching the default (\p def) are converted to zeroes, and the
  * non-default values are incremented by one if \p def is non-zero.  The resulting
  * table is returned, and its length is returned in \p ml_len if not NULL */
uint32_t *uni_rng_dat<<@sz>>_to_multi(const uni_chrrng_dat<<@sz>>_t *tab,
                                  uint32_t tab_len, uint32_t *ml_len,
				  uint<<@sz>>_t def)
@

<<Unicode property functions for generator>>=
<<[[uni_rng_dat]][[8]][[to_multi]]>>
<<[[uni_rng_dat]][[16]][[to_multi]]>>
<<[[uni_rng_dat]][[32]][[to_multi]]>>
@

<<[[uni_rng_dat]](@sz)[[to_multi]]>>=
<<[[uni_rng_dat]](@sz)[[to_multi]] Prototype>>
{
  uint32_t low, high, len, i;
  uint32_t *ml;
  uint<<@sz>>_t *bits;

  /* degenerate case:  always out-of-range */
  if(!tab_len) {
    uni_bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  for(i = tab_len; i > 0; i--)
    if(uni_chrrng_dat<<@sz>>_val(tab[i - 1]) != def)
      break;
  high = uni_chrrng_high<<@sz>>(tab[i - 1]);
  for(i = 0; i < tab_len; i++)
    if(uni_chrrng_dat<<@sz>>_val(tab[i]) != def)
      break;
  low = tab[i].low;
  len = high - low + 1;
  inisize(bits, len);
  /* Optimize(maybe): only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  for(; i < tab_len; i++)
    if(uni_chrrng_dat<<@sz>>_val(tab[i]) != def)
      for(uint<<@sz>>_t *p = bits + tab[i].low - low;
          p < bits + uni_chrrng_high<<@sz>>(tab[i]) + 1 - low; p++)
	*p = uni_chrrng_dat<<@sz>>_val(tab[i]) + !!def;
  const int bytes = <<@sz>> / 8;
  uni_bits_to_multi((uint8_t *)bits, len * bytes, low * bytes, high * bytes,
                    0, 0, &ml, ml_len);
  free(bits);
  return ml;
}
@

<<UCD parser local functions>>=
<<fixup_rng_dat[[8]]>>
<<fixup_rng_dat[[16]]>>
<<fixup_rng_dat[[32]]>>
@

<<fixup_rng_dat(@sz)>>=
static void fixup_rng_dat<<@sz>>(prop_t *p)
{
  uint32_t i;
  qsort(p->rng_dat<<@sz>>, p->len, sizeof(uni_chrrng_dat<<@sz>>_t), uni_cmprng_dat<<@sz>>);
  /* starting at top means only optimized entries are memmove'd */
  for(i = p->len - 1; i > 0; i--) {
    uint32_t j = i;
    while(i > 0 &&
          p->rng_dat<<@sz>>[i].low ==
	     uni_chrrng_high<<@sz>>(p->rng_dat<<@sz>>[i - 1]) + 1 &&
	  uni_chrrng_dat<<@sz>>_val(p->rng_dat<<@sz>>[i]) ==
	     uni_chrrng_dat<<@sz>>_val(p->rng_dat<<@sz>>[i - 1]))
      i--;
    if(i == j)
      continue;
    uint32_t k = i;
    <<Adjust [[p->rng_dat]][[<<@sz>>]] range [[k]] to range[[j]]>>
    if(k != j && j < p->len - 1)
        movebuf(p->rng_dat<<@sz>> + k + 1, p->rng_dat<<@sz>> + j + 1,
	        p->len - (j + 1));
    p->len -= j - k;
    if(!i)
      break;
  }
}
@

<<Adjust [[p->rng_dat]](@sz) range [[k]] to range[[j]]>>=
p->rng_dat<<@sz>>[k].high = p->rng_dat<<@sz>>[j].high;
@

<<Adjust [[p->rng_dat]][[32]] range [[k]] to range[[j]]>>=
uint32_t len = p->rng_dat32[j].low - p->rng_dat32[i].low + p->rng_dat32[j].len;
while(len >= 256) {
  p->rng_dat32[k++].len = 255;
  p->rng_dat32[k].low = p->rng_dat32[k - 1].low + 256;
}
p->rng_dat32[k].len = len;
@

<<Post-process property data>>=
<<Post-process [[p->rng_dat]][[8]]>>
<<Post-process [[p->rng_dat]][[16]]>>
<<Post-process [[p->rng_dat]][[32]]>>
@

<<Post-process [[p->rng_dat]](@sz)>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng_dat<<@sz>>) {
    fixup_rng_dat<<@sz>>(&parsed_props[i]);
    parsed_props[i].mt = uni_rng_dat<<@sz>>_to_multi(parsed_props[i].rng_dat<<@sz>>,
                                                  parsed_props[i].len,
                                                  &ml_len, parsed_props[i].def);
}
@

In addition to the property tables, a simple query function is
printed.  This calls a generic search function using the multi-level
table.  The test cases are printed as well, and added to the boolean
tests.

<<Dump character information as C code>>=
<<Dump [[parsed_props[i].rng_dat]][[8]]>>
<<Dump [[parsed_props[i].rng_dat]][[16]]>>
<<Dump [[parsed_props[i].rng_dat]][[32]]>>
@

<<Dump [[parsed_props[i].rng_dat]](@sz)>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng_dat<<@sz>>) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    const char *lname = i < num_prop_aliases ? prop_aliases[i].long_name :
                                               parsed_props[i].name;
    char nbuf[64];
    sprintf(nbuf, "uni_%s_rng.gen.c", name);
    open_wf(of, nbuf);
    fprintf(of, "#include \"uni_prop.h\"\n\n"
		"const uni_chrrng_dat<<@sz>>_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].len; j++) {
      fprintf(of, "\t{ 0x%04X, ", parsed_props[i].rng_dat<<@sz>>[j].low);
      uint<<@sz>>_t val = uni_chrrng_dat<<@sz>>_val(parsed_props[i].rng_dat<<@sz>>[j]);
      <<Dump [[val]]/[[parsed_props[i].rng_dat]][[<<@sz>>]][j]>>
      fprintf(of, " }%s\n", j < parsed_props[i].len - 1 ? "," : "");
    }
    fputs("};\n", of);
    fclose(of);
@

<<Dump [[val]]/[[parsed_props[i].rng_dat]][[8]][j]>>=
fprintf(of, "0x%04X, ", parsed_props[i].rng_dat8[j].high);
<<Print alias [[val]] if avail>>
else
  fprintf(of, "%d", (int)val);
@

<<Dump [[val]]/[[parsed_props[i].rng_dat]][[16]][j]>>=
{
  <<Print alias [[val]] if avail>>
  else
    fprintf(of, "%d", (int)val);
}
fprintf(of, "&0xFF, 0x%04X, ", parsed_props[i].rng_dat16[j].high);
{
  <<Print alias [[val]] if avail>>
  else
    fprintf(of, "%d", (int)val);
  fputs(">>8", of);
}
@

<<Dump [[val]]/[[parsed_props[i].rng_dat]][[32]][j]>>=
fprintf(of, "%d, ", parsed_props[i].rng_dat32[j].len);
<<Print alias [[val]] if avail>>
else
  fprintf(of, "0x%06X", (int)val);
@

<<Print alias [[val]] if avail>>=
uni_alias_t *aliases = val_aliases[i];

if(aliases && (!isdigit(aliases[0].short_name[0]) ||
               strchr(aliases[0].short_name, '.'))) {
  const uni_alias_t *alias = &aliases[val];
  const char *val_name = alias->short_name;
  if(isdigit(val_name[0]))
    val_name = alias->long_name;
  /* slow, but necessary with e.g. ID_Restrict_Type */
  fprintf(of, "UNI_%s_", name);
  for(const char *np = val_name; *np; np++) {
    if(isalnum(*np))
      putc(*np, of);
    else if(*np == ' ' || *np == '-' || *np == '_')
      putc('_', of);
    else
      fprintf(of, "_%02X", (int)(uint8_t)*np);
  }
}
@

<<Dump [[parsed_props[i].rng_dat]](@sz)>>=
    fprintf(gen_h, "/** Sorted range table for Unicode%s %s property */\n" 
                   "extern const uni_chrrng_dat<<@sz>>_t uni_%s_rng[];\n"
	           "/** Length of \\ref uni_%s_rng */\n"
	           "#define uni_%s_rng_len %d /* %d lookups max */\n"
	           "/** Retrieve value of%s %s property */\n",
	           enum_vals[i] ? " enumerated" : "",
	           lname, name, name, name, parsed_props[i].len,
	           lg2(parsed_props[i].len + 1),
	           enum_vals[i] ? " enumerated" : "", lname);
    if(enum_vals[i])
      fprintf(gen_h, "#define uni_%s_of(x) "
                       "(uni_%s_t)uni_x<<@sz>>_of(x, uni_%s_mtab, %d)\n",
		     name, name, name, (int)parsed_props[i].def);
    else
      fprintf(gen_h, "#define uni_%s_of(x) uni_x<<@sz>>_of(x, uni_%s_mtab, %d)\n",
		     name, name, parsed_props[i].def);
    print_mtab(name, lname, parsed_props[i].mt, gen_h);
    if(parsed_props[i].mt)
      fprintf(tstf, "dat<<@sz>>(%s, %d);\n", name, parsed_props[i].def);
  }
@

<<Unicode property exports>>=
<<uni_x[[8]]_of Prototype>>;
<<uni_x[[16]]_of Prototype>>;
<<uni_x[[32]]_of Prototype>>;
@

<<uni_x(@sz)_of Prototype>>=
/** Look up a <<@sz>>-bit value in a multi-level table.
  * The value associated with \p cp is looked up in \p tab, which is assumed
  * to follow the conventions of \ref uni_rng_dat<<@sz>>_to_multi.  That is,
  * zeroes (and lookup failures) are converted to \p def, and non-zero values
  * are decremented by one if \p def is non-zero */ 
uint<<@sz>>_t uni_x<<@sz>>_of(uint32_t cp, const uint32_t *tab, uint<<@sz>>_t def)
@

<<Unicode property functions>>=
<<uni_x[[8]]_of>>
<<uni_x[[16]]_of>>
<<uni_x[[32]]_of>>
@

<<uni_x(@sz)_of>>=
<<uni_x(@sz)_of Prototype>>
{
  const uint8_t *mr;
  uint<<@sz>>_t mv = uni_multi_tab_lookup(tab, cp * <<@sz>> / 8, &mr, 0);
  if(mr)
    mv = *(uint<<@sz>>_t *)mr;
  if(!mv)
    return def;
  else if(def)
    return mv - 1;
  else
    return mv;
}
@

<<range table data for [[i]]>>=
uni_x<<@sz>>_of(i, mtab, def)
@

\subsection{Support}

Some of the above properties are meant to be used as part of larger
algorithms, rather than on their own.  A few of the default
implementations of those algorithms are implemented here, mostly as an
example.

\subsubsection{Grapheme Cluster Boundary Determination}

The first algorithm is for grapheme cluster boundary determination
(UAX~\#29).  A grapheme cluster is essentially a logical character,
where code points are the physical characters. They usually map to a
single displayed glyph.  While the standard allows for locale-specific
tailoring of this algorithm, the CLDR provides no such tailoring at
present, so no support is provided.  However, there are root locale
tailorings in at least CLDR 39.

UTS~\#18 (Regular Expressions), RL3.2 does provide tailoring, by
calling all multi-character collation elements grapheme clusters.
However, I prefer calling them multi-character collation elements,
since some common examples (like ll and ch) appear as multiple
distinct glyphs.  On the other hand, it might be because some
multi-character clusters have accents applied to the cluster as a
whole rather than individual characters.  If that is a problem, the
application can deal with breaking differently.  As stated above, this
routine serves mostly as an example.

The first two rules merely state that grapheme clusters can't escape
the boundaries of the text.  There is no point in implementing them.
The rest deal only with pairs of characters, so the boundary
determination function does a switch on the first character, for those
rules that have a first character, and then on any failed rules, the
second character is checked.  The rules are not repeated here; look
them up in the standard.  Instead, the implementation of each rule is
labeled with the rule number (for UAX~\#29, revision 37).  While the
CLDR root locale has this in a machine-readable format, it expects
regular expressions to be used for checks.  It is better to just
update the routine if the standard ever changes.

In order to avoid looking up GCB for all middle characters twice, the
next character's GCB property is returned, and expected to be passed
in the next time around.  Speaking of the GCB property, its use pulls
in a property table, so the function is in its own object file.

Unicode-9.0.0 broke the abiliity (promised in prior versions, at
least) to determine the boundary using only two adjacent code points. 
Before, breaks were not permitted at all between Regional Indicators
(GB8a). As of 9.0.0, a break is allowed after an even number of
Regional Indicators (GB12-GB13).  Also, GB10 (and in Unicode-11.0.0,
GB11, when GB10 was dropped) requires a break dependent on an
arbitrarily long prefix that starts with a specific character class.

Unicode-9.0.0 and later also require emoji-related character
properties in addition to just the GCB property.  In versions 9 and
10, these where not even part of the UCD, and required additional
processing to determine from the CLDR data, so this is only computed
for versions 11 and up.  Recent CLDR also adds property queries to one
of its added root rules.

Rather than pull in multiple properites, a new property is generated:
[[GCBp]].
% Begin-doc GCBp
This encodes all additional property queries as
extensions to the original property.  The extra properties are:

\begin{itemize}
\item [[XX_EP]] == same as [[XX]], but [[uni_is_ExtPict]] is true (GB11)
\item [[XX_C]] == same as [[XX]], but [[uni_InSC_of]] is
[[UNI_InSC_Consonant]] (CLDR GB9.3)
\item [[EX_V]] == same as [[EX]], but [[uni_InSC_of]] is
[[UNI_InSC_Virama]] (CLDR GB9.3)
\item [[EX_0]] == same as [[EX]], but [[uni_ccc_of]] is $0$ (and not
[[EX_V]]) (CLDR GB9.3)
\end{itemize}
% End-doc GCBp

<<Parse UCD files>>=
decl_enum(GCBp, "XX");
prop_GCBp = add_prop("GCBp");
parsed_props[prop_GCBp].def =  enum_val(prop_GCB, def_GCBp);
<<Generate [[GCBp]] property>>
@

<<Library [[uni]] Members>>=
gcbrk.o
@

<<Unicode property exports>>=
/** Find boundary between grapheme clusters.
  * Grapheme clusters always start at the beginning of text and end at
  * the end; this function is for finding breaks in the middle.  Pass in
  * the two code points surrounding the check point (\p prev, \p next), for
  * every pair of code points, in order, from the start of text.  Pass in
  * zero for the first \p prevret, and the return code from the previous
  * call otherwise.  The return value is greater than zero if and only if
  * \p prev and \p next are in different grapheme clusters.  Unicode
  * defines two types of grapheme clusters:  legacy and extended.  To
  * select the legacy form, set \p legacy to true.  Passing values other
  * that zero to \p prevret is optional except when prev is a REGIONAL
  * INDICATOR SYMBOL LETTER (1F1E6..1F1FF) or there are any Extended
  * Pictographic code points and Zero Width Joiners (200D) earlier in
  * the string.  In the former case, a potential break between two
  * such symbols may not be detected, and in the latter case, a break
  * between a Zero Width Joiner and an Extended Pictographic may be
  * incorrectly indicated.  Otherwise, it is purely for performance:  this
  * function will return accurate results at random input locations by
  * passing zero for \p prevret.
  */
int uni_gc_brk(uint32_t prev, uint32_t next, int prevret, int legacy);
@

<<gcbrk.c>>=
<<Common C Header>>
#include "uni_prop.h"

/* GB1 and GB2 are covered in the explanatory text */
int uni_gc_brk(uint32_t prev, uint32_t next, int prevret, int legacy)
{
  enum { /* psuedo codes for prevret to track state */
    /* for GB11 */
    UNI_GCBp_EP_EX = UNI_NUM_GCBp,  /* EX|EX_0|EX_V after XX_EP|EP_EX */
    UNI_GCBp_EP_ZWJ, /* ZWJ after XX_EP|EP_EX */
    /* for CLDR GB9.3 */
    UNI_GCBp_EX_C, /* EX|ZWJ after XX_C */
    UNI_GCBp_EX_CV, /* EX_V after XX_C|EX_C|EX_CV or EX|EX_V after EX_CV */
  };

  uni_GCBp_t pgcb = !prevret ? uni_GCBp_of(prev) :
                     prevret < 0 ? -prevret - 1 : prevret - 1,
              ngcb = uni_GCBp_of(next);
  switch(pgcb) {
    case UNI_GCBp_CR:
      if(ngcb == UNI_GCBp_LF) /* GB3 */
        return -(ngcb + 1);
      /* fall through */
    case UNI_GCBp_LF:
    case UNI_GCBp_CN:
      return ngcb + 1; /* GB4 */
    default:
      break;
  }
  if(ngcb == UNI_GCBp_CR || ngcb == UNI_GCBp_LF || ngcb == UNI_GCBp_CN)
    return ngcb + 1; /* GB5 */
  switch((int)pgcb) {
    case UNI_GCBp_L:
      if(ngcb == UNI_GCBp_L || ngcb == UNI_GCBp_V || ngcb == UNI_GCBp_LV ||
         ngcb == UNI_GCBp_LVT) /* GB6 */
	return -(ngcb + 1);
      break;
    case UNI_GCBp_LV:
    case UNI_GCBp_V:
      if(ngcb == UNI_GCBp_V || ngcb == UNI_GCBp_T) /* GB7 */
        return -(ngcb + 1);
      break;
    case UNI_GCBp_LVT:
    case UNI_GCBp_T:
      if(ngcb == UNI_GCBp_T) /* GB8 */
        return -(ngcb + 1);
      break;
    case UNI_GCBp_RI:
      if(ngcb == UNI_GCBp_RI)
        return prevret < 0 ? ngcb + 1 : -(ngcb + 1); /* GB12-GB13 */
      break;
    case UNI_GCBp_PP:
      if(!legacy)
        return -(ngcb + 1); /* GB9b */
      break;
    case UNI_GCBp_XX_C: /* GB9.3 state tracking */
    case UNI_GCBp_EX_C:
      /* FIXME:  if(!cldr) break; */
      if(ngcb == UNI_GCBp_EX || ngcb == UNI_GCBp_ZWJ)
        return -(UNI_GCBp_EX_C + 1);
      if(ngcb == UNI_GCBp_EX_V)
        return -(UNI_GCBp_EX_CV + 1);
      break;
    case UNI_GCBp_EX_V:
      if(ngcb == UNI_GCBp_EX || ngcb == UNI_GCBp_ZWJ ||
         ngcb == UNI_GCBp_EX_V)
	return -(UNI_GCBp_EX_CV + 1);
      if(ngcb == UNI_GCBp_XX_C)
        return -(UNI_GCBp_XX_C + 1); /* GB9.3 */
      break;
    case UNI_GCBp_XX_EP: /* GB11, mixed w/ GB9 */
      if(ngcb == UNI_GCBp_EX || ngcb == UNI_GCBp_EX_V ||
         ngcb == UNI_GCBp_EX_0)
        return -(UNI_GCBp_EP_EX + 1);
      if(ngcb == UNI_GCBp_ZWJ)
        return -(UNI_GCBp_EP_ZWJ + 1);
      break;
    default:
      break;
  }
  if(ngcb == UNI_GCBp_EX || ngcb == UNI_GCBp_EX_V || /* GB9 */
     ngcb == UNI_GCBp_EX_0 || ngcb == UNI_GCBp_ZWJ ||
     (ngcb == UNI_GCBp_SM && !legacy)) { /* GB9b */
    if(pgcb == (int)UNI_GCBp_EP_EX && ngcb != UNI_GCBp_SM) /* GB11 */
      return -((ngcb == UNI_GCBp_ZWJ ? UNI_GCBp_EP_ZWJ : UNI_GCBp_EP_EX) + 1);
    return -(ngcb + 1);
  }
  if(prevret == -(UNI_GCBp_EP_ZWJ + 1) && ngcb == UNI_GCBp_XX_EP) /* GB11 */
    return -(ngcb + 1);
  return ngcb + 1; /* GB999 */
}
@

All needed properties should be parsed by now, so it's just a matter
of merging them.  The [[InSC]] checks also verify that the script is
correct.  The intent might have been to use [[scx]] for Gujr, but that
property is not yet available, and most documentation I have read
implies that the syntax used actually only references [[sc]].  I'm not
sure why the other script tests explicitly use [[sc=]] and Gujr does
not.

<<Generate [[GCBp]] property>>=
uni_alias_t *vap;
uni_valueof_t *ev;
#define new_elit_prep(np, op, n) \
  prop_t *op##prop = &parsed_props[prop_##op]; \
  fixup_rng_dat8(op##prop); \
  enum_vals_len[prop_##np] = enum_vals_len[prop_##op] + n; \
  max_val_aliases[prop_##np] = num_val_aliases[prop_##np] = \
    num_val_aliases[prop_##op] + n; \
  inisize(val_aliases[prop_##np], max_val_aliases[prop_##np]); \
  cpybuf(val_aliases[prop_##np], val_aliases[prop_##op], \
         num_val_aliases[prop_##op]); \
  inisize(enum_vals[prop_##np], enum_vals_len[prop_##np]); \
  cpybuf(enum_vals[prop_##np], enum_vals[prop_##op], enum_vals_len[prop_##op]); \
  vap = val_aliases[prop_##np] + num_val_aliases[prop_##op]; \
  ev = enum_vals[prop_##np] + enum_vals_len[prop_##op]
#define add_new_elit(p, n, lc) \
  uint8_t p##_##n = vap - val_aliases[prop_##p]; \
  ev->val = p##_##n; \
  vap->short_name = vap->long_name = #n; \
  vap->alt_name = vap->alt_name2 = NULL; \
  vap++; \
  ev++->name = #lc
#define finish_new_elits(p) \
  qsort(enum_vals[prop_##p], enum_vals_len[prop_##p], sizeof(**enum_vals), \
      uni_cmp_valueof)
new_elit_prep(GCBp, GCB, 4);
add_new_elit(GCBp, XX_EP, xx_ep);
add_new_elit(GCBp, XX_C, xx_c);
add_new_elit(GCBp, EX_V, ex_v);
add_new_elit(GCBp, EX_0, ex_0);
finish_new_elits(GCBp);
#define elit(p, n) \
  uint8_t p##_##n = enum_val(prop_##p, #n)
elit(GCB, EX); /* XX is def_GCB */
prop_t *ExtPict_prop = &parsed_props[add_prop("ExtPict")];
fixup_rng(ExtPict_prop);
prop_t *InSC_prop = &parsed_props[prop_InSC];
fixup_rng_dat8(InSC_prop);
uint8_t InSC_Consonant = enum_val(prop_InSC, "Consonant"),
        InSC_Virama = enum_val(prop_InSC, "Virama");
prop_t *sc_prop = &parsed_props[prop_sc];
fixup_rng_dat8(sc_prop);
uint8_t sc_Gujr = enum_val(prop_sc, "Gujr"),
        sc_Telu = enum_val(prop_sc, "Telu"),
        sc_Mlym = enum_val(prop_sc, "Mlym"),
        sc_Orya = enum_val(prop_sc, "Orya"),
        sc_Beng = enum_val(prop_sc, "Beng"),
	sc_Deva = enum_val(prop_sc, "Deva");
#define is_indic_scr(s) ((s == sc_Gujr || s == sc_Telu || s == sc_Mlym || \
                          s == sc_Orya || s == sc_Beng || s == sc_Deva))
prop_t *ccc_prop = &parsed_props[prop_ccc];
fixup_rng_dat8(ccc_prop);
int curinsc = -1;
<<Skip to next valid [[curinsc]]>>
for(uint32_t curgcb = 0, curep = 0,
             gcbl = GCBprop->rng_dat8[0].low,
	     gcbh = GCBprop->rng_dat8[0].high,
	     epl = ExtPict_prop->rng[0].low,
	     eph = ExtPict_prop->rng[0].high,
	     inscl = InSC_prop->rng_dat8[curinsc].low,
	     insch = InSC_prop->rng_dat8[curinsc].high;
      curgcb < GCBprop->len || curinsc < InSC_prop->len ||
      curep < ExtPict_prop->len; ) {
  if(epl <= gcbl && epl <= inscl) {
    if(gcbl <= eph) {
      fprintf(stderr, "GCBp: GCB of ExtPict %04X not Other\n", gcbl);
      exit(1);
    }
    if(inscl <= eph) {
      fprintf(stderr, "GCBp doesn't support Indic and ExtPict %04X\n", inscl);
      exit(1);
    }
    add_enum_rng(prop_GCBp, epl, eph, GCBp_XX_EP);
#define set_propp(pref, prop, pt) do { \
  if(cur##pref == prop->len) \
    pref##l = pref##h = ~0U; \
  else { \
    pref##l = prop->pt[cur##pref].low; \
    pref##h = prop->pt[cur##pref].high; \
  } \
} while(0)
#define inc_propp(pref, prop, pt) do { \
  ++cur##pref; \
  set_propp(pref, prop, pt); \
} while(0)
    inc_propp(ep, ExtPict_prop, rng);
  } else if(inscl <= gcbl) {
    if(epl <= insch) {
      fprintf(stderr, "GCBp doesn't support Indic and ExtPict %04X\n", epl);
      exit(1);
    }
    for(int inscc = inscl + 1; inscc <= insch; inscc++) {
      uint8_t sc = uni_chrrng_dat8(inscc, sc_prop->rng_dat8, sc_prop->len,
                                   sc_prop->def);
      if(!is_indic_scr(sc)) {
        fprintf(stderr, "GCBp doesn't support script change at %04X\n", inscc);
	exit(1);
      }
    }
    uint8_t insc = InSC_prop->rng_dat8[curinsc].dat;
    if(insc == InSC_Virama) {
      if(gcbl != inscl || gcbh < insch) {
        fprintf(stderr, "Virama %04X is not GCB EX\n", gcbh ? gcbh + 1 : gcbl);
	exit(1);
      }
      gcbl = insch + 1;
      if(gcbl > gcbh)
	inc_propp(gcb, GCBprop, rng_dat8);
    } else if(gcbl <= insch) {
      fprintf(stderr, "GCBp: GCB of Consonant %04X not Other\n", gcbl);
      exit(1);
    }
    add_enum_rng(prop_GCBp, inscl, insch,
                 insc == InSC_Virama ? GCBp_EX_V : GCBp_XX_C);
    <<Skip to next valid [[curinsc]]>>
    set_propp(insc, InSC_prop, rng_dat8);
  } else {
    uint32_t h = gcbh;
    if(inscl <= h)
      h = inscl - 1;
    if(epl <= h)
      h = epl - 1;
    uint8_t gcbc = GCBprop->rng_dat8[curgcb].dat;
    if(gcbc == GCB_EX) {
      uint32_t prev = 0, prev0 = 2;
      for( ; gcbl <= h; gcbl++) {
	uint8_t ccc = uni_chrrng_dat8(gcbl, ccc_prop->rng_dat8,ccc_prop->len,
	                              ccc_prop->def);
	if((!ccc) != prev0) {
	  if(prev)
	    add_enum_rng(prop_GCBp, prev, gcbl - 1,
	                 prev0 ? GCBp_EX_0 : GCB_EX);
	  prev0 = !ccc;
	  prev = gcbl;
	}
      }
      add_enum_rng(prop_GCBp, prev, gcbl - 1, prev0 ? GCBp_EX_0 : GCB_EX);
    } else
      add_enum_rng(prop_GCBp, gcbl, h, gcbc);
    if(h != gcbh)
      gcbl = h + 1;
    else
      inc_propp(gcb, GCBprop, rng_dat8);
  }
}
@

<<Skip to next valid [[curinsc]]>>=
while(1) {
  while(++curinsc < InSC_prop->len &&
        InSC_prop->rng_dat8[curinsc].dat != InSC_Consonant &&
        InSC_prop->rng_dat8[curinsc].dat != InSC_Virama);
  if(curinsc == InSC_prop->len)
    break;
  uint8_t sc = uni_chrrng_dat8(InSC_prop->rng_dat8[curinsc].low,
                               sc_prop->rng_dat8, sc_prop->len, sc_prop->def);
  if(is_indic_scr(sc))
    break;
}
@

\lstset{language=txt}
<<FIXME>>=
not sure what \p{Gujr} means:  sc, scx, or sc+scx?
test 9.3 somehow
need to document yet another exception to the independence rule (9.3),
  (or not mention the independence rule at all):  no tracking means
  possible invalid break before Indic Linking Consonant
add a function to help finding GCBs at a random location:
  - back up to nearest break:
     LF preceeded by CR
     L|V|LV|LVT preceeded by L
     V|T preceeded by LV|V
     T preceeded by LVT|T
     Extend|ZWJ preceeded by anything
     anything preceeded by Prepend
  - back out of GB12/GB13:
     RI preceeded by RI, then forward again to set state
@

\subsubsection{Word Boundary Determination}

The next larger element of text that is useful to extract is a word.
One way to do this is to simply assign a boolean property to each
character which makes it part of or not part of a word, and assign
word boundaries after any contiguous group.  UTS~\#18, RL1.4 defines
such a class.  The only complication is that non-spacing marks inherit
their class from their preceding character.  This is reflected in a
special return value.  Once again, a property table or two is pulled
in, so the function is in its own object file.

\lstset{language=C}
<<Unicode property exports>>=
/** Perform a simple Unicode word boundary check.
  * If \p cp is definitely not part of a word, zero is returned.  If
  * it is definitely part of a word, a value greater than zero is returned.
  * Otherwise, a value less than zero is returned, and \p cp has the
  * same status as the previous character in the string, or, if it is the
  * first character, it is definitely part of a word */
int uni_is_simple_word(uint32_t cp);
@

<<Library [[uni]] Members>>=
wb_simp.o
@

<<wb_simp.c>>=
<<Common C Header>>
#include "uni_prop.h"

int uni_is_simple_word(uint32_t cp)
{
  if(cp == 0x200C || cp == 0x200D)
    return 1;
  uni_gc_t gc = uni_gc_of(cp);
  if(gc != UNI_gc_Mc && (uni_gc_trans[UNI_gc_M] & (1 << gc)))
    return -1;
  return !!((uni_gc_trans[UNI_gc_L] & (1 << gc)) ||
            (uni_gc_trans[UNI_gc_N] & (1 << gc)));
}
@

However, a slightly more sophisticated (but still useless in most
eastern locales) algorithm is also specified by UAX~\#29.  Word
boundaries may also be altered using locale-specific overrides.  The
CLDR provides a few, but its entire specification of word boundary
rules expects a regular expression storage format.  This library does
not use or implement pure regular expressions, though, so reading the
data files will not work.  Instead, the locale-specific alterations
are provided in a different form: alternate property lookup tables and
special lookup codes to enable each of the current overrides.  This
has the huge disadvantage that future revisions of the CLDR will
likely require additional hand-coded overrides, which must be detected
by human perusal of the CLDR files.  The regular expressions will not
be provided as a property, either, so an application which needs to
use the regular expression form will either have to use ICU, which
also includes a regular expression matcher that understands them, or
read the CLDR manually.

Once again the first two rules are ignored, as they simply express
that words cannot extend past text boundaries.  The rest of the rules
are implemented in a switch and post-switch check, just like the
grapheme cluster check.  In addition to using rules from UAX~\#29,
revision 37, the CLDR overrides from CLDR version 39 are implemented.
There are other suggested locale-specific tailorings in the UAX, but
since they are not implemented in the CLDR, I won't implement them
here, either.  The ones which only involve character classification
changes can be implemented by substituting the table. The only one
that might be worthwhile implementing is WB5a, to divide between
apostrophe and vowels.

Rules WB6, WB7, WB7b, WB7c, WB11, and WB12 are difficult, though: 
they require three characters rather than just two to make their
decision.  When combined with WB4, essentially unlimited lookahead is
required.  The rules come in pairs, basically binding all three
together if they appear together, and, due to the default rule,
allowing splits between them otherwise.  To track this, I used to use
$1$ and $-1$ as tokens, but that might break in the future, and
complicates testing for return values, so I track the expected third
element using positive return values, and always return zero for word
breaks (instead of positive values).  Basically, while WB4 is being
applied, the previous return value is passed on if positive, or the
negative of the previous character's WB property, otherwise.

As with the other functions, a different property table is pulled in,
so the function gets its own object file.  And, as with grapheme
cluster breaks, this algorithm started pulling in other properties, so
a new property should be synthesized.  The Japanese locale extensions
also require extra enumeration literals.
% Begin-doc WBp
The extra properties are:

\begin{itemize}
\item [[XX_EP]] == same as [[XX]], but [[uni_is_ExtPict]] is true (WB3c)
\item [[LE_EP]] == same as [[LE]], but [[uni_is_ExtPict]] is true (WB3c)
\item [[XX_HI]] == Hiragana characters (only in appropriate locales)
\item [[LE_ID]], [[XX_ID]] == Ideographic characters (only in
appropriate locales).  Ideographic characters with the [[Extend]] WB
property are left as [[Extend]].
\end{itemize}
% End-doc WBp

<<Parse UCD files>>=
decl_enum(WBp, "XX");
prop_WBp = add_prop("WBp");
parsed_props[prop_WBp].def = enum_val(prop_WB, def_WBp);
<<Generate [[WBp]] property>>
@

<<Unicode property exports>>=
/** Perform full Unicode word boundary detection.
  * The beginning and end of a string are always word boundaries. Otherwise,
  * they happen between code points; that is what this function detects.
  * Unlike \ref uni_is_simple_word, there are no distinct non-word code
  * points, just boundaries between words.  The code points to check
  * between (\p prev, \p next) must be accompanied by the previous return
  * value (zero initially).  If the return value is less than zero, no
  * break occurs.  If it is zero, a break occurs.  If it is greater
  * than zero, it is part of a backtracking group.  The group finishes
  * when the return value is zero or less.  The first member of the
  * group is then a word boundary if the last one is, and not otherwise.
  * All other members of the group are not word boundaries.  For example,
  * 1 1 1 0 breaks at the first 1, but not the other two, and
  * 1 1 1 -1 does not break anywhere.  1 1 1 end-of-string breaks at
  * the first 1, since end-of-string is a break point, but not at the other
  * two.
  *
  * Due to the need for backtracking and state tracking, it is not generally
  * possible to begin detecting words in the middle of a string.  Instead,
  * it must start at the latest at the last known word boundary (i.e.,
  * a return value of 0).
  *
  * Localization can be accomplished by passing in an alternate lookup table
  * for the WB property in \p tab.  If NULL is passed in, \ref uni_WBp_mtab
  * is used. */
int uni_word_brk(uint32_t prev, uint32_t next, int prevret, const uint32_t *tab);
@

<<Library [[uni]] Members>>=
wbrk.o
@

<<wbrk.c>>=
<<Common C Header>>
#include "uni_prop.h"

/* extra WB states */
#define UNI_WBp_HL_SQ (UNI_NUM_WBp + 1) /* WB7/WB7a */
#define UNI_WBp_RI_Extend (UNI_WBp_HL_SQ + 1) /* WB15/WB16+WB4 */

/* WB1 and WB2 are covered in the explanatory text */
int uni_word_brk(uint32_t prev, uint32_t next, int prevret, const uint32_t *tab)
{
  if(!tab)
    tab = uni_WBp_mtab;
#define get_WB(x) uni_x8_of(x, tab, UNI_WBp_XX)
  uni_WBp_t pwb, nwb = get_WB(next);
  if(prevret > 0)
    pwb = prevret - 1;
  else if(prevret < 0)
    pwb = -prevret - 1;
  else
    pwb = get_WB(prev);
  switch(pwb) {
    case UNI_WBp_CR:
      if(nwb == UNI_WBp_LF) /* WB3 */
        return -(nwb + 1);
      /* fall through */
    case UNI_WBp_LF:
    case UNI_WBp_NL:
      return 0; /* WB3a */
    case UNI_WBp_ZWJ:
      if(nwb == UNI_WBp_XX_EP || nwb == UNI_WBp_LE_EP)
        return -(nwb + 1); /* WB3c */
      break;
    case UNI_WBp_WSegSpace:
      if(nwb == UNI_WBp_WSegSpace)
        return -(nwb + 1); /* WB3d */
    default:
      break;
  }
  if(nwb == UNI_WBp_CR || nwb == UNI_WBp_LF || nwb == UNI_WBp_NL)
    return 0; /* WB3b */
  /* WB4: don't break, but pass on prefix for future rules */
  /* which also means: don't trigger prior rules later */
  /* and don't bypass prior rules: */
  if(prevret < 0 && get_WB(prev) == UNI_WBp_ZWJ &&
     (nwb == UNI_WBp_XX_EP || nwb == UNI_WBp_LE_EP))
    return -(nwb + 1); /* WB3c */
  if(nwb == UNI_WBp_Extend || nwb == UNI_WBp_FO || nwb == UNI_WBp_ZWJ) {
    if(prevret > 0 || pwb == UNI_WBp_RI_Extend)
      return prevret;
    if(!prevret && pwb == UNI_WBp_RI) /* WB15/WB16: remember if 0 */
      return -(UNI_WBp_RI_Extend + 1);
    if(pwb == UNI_WBp_ZWJ || pwb == UNI_WBp_WSegSpace)
      return -(nwb + 1); /* bypass WB3c, WB3d */
    return -(pwb + 1);
  }
  switch((int)pwb) { /* cast is to allow UNI_WBp_ID w/o warning */
    case UNI_WBp_HL:
      if(nwb == UNI_WBp_SQ)
        return -(UNI_WBp_HL_SQ + 1); /* WB7/WB7a */
      if(prevret > 0) { /* WB7b/WB7c */
        /* this is actually DQ after HL */
	if(nwb == UNI_WBp_HL)
	  return -(nwb + 1);
	break;
      }
      if(nwb == UNI_WBp_DQ) /* WB7b/WB7c */
        return UNI_WBp_HL + 1;
      /* drop through */
    case UNI_WBp_LE:
      if(prevret > 0) { /* WB6/WB7 */
        /* this is actually ML|MB|SQ after LE|HL */
        if(nwb == UNI_WBp_LE || nwb == UNI_WBp_LE_ID || nwb == UNI_WBp_HL)
	  return -(nwb + 1);
        break;
      }
      /* drop through */
    case UNI_WBp_LE_ID:
    case UNI_WBp_LE_EP:
      if(nwb == UNI_WBp_SQ || nwb == UNI_WBp_ML || nwb == UNI_WBp_MB) /* WB6/WB7 */
        return UNI_WBp_LE + 1;
      if(nwb == UNI_WBp_LE    || nwb == UNI_WBp_LE_ID ||
         nwb == UNI_WBp_LE_EP || nwb == UNI_WBp_HL    || /* WB5 */
         nwb == UNI_WBp_NU    || /* WB9 */
	 nwb == UNI_WBp_EX) /* WB13a */
        return -(nwb + 1);
      if(pwb == UNI_WBp_LE_ID &&
         (nwb == UNI_WBp_XX_ID || nwb == UNI_WBp_LE_ID)) /* WB13.4; ja only */
        return -(nwb + 1);
      break;
    case UNI_WBp_HL_SQ: /* WB7 */
      if(nwb == UNI_WBp_HL)
        return -(nwb + 1);
      break;
    case UNI_WBp_NU:
      if(prevret > 0) { /* WB11/WB12 */
        /* this is actually MN|MB|SQ after NU */
        if(nwb == UNI_WBp_NU)
	  return -(nwb + 1);
        break;
      }
      if(nwb == UNI_WBp_SQ || nwb == UNI_WBp_MN || nwb == UNI_WBp_MB) /* WB11/WB12 */
        return pwb + 1;
      
      if(nwb == UNI_WBp_NU    || /* WB8 */
         nwb == UNI_WBp_LE    || nwb == UNI_WBp_LE_ID ||
	 nwb == UNI_WBp_LE_EP || nwb == UNI_WBp_HL    || /* WB10 */
	 nwb == UNI_WBp_EX) /* WB13a */
        return -(nwb + 1);
      break;
    case UNI_WBp_KA:
      if(nwb == UNI_WBp_KA || /* WB13 */
        nwb == UNI_WBp_EX) /* WB13a */
      return -(nwb + 1);
      break;
    case UNI_WBp_EX:
      if(nwb == UNI_WBp_EX || /* WB13a */
         nwb == UNI_WBp_LE || nwb == UNI_WBp_HL || nwb == UNI_WBp_NU || /* WB13b */
         nwb == UNI_WBp_KA || nwb == UNI_WBp_LE_ID || nwb == UNI_WBp_LE_EP)
        return -(nwb + 1);
      break;
    case UNI_WBp_XX_HI:
      if(nwb == UNI_WBp_XX_HI) /* WB13.3; ja only */
        return -(nwb + 1);
      break;
    case UNI_WBp_XX_ID:
      if(nwb == UNI_WBp_XX_ID || nwb == UNI_WBp_LE_ID) /* WB13.4; ja only */
        return -(nwb + 1);
      break;
    case UNI_WBp_RI_Extend:
      prevret = 0;
      /* fall through */
    case UNI_WBp_RI:
      if(nwb == UNI_WBp_RI && !prevret)
        return -(nwb + 1); /* WB15-WB16 */
      break;
    default:
      break;
  }
  return 0; /* WB999 */
}
@

<<Generate [[WBp]] property>>=
new_elit_prep(WBp, WB, 5);
add_new_elit(WBp, XX_EP, xx_ep);
add_new_elit(WBp, LE_EP, le_ep);
add_new_elit(WBp, XX_HI, xx_hi);
add_new_elit(WBp, XX_ID, xx_id);
add_new_elit(WBp, LE_ID, le_id);
finish_new_elits(WBp);
elit(WB, LE); /* XX is def_WB */
for(uint32_t curwb = 0, curep = 0,
             wbl = WBprop->rng_dat8[0].low,
	     wbh = WBprop->rng_dat8[0].high,
	     epl = ExtPict_prop->rng[0].low,
	     eph = ExtPict_prop->rng[0].high;
      curwb < WBprop->len || curep < ExtPict_prop->len; ) {
  uint32_t h;
  if(epl < wbl) {
    h = eph;
    if(h >= wbl)
      h = wbl - 1;
    add_enum_rng(prop_WBp, epl, h, WBp_XX_EP);
    epl = h + 1;
  }
  if(wbl < epl) {
    h = wbh;
    if(h >= epl)
      h = epl - 1;
    add_enum_rng(prop_WBp, wbl, h, WBprop->rng_dat8[curwb].dat);
    wbl = h + 1;
  }
  if(wbl == epl && wbl <= wbh && epl <= eph) {
    if(WBprop->rng_dat8[curwb].dat != WB_LE) {
      fprintf(stderr, "WBp: WB of ExtPict %04X is not Other or ALetter\n", wbl);
      exit(1);
    }
    h = wbh;
    if(h > eph)
      h = eph;
    add_enum_rng(prop_WBp, wbl, h, WBp_LE_EP);
    wbl = h + 1;
    epl = h + 1;
  }
  if(wbl > wbh)
    inc_propp(wb, WBprop, rng_dat8);
  if(epl > eph)
    inc_propp(ep, ExtPict_prop, rng);
}
@

\lstset{language=txt}
<<FIXME>>=
Provide WB5a as locale ext (but what is apostrophe?  SQ?  vowels are LE)
  apostrophe x vowels
@

\subsubsection{Sentence Boundary Determination}

The next larger boundary is a sentence, also specfified by UAX~\#29. 
Again, CLDR overrides may be provided, but they are in an unusable
format.  Once again simple locale overrides are supported by replacing
the lookup table.  In addition, version 24 CLDR introduces known
abbreviations (mostly titles) as non-sentence overrides
(suppressions).  They are present in seven locales.  They are listed
in UTS~\#35 as a ``technology preview'', so perhaps spending a great deal
of effort supporting them is pointless.  Note that matching in general
requires normalization of both the pattern and the text to match
against, but normalization is beyond the scope of this function.  It
is up to the user to ensure they are both normalized the same way.

\lstset{language=C}
<<Unicode property exports>>=
<<[[uni_sb_locale_t]] deps>>
<<[[uni_sb_locale_t]]>>
@

<<[[uni_sb_locale_t]]>>=
/** Localization parameters for sentence break detection */
typedef struct {
  const uint32_t *tab; /**< lookup table for SB property if non-NULL */
  <<SB Suppression Table>>
} uni_sb_locale_t;
@

<<Known Data Types>>=
uni_sb_locale_t,%
@

This break algorithm is the most complex so far.  SB5 is mostly
simple, requiring passing codes forward, but, like with WB4, any
lookahead becomes infinite, requiring backtracking.  The only rules
which separate sentences are SB4 and SB11.  SB4 is unconditional, so
it is easy to implement.  However, it should be noted that SB4 assumes
that explicit newlines separate paragraphs; if this is not desired, a
table override should remap them to [[SP]].  SB8 is the only problem:
not only does it require lookahead, it requires infinite lookahead
even without SB5 due to having to skip a large variety of characters
before finding its [[LO]].  Like with word breaking, a positive return
value is used for this backtracking.

In addition, the suppression lookups require \emph{separate}
backtracking, as well as complex extra tracking state (i.e., the
current match state).  I used to use a separate state tracker for
suppressions, but now I'm sharing this with the main state (i.e., the
main return value and [[prevret]]).  The sign and the lower byte are
the main state (although that could be reduced, if necessary).  The
remaining 23 bits can be used for the suppression state.

The purpose of the suppressions is to suppress SB11 from triggering
within or after a match.  They can affect backtracking in several
ways:

\begin{enumerate}
\item While no match is in progress, a search for [[LO]] begins for
SB8.  This starts backtracking at the potential break for SB11
([[+AT_LO]]).  There will be no other potential break points before
this is resolved, so starting a suppression match while this in
progress does not affect anything, so long as the match state is
retained.  When backtracking ends, a token ([[-AT_LO_NBRK]]) is returned
to indicate SB8, and a regular return value ([[-nsb]]) is returned to
indicate SB11.
\item \label{item:sb11-mip}While a match is in progress, and no
suppression backtracking is in progress, there is a break due to SB11,
which is not affected by SB8.  This will start backtracking for
suppression ([[+nsb]]).  Since there is a greater variety of possible
code points encountered while suppression matching, internal state
must be set to indicate that it's in progress, and all return values
become positive.  This can be detected by positive [[prevret]] values
that aren't [[AT_LO]].
\item \label{item:sb8sim}While a match is in progress, and no
suppression backtracking is in progress, there is a potential break
due to SB8/SB11 ambiguity.  This will start backtracking for both
simultaneously ([[+AT_LO_SUP]]).  The ambiguity is resolved before a
match is made.  When the ambiguity is resolved in favor of SB8,
backtracking ends normally ([[-AT_LO_NBRK]]).  When it is resolved in
favor of SB11, backtracking continues as if it were the the second
case; i.e., it becomes plain suppression backtracking ([[+nsb]]).
\item \label{item:sb8simcont}Same as above, but a match occurs before
or at the same time as the ambiguity is resolved.  This ends
backtracking, returning a token to indicate suppression.  Given the
kinds of characters which end SB8 resolution, this token can simply
indicate that the next character's SB code needs to be re-evaluated
([[-SUP_NBRK]]), although it could be skipped if that character is
[[UP]] or [[LO]] by returning [[-AT_LO_NBRK]].
\item \label{item:sb11-mbtip}While suppression backtracking, an
unambiguous SB11 break occurs.  This can be marked with a special
token that indicates a break if the start of backtracking resolves to
a break ([[+SUP_BRK]]).
\item \label{item:sb8-mbtip}While suppression backtracking, a
potential break with SB8/SB11 ambiguity is found.  This starts a
sub-group, marked with a token that breaks unless canceled
([[AT_LO_SUB]]).  The cancelation is indicated by the next token or
end of backtracking: cancel if [[LO]] was found by returning a special
token ([[+AT_LO_NBRK]]).
\item \label{item:supfail}While suppression backtracking, and not SB8
backtracking at the same time, a full match failure occurs.  This ends
suppression backtracking, and resolves the start and all intermediate
uncanceled breaks to break ([[-nsb]] or 0).
\item \label{item:supfailsb8}While suppression backtracking, and also
SB8 backtracking, a full match failure occurs.  Internal state must be
set to ensure that suppression backtracking ends when SB8 backtracking
completes.  For example, return and retain a different SB value
([[AT_LO_NSUP]]).  Once SB8 backtracking is finished, if SB11, return
a negative value ([[-nsb]]), and if SB8, delay resolutioon by one more
code point by returning end-of-subgroup ([[+AT_LO_NBRK]]), followed by
a negative value ([[-nsb]]).
\item \label{item:supmatch}While suppression backtracking, a match
occurs, regardless of whether or not SB8 backtracking as well, since
this renders SB8 moot.  A special token must be returned to indicate
that neither the first nor any intermediate breaks occur.  If the
matching code point is [[AT]], [[ST]], or part of a match which
includes one of those, any token would suffice, as it suppresses the
SB11 which includes that, as well ([[-SUP_NBRK]]).  However, if it
might be a prefix for another [[AT]] or [[ST]] rule, it needs to
retain that information.  The only rule affected is SB7 ([[UP|LO]]
should become [[-AT_LO_NBRK]], which is interpreted as [[LO]]).
\item \label{item:suppartfail}While suppression backtracking, a partial
match failure occurs.  This means that while attempting to match
multiple strings, all of the strings which started at the start of
matching have failed to match, causing the matcher to move the
possible match start forward.  If the failure does not move the match
start past the start of backtracking, this has no effect.  Otherwise,
if it only crosses one break point (i.e., there are no intermediate
uncanceled potential breaks), this is the same as a full match
failure; there are four such patterns in CLDR 39.  Otherwise, this is
too difficult to manage with simple return values; there would need to
be a way to say, ``allow the first N  breaks regardless, but continue
backtracking.''  There are no patterns in CLDR 39 which have this problem.
\item \label{item:suppartmatch}While suppression backtracking, a match
occurs, but there are still other matches in progress.  If the match
extends to the start of backtracking, it can be considered a full
match (there are 67 such patterns in CLDR 39).  If it does not span
over any possible breaks (really only possible if the suppression
itself has no possible breaks, which usually means the suppression was
invalid in the first place; there are three such patterns in CLDR 39),
it can be ignored. Otherwise, it complicates things further by
possibly non-breaking in the middle.  Fortunately, there are no such
patterns in CLDR 39.
\end{enumerate}

<<Unicode property exports>>=
/** Perform full Unicode sentence boundary detection.
  * The beginning and end of a string are always sentence boundaries.
  * Otherwise, they happen between code points; that is what this function
  * detects.  The code points to check between (\p prev, \p next) must be
  * accompanied by the previous return value (zero initially).  If the
  * return value is less than zero, no break occurs.  If it is zero, a
  * break occurs.  The first result greater than zero starts a backtracking
  * group.  The group ends at the next negative result or the end of
  * text.  */
<<Doxymentation for Post-backtrack SB Result>>
/**
  * Due to the need for backtracking and state tracking, it is not
  * generally possible to begin detecting sentences in the middle of a
  * string.  Instead, it should start at the last known sentence
  * boundary.  Use of this routine to find the boundaries of the
  * sentence surrounding a given point (e.g. for selection) is not
  * recommended.
  *
  # Localization can be accomplished by passing in a non-NULL \p locale.
  * If \p locale is non-NULL and \p locale->tab is also non-NULL, it is used
  * as the SB property lookup table (otherwise, \ref uni_SB_mtab is used). */
<<Suppression Doxymentation>>
int32_t uni_sentence_brk(uint32_t prev, uint32_t next, int32_t prevret,
                         const uni_sb_locale_t *locale);
@

<<Library [[uni]] Members>>=
sbrk.o
@

<<sbrk.c>>=
<<Common C Header>>
#include "uni_prop.h"

enum {
  UL_AT = UNI_NUM_SB, /* UP|LO AT for SB7 */
  AT_CL, /* AT CL for SB8-SB11 */ ST_CL, /* ST CL for SB8a-SB11 */
  AT_SP, /* AT CL* SP for SB8-SB11 */ ST_SP, /* ST CL* SP for SB8-SB11 */
  AT_LO, /* AT CL* SP* for SB8, SB11 (scan forward for LO) */
  AT_LO_SUP, /* AT_LO when sup matching but not backtracking yet */
  AT_LO_NSUP, /* AT_LO after suppression match failure */
  /* tokens for backtracking; not shifted */
#define max_SB_tokens 8 /* all others are shifted by this much */
  /* start-of-group is first positive result */
  /* end-of-group is first negative or end-of-string after start-of-group: */
  /* if token:  entire group is non-break */
  /* if non-token:  beginning of group and mid-group may-breaks break */
  SUP_BRK = 1, /* always +: break the same as start of group */
  AT_LO_SUB, /* always +: start of sub-group */
    /* sub-group ends on next token or end of group */
    /* if end is token AT_LO_NBRK, non-break; else breaks like group first */
    /*  if +AT_LO_NBRK: non-break */
    /*  if +any_other_token or end of group: break the same as start of group */
  AT_LO_NBRK, /* becomes LO; ends group if -, ends sub-group if + */
  SUP_NBRK, /* becomes 0/re-lookup prev's SB; always - */
};
@

<<Doxymentation for Post-backtrack SB Result>>=
/** Each return value within the group, as well as the terminating
  * negative return vlue, has a token to determine breaking.  The token is
  * the lower 8 bits as a signed character (the sign is also shared with the
  * return value as a whole).  If the token of the terminating negative return
  * value is greater than -8, no breaking occurs within the group.  Otherwise
  * (and when the group is terminated by end-of-string), the start of the group
  * and mid-group breaks break.  Mid breaks are indicated by some postive return
  * values with tokens less than 8.  If that token is 1, a break occurs if
  * the start of the group breaks.  If it is 2, a break does not occur if
  * there is a 3 token before the next 2 or the end of the group. */
@

<<sbrk.c>>=
/* SB1 and SB2 are covered in the explanatory text */
int32_t uni_sentence_brk(uint32_t prev, uint32_t next, int32_t prevret,
                         const uni_sb_locale_t *locale)
{
  const uint32_t *tab = locale && locale->tab ? locale->tab : uni_SB_mtab;
#define get_SB(x) uni_x8_of(x, tab, UNI_SB_XX)
  uint8_t psb;
  uni_SB_t nsb = get_SB(next);
  uint32_t prevret_abs = prevret < 0 ? -prevret : prevret;
  uint8_t curstate = prevret_abs & 0xff;
  uint32_t supp_state = prevret_abs >> 8;
#define sb_ret_nbtok(n) return (-((n)|(supp_state<<8)))
#define sb_ret_nb(n) sb_ret_nbtok((n)+max_SB_tokens)
#define sb_ret_bttok(n) return ((n)|(supp_state<<8))
#define sb_ret_bt(n) sb_ret_bttok((n)+max_SB_tokens)

  if(curstate >= max_SB_tokens)
    psb = curstate - max_SB_tokens;
  else if(curstate == AT_LO_NBRK)
    psb = UNI_SB_LO;
  else if(curstate == AT_LO_SUB || curstate == AT_LO_NSUP)
    psb = AT_LO;
  else
    psb = get_SB(prev);
  switch(psb) {
    case UNI_SB_CR:
      if(nsb == UNI_SB_LF) /* SB3 */
        return -(nsb+max_SB_tokens); /* supp_state=0 */
      /* fall through */
    case UNI_SB_LF:
    case UNI_SB_SE:
      return 0; /* SB4 */ /* supp_state=0 */
  }
  <<Advance suppression state>>
  if(nsb == UNI_SB_EX || nsb == UNI_SB_FO) { /* SB5 */
    if(prevret)
      return prevret;
    sb_ret_nb(psb);
  }
  if(psb == UL_AT) {
    if(nsb == UNI_SB_UP)
      sb_ret_nb(nsb); /* SB7 */
    psb = UNI_SB_AT; /* otherwise, consider (UP|LO) AT as just AT */
  }
  switch(psb) {
    case UNI_SB_AT:
      if(nsb == UNI_SB_NU)
        sb_ret_nb(nsb);  /* SB6 */
      /* drop through */
    case AT_CL: /* AT|AT_CL */
      if(nsb == UNI_SB_CL)
        sb_ret_nb(AT_CL); /* SB9 */
      /* drop through */
    case AT_SP: /* AT|AT_CL|AT_SP */
      switch(nsb) {
        /* note:  SC does not affect SB8, as stream of CL|NU|SC|SP|XX LO combines */
        case UNI_SB_SC: /* SB8a */
	case UNI_SB_AT:
	case UNI_SB_ST:
	case UNI_SB_LO: /* SB8 */
	case UNI_SB_CR: /* SB9, SB10 */
	case UNI_SB_LF:
	case UNI_SB_SE:
	  sb_ret_nb(nsb);
        case UNI_SB_SP:
          sb_ret_nb(AT_SP); /* SB9, SB10 */
	case UNI_SB_CL: /* only possible if psb==UNI_SB_SP */
	case UNI_SB_NU:
	/* case UNI_SB_SC: */
	case UNI_SB_XX:
	  <<Maybe suppress SB8/SB11 ambiguity>>
	  sb_ret_bt(AT_LO); /* SB8/SB11 ambig */
	default:
	  <<Maybe suppress SB11>>
	  return 0; /* SB11; supp_state=0 */
      }
    case AT_LO:
      switch(nsb) {
        case UNI_SB_CL:
	case UNI_SB_NU:
	case UNI_SB_SC:
	case UNI_SB_SP:
	case UNI_SB_XX:
	  <<Maybe suppress SB8/SB11 ambiguity>>
	  sb_ret_bt(AT_LO); /* SB8 / SB11, SB998 ambig */
	case UNI_SB_LO:
	  <<Handle SB8 ambiguity resolution with suppression>>
	  sb_ret_nbtok(AT_LO_NBRK); /* resolved: SB8 (no break) */
	default:
	  sb_ret_nb(nsb); /* resolved:  SB11 (break) */
      }
    case UNI_SB_UP:
    case UNI_SB_LO:
      if(nsb == UNI_SB_AT)
        /* FIXME: return -UNI_SB_XX if sup matched */
        sb_ret_nb(UL_AT); /* SB998, SB7 */
      break;
    case UNI_SB_ST:
    case ST_CL:
      if(nsb == UNI_SB_CL)
        sb_ret_nb(ST_CL); /* SB9 */
      /* drop through */
    case ST_SP: /* ST|ST_CL|ST_SP */
      switch(nsb) {
        case UNI_SB_SC: /* SB8a */
	case UNI_SB_AT:
	case UNI_SB_ST:
	case UNI_SB_CR: /* SB9, SB10 */
	case UNI_SB_LF:
	case UNI_SB_SE:
	  sb_ret_nb(nsb);
        case UNI_SB_SP:
	  sb_ret_nb(ST_SP); /* SB9, SB10 */
	default:
	  <<Maybe suppress SB11>>
	  return 0; /* SB11; supp_state=0 */
      }
  }
  sb_ret_nb(nsb); /* SB998 */
}
@

My old suppression matching algorithm used a sorted table of strings,
and kept track of where it was by storing the string number and offset
as the state.  When a mismatch occured, the table was linearly
searched forward for a possible continuation.  If that failed, it cut a
character off the start of the match and found a new string to search
(using binary search this time).  This was not efficient, but it was
good enough for a mostly unused feature.  I have decided to make this
more efficient.  Rather than perform the first linear search, or even
requiring an initial sort on the string list, the strings are prepared
into a finite state machine.
% Begin-doc trie
A trie%
\footnote{de la Briandais, Ren\'e (1959). File searching using variable
length keys. Proc. Western J. Computer Conf. pp. 295--298.  Note that I
got this reference off of Wikipedia, and did not read the original
paper myself as I have no access at the moment.}
is a type of tree (directed acylcic graph with only one node, the
root, having no edges incoming) whose nodes represent text matched so
far, and whose edges are labeled with the character needed to
transition to that node.  Nodes are anonymous, but the node
representing the last character of any string to match is marked with
all of the strings it matches.  This can be used as a finite state machine
to match multiple strings at once from a given starting point.  The
state is represented by the node.  The next node is determined by the
edge labeled by the next character available for matching.  If there
is no such edge, all strings have mismatched.  If the target node is
labeled with match strings, all of those strings match the text from
the starting position to the current position; more strings may match
if there are edges leaving this node.

Finding the longest match, of length $n$, is $O(n)$ if accessing the
current state, finding an edge, and following that edge are $O(1)$.
The first is true if the state is a simple pointer or array index for
an array with $O(1)$ access.  The second is true if the character can
be used as an index for an array with $O(1)$ access.  The third is
true if the first is true, and the second's array value is the simple
pointer or array index.  The [[uni_utfN_strings_to_trie]] functions
use a simple array index for the state; the array element is the edge
array.  The edge array's contents are the next state (index).  The
edge array, indexed by a Unicode code point, is a sparse array.  Using
a multi-level table for this makes all three true, for $O(n)$.
However, generating and manipulating multi-level tables is difficult,
so these functions generate [[uni_chrrng_dat]]N tables, whose access is
$O(\textrm{lg}m)$, making array access $O(n\textrm{lg}m)$, where $m$
is the number of strings to match.  Once manipulations are complete,
the trie can be converted to using $O(1)$ multi-level tables using
[[uni_chrrng_trie_to_multi]].  Tries can be freed when finished using
[[uni_free_chrrng_trie]] and [[uni_free_trie]].  If you don't store
the number of states, the functions needing that value can be supplied
this information by scanning the trie with [[uni_chrrng_trie_nstates]]
or [[uni_trie_nstates]].  As a convenience, passing 0 for [[nstates]]
will call these functions for you.

% End-doc trie

Building the trie is also advertised as $O(N)$, where $N$ is the
combined length of all strings, again, assuming that all
sub-operations are $O(1)$.  For the same reasons as above, the actual
speed is $O(N\textrm{lg}m)$.  In fact, since I expect to use the
multi-level tables, there is an added large (as-yet unanalyzed)
increase in processing time.  However, I expect this sort of
preprocessing to be done while parsing the CLDR, rather than at
run-time, so its increased run time is not that critical.

Acceptance would most simply be implemented as a flag.  The flag does
not tell you which string matched, although you can deduce that from
the start position.  It's also not needed for the suppression matcher.
However, I want to keep the trie creator generic, so I will support
knowing at least one matching string by reserving the lower state
numbers as accept states.

% Begin-doc trie
Rather than marking states specially, state 0 is always the start
state, and the following [[nstr]] states, where [[nstr]] is the number
of strings being matched, are all accept states.  This means that the
number of strings that were used to generate the trie must be known in
order for the trie state machine to work.  While the information
conveyed by the specific accepting state number can be safely ignored,
state [[n]] corresponds to the [[n]]th string (i.e., string array
index [[n]]-1).  Duplicate strings only accept the last string found;
in order to mark the others, a separate, optional array ([[dups]]) can
be passed in, which is filled in with zeroes for unduplicated strings,
and the index of the previous identical string plus one for duplicated
ones.

The [[uni_chrrng_trie_next]] and [[uni_trie_next]] functions provide a
simple way to obtain the next state, given the current state and the
next code point ([[c]]).  For either one, start with 0, and if the
next state is 0, a mismatch occured.  Matching is determined by the
non-zero low-numbered states, as mentioned above.
% End-doc trie

\lstset{language=txt}
<<FIXME>>=
Implement dups parm
@

\lstset{language=C}
<<[[uni_sb_locale_t]] deps>>=
<<Trie Types>>
@

<<Trie Types>>=
/** Range table tries; requries size_t len[] to define each table length */
typedef uni_chrrng_dat16_t **uni_chrrng_trie_t;
/** Fully constant range table tries; requires const size_t len[] to
  * define each table length */
typedef const uni_chrrng_dat16_t * const *uni_const_chrrng_trie_t;
@

<<Known Data Types>>=
uni_chrrng_trie_t,uni_const_chrrng_trie_t,%
@

<<Unicode property exports>>=
<<[[uni_utf]][[8]][[_strings_to_trie]] Prototype>>;
<<[[uni_utf]][[16]][[_strings_to_trie]] Prototype>>;
<<[[uni_utf]][[32]][[_strings_to_trie]] Prototype>>;
@

<<[[uni_utf]](@sz)[[_strings_to_trie]] Prototype>>=
/** Convert \p nstr utf-<<@sz>> \p strings to a trie.  If non-NULL, \p lens
  * is an array of length \p nstr giving the length of each string.
  * Otherwise, or if a given length is zero, the string is expected to be
  * zero-terminated.  The return value is a finite state machine whose
  * nodes are labeled by a state number.  The start state is state 0.  States
  * 1 through \p nstr are (the only) accepting states, and correspond to
  * accepting the first through \p nstr'th string from \p strings,
  * respectively.  If there are duplicate strings, only the last one will
  * ever match.  To execute the state machine, find the next state with
  * \ref uni_chrrng_trie_next with the return value and \p *ret_lens.
  * A next state of 0 indicates no possible further matches, requiring
  * backtracking to the start of matching and advancing a character if
  * more matches need to be found.
  *
  * The return values are the trie (a plain array of \ref uni_chrrng_dat16_t
  * arrays), the lengths of each range array (\p *ret_lens), and, if
  * \p nstates is non-NULL, the total length of the plain array (\p *nstates).
  * NULL will be returned as the primary return value and in \p *ret_lens,
  * as well as 0 in \p *nstates if \p nstates is non-NULL, if there are
  * errors such as no strings to match, zero-length strings, or more than
  * 65536 states.  The number of generated states will never exceed one plus
  * the combined length, in code points, of all \p strings */
uni_chrrng_trie_t uni_utf<<@sz>>_strings_to_trie(
    const uint<<@sz>>_t * const *strings, const size_t *lens, size_t nstr,
    uint32_t **ret_lens, size_t *nstates)
@

<<Unicode property exports>>=
/** Find next state of \p trie / \p trie_lens given current \p state and
    next character \p c */
uint16_t uni_chrrng_trie_next(uni_const_chrrng_trie_t trie,
			      const uint32_t *trie_lens, uint16_t state,
			      uint32_t c);
#define uni_chrrng_trie_next(trie, trie_lens, state, c) \
  (trie[state] ? uni_chrrng_dat16(c, trie[state], trie_lens[state], 0) : 0)
/** Find the number of states in \p trie / \p trie_lens */
size_t uni_chrrng_trie_nstates(uni_const_chrrng_trie_t trie,
                               const uint32_t *trie_lens);
/** Free range table \p trie / \p trie_lens of size \p nstates (0 to compute) */
void uni_free_chrrng_trie(uni_chrrng_trie_t trie, uint32_t *trie_lens,
                          size_t nstates);
@

<<uni_prop.c>>=
<<[[uni_utf]][[8]][[_strings_to_trie]]>>
<<[[uni_utf]][[16]][[_strings_to_trie]]>>
<<[[uni_utf]][[32]][[_strings_to_trie]]>>
@

<<[[uni_utf]](@sz)[[_strings_to_trie]]>>=
<<[[uni_utf]](@sz)[[_strings_to_trie]] Prototype>>
{
  if(!nstr) {
    if(nstates)
      *nstates = 0;
    *ret_lens = NULL;
    return NULL;
  }
  uni_chrrng_dat16_t **ret;
  int ret_alloc, nret = nstr + 1, *ret_allocn;
  for(ret_alloc = 16; ret_alloc < nret; ret_alloc *= 2);
  inisize(ret, ret_alloc);
  inisize(*ret_lens, ret_alloc);
  inisize(ret_allocn, ret_alloc);
  clearbuf(ret, ret_alloc);
  /* valgrind would not be happy w/ moving uninitialized values */
  clearbuf(*ret_lens, ret_alloc);
  clearbuf(ret_allocn, ret_alloc);
  for(int i = 0; i < nstr; i++) {
    size_t len = lens ? lens[i] : 0;
    if(!len && !*strings[i]) {
      <<Return empty trie due to error>>
    }
    unsigned int clen;
    uint16_t st = 0;
    int last_chr = 0;
    
    for(const uint<<@sz>>_t *p = strings[i]; !last_chr; p += clen) {
      uint32_t c = uni_int_utf<<@sz>>_decode(p, &clen);
      last_chr = len == (int)(p - strings[i]) + clen || (!len && !p[clen]);
      if(!ret[st]) {
        inisize(ret[st], (ret_allocn[st] = 8));
	(*ret_lens)[st] = 0;
      }
      int h = (int)(*ret_lens)[st] - 1, l = 0;
      while(l <= h) {
        int m = (l + h) / 2;
        if(c < ret[st][m].low)
          h = m - 1;
        else if(c > ret[st][m].high)
          l = m + 1;
        else {
          uint16_t nst = uni_chrrng_dat16_val(ret[st][m]);
	  if(last_chr) {
	    /* if(st <= nstr) warning("duplicate string"); */
	    uni_set_chrrng_dat16_val(ret[st][m], i + 1);
	    ret[(st = i + 1)] = ret[nst];
	    (*ret_lens)[i + 1] = (*ret_lens)[nst];
	    ret_allocn[i + 1] = ret_allocn[nst];
	    ret[nst] = NULL;
	    (*ret_lens)[nst] = 0;
	    ret_allocn[nst] = 0;
	  } else
	    st = nst;
	  break;
        }
      }
      if(l > h) {
        if(ret_allocn[st] == (*ret_lens)[st])
          resize(ret[st], (ret_allocn[st] *= 2));
        movebuf(ret[st]+l+1, ret[st]+l, (*ret_lens)[st] - l);
	(*ret_lens)[st]++;
	uni_chrrng_dat16_t *tran = &ret[st][l];
	tran->low = tran->high = c;
        st = last_chr ? i + 1 : nret++;
        uni_set_chrrng_dat16_val(*tran, st);
        if(st == ret_alloc) {
          if(ret_alloc == ((uint16_t)~0U)>>1) {
	    <<Return empty trie due to error>>
	  }
	  resize(ret, (ret_alloc *= 2));
	  clearbuf(ret + ret_alloc / 2, ret_alloc / 2);
	  resize((*ret_lens), ret_alloc);
	  resize(ret_allocn, ret_alloc);
	  clearbuf(*ret_lens + ret_alloc / 2, ret_alloc / 2);
	  clearbuf(ret_allocn + ret_alloc / 2, ret_alloc / 2);
        }
      }
    }
  }
  if(nstates)
   *nstates = nret;
  free(ret_allocn);   
  return ret;
}
@

<<Return empty trie due to error>>=
uni_free_chrrng_trie(ret, *ret_lens, nret);
free(ret_allocn);
if(nstates)
  *nstates = 0;
*ret_lens = NULL;
return NULL;
@

<<uni_prop.c>>=
size_t uni_chrrng_trie_nstates(uni_const_chrrng_trie_t trie,
                               const uint32_t *trie_lens)
{
  if(!trie || !trie_lens)
    return 0;
  size_t nstates = 1;
  for(size_t curstate = 0; curstate < nstates; curstate++) {
    if(!trie[curstate] || !trie_lens[curstate])
      continue;
    for(uint32_t i = 0; i < trie_lens[curstate]; i++) {
      uint16_t next_state = uni_chrrng_dat16_val(trie[curstate][i]);
      if(next_state >= nstates)
	nstates = next_state + 1;
    }
  }
  return nstates;
}
@

<<uni_prop.c>>=
void uni_free_chrrng_trie(uni_chrrng_trie_t trie, uint32_t *trie_lens,
                          size_t nstates)
{
  if(!trie)
    return;
  if(!nstates)
    nstates = uni_chrrng_trie_nstates((uni_const_chrrng_trie_t)trie, trie_lens);
  for(size_t i = 0; i < nstates; i++)
    if(trie[i])
      free(trie[i]);
  free(trie);
  free(trie_lens);
}
@

<<Trie Types>>=
/** Multi-level table tries */
typedef uint32_t **uni_trie_t;
/** Fully constant multi-level table tries */
typedef const uint32_t * const *uni_const_trie_t;
@

<<Known Data Types>>=
uni_trie_t,uni_const_trie_t,%
@

<<Unicode property exports>>=
/** Convert \p trie with \p nstates states (0 to compute) using
  * uni_chrrng_dat16_t lookup tables whose lengths are in \p trie_lens to
  * one using multi-level table lookups.  If \p mt_lens is not NULL, it
  * must point to an array of \p nstates values, which are filled in with
  * the multi-level table lengths */
uni_trie_t uni_chrrng_trie_to_multi(uni_const_chrrng_trie_t trie,
				    const uint32_t *trie_lens,
				    size_t nstates, uint32_t *mt_lens);
/** Find the number of states in \p trie */
size_t uni_trie_nstates(uni_const_trie_t trie);
/** Free multi-level table \p trie of size \p nstates */
void uni_free_trie(uni_trie_t trie, size_t nstates);
@

<<uni_prop.c>>=
uni_trie_t uni_chrrng_trie_to_multi(uni_const_chrrng_trie_t trie,
				    const uint32_t *trie_lens,
				    size_t nstates, uint32_t *mt_lens)
{
  uint32_t **ret;
  if(!trie || !trie_lens)
    return NULL;
  if(!nstates)
    nstates = uni_chrrng_trie_nstates(trie, trie_lens);
  inisize(ret, nstates);
  clearbuf(ret, nstates);
  if(mt_lens)
    clearbuf(mt_lens, nstates);
  uint32_t tmp_mt_len;
  for(int i = 0; i < nstates; i++)
    if(trie[i])
      ret[i] = uni_rng_dat16_to_multi(trie[i], trie_lens[i],
                                      mt_lens ? &mt_lens[i] : &tmp_mt_len, 0);
  return ret;
}
@

<<uni_prop.c>>=
size_t uni_trie_nstates(uni_const_trie_t trie)
{
  if(!trie)
    return 0;
  size_t nstates = 1;
  for(size_t curstate = 0; curstate < nstates; curstate++) {
    const uint32_t *mt = trie[curstate];
    if(!mt)
      continue;
    <<Multi-level Table Info Vars>>
    <<Multi-level Table Info Pointers Loop>>
      i += skip;
    <<Multi-level Table Info Pointers End of Loop>>
    for(uint32_t j = 0; j < skip; j++, i++) {
      uint16_t next_state = mt[i] >> 16;
      if(next_state >= nstates)
        nstates = next_state + 1;
      next_state = mt[i] & 0xffff;
      if(next_state >= nstates)
        nstates = next_state + 1;
    }
  }
  return nstates;
}
@

<<uni_prop.c>>=
void uni_free_trie(uni_trie_t trie, size_t nstates)
{
  if(!trie)
    return;
  if(!nstates)
    nstates = uni_trie_nstates((uni_const_trie_t)trie);
  for(size_t i = 0; i < nstates; i++)
    if(trie[i])
      free(trie[i]);
  free(trie);
}
@

Here is a simple example of using the trie to scan UTF-8 data:

<<trie-sample>>=
<<Create trie sample data>>

...

<<Create sample trie>>
<<Free sample range trie>>

...

<<Match sample trie>>
@

<<Create trie sample data>>=
/* the strings to match.  UTF-8 for this example. */
const uint8_t * const *strings[] = { "string1", "string2", ... };
const int nstr = sizeof(strings) / sizeof(*strings);
@

<<Create sample trie>>=
/* convert string table to multi-level table trie */
uint32_t *trie_lens;
uni_chrrng_trie_t trie_rng =
   uni_utf8_strings_to_trie(strings, NULL, nstr, &trie_lens, NULL);
uni_trie_t trie = uni_chrrng_trie_to_multi((uni_const_chrrng_trie_t)trie_rng,
                                            trie_lens, 0, NULL);
@

<<Free sample range trie>>=
/* trie_rng no longer needed */
uni_free_chrrng_trie(trie_rng, trie_lens, 0);
@

<<Match sample trie>>=
/* match buf against all strings */
uint8_t *matching = NULL; /* start of match */
uint16_t curstate = 0;
unsigned int clen;
for(uint8_t *p = buf; *p; p += clen) {
  uint32_t c = uni_int_utf8_decode(p, &clen);
  curstate = uni_trie_next(trie, curstate, c);
  if(!curstate) { /* mismatch */
    if(matching) {
      p = matching; /* backtrack to start */
      clen = uni_utf8_nextc(p);
      matching = NULL;
    }
    continue;
  }
  if(!matching) /* first match: mark start of match */
    matching = p;
  if(curstate <= nstr) /* match! */
    printf("match [%s] at %d len %d\n", strings[curstate - 1],
           (int)(matching - buf), (int)(p - matching) + clen);
}
@

% Begin-doc Aho-Corasick
Using tries still suffers from having to start back at the beginning
and trying at the next character%
% End-doc Aho-Corasick
, like my old code%
% Begin-doc Aho-Corasick
.  The Aho-Corasick
algorithm%
\footnote{Aho, Alfred V.; Corasick, Margaret J. (June 1975).
"Efficient string matching: An aid to bibliographic search".
Communications of the ACM. 18 (6): 333--340.  Note that like the trie
reference, this comes from Wikipedia, as is terminology.  Again, I do
not currently have access to the paper to read it myself, or even the
Knuth books I originally learned this from.}
cures this by backtracking during preprocssing.  The ``suffix'' edge
associated with each state points to the next longest possible match in
progress if the match fails.  This implicitly moves the match start
forward, and there is no way, at least with the unmodified algorithm,
to know by how much until matching succeeds.
% End-doc Aho-Corasick
This means that following the suffix edge should only be done while
not backtracking, unless I can find a way to modify the algorithm to
provide that information.
% Begin-doc Aho-Corasick
This algorithm also provides a way to know \emph{all} the matching
strings, rather than just one, via the ``dict suffix'' edge%
% End-doc Aho-Corasick
, although, again, this is not needed for suppression matching%
% Begin-doc Aho-Corasick
.
% End-doc Aho-Corasick

\lstset{language=C}
<<Unicode property exports>>=
/** Generate Aho-Corasick graph edges for \p trie (as generated by e.g.
    \ref uni_utf8_strings_to_trie) with \p nstates states, whose
    lookup table lengths are in \p trie_lens.  If \p suffix is
    non-NULL, it is filled with \p nstates backtrack transitions for
    match failure (the maximal suffix links).  If \p dict_suffix is
    non-NULL, it is filled with \p nstates links to other states which
    also match when this state matches.  If \p dict_suffix is
    non-NULL, the number of strings to match must be given in \p nstr
    as well, since that determines what states match.  Also, like the
    trie generator, duplicates only match the last one. */
void uni_aho_corasick_prep(uni_const_chrrng_trie_t trie, const uint32_t *trie_lens,
			   size_t nstates, size_t nstr,
			   uint16_t *suffix, uint16_t *dict_suffix);
/** Find next state of \p trie given current \p state and next character \p c */
uint16_t uni_trie_next(const int32_t * const *trie, uint16_t state,
			      uint32_t c);
#define uni_trie_next(trie, state, c) \
  (trie[state] ? uni_x16_of(c, trie[state], 0) : 0)
@

<<uni_prop.c>>=
void uni_aho_corasick_prep(uni_const_chrrng_trie_t trie, const uint32_t *trie_lens,
			   size_t nstates, size_t nstr,
			   uint16_t *suffix, uint16_t *dict_suffix)
{
  if(!nstates || (!suffix && !dict_suffix))
    return;
  uint16_t *suf;
  if(suffix)
    suf = suffix;
  else
    inisize(suf, nstates);
  clearbuf(suf, nstates);
  if(dict_suffix)
    clearbuf(dict_suffix, nstates);
  uint16_t *proc, nproc;
  inisize(proc, nstates - 1);
  for(nproc = 0; nproc < trie_lens[0]; nproc++)
    proc[nproc] = uni_chrrng_dat16_val(trie[0][nproc]);
  for(uint16_t curproc = 0; curproc < nproc; curproc++) {
    uint16_t cur = proc[curproc];
    for(int i = 0; i < trie_lens[cur]; i++) {
      uint16_t v = uni_chrrng_dat16_val(trie[cur][i]);
      for(uint32_t c = trie[cur][i].low; c <= trie[cur][i].high; c++) {
        uint16_t s_c;
        for(uint16_t s = suf[cur];
             !(s_c = uni_chrrng_trie_next(trie, trie_lens, s, c)) && s;
			s = suf[s]);
        if(s_c)
          suf[v] = s_c;
      }
      proc[nproc++] = v;
    }
    if(dict_suffix)
      dict_suffix[cur] = suf[cur] && suf[cur] <= nstr ?
	                    suf[cur] : dict_suffix[suf[cur]];
  }
  free(proc);
  if(!suffix)
    free(suf);
}
@

Here is  simple example of using the Aho-Corasick matcher:

<<Aho-Corasick-sample>>=
<<Create trie sample data>>

...

<<Create sample Aho-Corasick trie>>
<<Free sample range trie>>

...

<<Match sample Aho-Corasick trie>>
@

<<Create sample Aho-Corasick trie>>=
<<Create sample trie>>
/* Generate Aho-Corasick links */
size_t nstates = uni_chrrng_trie_nstates(trie_rng, trie_lens);
uint16_t suffix[nstates], dict_suffix[nstates];
uni_aho_corasick_prep((uni_const_chrrng_trie_t)trie_rng, trie_lens, nstates,
                       nstr, suffix, dict_suffix);
@

<<Match sample Aho-Corasick trie>>=
/* match buf against all strings */
uint16_t curstate = 0;
unsigned int clen;
for(uint8_t *p = buf; *p; p += clen) {
  uint32_t c = uni_int_utf8_decode(p, &clen);
  while(1) {
    uint16_t newstate = uni_trie_next(trie, curstate, c);
    if(newstate || !curstate) { /* match or at root: take what we found */
      curstate = newstate;
      break;
    }
    /* mismatch: follow suffix link */
    curstate = suffix[curstate];
  }
  if(curstate && curstate <= nstr) /* match! */
    /* possible multi-match; follow dict-suffix */
    for(; curstate; curstate = dict_suffix[curstate]) {
      int matchlen = strlen(strings[curstate - 1]);
      printf("match [%s] at %d len %d\n", strings[curstate - 1],
             (p - buf) + clen - matchlen, matchlen);
    }
}
@

Now that the matcher is finished, it's time to create the necessary
locale data from a suppression list.  I will only support UTF-8 for
this.  For matching, we will need the trie, the number of strings to
match (for accept state identification) and the suffix links.

<<Suppression Doxymentation>>=
/** If \p locale is non-NULL and \p locale->suppression_trie is also
  * non-NULL, that trie will be used to perform break suppressions.
  * In addition to the trie, \p locale->num_suppressions must be set
  * to the number of strings used to generate that trie.  This is used
  * to determine matches.  Aho-Corasick suffix edges appropriate for
  * suppression matching must also be supplied in \p suppression_suffix.
  * All three of these can be set using \ref uni_utf8_suppressions. */
@

<<SB Suppression Table>>=
/** If non-NULL, a trie of suppressions generated by
  * \ref uni_chrrng_trie_to_multi. */
uni_const_trie_t suppression_trie;
/** If suppression_trie is non-NULL, the number of matching strings.  This
  * is used to determine if a state matches. */
size_t num_suppressions;
/** If suppression_trie is non-NULL, an array of Aho-Corasick suffix edges,
  * with edges that pass breaks specially flagged. */
const uint16_t *suppression_suffix;
@

<<Unicode property exports>>=
/** Convert \p nstr \p strs into a suppression trie in \p locale.  If an
    error occurred during processing, or no non-empty strings were given,
    the trie parameters for \p locale are zeroed out, and zero is returned.
    Otherwise, non-zero is returned.  If \p warn is non-NULL, warnings
    and errors are printed to standard error prefixed by \p warn */
int uni_utf8_suppressions(uni_sb_locale_t *locale,
                          const uint8_t * const *strs, size_t nstr,
			  const char *warn);
/** Free suppression-related data in \p locale */
void uni_free_suppressions(uni_sb_locale_t *locale);
@

<<sbrk.c>>=
int uni_utf8_suppressions(uni_sb_locale_t *locale,
                          const uint8_t * const *strs, size_t nstr,
			  const char *warn)
{
  locale->suppression_trie = NULL;
  locale->num_suppressions = 0;
  locale->suppression_suffix = NULL;
  const uint8_t **sups;
  size_t nsup = 0;
  inisize(sups, nstr);
  <<Validate [[nstr]] suppressions in [[strs]] into [[sups]]/[[nsup]]>>
  if(!nsup) {
    <<Clean up suppression validation>>
    return 0;
  }
  size_t nstates;
  uint32_t *trie_lens;
  uni_chrrng_trie_t rtrie = uni_utf8_strings_to_trie(sups, NULL, nsup,
                                                     &trie_lens, &nstates);
  if(!rtrie) {
    free(sups);
    <<Clean up suppression validation>>
    return 0;
  }
  if(nstates > 0x7fff) { /* need room for flag */
    uni_free_chrrng_trie(rtrie, trie_lens, nstates);
    free(sups);
    <<Clean up suppression validation>>
    return 0;
  }
  locale->suppression_trie =
    (uni_const_trie_t)uni_chrrng_trie_to_multi((uni_const_chrrng_trie_t)rtrie,
                                               trie_lens, nstates, NULL);
  locale->num_suppressions = nsup;
  uint16_t *suf;
  inisize(suf, nstates);
  uni_aho_corasick_prep((uni_const_chrrng_trie_t)rtrie, trie_lens, nstates,
                        nstr, suf, NULL);
  locale->suppression_suffix = suf;
  uni_free_chrrng_trie(rtrie, trie_lens, nstates);
  <<Validate/adjust suppressions in [[locale]]>>
  free(sups);
  <<Clean up suppression validation>>
  return 1;
}
@

<<sbrk.c>>=
void uni_free_suppressions(uni_sb_locale_t *locale)
{
  if(locale->suppression_trie)
    uni_free_trie((uni_trie_t)locale->suppression_trie, 0);
  if(locale->suppression_suffix)
    free((uint16_t *)locale->suppression_suffix);
  locale->suppression_trie = NULL;
  locale->suppression_suffix = NULL;
  locale->num_suppressions = 0;
}
@

To validate suppressions and adjust suffix links, we need to first
know where breaks might occur, using plain [[uni_sentence_brk]].
Since the SB property table might affect this, the input [[locale]] is
passed along.  Blank strings are invalid for tries, so they are
dropped.  Suppressions which do not actually suppress anything are
invalid as well (see backtracking case \ref{item:suppartmatch}
above), so they are also dropped.

<<Validate [[nstr]] suppressions in [[strs]] into [[sups]]/[[nsup]]>>=
uint8_t **breaks;
inisize(breaks, nstr);
for(int i = 0; i < nstr; i++) {
  if(!strs[i] || !*strs[i]) {
    if(warn)
      fprintf(stderr, "%s: warning: ignoring blank suppression %d\n", warn, i);
    breaks[i] = NULL;
    continue;
  }
  sups[nsup++] = strs[i];
  inisize(breaks[i], strlen((const char *)strs[i]));
  unsigned int clen;
  int32_t sbret = 0;
  const uint8_t *p = strs[i];
  uint8_t *bp = breaks[i], *btbp = NULL /* for backtrack */;
  int nbrk = 0;
  uint32_t prev = uni_int_utf8_decode(p, &clen), next;
  /* CR/LF/SE always force a break */
  const uint32_t *tab = locale->tab ? locale->tab : uni_SB_mtab;
  uni_SB_t sb = get_SB(prev); /* get_SB from uni_sentence_brk() */
  if(warn && (sb == UNI_SB_CR || sb == UNI_SB_LF || sb == UNI_SB_SE))
    fprintf(stderr, "%s: warning:  CR/LF/SE '%04X' in pattern may be ignored\n",
            warn, (int)prev);
  while((next = uni_int_utf8_decode((p += clen), &clen))) {
    sb = get_SB(next);
    if(warn && (sb == UNI_SB_CR || sb == UNI_SB_LF || sb == UNI_SB_SE))
      fprintf(stderr, "%s: warning:  CR/LF/SE '%04X' in pattern may be ignored\n",
              warn, (int)next);
    sbret = uni_sentence_brk(prev, next, sbret, locale);
    prev = next;
    if(!btbp && sbret > 0)
      btbp = bp;
    *bp++ = !sbret;
    if(!sbret)
      nbrk++;
    if(btbp && sbret <= 0) {
      *btbp = sbret < -8 || !sbret;
      if(*btbp)
        nbrk++;
      btbp = NULL;
    }
  }
  /* might break after last char */
  if(btbp) {
    *btbp = 1; /* worst case: resolves to SB11 */
    nbrk++;
    *bp = 0; /* other rules ensure string after ambiguity is non-breaking */
  } else {
    /* next can't be CUR|LF|SE (SB9/SB10), NU (SB6), UP (SB7),
     * EX|FO (SB5), ST|AT (SB8a), CL|SP|SC|XX|LO (SB8) */
    /* This leaves LE, but it's probably safer to just use: */
    /*   0 = XX forces SB8 ambiguity if last char could've been SB11 */
    sbret = uni_sentence_brk(prev, 0, sbret, NULL);
    *bp = sbret > 0;
    if(*bp)
      nbrk++;
  }
  /* No breaks: invalid/skip */
  if(!nbrk) {
    free(breaks[i]);
    breaks[i] = NULL;
    nsup--;
    if(warn)
      fprintf(stderr, "%s: warning: no breaks in [%s]\n", warn, strs[i]);
  }
}
@

<<Clean up suppression validation>>=
for(int j = 0; j < nstr; j++)
  if(breaks[j])
    free(breaks[j]);
free(breaks);
@

In order to take care of backtracking case \ref{item:suppartfail},
any suffix link which shifts the start point past the first SB11 break
in the string being matched must be treated like a full match failure.
Such suffix links are marked here with a flag, and a warning is
issued.  If more than one break is skipped, the entire trie is
rejected with an error, because the code cannot suppoprt this.
Technically, only suffix links whose targets cannot match anything the
original node could should be considered, but this is expensive to
track and not necessary in CLDR 39. It may overzealously reject valid
tries in the future, though.  In order to figure out if breaks were
skipped, the length of text that the node represents so far must be
known.  This can be found by following any edge forward until an
accepting state is found, counting the number of transitions, and
subtracting that from the matched string length.

<<Validate/adjust suppressions in [[locale]]>>=
uni_const_trie_t trie = locale->suppression_trie;
for(int i = 0; i < nstr; i++) {
  if(!breaks[i])
    continue;
  unsigned int clen;
  uint16_t curstate = 0;
  int hadbreak = 0;
  for(const uint8_t *p = strs[i], *bp = breaks[i]; *p; p += clen) {
    uint32_t c = uni_int_utf8_decode(p, &clen);
    curstate = uni_x16_of(c, trie[curstate], 0);
    if(warn && curstate <= nsup && p[clen])
      fprintf(stderr, "%s: warning: overlapping match [%s] [%s]\n",
	      warn, sups[curstate - 1], strs[i]);
    if(p == strs[i]) /* mismatch @ 1st char is full mismatch */
      continue;
    hadbreak |= *bp++; /* bp is actually 1 char behind at this point */
    if(!hadbreak) /* mismatch before/at 1st break is irrelevant */
      continue;
    /* FIXME:  skipchars = any_valid_chars(curstate) */
    for(uint16_t *s = &suf[curstate]; *s && !(*s & 0x8000); s = &suf[*s]) {
      /* FIXME: schars = any_valid_chars(*s) */
      /* fIXME: skip if(schars - skipchars = empty) */
      /* FIXME: skipchars |= schars (could do this @ end of loop) */
      /* follow any match to completion to determine prefix len */
      int suflen = 0;
      uint16_t sstate;
      for(sstate = *s & 0x7fff; sstate > nsup;
          /* *mtab == low which is *2 due to 16-bit data */
	  sstate = uni_x16_of(trie[sstate][0]/2, trie[sstate], 0))
	suflen++;
      int preflen = uni_utf8_strlen(sups[sstate - 1]) - suflen, didpass = 0;
      for(uint8_t *pass = breaks[i]; pass < bp - preflen; pass++)
        if(*pass) {
          if(didpass++)
	    break;
	  *s |= 0x8000;
        }
      if(didpass > 1) {
        if(warn)
          fprintf(stderr, "%s: error: [%s] passes multiple breaks in [%s]\n",
	          warn, sups[sstate - 1], strs[i]);
        uni_free_trie((uni_trie_t)trie, nstates);
	free(suf);
	locale->suppression_trie = NULL;
	locale->num_suppressions = 0;
	locale->suppression_suffix = NULL;
	<<Clean up suppression validation>>
	return 0;
      } else if(warn && didpass)
	fprintf(stderr, "%s: warning: [%s] passes break in [%s]\n",
		warn, sups[sstate - 1], strs[i]);
      if(didpass) /* link broken, so stop looking */
        break;
    }
  }
}
@

So now, the current state is the 16-bit state array index, and no
other information.  It gets reset whenever 0 is returned, which is
only done for unsuppressable breaks (after SE, CR, LF) and SB11 when
no match is in progress, anyway.  Instead of returning 0 for SB11, a
token is always returned when a match is in progress.  This takes care
of the initial return value for backtracking cases \ref{item:sb11-mip}
and \ref{item:sb11-mbtip}.

<<Maybe suppress SB11>>=
if(supp_state) {
  if(prevret > 0)
    sb_ret_bttok(SUP_BRK);
  else
    sb_ret_bt(nsb);
}
@

Actual backtracking requires returning positive values.  The most
convenient way to do this is to redefine the non-backtracking macros
defined above to backtrack if supression backtracking is in progress.
This takes care of the rest of backtracking case \ref{item:sb11-mip},
as well as the SB11 resolution for case \ref{item:sb8sim}.

<<Advance suppression state>>=
/* could set this to 1/-1 instead of below, but I'll give the optimizer
 * as big a chance as possible of converting * into single negate instr */
int is_sup_bt = prevret > 0 && psb != AT_LO;
#undef sb_ret_nbtok
#define sb_ret_nbtok(n) return ((is_sup_bt ? 1 : -1) * ((n)|(supp_state<<8)))
#undef sb_ret_nb
#define sb_ret_nb(n) sb_ret_nbtok((prevret == AT_LO && supp_state ? -1 : 1) * \
   ((n)+max_SB_tokens))
@

Sub-backtracking is started instead of SB8 backtracking by returing
[[AT_LO_SUB]] (for backtracking case \ref{item:sb8simcont}).  Starting
both simultaneously is done by passing [[AT_LO_SUP]] (for backtracking
case \ref{item:sb8-mbtip}).  The simple end of sub-backtracking for
backtracking case \ref{item:sb8-mbtip} is already taken care of by making
[[AT_LO_NBRK]] positive above.  A mismatch during simultaneous
backtracking requires switching to pure SB8 backtracking with a
special ending, by returning [[AT_LO_NSUP]].

<<Maybe suppress SB8/SB11 ambiguity>>=
if(supp_state) {
  if(is_sup_bt)
    sb_ret_bttok(AT_LO_SUB);
  else if(psb == AT_LO && curstate != AT_LO + max_SB_tokens)
    sb_ret_bt(AT_LO_NSUP);
  else
    sb_ret_bt(AT_LO_SUP);
} else if(psb == AT_LO && curstate != AT_LO + max_SB_tokens)
  sb_ret_bt(AT_LO_NSUP);
@

<<Handle SB8 ambiguity resolution with suppression>>=
if(curstate != AT_LO + max_SB_tokens)
  sb_ret_bttok(AT_LO_NBRK);
@

To advance the state, just walk the trie.  On mismatch, we continue
using the Aho-Corasick next-best-match link, but also consider it the
same as a full mismatch if currently backtracking and a break point is
passed.

<<Update suppression state for (@char)>>=
uint16_t newstate, rej = 0;
while(1) {
  newstate = uni_trie_next(locale->suppression_trie, supp_state, <<@char>>);
  if(!newstate && supp_state) {
    supp_state = locale->suppression_suffix[supp_state];
    rej |= supp_state & 0x8000;
    supp_state &= 0x7fff;
  } else
    break;
}
supp_state = newstate;
int matching = !!supp_state && (!rej || !is_sup_bt);
if(!matching) {
  <<Process full suppression mismatch>>
} else if(supp_state <= locale->num_suppressions) {
  <<Process suppression match>>
}
@

When not backtracking, match failures are ignored.  For backtracking
case \ref{item:supfail}, failure simply toggles return codes back to
normal, by resetting the backtracking flag.  Case
\ref{item:supfailsb8} is taken care of by returning [[AT_LO_NSUP]]
above.

<<Process full suppression mismatch>>=
is_sup_bt = 0;
curstate = AT_LO_NSUP + max_SB_tokens;
@

Partial failures are already covered, so we can be assured that
partial matches (case \ref{item:supmatch}) are as good as full
matches.  For both full match cases (\ref{item:sb8simcont} and
\ref{item:supmatch}), this just returns an appropriate code to
continue the normal algorithm, pretending that the current character
is definitely not part of an SB11 break.

<<Process suppression match>>=
if(psb == UNI_SB_LO || psb == UNI_SB_UP)
  return -AT_LO_NBRK;
else
  return -SUP_NBRK;
@

I will assume that spaces in any of the patterns mean [[SP+]], and
that [[FO]] should be ignored (since they just alter appearance, and
due to SB5).  I will \emph{not} assume that [[EX]] should be ignored,
in spite of SB5, since they alter the meaning of a character (although
perhaps I should ignore ZWJ and ZWNJ).  Again, the user should ensure
both patterns and source text are normalized the same way for matches
to work.  UTS~\#35 gives no guidance on what to do with these three
character types.

<<Advance suppression state>>=
if(locale && locale->suppression_trie) {
  if(!supp_state) {
    <<Update suppression state for [[prev]]>>
  }
  if(!supp_state ||
     /* skip FO and match SP as SP+ */
     (nsb != UNI_SB_FO && (nsb != UNI_SB_SP || get_SB(prev) != UNI_SB_SP))) {
    <<Update suppression state for [[next]]>>
  }
}
@

\subsubsection{Line Breaking}

The next larger boundary is a paragraph.  There is no implementation
here; it is generally up to the text file format to explicitly
indicate paragraph boundaries.  When line breaks are explicit, this is
often done using parapragh separators (gc = Zp) or multiple consective
line separators.  When they are implicit, these methods may also be
used, or simple line separators may function as paragraph separators
as well.  As mentioned above, the sentence breaking algorithm assumes
that single line separators are paragraph separators.

When paragraphs are known, a common operation is to narrow the view of
the paragraph to a limited width.  When doing so, the paragraph must
be split into multiple lines via word wrapping (or line breaking).
Unicode UAX~\#14 specifies a line breaking algorithm of limited use.  A
real line breaking algorithm is very context dependent.  The only
application that would benefit from the algorithm without the addition
of at least a hyphenation dictionary is a plain text formatter, such
as the automatic wrapping mode in a plain text editor.  Nonetheless,
the following function implements the algorithm specified in UAX~\#14,
revision 45.

This algorithm has many more rules than the boundary detection
algorithms.  It also has three return types, rather than just two: in
addition to non-break and possible break locations, mandatory breaks
are marked differentlly (all breaks in the previous algorithms are, in
essence, mandatory).  Like the others, a modifier rule (LB9) extends
any non-zero lookahead to infinity, and like the sentence break rules,
some of the rules require infinite lookahead in and of themselves.  At
least there aren't two independent sources of backtracking, since I do
not implement hyphenation dictionary lookups.  The need for three
different return codes means that just simple positive and negative
return codes are inadequate, but we can just use zero for manadatory
breaks (which also reset the state, since no other information can be
carried), negative for non-breaks, and a special token (just $1$) to
indicate optional breaks.  All other positive values are for
backtracking.

Like the boundary determination algorithms, locale-specific tailoring
should be supported.  Once again, the CLDR does not provide tailorings
in a usable format.  Tailorings are hand-coded and enabled using
flags. Some of the standard-recommended tailorings involve the use of
hyphenation dictionaries, which will likely never be implemented here.
In fact, breaking using a hyphenation dictionary would require use of
the same dictionary by the user as well, as Unicode provides no
mechanism for \TeX{}-like three-parameter soft hyphens.  As of CLDR
39, there are locale-specific tailorings for ja and zh locales, and a
former locale-specific tailoring of fi has been made universal.  CLDR
also adopts a more complex form of number detection, described by
example tailoring 7.  In fact, I don't even implement the standard
LB25 and LB13 at all, but only use the more complex form.  This
appears to be the intended use, since some tests fail with the simple
rules.

Like the grapheme cluster and word boundary algorithms, this one pulls
in more than just its associated lb property.  To correct this, and
also to do less work on startup to support LB1, a custom property is
generated.
% Begin-doc lbp
The needed properties are:

\begin{itemize}
\item For LB1, [[AI]], [[SG]], and [[XX]] (the lb default) is changed
to [[AL]].  The default of the new table is [[AL]] as well.
\item For LB1, [[SA]] is changed to [[CM]] if the gc property is [[Mn]]
or [[Mc]], and to [[AL]] otherwise.
\item For LB1, [[CJ]] is changed to [[NS]].
\item For LB30, [[OP]] with an ea property of [[F]], [[W]], or [[H]]
is converted to [[OP_ea_FWH]].
\item For LB30, [[CP]] with an ea property of [[F]], [[W]], or [[H]]
is converted to [[CP_ea_FWH]].
\item For LB30b, [[XX]]/[[AL]] with the ExtPict property and with a gc
of [[Cn]] is converted to [[AL_EP]].
\end{itemize}
% End-doc lbp

<<Parse UCD files>>=
decl_enum(lbp, "AL");
prop_lbp = add_prop("lbp");
parsed_props[prop_lbp].def = enum_val(prop_lb, def_lbp);
<<Generate [[lbp]] property>>
@

<<Unicode property exports>>=
/** Perform full Unicode line break opportunity detection.
  * The beginning of a string is never a line break opportunity, and the end
  * is a mandatory one.  Otherwise, they happen between code points; that is
  * what this function detects.  The code points to check between (\p prev,
  * \p next) must be accompanied by the previous return value (zero
  * initially).  If the return value is zero, a mandatory break occurs.
  * If it is less than zero, no break may occur.  Otherwise, if it is
  * greater than or equal to 8, an optional break occurs.  If it is less
  * than 8, backtracking starts.  At the end of backtracking, a value less
  * than zero but greater than -8 indicates that the start of backtracking
  * cannot break.  Otherwise, the start of backtracking breaks.  No breaks
  * may occur mid-backtracking.
  *
  * Due to the need for backtracking and state tracking, it is not
  * generally possible to begin detecting line break opportunities in
  * the middle of a string.  Instead, it must start at the latest at the 
  * last known break opportunity from a return value less than -1, and
  * perhaps even earlier.  Localization can be accomplished by passing in a
  * non-NULL \p tab to specify the lb property.  If NULL is passed in,
  * \ref uni_lb_mtab is used */
int uni_line_brk(uint32_t prev, uint32_t next, int prevret, const uint32_t *tab);
@

<<Library [[uni]] Members>>=
lbrk.o
@

<<lbrk.c>>=
<<Common C Header>>
#include "uni_prop.h"

typedef enum {
  st_reset = 0,
  <<Line break special tokens>>
  lb_num_st
} lb_state_t;
#define lb_num_st 8 /* give it some room */

enum {
  lb_ext0 = UNI_NUM_lbp,
  <<Line break extra classes>>
  num_lb_ext
};

/* LB2, LB3 taken care of in description */
int uni_line_brk(uint32_t prev, uint32_t next, int prevret, const uint32_t *tab)
{
  if(!tab)
    tab = uni_lbp_mtab; /* LB1 */
#define get_lb(x) uni_x8_of(x, tab, UNI_lbp_AL)
  uint8_t plb;
  uni_lbp_t nlb = get_lb(next);
  if(prevret)
    plb = (prevret > 0 ? prevret : -prevret) - lb_num_st;
  <<Adjust lb [[prevret]]>>
  if(!prevret) {
    plb = get_lb(prev);
    prevret = -(plb + lb_num_st);
  }
  switch(plb) {
    <<Handle pre-LB9 breaks for [[plb]]>>
  }
  switch((uint8_t)nlb) {
    <<Handle unconditional breaks for [[nlb]]>>
  }
  switch(plb) {
    <<Handle post-LB9 breaks>>
  }
  <<Perform final lb fixups>>
  <<Return break before [[nlb]]>> /* LB31 */
}
@

LB9 is the major break point.  While the rules before that are
simplified, spaces must be handled separately because some later rules
require supporting arbitrary space suffixes, similar to LB9.  LB10 i
shandled here as well, since [[plb]] should nver be [[ZWJ]] or [[CM]]
if it has been handled as a [[nwb]] for LB9.

<<Handle pre-LB9 breaks for [[plb]]>>=
case UNI_lbp_CR:
  if(nlb == UNI_lbp_LF) /* LB5 */
    return -(lb_num_st + UNI_lbp_LF);
  /* fall through */
case UNI_lbp_BK: /* LB4 */
case UNI_lbp_LF: /* LB5 */
case UNI_lbp_NL: /* LB5 */
  return 0; /* this is the only place ! is returned */
case UNI_lbp_ZW:
  if(nlb == UNI_lbp_SP)
    return -(lb_num_st + plb); /* LB7, LB8 */
  if(nlb == UNI_lbp_BK || nlb == UNI_lbp_CR || nlb == UNI_lbp_LF ||
     nlb == UNI_lbp_NL || /* LB6 */
     nlb == UNI_lbp_ZW) /* LB7 */
    return -(lb_num_st + nlb);
  return lb_num_st + nlb; /* LB8 */
@

<<Handle unconditional breaks for [[nlb]]>>=
case UNI_lbp_BK:
case UNI_lbp_CR:
case UNI_lbp_LF:
case UNI_lbp_NL: /* LB6 */
case UNI_lbp_ZW: /* LB7 */
  /* SP of LB7 is handled differently due to SP* tracking */
  return -(lb_num_st + nlb);
@

LB9 has a few issues compared to the equivalent rules in the boundary
detection algorithms.  First of all, even though the entire string of
[[CM]] and [[ZWJ]] characters following X is bound together (LB10 +
LB28), there is nothing binding X to the string itself.  For now, I
will assume that the wording of LB9 (``Do not break a combining
character sequence'') implies a new rule LB9a: X  [[CM|ZWJ]].
Second, LB8a throws a bit of a wrench into the works.  For one, it
requires tracking whether or not a [[ZWJ]] was found.  Since I have to
retain the previous state which tracking this at the same time, I add
a flag to the state, and filter it out after use.  For another, it's
hard to say if it should only be honored if at the end, or if it
should be honored in the middle as well (due to LB9 itself).

Since most of the listed exceptions for LB9 have already been handled
above, the only one left to be dealt with here is [[SP]].

<<Line break extra classes>>=
#define st_fl_ZWJ 128U /* add to state to indicate ZWJ for LB8a */
@

<<Adjust lb [[prevret]]>>=
/* strip ZWJ flag out (LB8a/LB9 conflict) */
int was_zwj = prevret & st_fl_ZWJ;
if(prevret < 0) {
  was_zwj = !was_zwj;
  prevret |= st_fl_ZWJ;
} else
  prevret &= ~st_fl_ZWJ;
@

<<Handle pre-LB9 breaks for [[plb]]>>=
case UNI_lbp_ZWJ:
case UNI_lbp_CM:
  was_zwj = plb == UNI_lbp_ZWJ ? st_fl_ZWJ : 0; /* LB8a */
  plb = UNI_lbp_AL; /* LB10 */
  /* if nlb is also CM/ZWJ, pass non-break */
  prevret = -(lb_num_st + plb);
  break;
@

<<Return break before [[nlb]]>>=
return (was_zwj ? -1 : 1) * (lb_num_st + nlb); /* LB8a */
@

<<Handle unconditional breaks for [[nlb]]>>=
case UNI_lbp_ZWJ:
case UNI_lbp_CM: /* LB9 */
  switch(plb) {
    case UNI_lbp_SP: /* all other exceptions already handled */
    <<Other lbp [[SP]] aliases>>
      break;
    default:
      was_zwj = nlb == UNI_lbp_ZWJ ? st_fl_ZWJ : 0; /* LB8a */
      if(!prevret)
	return -(lb_num_st + plb + was_zwj);
      if(prevret > 0 && prevret >= lb_num_st)
        prevret = -prevret;
      if(was_zwj) {
	if(prevret < 0)
	  prevret &= ~st_fl_ZWJ;
	else
	  prevret |= st_fl_ZWJ;
      }
      return prevret;
  }
  break;
@

After this point, only LB18, LB20, and LB31 have breaks.  Any rules
suppressing all breaks can be dealt with simply.

<<Handle unconditional breaks for [[nlb]]>>=
case UNI_lbp_WJ: /* LB11 */
  return -(lb_num_st + nlb);
case UNI_lbp_GL: /* LB12a */
  /* not really suppressing all breaks:  exceptions are SP, BA, HY */
  switch(plb) {
    case UNI_lbp_SP:
    case UNI_lbp_BA:
    case UNI_lbp_HY:
    <<Other lbp [[SP]], [[BA]], [[HY]] aliases>>
      break;
    default:
      return -(lb_num_st + nlb);
  }
  break;
case UNI_lbp_EX:
  return -(lb_num_st + nlb); /* LB13 */
@

<<Handle post-LB9 breaks>>=
case UNI_lbp_WJ: /* LB11 */
case UNI_lbp_GL: /* LB12 */
  return -(lb_num_st + nlb);
@

LB13 has additional full suppressions in the default algorithm, but
the full number tailoring makes those conditional.  The condition is
simple enough to include in the first switch, though.  Basically, LB25
requires tracking whether or not the preceeding character was a
number.  The specific patterns need to track this as well as the
actual next character, so [[NU_EXT_*]] classes are added for each of
them.  Any other rules must check for these extended classes as well
as the base class.

<<Line break extra classes>>=
NU_EXT_NU, NU_EXT_SY, NU_EXT_IS, /* LB25; NU (NU|SY|IS)+ */
NU_EXT_CL, NU_EXT_CP, NU_EXT_CP_ea_FWH, /* LB25: NU (NU|SY|IS)* (CL|CP|CP_ea_FWH) */
@

<<Handle unconditional breaks for [[nlb]]>>=
/* LB25 rules 5: NU (x NU|SY|IS)* (x CL|CP|CP_ea_FWH)? x */
/*  due to LB13, LB25 rule 3-4, x's on lhs are always x */
#define is_NU /* LB25 in progress has special NU codes */ \
  plb == UNI_lbp_NU || plb == NU_EXT_NU || plb == NU_EXT_SY ||  plb == NU_EXT_IS
case UNI_lbp_IS:
case NU_EXT_IS:
  if(is_NU)
    return -(lb_num_st + NU_EXT_IS); /* LB13/LB25 */
case UNI_lbp_SY:
case NU_EXT_SY:
  if(is_NU)
    return -(lb_num_st + NU_EXT_SY); /* LB13/LB25 */
/* NU (NU|SY|IS)* (x CL|CP|CP_ea_FWH)? -> NU_EXT_CL... */
case UNI_lbp_CL:
case NU_EXT_CL:
  if(is_NU)
    return -(lb_num_st + NU_EXT_CL); /* LB13/LB25 */
case UNI_lbp_CP:
case NU_EXT_CP:
  if(is_NU)
    return -(lb_num_st + NU_EXT_CP); /* LB13/LB25 */
case UNI_lbp_CP_ea_FWH:
case NU_EXT_CP_ea_FWH:
  if(is_NU)
    return -(lb_num_st + NU_EXT_CP_ea_FWH); /* LB13/LB25 */
  return -(lb_num_st + nlb); /* LB13 */
@

Dealing with any unlimited non-breaks after LB18 requires skipping
[[SP]].  This is complicated by the fact that LB14 through LB17 allow
optional [[SP]] characters.  Special codes are returned to retain this
information.

<<Line break extra classes>>=
OP_SP, /* LB14: OP|OP_ea_LWH SP+ */
QU_SP, /* LB15: QU SP+ */
CLP_SP, /* LB16: CL|CP|CP_ea_FWH SP+ */
B2_SP, /* LB17: B2 SP+ */
@

<<Handle unconditional breaks for [[nlb]]>>=
case UNI_lbp_SP: /* LB7 */
  switch(plb) {
    case UNI_lbp_OP:
    case UNI_lbp_OP_ea_FWH:
    case OP_SP:
      return -(lb_num_st + OP_SP); /* for LB14 */
    case UNI_lbp_QU:
    case QU_SP:
      return -(lb_num_st + QU_SP); /* for LB15 */
    case UNI_lbp_CL:
    case UNI_lbp_CP:
    case UNI_lbp_CP_ea_FWH:
    case CLP_SP:
      return -(lb_num_st + CLP_SP); /* for LB16 */
    case UNI_lbp_B2:
    case B2_SP:
      return -(lb_num_st + B2_SP); /* for LB17 */
  }
  return -(lb_num_st + UNI_lbp_SP);
@

<<Handle post-LB9 breaks>>=
case UNI_lbp_SP:
  <<Return break before [[nlb]]>> /* LB18 */
case UNI_lbp_OP:
case UNI_lbp_OP_ea_FWH:
case OP_SP:
  <<Adjust LB25 [[OP]] return>>
  return -(lb_num_st + nlb); /* LB14 */
case QU_SP:
  if(nlb == UNI_lbp_OP || nlb == UNI_lbp_OP_ea_FWH)
    return -(lb_num_st + nlb); /* LB15 */
  <<Return break before [[nlb]]>> /* LB18 */
case UNI_lbp_CL:
case UNI_lbp_CP:
case UNI_lbp_CP_ea_FWH:
  if(nlb == UNI_lbp_NS)
    return -(lb_num_st + nlb); /* LB16 */
  break;
case CLP_SP:
  if(nlb == UNI_lbp_NS)
    return -(lb_num_st + nlb); /* LB16 */
  <<Return break before [[nlb]]>> /* LB18 */
case UNI_lbp_B2:
  if(nlb == UNI_lbp_B2)
    return -(lb_num_st + nlb); /* LB17 */
  break;
case B2_SP:
  if(nlb == UNI_lbp_B2)
    return -(lb_num_st + nlb); /* LB17 */
  <<Return break before [[nlb]]>> /* LB18 */
@

<<Other lbp [[SP]] aliases>>=
case OP_SP:
case QU_SP:
case CLP_SP:
case B2_SP:
@

<<Other lbp [[SP]], [[BA]], [[HY]] aliases>>=
<<Other lbp [[SP]] aliases>>
@

Since [[CB]] only appears in one rule, it is easier to deal with than
[[SP]].  It is, however, still affected by other rules.  The prefix
rule is already overridden by previous suffix rules.  The suffix fule,
though, is overridden by LB11, LB12, LB14 and LB19.  The check is
still simple enough to include it in the initial [[nlb]] check.
However, any unlimited suffix rules following LB20 must exclude
[[CB]].  Rather than testing for this in the first [[nlb]] switch,
another [[nlb]] switch is executed in post-processing.  The only odd
one out is [[QU]], which is after LB18, but before LB20.

<<Handle unconditional breaks for [[nlb]]>>=
case UNI_lbp_CB:
  switch(plb) {
    case UNI_lbp_WJ: /* LB11 */
    case UNI_lbp_GL: /* LB12 */
    case UNI_lbp_OP: /* LB14 */
    case UNI_lbp_OP_ea_FWH:
    case OP_SP:
    case UNI_lbp_QU: /* LB19 */
      break;
    default:
      <<Return break before [[nlb]]>> /* LB20 */
  }
  break;
@

<<Handle post-LB9 breaks>>=
case UNI_lbp_CB:
  if(nlb == UNI_lbp_QU)
    return -(lb_num_st + nlb); /* LB19 */
  <<Return break before [[nlb]]>> /* LB20 */
@

<<Perform final lb fixups>>=
switch((uint8_t)nlb) {
  <<Handle post-LB20 unconditional breaks for [[nlb]]>>
}
@

<<Handle post-LB20 unconditional breaks for [[nlb]]>>=
case UNI_lbp_QU: /* LB19 */
case UNI_lbp_NS: /* LB21 */
case UNI_lbp_IN: /* LB22 */
  return -(lb_num_st + nlb);
@

While the result is always negative, [[HY]] and [[BA]] play a role in
LB21a and LB25 as well, so they need to return a special code.  LB21a
codes don't have to be used as [[GL]] exceptions above, since they don't
break after the [[HY]] or [[BA]], anyway.

<<Line break extra classes>>=
HL_HY, /* LB21a: HL HY */ HL_BA, /* LB21a: HL BA */
PRPO_HY, /* LB25 */
@

<<Handle post-LB20 unconditional breaks for [[nlb]]>>=
case UNI_lbp_BA: /* LB21 */
  if(plb == UNI_lbp_HL)
    return -(lb_num_st + HL_BA); /* LB21a */
case UNI_lbp_HY:
  if(plb == UNI_lbp_HL)
    return -(lb_num_st + HL_HY); /* LB21a */
  else if(plb == UNI_lbp_PR || plb == UNI_lbp_PO)
    return -(lb_num_st + PRPO_HY); /* LB25 */
  return -(lb_num_st + nlb);
@

<<Other lbp [[SP]], [[BA]], [[HY]] aliases>>=
case PRPO_HY:
@

The pure prefix rules can be handled in the post-LB9 [[prevret]]
switch, since both of the explicit breaks have been handled now.

<<Handle post-LB9 breaks>>=
case UNI_lbp_QU: /* LB19 */
case UNI_lbp_BB: /* LB21 */
case HL_BA: /* LB21a */
case HL_HY:
  return -(lb_num_st + nlb);
@

Most of the other rules can be simply implemented as well.

<<Handle post-LB9 breaks>>=
case UNI_lbp_HY:
case PRPO_HY:
  if(((nlb == UNI_lbp_AL || nlb == UNI_lbp_AL_EP) && prevret >= 0) || /* LB20.09 */
     nlb == UNI_lbp_NU) /* LB25 */
    return -(lb_num_st + nlb);
  break;
case UNI_lbp_NU:
case NU_EXT_NU:
  switch((uint8_t)nlb) {
    case UNI_lbp_AL: /* LB23 */
    case UNI_lbp_AL_EP:
    case UNI_lbp_HL:
    case UNI_lbp_OP: /* LB30 */
      return -(lb_num_st + nlb);
  }
  /* Fall through */
case NU_EXT_SY:
case NU_EXT_IS:
  switch((uint8_t)nlb) {
    case UNI_lbp_NU: /* LB25 */
      return -(lb_num_st + NU_EXT_NU);
    case UNI_lbp_SY:
      return -(lb_num_st + NU_EXT_SY);
    case UNI_lbp_IS:
      return -(lb_num_st + NU_EXT_IS);
    case UNI_lbp_CL:
      return -(lb_num_st + NU_EXT_CL);
    case UNI_lbp_CP:
      return -(lb_num_st + NU_EXT_CP);
    case UNI_lbp_CP_ea_FWH:
      return -(lb_num_st + NU_EXT_CP_ea_FWH);
  }
  /* Fall through */
case NU_EXT_CL:
case NU_EXT_CP:
case NU_EXT_CP_ea_FWH:
  if(nlb == UNI_lbp_PO || nlb == UNI_lbp_PR)
    return -(lb_num_st + nlb);
  break;
case UNI_lbp_AL_EP:
  if(nlb == UNI_lbp_EM)
    return -(lb_num_st + nlb); /* LB30b */
  /* Fall through */
case UNI_lbp_HL:
case UNI_lbp_AL:
  switch((uint8_t)nlb) {
    case UNI_lbp_NU: /* LB23 */
    case UNI_lbp_PR: /* LB24 */
    case UNI_lbp_PO:
    case UNI_lbp_AL: /* LB28 */
    case UNI_lbp_AL_EP:
    case UNI_lbp_HL:
    case UNI_lbp_OP: /* LB30 */
      return -(lb_num_st + nlb);
  }
  break;
case UNI_lbp_JL:
  switch((uint8_t)nlb) {
    case UNI_lbp_JL: /* LB26 */
    case UNI_lbp_JV:
    case UNI_lbp_H2:
    case UNI_lbp_H3:
    case UNI_lbp_PO: /* LB27 */
      return -(lb_num_st + nlb);
  }
  break;
case UNI_lbp_JV:
case UNI_lbp_H2:
  if(nlb == UNI_lbp_JV || nlb == UNI_lbp_JT || /* LB26 */
     nlb == UNI_lbp_PO) /* LB27 */
    return -(lb_num_st + nlb);
  break;
case UNI_lbp_JT:
case UNI_lbp_H3:
  if(nlb == UNI_lbp_JT /* LB26 */ || nlb == UNI_lbp_PO /* LB27 */)
    return -(lb_num_st + nlb);
  break;
case UNI_lbp_EB:
  if(nlb == UNI_lbp_EM)
    return -(lb_num_st + nlb); /* LB30b */
  /* Fall through */
case UNI_lbp_ID:
case UNI_lbp_EM:
  if(nlb == UNI_lbp_PO)
    return -(lb_num_st + nlb); /* LB23a */
  break;
case UNI_lbp_RI:
  if(nlb == UNI_lbp_RI)
    return -(lb_num_st + RI_RI); /* LB30a */
  break;
/* no RI_RI:  LB30a */
@

<<Line break extra classes>>=
RI_RI, /* LB30a: After odd # of RIs, suppress, but don't suppress next time */
@

<<Perform final lb fixups>>=
switch(plb) {
  case NU_EXT_CP:
  case UNI_lbp_CP:
    if(nlb == UNI_lbp_NU)
      return -(lb_num_st + nlb); /* LB30 */
  case NU_EXT_IS:
  case UNI_lbp_IS:
    if(nlb == UNI_lbp_AL || nlb == UNI_lbp_AL_EP)
      return -(lb_num_st + nlb); /* LB29/LB30 */
    /* Fall thorugh */
  case NU_EXT_SY:
  case UNI_lbp_SY:
    if(nlb == UNI_lbp_HL)
      return -(lb_num_st + nlb); /* LB21b/LB29/LB30 */
    break;
}
@

The troublesome one is the first rule of LB25, since it requires
backtracking.

<<Handle post-LB9 breaks>>=
case UNI_lbp_PR:
  switch((uint8_t)nlb) {
    case UNI_lbp_ID: /* LB23a */
    case UNI_lbp_EB:
    case UNI_lbp_EM:
    case UNI_lbp_JL: /* LB27 */
    case UNI_lbp_JV:
    case UNI_lbp_JT:
    case UNI_lbp_H2:
    case UNI_lbp_H3:
      return -(lb_num_st + nlb);
  }
  /* Fall through */
case UNI_lbp_PO:
  switch((uint8_t)nlb) {
    case UNI_lbp_AL: /* LB24 */
    case UNI_lbp_AL_EP:
    case UNI_lbp_HL:
    case UNI_lbp_NU: /* LB25 */
      return -(lb_num_st + nlb);
    case UNI_lbp_OP: /* LB25 */
    case UNI_lbp_OP_ea_FWH:
      return BT_PRPO_OP;
  }
  break;
@

<<Adjust lb [[prevret]]>>=
if(prevret == BT_PRPO_OP)
  plb = UNI_lbp_OP;
else if(prevret == -BT_PRPO_OP) {
  plb = UNI_lbp_NU;
  prevret = -(lb_num_st + plb);
}
@

<<Adjust LB25 [[OP]] return>>=
if(prevret == BT_PRPO_OP && nlb == UNI_lbp_NU)
  return -BT_PRPO_OP; /* LB25 backtrack finished: don't break */
/* else LB25 backtrack canceled (may have already been canceled above) */
@  

<<Line break special tokens>>=
BT_PRPO_OP,  /* LB25 */ /* PR|PO ? OP|OP_ea_FWH */
@

And now some final cleanup:  LB8a inverts the sign of the default
return, and LB1, LB30, and LB30b still need a new property.

<<Generate [[lbp]] property>>=
new_elit_prep(lbp, lb, 3);
add_new_elit(lbp, OP_ea_FWH, op_ea_fwh);
add_new_elit(lbp, CP_ea_FWH, cp_ea_fwh);
add_new_elit(lbp, AL_EP, al_eb);
finish_new_elits(lbp);
elit(lb, AI); /* XX is def_lb; AL is def_lbp */
elit(lb, SG);
elit(lb, SA);
elit(lb, CM);
elit(lb, CJ);
elit(lb, NS);
elit(lb, OP);
elit(lb, CP);
prop_t *gc_prop = &parsed_props[prop_gc];
fixup_rng_dat8(gc_prop);
elit(gc, Mn); /* Cn is def_gc */
elit(gc, Mc);
prop_t *ea_prop = &parsed_props[prop_ea];
fixup_rng_dat8(ea_prop);
elit(ea, F);
elit(ea, W);
elit(ea, H);
for(uint32_t curlb = 0, curep = 0,
             lbl = lbprop->rng_dat8[0].low,
	     lbh = lbprop->rng_dat8[0].high,
	     epl = ExtPict_prop->rng[0].low,
	     eph = ExtPict_prop->rng[0].high;
      curlb < lbprop->len || curep < ExtPict_prop->len; ) {
  uint8_t curdat, op;
  if(curlb < lbprop->len) {
    curdat = lbprop->rng_dat8[curlb].dat;
    if(curdat == lb_AI || curdat == lb_SG) {
      inc_propp(lb, lbprop, rng_dat8); /* skip/convert to default (AL) */
      continue;
    }
    if(curdat == lb_SA) {
      while(lbl <= lbh &&
            (op = uni_chrrng_dat8(lbl, gc_prop->rng_dat8, gc_prop->len,
                                  gc_prop->def)) != gc_Mn && op != gc_Mc)
        lbl++; /* skip/convert to default (AL) */
      if(lbl > lbh) {
	inc_propp(lb, lbprop, rng_dat8);
	continue;
      }
    } else if(curdat == lb_CJ)
      curdat = lb_NS;
    else if(curdat == lb_OP || curdat == lb_CP)
      if((op = uni_chrrng_dat8(lbl, ea_prop->rng_dat8, ea_prop->len,
			       ea_prop->def)) == ea_F ||
         op == ea_W || op == ea_H)
	curdat = curdat == lb_OP ? lbp_OP_ea_FWH : lbp_CP_ea_FWH;
  } else
    curdat = lbprop->def;
  for(; epl <= eph && epl < lbl; epl++) /* assuming lb(Cn) == XX */
    if(uni_chrrng_dat8(epl, gc_prop->rng_dat8, gc_prop->len,
                       gc_prop->def) == gc_prop->def) {
      uint32_t nl = epl + 1;
      while(++nl <= eph &&
            uni_chrrng_dat8(epl, gc_prop->rng_dat8, gc_prop->len,
	                    gc_prop->def) == gc_prop->def);
      add_enum_rng(prop_lbp, epl, nl - 1, lbp_AL_EP);
      epl = nl;
      break;
    }
  if(epl > eph) {
    inc_propp(ep, ExtPict_prop, rng);
    continue;
  }
  if(curdat == lbprop->def)
    continue;
  uint32_t nl = lbl + 1;
  if(curdat == lb_SA) {
    while(nl <= lbh && ((op = uni_chrrng_dat8(nl, gc_prop->rng_dat8,
			 		      gc_prop->len, gc_prop->def)) == gc_Mn ||
			op == gc_Mc))
      nl++;
    curdat = lb_CM;
  } else if(curdat == lb_OP || curdat == lb_CP)
    while(nl <= lbh && (op = uni_chrrng_dat8(nl, ea_prop->rng_dat8,
			 		     ea_prop->len, ea_prop->def)) != ea_F &&
			op != ea_W && op != ea_H)
      nl++;
  else if(curdat == lbp_OP_ea_FWH || curdat == lbp_CP_ea_FWH)
    while(nl <= lbh && ((op = uni_chrrng_dat8(nl, ea_prop->rng_dat8,
			 		      ea_prop->len, ea_prop->def)) == ea_F ||
			op == ea_W || op == ea_H))
      nl++;
  else
    nl = lbh + 1;
  add_enum_rng(prop_lbp, lbl, nl - 1, curdat);
  lbl = nl;
  if(lbl > lbh)
    inc_propp(lb, lbprop, rng_dat8);
}
@

\lstset{language=txt}
{\let\Tt\unimonox
<<FIXME>>=
Support u-lb locale extension.  Seems to be a reference to css3-text:
http://www.w3.org/TR/css3-text/#line-break-property
  values: auto strict normal loose
  The precise set of rules in effect for each level is up to the UA
  and should follow language conventions. However, this specification
  does require that:
    Following breaks be forbidden in strict line breaking and
    allowed in normal and loose: 
      breaks before Japanese small kana or the Katakana-Hiragana
      prolonged sound mark: i.e. characters with the Unicode Line Break
      property CJ. (See LineBreak.txt in [UNICODE].)  
    If the content language is Chinese or Japanese, then
    additionally allow (but otherwise forbid) for normal and loose:
      breaks before hyphens:
         U+2010,  U+2013,  U+301C,  U+30A0 
    Following breaks be forbidden in normal and strict line
    breaking and allowed in loose:
       breaks before iteration marks:
          U+3005,  U+303B,  U+309D,  U+309E,  U+30FD,  U+30FE
       breaks between inseparable characters such as  U+2025, 
         U+2026 i.e. characters with the Unicode Line Break property IN. (See
        LineBreak.txt in [UNICODE].)  
    If the content language is Chinese or Japanese, then additionally
    allow (but otherwise forbid) for loose: 
      breaks before certain centered punctuation marks:
        : U+003A, ; U+003B,  U+30FB,  U+FF1A,  U+FF1B,  U+FF65,
        ! U+0021, ? U+003F,  U+203C,  U+2047,  U+2048,  U+2049,
	 U+FF01,  U+FF1F
      breaks before suffixes:
         % U+0025,  U+00A2,  U+00B0,  U+2030,  U+2032,  U+2033,
	  U+2103,  U+FF05,  U+FFE0
      breaks after prefixes:
          U+2116 and all currency symbols (Unicode general category
         Sc) other than  U+00A2 and  U+FFE0
@

}

\subsection{Support Testing}

While the machine-readable breaking rules are unusable, the test files
are not.  A generic test driver can be used to test all of the
breaking functions, with special asides to deal with different calling
conventions.

\lstset{language=make}
<<C Test Support Executables>>=
tstbrk \
@

<<Additional Tests>>=
./tstbrk g <$(UCD_LOC)/auxiliary/GraphemeBreakTest.txt
./tstbrk w <$(UCD_LOC)/auxiliary/WordBreakTest.txt
./tstbrk s <$(UCD_LOC)/auxiliary/SentenceBreakTest.txt
./tstbrk l <$(UCD_LOC)/auxiliary/LineBreakTest.txt
@

Each line of a test file is either a comment or a test case followed
by a comment.  While the comments following test cases have a specific
format, all comments are ignored.  The line number is tracked even for
comments, though, so that error messages can show where the test
failed.  The tests run very quickly, so a dot is printed after every
10 tests to indicate that the program isn't simply exiting without
testing.

\lstset{language=C}
<<tstbrk.c>>=
<<Common C Header>>
#include "uni_prop.h"

/* longest line == 1788 chars */
char lbuf[4096];

int main(int argc, const char **argv)
{
  char tst = 'g';
  if(argc > 1)
    tst = **++argv;
  int lno;
  for(lno = 1; fgets(lbuf, sizeof(lbuf), stdin); lno++) {
    uint8_t *s = (uint8_t *)strchr(lbuf, '#');
    if(s)
      *s = 0;
    for(s = (uint8_t *)lbuf; isspace(*s); s++);
    if(!*s)
      continue;
    <<Process a break test line>>
    if(!((lno + 1) % 10)) {
      putchar('.');
      fflush(stdout);
    }
  }
  putchar('\n');
  return 0;
}
@

The test text is a string of code points, with marks for what the
breaking function should return:  for 1, and  for 0.  Each string is
surrounded by  to indicate the fact that breaks always occur at
start-of-text and end-of-text, which are not implemented in the braek
functions.

The main difference between the functions, other than of course the
specific function to call, is whether or not back tracking is
required.  For back tracking, the expected status at the backtrack
point is saved, and when it is resolved, it is checked.

<<Process a break test line>>=
/* ignore bot/eot marks */
unsigned int clen;
if(uni_utf8_decode(s, 4, &clen) != (tst == 'l' ? L'' : L'')) {
  fprintf(stderr, "Invalid line %d: %ssot break\n", lno, tst == 'l' ? "" : "no ");
  return 1;
}
for(s += clen; isspace(*s); s++);
uint8_t *e;
for(e = s + strlen((char *)s); isspace(e[-1]); e--);
e += uni_utf8_prevc(e);
if(uni_utf8_decode(e, 4, NULL) != L'') {
  fprintf(stderr, "Invalid line %d: no eot break\n", lno);
  return 1;
}
while(isspace(e[-1]))
  e--;
uint32_t p, n, pp = 0, pn = 0; /* init to shut gcc up */
int ret = 0, pexp_brk = 0;
n = strtoul((char *)s, (char **)&s, 16);
while(s < e) {
  p = n;
  while(isspace(*s))
    s++;
  int exp_brk = uni_utf8_decode(s, 4, &clen) == L'';
  for(s += clen; isspace(*s); s++);
  n = strtoul((char *)s, (char **)&s, 16);
  switch(tst) {
    case 'g':
      ret = uni_gc_brk(p, n, ret, 0);
      break;
    case 'w':
      ret = uni_word_brk(p, n, ret, NULL);
      break;
    case 's':
      ret = uni_sentence_brk(p, n, ret, NULL);
      break;
    case 'l':
      ret = uni_line_brk(p, n, ret, NULL);
      break;
  }
  int match = 0;
  if(ret > 0 && tst != 'g' && (tst != 'l' || ret < 8)) {
    if(!pexp_brk) {
      pexp_brk = exp_brk + 1;
      pp = p;
      pn = n;
      continue;
    }
    match = !exp_brk;
  } else if(pexp_brk) {
    if(tst == 'w')
      match = (!ret) == exp_brk && (!ret) == pexp_brk - 1;
    else if(tst == 's')
      match = (!ret) == exp_brk && pexp_brk - 1 == (!ret || ret < -8);
    else /* tst == 'l' */
      match = (ret >= 0) == exp_brk && (ret < 0 && ret > -8) != pexp_brk - 1;
    pexp_brk = 0;
  } else
    match = (ret >= 0) == exp_brk;
  if(!match) {
    if((ret >= 0) == exp_brk) {
      p = pp;
      n = pn;
    }
    fprintf(stderr, "Failed line %d %04X %s %04X\n",
                    lno, (int)p, exp_brk ? "" : "", (int)n);
    return 1;
  }
}
if(pexp_brk && !(pexp_brk - 1)) {
  fprintf(stderr, "Failed line %d %04X  %04X\n", lno, (int)pp, (int)pn);
  return 1;
}
@

\section{Numeric Properties}

Numeric properties are a subset of enumerated properties.  The
property's primary values are integers or two dot-separated integers
(i.e., floating point values).  They may have non-numeric values and
aliases as well.  The main difference is that the sloppy matching
should also support matching the plain integer against any like-valued
integer (e.g., 01 matches 1) and the dotted value against any
like-valued floating point number (e.g., 01.10 matches 1.1).  In
addition, the age property is meant to match equal to or less than the
search value.  For example, 3.0 matches 1.1 as well.

\subsection{Storage Methods}

Numeric values are stored as rational numbers (fractions).  Since the
current standard has no numbers requiring denominators over 127, the
denominator is stored in a single byte.  The numerator needs no more
than 19 bits, so it is stored in the remaining 3 bytes.  The generic
32-bit data type developed in the last section can be used to store
these, by casting the 32-bit value back and forth between the native
fraction structure ([[uni_frac_t]]).

\lstset{language=C}
<<[[uni_frac_t]]>>=
/** 32-bit structure for storing rational numbers (fractions) */
typedef struct {
    int32_t num:24; /**< Numerator */
    uint32_t denom:8; /**< Denominator; could be zero if num is zero */
} uni_frac_t;
@

<<Unicode property exports for generator>>=
<<[[uni_frac_t]]>>
/** Look up rational number in a range table
  * The numerator (\p num) and denominator (\p denom) of a rational
  * number stored in \p tab/\p tab_len for \p cp is returned.  The
  * table must be sorted as per \ref uni_cmprng_dat32.  The 32-bit
  * data must be formatted as a \ref uni_frac_t structure */
void uni_chrrng_val(uint32_t cp, const uni_chrrng_dat32_t *tab,
                    uint32_t tab_len, int32_t *num, uint8_t *denom);
@

<<Known Data Types>>=
uni_frac_t,%
@

<<Unicode property functions for generator>>=
void uni_chrrng_val(uint32_t cp, const uni_chrrng_dat32_t *tab,
                    uint32_t tab_len, int32_t *num, uint8_t *denom)
{
  uint32_t res = uni_chrrng_dat32(cp, tab, tab_len, 0);
  uni_frac_t *f = (uni_frac_t *)&res;
  if(!f->num)
    f->denom = 1;
  *num = f->num;
  *denom = f->denom;
}
@

<<Property parsed contents>>=
uni_chrrng_dat32_t *rng_dat32;
int rng_num; /* flag: rng_dat32's data is actually uni_frac_t */
@

<<UCD parser local functions>>=
static void add_dat32_rng(prop_t *p, uint32_t low, uint32_t high,
                          uint32_t dat)
{
  if(!p->max_len)
    inisize(p->rng_dat32, (p->max_len = 8));
  if(p->len) {
    uni_chrrng_dat32_t *last = &p->rng_dat32[p->len -1];
    if(last->len < 255 && last->low + last->len == low - 1 &&
       last->dat == dat) {
      if(high < last->low + 256) {
        last->len = high - last->low;
	return;
      }
      last->len = 255;
      low = last->low + 256;
    }
  }
  while(1) {
    check_size(p->rng_dat32, p->max_len, p->len + 1);
    p->rng_dat32[p->len].dat = dat;
    p->rng_dat32[p->len].low = low;
    ++p->len;
    if(high < low + 256) {
      p->rng_dat32[p->len - 1].len = high - low;
      return;
    }
    p->rng_dat32[p->len - 1].len = 255;
    low += 256;
  }
}

static void add_num_rng(prop_t *p, uint32_t low, uint32_t high,
                        int32_t num, uint8_t denom)
{
  uni_frac_t f = {num, denom};
  uint32_t *v = (uint32_t *)&f;
  if(!num) /* store 0 as 0/0 for consistency */
    f.denom = 0;
  p->rng_num = 1;
  add_dat32_rng(p, low, high, *v);
}
@

<<UCD parser local definitions>>=
#define decl_num(n) int prop_##n = -1
#define add_num(n, num, denom) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  add_num_rng(&parsed_props[prop_##n], low, high, num, denom); \
} while(0)
@

\subsection{Testing}

To test the table implementations, a different, but nearly identical,
set of routines is used compared to the ones one for booleans.

<<Functions to help test generated tables>>=
#define num(x) doit_num(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab)

static void doit_num(const char *name, const uni_chrrng_dat32_t *rng, uint32_t nent,
                     const uint32_t *mtab)
{
    uint32_t i;

    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      int32_t rn, mn;
      uint8_t rd, md;
      uni_chrrng_val(i, rng, nent, &rn, &rd);
      <<range table val for [[i]]>>;
      if(rn != mn || rd != md) {
        fprintf(stderr, "mismatch %s@%d %d/%d %d/%d\n", name, i,
	                (int)rn, (int)rd, (int)mn, (int)md);
	exit(1);
      }
    }
    /* check performance */
    int j;
    double tr, tt;
    tstart();
    int32_t mn;
    uint8_t md;
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        uni_chrrng_val(i, rng, nent, &mn, &md);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        <<range table val for [[i]]>>;
    tt = tend();
    printf("  r%.0f t%.0f %.2fx\n", tr, tt, tr / tt);
}
@

\subsection{Parsing the UCD}

Numeric properties from [[UnicodeData.txt]] are ccc (field 4) and nv
(field 9)%
\footnote{[[extracted/DerivedNumericValues.txt]] might be a better
source for nv, as it includes Unihan data.}%
.  Since ccc has aliases, it is already added as an enumeration.  The
nv field is already in normal rational form.

<<Initialize UCD files>>=
decl_num(nv);
@

<<Process a line of [[UnicodeData.txt]]>>=
if(*fields[8]) {
  int32_t num = strtol(fields[8], &s, 10);
  uint8_t denom = *s ? strtol(s + 1, NULL, 10) : 1;
  add_num(nv, num, denom);
}
@

An optional numeric property comes from [[Unihan_IRGSources.txt]]:
cjkRSUnicode%
\footnote{Listed in UniHan as Informative, but still needed for regex.}%
.  It is, like all unihan files, a tab-separated file, with the single
code point in U+ notation in field 1.  Note that the actual optional
property is CJK\_Radical, which I interpret as meaning the same as
cjkRSUnicode. However, I will not add a non-normative alias
(especially considering that the library currently only supports four
names, and there are already four names for this property).

There is another, related file ([[CJKRadicals.txt]]) which provides
additional interpretation for the radical numbers returned by this
property.  Since this file's values have no property label, I will not
provide them as a property for now.

<<UCD parser local functions>>=
static void split_line_tab(char *buf)
{
  if(!max_fields)
    inisize(fields, (max_fields = 16));
  num_fields = 0;
  while(*buf != '\t' && isspace(*buf)) buf++;
  if(!*buf || *buf == '#')
    return;
  while(1) {
    while(isspace(*buf) && *buf != '\t')
      buf++;
    char *f = buf, *nf, fc;
    for(nf = buf; *nf && *nf != '\t'; nf++);
    fc = *nf;
    buf = nf + 1;
    while(nf > f && isspace(nf[-1])) --nf;
    *nf = 0;
    check_size(fields, max_fields, num_fields + 1);
    fields[num_fields++] = f;
    if(!fc)
      return;
  }
}
@

<<Initialize Unihan files>>=
decl_num(cjkRSUnicode);
@

<<Parse Unihan files>>=
open_f("Unihan_IRGSources.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse Unihan cp>>
  <<Parse [[kRSUnicode]] property>>
  <<Further processing of [[IRGSources]]>>
}
fclose(f);
@

<<Parse [[kRSUnicode]] property>>=
if(!strcmp(fields[1], "kRSUnicode")) {
  /* due to complexity of this field, the num/denom is interpreted */
  /* differently: */
  /* denom is number after .; if after !., then it is negated */
  /* num is 3 byte fields; lowest field is left of . or !. */
  /* if there are two values, then the 2nd value is in the upper bytes */
  int32_t num;
  uint8_t denom;
  num = strtol(fields[2], &s, 10);
  if(*s == '!')
    denom = -strtol(s + 2, &s, 10);
  else
    denom = strtol(s + 1, &s, 10);
  if(*s) {
    num |= strtol(s, &s, 10) << 8;
    if(*s == '!')
      num |= -strtol(s + 2, &s, 10) << 16;
    else
      num |= strtol(s + 1, &s, 10) << 16;
  }
  add_num(cjkRSUnicode, num, denom);
}
@

<<Parse Unihan cp>>=
split_line_tab(lbuf);
if(num_fields < 2)
  continue;
low = high = strtol(lbuf + 2, &s, 16);
if(!low || *s) { /* should never happen, but if it does: ignore */
  fprintf(stderr, "bad col 1: %s\n", lbuf);
  continue;
}
@

<<Unicode property exports>>=
/** Extract first number pair for cjkRSUnicode property.
  * The first number pair in the cjkRSUnicode property is written as
  * l.r or l!.r.  This macro takes the "numerator" \p n and "denominator"
  * \p d values from its numeric property value, and sets lvalues \p l and \p r.
  * The presence of an exclamation point is indicated by a negative \p r, in
  * which case the actual \p r is its absolute value. */
#define uni_cjkRSUnicode_val1(n, d, l, r) do { \
  l = (uint8_t)(n); \
  r = (int8_t)d; \
} while(0)
/** Extract second number pair for cjkRSUnicode property.
  * The second number pair in the cjkRSUnicode property is written as
  * l.r or l!.r.  This macro takes the "numerator" \p n and "denominator"
  * \p d values from its numeric property value, and sets lvalues \p l and \p r.
  * The presence of an exclamation point is indicated by a negative \p r, in
  * which case the actual \p r is its absolute value. */
#define uni_cjkRSUnicode_val2(n, d, l, r) do { \
  l = (uint8_t)(n >> 8); \
  r = (int32_t)n >> 16; \
} while(0)
@

<<C Prototypes>>=
void uni_cjkRSUnicode_val1(int32_t num, uint8_t denom, uint8_t &l,
                           int8_t &r);
void uni_cjkRSUnicode_val2(int32_t num, uint8_t denom, uint8_t &l,
                           int8_t &r);
@

Another property is the Equivalent\_Unified\_Ideograph property, which
lists unified CJK codes that look approximately like a CJK radical or
stroke.  While the value could fit in the 24-bit numerator, it's
stored in a full 32-bit word, instead.

<<Initialize UCD files>>=
decl_num(EqUIdeo);
@

<<Parse UCD files>>=
open_f("EquivalentUnifiedIdeograph.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_num32(EqUIdeo, fields[1]);
}
fclose(f);
@

<<UCD parser local definitions>>=
#define add_num32(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  add_dat32_rng(&parsed_props[prop_##n], low, high, atol(v)); \
} while(0)
@

\lstset{language=txt}
<<FIXME>>=
other properties from IRGSources?  RadicalStrokeCounts?
CJKRadicals.txt:
  field 1 = radical # (index); may include ' (maps to !?)
  field 2 = CJK Radical character
  field 3 = CJK Unified Ideograph character
 2 & 3 are always 1 16-bit char, so could store as 32-bit value per index
TangutSources.txt, NushuSources.txt:
  field 1 = U+-cp
  field 2 = tag (includig kRSTUnicode)
  field 3 = value
EmojiSources.txt:
  field 1 = cp sequence
  field 2-4 = shift-JIS code for various phone companies.  Who cares?
@

The only other numeric property currently supported is age, which
comes from [[DerivedAge.txt]].  Since it has aliases, it is already
added as an enumeration.

\subsection{Generating the Static Data}

The tables have alreaady been dumped above.  In addition to the
property tables, a simple query function for fractions is provided.

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng_dat32 && parsed_props[i].rng_num) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    const char *lname = i < num_prop_aliases ? prop_aliases[i].long_name :
                                               parsed_props[i].name;
    fprintf(gen_h, "/** Retrieve value of numeric %s property \\p x.\n"
                   "  * \\p *n is set to the numerator, and \\p *d is set\n"
		   "  * to the denominator */\n", lname);
    fprintf(gen_h, "#undef uni_%s_of\n", name); /* set by 32-bit export */
    fprintf(gen_h, "#define uni_%s_of(x, n, d) uni_x_val(x, uni_%s_mtab, n, d)\n",
	         name, name);
    fprintf(tstf, "num(%s);\n", name);
  }
}
@

<<Unicode property exports>>=
/** Look up rational number in a mutli-level table
  * The numerator (\p num) and denominator (\p denom) of a rational
  * number stored in \p tab for \p cp is returned.  The 32-bit
  * data must be formatted as a \ref uni_frac_t structure */
void uni_x_val(uint32_t cp, const uint32_t *tab, int32_t *num, uint8_t *denom);
@

<<Unicode property functions>>=
void uni_x_val(uint32_t cp, const uint32_t *tab, int32_t *num, uint8_t *denom)
{
  const uint8_t *mr;
  uni_multi_tab_lookup(tab, cp * 4, &mr, 0);
  if(!mr)
    *num = 0;
  else {
    const uni_frac_t *v = (const uni_frac_t *)mr;
    *num = v->num;
    *denom = v->denom;
  }
  if(!*num)
    *denom = 1;
}
@

<<range table val for [[i]]>>=
uni_x_val(i, mtab, &mn, &md);
@

<<Additional property type names>>=
/** Numeric;  range table is \ref uni_chrrng_dat32_t, and multi-level table
 ** is 32 bits per code point.  32-bit value is actually \ref uni_frac_t */
UNI_PROP_TYPE_NUM,
@

<<Set prop type for export>>=
if(parsed_props[i].rng_num)
  t = UNI_PROP_TYPE_NUM;
@

<<Functions to help test generated tables>>=
<<[[dat]][[32]] test>>
@

\lstset{language=txt}
<<FIXME>>=
numeric (UAX44-LM1, which provides example and words but no algorithm):
  compare as floating-point number w/ limited precision
  e.g. 01.00 == 1, 0.3333 = 1/3 (but how limited precision?)
or, alternatively:
  - integer parts must match exactly (i.e. floor(A/B) == floor(C/D))
    a=rem(A/B), b=B, c=rem(C/D), d=D
  - fractional parts are multiplied by a fixed amount, say 1000:
    x=floor(a*1000/b); y=floor(c*1000/d)
    or, say, enough that multiplying by 10 more would make it > 1000
    thus 1/20 would become 500 rather than just 50.
  - if |x-y|<=10 (or some other slop factor), OK.
  1/3 = 333; .3 (33) bad .33 (3) OK .333 (0) best
  2/3 = 666; .7 (34) bad .67 (4) OK .667 (1) OK .6667 (0) best
  - note that if I used round instead of floor, .667 would be best
@

<<FIXME>>=
Unihan_NumericValues.txt: kAccountingNumeric, kOtherNumeric, kPrimaryNumeric
  (may need inclusion in nt/nv)
  (note: listed in unihan as "informative")
  (note: no character has more than one of these)
Unihan_DictionaryLikeData.txt:  kFourCornerCode, kFrequency, kGradeLevel,
                                kHKGlyph, kTotalStrokes
				kCHaiT: maybe; maybe needs alt. rep.
				kFenn, kPhonetic: could put last bit in denom
Unihan_DictionaryIndices.txt: lots.  may need alt. rep to fit values
Unihan_OtherMappings.txt: lots (may need string for some)
@

\section{String Properties}

String properties are those which have a value that cannot be
represented using the previously described methods.  Since the UCD is
text, this is generally either an ASCII or Unicode string.  Useful
operations include:

\begin{itemize}
\item Find out the string-valued value \emph{of} the property for a
character.  For variable-length strings, this should support two-part
retrieval: first obtain the length, and then obtain the string.  If
there is a maximum length, it should be listed so that the sizing step
can be avoided.
\end{itemize}

\subsection{Storage Methods}

The storage methods used for boolean properties can be used for
strings as well, with some adjustments.  First of all, there are very
few consecutive characters with the same value (other than the default
value, which is an empty string), so using a range list is wasteful.
Instead, a simple code point list with value is used.  One way to
store the string would be as a pointer to a zero-terminated string (no
strings use zero as a component value).  However, this requires
computing string length at every access, so a better way would be to
store a pointer and a length.  For 64-bit systems, this would require
at least 8 bytes for the pointer, and probably another 8 bytes to
align the structure, so 4 bytes for the code point and 4 bytes for the
length.  However, a more efficient storage method would be to use a
32-bit pointer into a single string containing all possible values,
reducing storage by 4 bytes.  Additional savings can be had if the
combined string size can be kept below 64K, in which case 16-bit
integers can represent the offset and length.  The length could be
stored in the string as well, eliminating the need for a separate
length, and generally reducing the space required for that to a byte.
Since fewer than 24 bits are used for the code point, the length could
be stored in the code point's lower byte as well; masking would be
required to test equality, though.  On the other hand, using UTF-16 to
encode strings will, on average, consume significantly less space than
UTF-32, but will not give space for an extra length byte.  Another
space-saving measure would be to store similar properties together.
For example, the main string properties are either case
folding-related or normalization-related, and some have the others as
their default values.  These could be combined when they have many
elements in common.  For now, though, plain tables are built using
32-bit pointers and lengths.  The merging into a single string table
can be done during post-processing. However, to assist with this,
multiple tables sharing the same string table can use the [[flags]]
field to distinguish themselves.

\lstset{language=C}
<<[[prop_t]] prerequisites>>=
typedef struct {
  uint32_t cp, off, flags: 8, len: 24;
} raw_cp_str_t;
@

<<Known Data Types>>=
raw_cp_str_t,%
@

<<Property parsed contents>>=
raw_cp_str_t *str_arr;
uint32_t *strs;
uint32_t strs_len, max_strs;
@

<<UCD parser local functions>>=
static void add_str_rng(prop_t *p, uint32_t low, uint32_t high, const uint32_t *val,
                        uint32_t len)
{
  uint32_t off = p->strs_len;
  if(!p->max_len) {
    inisize(p->str_arr, (p->max_len = 8));
    inisize(p->strs, (p->max_strs = 32));
  }
  check_size(p->strs, p->max_strs, off + len);
  cpybuf(p->strs + off, val, len);
  p->strs_len += len;
  for(; low <= high; low++) {
    check_size(p->str_arr, p->max_len, p->len + 1);
    p->str_arr[p->len].cp = low;
    p->str_arr[p->len].off = off;
    p->str_arr[p->len].len = len;
    p->str_arr[p->len].flags = 0;
    ++p->len;
  }
}
@

<<UCD parser local definitions>>=
#define decl_str(n) \
  int prop_##n = -1
#define add_str(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[64]; /* max known value == 30, so 64 should be enough */ \
    uint32_t len; \
    for(s = v, len = 0; *s; len++) \
      str[len] = strtol(s, &s, 16); \
    add_str_rng(&parsed_props[prop_##n], low, high, str, len); \
  } \
} while(0)
@

\subsection{Parsing the UCD -- Decomposition}

The first string field from [[UnicodeData.txt]] is the decomposition
mapping (dm) field.  For compatibility decompositions, the initial
part of the field is the decomposition type in angle brackets, which
has already been encoded as an enumeration (dt), so it is skipped,
along with the space which always follows it%
\footnote{The [[NormalizationCorrections.txt]] file provides previous
versions of this field, but since I see no use for it, and no property
name for it, it will not be provided.}%
.

<<Initialize UCD files>>=
decl_str(dm);
@

<<Process a line of [[UnicodeData.txt]]>>=
s = fields[5];
if(*s == '<')
  s = strchr(s, '>') + 2;
add_str(dm, s);
@

While the default values for dt and dm are normally blank, one range
actually has defined canonical values: the Hangul
Syllables\footnote{Note that I am not Korean, so I may refer to things
incorrectly or inappropriately.  Everything I know about Korean is
what I read about in the Unicode standard (and maybe a little watching
of subtitled Korean TV dramas).}, from AC00 through D7A3.  These are
only described in the standard, and can be generated with a simple
algorithm.  An entry in this range consists of three parts, called L,
V, and T.  There are normal, named code points for each L, V, and T
outside of this range, and every LV and LVT combination is contained
within this range.  There are defined decomposition mappings from LVT
to LV and T, and from LV to L and V. Rather than fill the dm string
table with easily generated values, the value is represented as an
empty result.  A function is then provided to convert that empty
string into the correct decomposition mapping.  The canonical
dm entry for LVT is LV T, so a flag is used to select the full L V T
decomposition directly.

<<Process a line of [[UnicodeData.txt]]>>=
if(low == 0xAC00) {
  /* hangul syllables */
  add_enum_rng(prop_dt, low, high, enum_val(prop_dt, "can"));
  add_str_rng(&parsed_props[prop_dm], low, high, NULL, 0);
}
@

<<Unicode property exports>>=
/** Decompose Hangul syllable \p cp
  * Decomposes Hangule syllable LVT into LV and T (if \p full is false)
  * or L, V and T (if \p full is true), and decomposes LV into L and V.
  * If \p full is true, \p res should have space for three words.
  * Otherwise, or if \p cp is known to be LV, only two words are required.
  * The return value is negative if \p cp is not a decomposable Hangul syllable,
  * or the length of the return value otherwise */
int uni_hangul_syllable_decomp(uint32_t cp, uint32_t *res, int full);
@

<<Unicode property functions>>=
int uni_hangul_syllable_decomp(uint32_t cp, uint32_t *res, int full)
{
  int L, V, T;
  if(cp < 0xAC00 || cp > 0xD7A3)
    return -1;
  cp -= 0xac00;
  T = cp % 28;
  V = (cp / 28) % 21;
  L = cp / (28 * 21);
  if(!T) { /* LV -> L V */
    res[0] = 0x1100 + L;
    res[1] = 0x1161 + V;
    return 2;
  }
  if(!full) { /* LVT -> LV T */
    res[0] = 0xAC00 + 28 * (L * 21 + V);
    res[1] = 0x11A7 + T;
    return 2;
  }
  /* LVT -> L V T */
  res[0] = 0x1100 + L;
  res[1] = 0x1161 + V;
  res[2] = 0x11A7 + T;
  return 3;
}
@

The decomposition mappings are more useful if they are fully
decomposed.  Each character is given both a canonical and
compatibility full decomposition.  There is no real problem with
storing two entries with the same key in a sorted code point table, so
both are stored in the same table, with different flags.  Bit zero
indicates compatibility decomposition.

While I think the raw dm value is worthless, the Unicode standard
requires that it be provided for regular expressions.  It is stored in
the same string table as well, marked with bit one set if different
from the full decomposition.  Like the full decomposition, bit zero is
used to indicate compatibility decompositions, in order to avoid an
extra dt table lookup.

<<Initialize UCD files>>=
decl_str(dm_full);
@

<<Parse UCD files>>=
prop_dm_full = add_prop("dm_full");
prop_t *dmf_prop = &parsed_props[prop_dm_full];
prop_t *dm_prop = &parsed_props[prop_dm];
uint8_t can_val = enum_val(prop_dt, "can");
uint8_t none_val = enum_val(prop_dt, "None");
prop_t *dt_prop = &parsed_props[prop_dt];
for(i = 0; i < dm_prop->len; i++) {
  uint32_t cp = dm_prop->str_arr[i].cp;
  uint32_t str[64], len = dm_prop->str_arr[i].len;
  int is_can = uni_chrrng_dat8(cp, dt_prop->rng_dat8, dt_prop->len,
                               none_val) == can_val;
  int do_dec = !is_can;
  int did_repl = 0;
  raw_cp_str_t *unex;

  /* add unexpanded decomp */
  add_str_rng(dmf_prop, cp, cp, NULL, 0);
  if(!dmf_prop->strs_len) { /* deferred until first alloc */
    free(dmf_prop->strs);
    dmf_prop->strs = dm_prop->strs;
    dmf_prop->max_strs = dm_prop->max_strs;
    dmf_prop->strs_len = dm_prop->strs_len;
    dm_prop->strs = NULL;
    dm_prop->max_strs = dm_prop->strs_len = 0;
  }
  unex = &dmf_prop->str_arr[dmf_prop->len - 1];
  unex->off = dm_prop->str_arr[i].off;
  unex->len = len;
  cpybuf(str, dmf_prop->strs + unex->off, len);
  if(is_can)
    for(j = 0; j < len; j++) {
      uint8_t dt = uni_chrrng_dat8(str[j], dt_prop->rng_dat8, dt_prop->len,
                                   none_val);
      if(dt == can_val) {
        raw_cp_str_t *dec = bsearch(str + j, dm_prop->str_arr, dm_prop->len,
				    sizeof(raw_cp_str_t), uni_cmp_cp);
        if(dec->len > 1)
          movebuf(str + j + dec->len, str + j + 1, len - j - 1);
        cpybuf(str + j, dec->off + dmf_prop->strs, dec->len);
        len += dec->len - 1;
	did_repl = 1;
        j--; /* recheck 1st char of replacement */
      } else if(dt != none_val)
        do_dec = 1;
  }
  if(did_repl) {
    unex->flags = 2;
    add_str_rng(dmf_prop, cp, cp, str, len);
  }
  if(do_dec) {
    for(j = 0; j < len; j++) {
      uint8_t dt = uni_chrrng_dat8(str[j], dt_prop->rng_dat8, dt_prop->len,
                                   none_val);
      if(dt != none_val) {
        raw_cp_str_t *dec = bsearch(str + j, dm_prop->str_arr, dm_prop->len,
				    sizeof(raw_cp_str_t), uni_cmp_cp);
        if(dec->len > 1)
          movebuf(str + j + dec->len, str + j + 1, len - j - 1);
        cpybuf(str + j, dec->off + dmf_prop->strs, dec->len);
	did_repl = 1;
        len += dec->len - 1;
        j--; /* recheck 1st char of replacement */
      }
    }
    if(did_repl) {
      if(!is_can)
        unex->flags = 3;
      add_str_rng(dmf_prop, cp, cp, str, len);
      dmf_prop->str_arr[dmf_prop->len - 1].flags = 1;
    } else if(!is_can)
      unex->flags = 1;
  }
}
@

\subsection{Generating the Static Data}

This string table can now be printed.  Before doing so, the strings
are sorted, and any strings which are either the same or overlap from
the start share space.  This does not save as much as true common
substring elimination would, but it does save a little.  Additional
space could be saved by storing 24-bit (or even 21-bit) words, at the
expense of retrieval inefficiency.  Alternatively, the words could be
stored in UTF-16 format, which is somewhat natural and nearly always
takes up less space than UTF-32.  The same could be said of UTF-8, but
UTF-8 is not always smaller than UTF-16, and requires more complicated
encoding and decoding.  A property-wide parameter selects the format:
UTF-16 or UTF-32.

If 8-bit data is really wanted, it should probably be ASCII.  In that
case, the 32 bits per character are excessive, and should instead be
filled 4 characters per word.  This format is supported as well;
strings are assumed to be zero-terminated, or exactly four times the
length of the 32-bit string if no zeroes are encountered first.  The
terminating zero must always be in the last word.  Note that merging
strings works identically for 16-bit and 32-bit strings, but 8-bit
strings need a second pass to detect end-of-string overlaps.

<<Property parsed contents>>=
int strs_char_size; /* 8, 16, 32, or 0 (16) */
@

<<UCD parser local functions>>=
/* qsort has no user data, so this is a static */
const uint32_t *sort_strs;
static int cmp_strs(const void *a, const void *b)
{
   const raw_cp_str_t *A = a, *B = b;
   uint32_t len = A->len;
   int c;

   if(len > B->len)
     len = B->len;
   c = cmpbuf(sort_strs + A->off, sort_strs + B->off, len);
   if(c)
     return c;
   return (int32_t)A->len - (int32_t)B->len;
}
@

<<UCD parser local functions>>=
static void merge_strs(prop_t *p)
{
  uint32_t i;

  sort_strs = p->strs;
  qsort(p->str_arr, p->len, sizeof(*p->str_arr), cmp_strs);
  for(i = p->len - 1; i; i--)
    if(p->str_arr[i - 1].len <= p->str_arr[i].len &&
       !cmpbuf(p->strs + p->str_arr[i - 1].off,
               p->strs + p->str_arr[i].off,
	       p->str_arr[i - 1].len)) {
      p->str_arr[i - 1].off = p->str_arr[i].off;
  }
}
@

<<UCD parser local functions>>=
static void dump_strs(prop_t *p, const char *lname, FILE *gen_h)
{
  int i;
  unsigned int off, saved = 0;
  char nbuf[64];
  int char_size = p->strs_char_size;

  if(!char_size)
    char_size = 16;
  sprintf(nbuf, "uni_%s_strs.gen.c", p->name);
  open_wf(of, nbuf);
  fprintf(of, "#include <stdint.h>\n\n"
              "const uint%d_t uni_%s_strs[] = {\n\t", char_size, p->name);
  const char *prefix = "";
  merge_strs(p);
  for(i = p->len - 1, off = 0; i >= 0; i--) {
    int j, len = p->str_arr[i].len;
    unsigned int soff = p->str_arr[i].off;
    if(!len) {
      p->str_arr[i].off = 1;
      continue;
    }
    /* adjust 8-bit len */
    if(char_size == 8) {
      const char *s = (const char *)(p->strs + soff + len);
      len *= 4;
      while(len && !*--s)
        len--;
      p->str_arr[i].len = len;
    }

    p->str_arr[i].off = off;
    if(char_size > 8)
      for(j = 0; j < len; j++) {
        uint32_t c = p->strs[soff + j];
        if(char_size == 16 && c > 0xFFFF) {
          c -= 0x10000;
          fprintf(of, "%s0x%X,", prefix, 0xD800 + (c >> 10));
	  prefix = !((off + j + 1) % 8) ? "\n\t" : " ";
	  c = 0xDC00 + (c & 0x3ff);
	  p->str_arr[i].len++;
	  off++;
        }
        fprintf(of, "%s0x%04X", prefix, c);
	prefix = !((off + j + 1) % 8) ? ",\n\t" : ", ";
      }
    else {
      const char *s = (const char *)(p->strs + soff);
      for(j = 0; j < len; j++, s++) {
        if(*s == '\\' || *s == '\'')
	  fprintf(of, "%s'\\%c'", prefix, *s);
        else if(isprint(*s))
	  fprintf(of, "%s '%c'", prefix, (int)*s);
	else
	  fprintf(of, "%s0x%02X", prefix, (int)(uint8_t)*s);
	prefix = !((off + j + 1) % 8) ? ",\n\t" : ", ";
      }
    }
    off += len;
    /* skip predecessor if prefix of string already in table */
    /* but adjust length if necessary */
    while(i > 0 && p->str_arr[i - 1].off == soff) {
      if(char_size == 16) {
        for(j = len = 0; j < p->str_arr[i - 1].len; j++)
	  len += uni_utf16_enclen(p->strs[soff + j]);
	p->str_arr[i - 1].len = len;
	saved += len;
      } else if(char_size == 8) {
        len = p->str_arr[i - 1].len * 4;
	if(len > p->str_arr[i].len)
	  len = p->str_arr[i].len;
	p->str_arr[i - 1].len = len;
	saved += len;
      } else
        saved += p->str_arr[i - 1].len;
      p->str_arr[i - 1].off = p->str_arr[i].off;
      --i;
    }
    if(char_size == 8) {
      while(i > 0 && p->str_arr[i - 1].len &&
                     p->str_arr[i - 1].len * 4 <= p->str_arr[i].len) {
        len = p->str_arr[i - 1].len * 4;
        const char *s = (const char *)(p->strs + soff),
                   *t = (const char *)(p->strs + p->str_arr[i - 1].off),
		   *e = t + len;
        while(len && !*--e)
	  len--;
        if(!memcmp(s, t, len)) {
	  for(j = --i; j > 0; j--)
	    if(p->str_arr[j - 1].off != p->str_arr[i].off)
	      break;
	  for(; i >= j; i--) {
	    int plen = p->str_arr[i].len * 4;
	    if(plen > len)
	      plen = len;
	    p->str_arr[i].len = plen;
            p->str_arr[i].off = p->str_arr[i + 1].off;
	    saved += plen;
	  }
	  i++; /* last loop goes under by one */
        } else
	  break;
      }
      if(!i)
        break;
    }
  }
  fputs("\n};\n", of);
  fclose(of);
  fprintf(gen_h, "/** Bulk string data for Unicode %s property */\n"
                 "extern const uint%d_t uni_%s_strs[];\n"
		 "/** Length of \\ref uni_%s_strs */\n"
                 "#define uni_%s_strs_len %d /* (%d bytes) (%d words saved) */\n",
		 lname, char_size, p->name, p->name, p->name, off,
		 off * char_size / 8, saved);
}
@

<<Dump character information as C code>>=
/* ensure that each property has like-named string table */
s = (char *)dmf_prop->name;
dmf_prop->name = "dm";
dump_strs(dmf_prop, "Deomposition_Mapping", gen_h);
dmf_prop->name = s;
fputs("/** Bulk string data for full canonical decomposition property */\n"
      "#define uni_canon_decomp_strs uni_dm_strs\n"
      "/** Length of \\ref uni_canon_decomp_strs */\n"
      "#define uni_canon_decomp_strs_len uni_dm_strs_len\n"
      "/** Bulk string data for full compatibility decomposition property */\n"
      "#define uni_compat_decomp_strs uni_dm_strs\n"
      "/** Length of \\ref uni_compat_decomp_strs */\n"
      "#define uni_compat_decomp_strs_len uni_dm_strs_len\n", gen_h);
@

Now that the string offsets and lengths are correct, the plain table
is ready to be converted into a multi-level table, and then both can
be printed.  The plain table could simply be the same as the raw
table, using the flags to distinguish entries.  The multi-level table,
though, expects only one value per index, and each value must be the
same size.  Rather than figure out a way to squeeze all three values
into an efficient representation, three separate tables are generated.
While it would definitely be possible to encode the offset and length
into 24 bits, the current multi-level table implementation only
supports sizes which are powers of two.  Encoding into 16 bits might
be possible, but is likely too complex to be useful.  Other string
propreties may also benefit from more breathing room.  Instead, 32
bits are used: 16 for the offset and 16 for the length.  The raw dm
property also needs to distinquish between canonical and compatibility
results.  There is a binary property to help, but that requires an
extra lookup.  Instead, a flag is ored into the length field. The
compatibility decomposition table uses entries from the canonical
table where no compatibility decomposition exists.  Rather than make
the plain tables behave completely differently than the multi-level
tables (i.e., be a single merged table), three separate plain tables
will be generated as well.

<<UCD parser local functions>>=
/* sort by flag first, then cp */
static int cmp_cp_flg(const void *a, const void *b)
{
   const raw_cp_str_t *A = a, *B = b;
   int32_t c;

   c = (int32_t)A->cp - (int32_t)B->cp;
   if(c)
     return c;
   return (int32_t)A->flags - (int32_t)B->flags;
}
@

<<[[uni_str_arr_t]]>>=
/** A 64-bit structure for associating a code point with a string descriptor */
typedef struct {
  uint32_t cp; /**< The code point */
  uint32_t off:20, /**< Offset into bulk string */ len:12; /**< String length */
} uni_str_arr_t;
@

<<[[uni_str_ptr_t]]>>=
/** A 32-bit string descriptor */
typedef struct {
  uint32_t off:20, /**< Offset into bulk string */ len:12; /**< String length */
} uni_str_ptr_t;
@

<<Unicode property exports for generator>>=
<<[[uni_str_arr_t]]>>
<<[[uni_str_ptr_t]]>>
@

<<Known Data Types>>=
uni_str_arr_t,uni_str_ptr_t,%
@

<<Dump character information as C code>>=
qsort(dmf_prop->str_arr, dmf_prop->len, sizeof(*dmf_prop->str_arr), cmp_cp_flg);
uni_str_arr_t *dec;
inisize(dec, dmf_prop->len);
/* the easy table: canon full mapping is all w/ flags == 0 */
open_wf(cdf, "uni_canon_decomp_arr.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_arr_t uni_canon_decomp_arr[] = {\n\t", cdf);
uint32_t curent;
for(i = 0; dmf_prop->str_arr[i].flags; i++);
for(curent = 0; i < dmf_prop->len; i++, curent++) {
  dec[curent].cp = dmf_prop->str_arr[i].cp;
  dec[curent].off = dmf_prop->str_arr[i].off;
  dec[curent].len = dmf_prop->str_arr[i].len;
  while(i < dmf_prop->len - 1 && dmf_prop->str_arr[i + 1].flags)
    ++i;
  fprintf(cdf, "{ 0x%04X, %d, %d }%s", (int)dec[curent].cp, (int)dec[curent].off,
                                      (int)dec[curent].len,
				      i < dmf_prop->len - 1 ? ",\n\t" : "\n};\n");
}
fclose(cdf);
fprintf(gen_h, "/** Sorted string pointer array for full canonical decomposition */\n"
	       "extern const uni_str_arr_t uni_canon_decomp_arr[];\n"
	       "/** Length of \\ref uni_canon_decomp_arr */\n"
	       "#define uni_canon_decomp_arr_len %d /* %d lookups max */\n",
	       curent, lg2(curent + 1));
@

<<Clean up after parsing UCD files>>=
free(dec);
@

<<[[uni_cp_val_t]]>>=
/** A 64-bit structure for associating a single code point with a 32-bit value */
typedef struct {
  uint32_t cp; /**< The code point */
  uint32_t val; /**< The associated value */
} uni_cp_val_t;
@

<<Unicode property exports for generator>>=
<<[[uni_cp_val_t]]>>
/** Convert array of code points with 32-bit values to a multi-level table
  * The table \p tab/\p tab_len must sorted as per \ref uni_cmp_cp.  The
  * table is the return value; its length is returned in \p ml_len if
  * non-NULL */
uint32_t *uni_cp_val_to_multi(const uni_cp_val_t *tab, uint32_t tab_len,
                              uint32_t *ml_len);
@

<<Known Data Types>>=
uni_cp_val_t,%
@

<<Unicode property functions for generator>>=
uint32_t *uni_cp_val_to_multi(const uni_cp_val_t *tab, uint32_t tab_len,
                              uint32_t *ml_len)
{
  uint32_t low, high, len, i;
  uint32_t *ml;
  uint32_t *bits;

  /* degenerate case:  always out-of-range */
  if(!tab_len) {
    uni_bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  low = tab[0].cp;
  high = tab[tab_len - 1].cp;
  len = high - low + 1;
  inisize(bits, len);
  /* Optimize(maybe): only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  for(i = 0; i < tab_len; i++)
    bits[tab[i].cp - low] = tab[i].val;
  uni_bits_to_multi((uint8_t *)bits, len * 4, low * 4, high * 4, 0, 4, &ml, ml_len);
  free(bits);
  return ml;
}
@

<<Dump character information as C code>>=
uint32_t *tmt = uni_cp_val_to_multi((uni_cp_val_t *)dec, curent, &ml_len);
print_mtab("canon_decomp", "full canonical decomposition", tmt, gen_h);
free(tmt);
@

<<Dump character information as C code>>=
/* the next harder table: flags == 1 -> full compat decomp */
/* needs to duplicate full canon decomp when empty */
open_wf(kf, "uni_compat_decomp_arr.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_arr_t uni_compat_decomp_arr[] = {\n\t", kf);
for(i = curent = 0; i < dmf_prop->len; i++, curent++) {
  raw_cp_str_t *cur = &dmf_prop->str_arr[i];
  /* could be canon followed by compat, or compat followed by dm */
  if(i < dmf_prop->len - 1 && cur[1].cp == cur->cp &&
     !(cur->flags & 1) && (cur[1].flags & 1)) {
    ++i;
    ++cur;
  }
  dec[curent].cp = cur->cp;
  dec[curent].off = cur->off;
  dec[curent].len = cur->len;
  if(cur->flags & 1)
    dec[curent].off |= 0x80000;
  /* could be canon/compat followed by dm */
  if(i < dmf_prop->len - 1 && cur->cp == cur[1].cp)
    ++i;
  fprintf(kf, "{ 0x%04X, %d%s, %d }%s",
              (int)cur->cp, (int)cur->off,
	      (cur->flags & 1) ? " | 0x80000" : "",  (int)cur->len,
	      i < dmf_prop->len - 1 ? ",\n\t" : "\n};\n");
}
fclose(kf);
fprintf(gen_h, "/** Sorted string pointer array for full canonical decomposition */\n"
	       "extern const uni_str_arr_t uni_compat_decomp_arr[];\n"
	       "/** Length of \\ref uni_compat_decomp_arr */\n"
	       "#define uni_compat_decomp_arr_len %d /* %d lookups max */\n",
	       curent, lg2(curent + 1));
tmt = uni_cp_val_to_multi((uni_cp_val_t *)dec, curent, &ml_len);
print_mtab("compat_decomp", "full compatibility decomposition", tmt, gen_h);
free(tmt);
@

<<Dump character information as C code>>=
/* the hardest table: 2/3 -> dm */
/* needs to duplicate full canon decomp when empty */
/* needs to duplicate full compat decomp when empty & full canon empty*/
open_wf(dm, "uni_dm_arr.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_arr_t uni_dm_arr[] = {\n\t", dm);
for(i = curent = 0; i < dmf_prop->len; i++) {
  raw_cp_str_t *cur = &dmf_prop->str_arr[i];

  if(i < dmf_prop->len - 1 && cur[1].cp == cur->cp) {
    /* possibilities: canon,compat,dm canon,dm canon,compat compat,dm */
    if(cur[1].flags & 2) {
      ++i; /* compat,dm or canon,dm: skip full */
      cur++;
    } else if(i < dmf_prop->len - 2 && cur->cp == cur[2].cp) {
      i += 2; /* canon,compat,dm: skip both full */
      cur += 2;
    } else /* canon,compat; skip compat when done */
      i++;
  }
  dec[curent].cp = cur->cp;
  dec[curent].off = cur->off;
  dec[curent].len = cur->len;
  if(cur->flags & 1)
    dec[curent].off |= 0x80000;
  fprintf(dm, "{ 0x%04X, %d%s, %d }%s", (int)cur->cp, (int)cur->off,
				        (cur->flags & 1) ? " | 0x80000" : "",
                                        (int)cur->len,
				        i < dmf_prop->len - 1 ? ",\n\t" : "\n};\n");
  curent++;
}
fclose(dm);
fprintf(gen_h, "/** Sorted string pointer array for Decomposition_Mapping property.\n"
               "  * Note that the len field has its upper bit set if this\n"
	       "  * is a compatibility decomposition */\n"
	       "extern const uni_str_arr_t uni_dm_arr[];\n"
	       "/** Length of \\ref uni_dm_arr */\n"
	       "#define uni_dm_arr_len %d /* %d lookups max */\n",
	       curent, lg2(curent + 1));
tmt = uni_cp_val_to_multi((uni_cp_val_t *)dec, curent, &ml_len);
print_mtab("dm", "Decomposition_Mapping", tmt, gen_h);
free(tmt);
@

The generic all-property table needs to have a pointer to the string
table for each property.

<<Property parsed contents>>=
const char *strs_name;
@

<<Parse UCD files>>=
#define do_dm_prop(p) do { \
  prop_t *_p = &parsed_props[add_prop(#p)]; \
  _p->strs_name = "dm"; \
  enable_str_mt(_p); \
} while(0)
do_dm_prop(canon_decomp);
do_dm_prop(compat_decomp);
do_dm_prop(dm);
@

<<Additional property type names>>=
/** String: "range" table is actually \ref uni_str_arr_t, and multi-level
 ** table is 32 bits per code point.  32-bit value is actually
 ** \ref uni_str_ptr_t */
UNI_PROP_TYPE_STR,
@

<<Set prop type for export>>=
if(parsed_props[i].strs_name || parsed_props[i].strs)
  t = UNI_PROP_TYPE_STR;
@

<<Additional property structure members>>=
const uint8_t *strs8;  /**< The string table for 8-bit string properties */
const uint16_t *strs16;  /**< The string table for 16-bit string properties */
const uint32_t *strs32;  /**< The string table for 32-bit string properties */
uint32_t strs_len;  /**< The number of words in the string table */
@

<<Print additional property structure members>>=
const char *strs_name = parsed_props[i].strs_name;
if(parsed_props[i].strs || strs_name) {
  if(!strs_name)
    strs_name = name;
  int char_size = parsed_props[i].strs_char_size;
  if(!char_size)
    char_size = 16;
  fprintf(pnf, "%s%s, uni_%s_strs%s%s, uni_%s_strs_len",
               char_size > 8 ? ", NULL" : "", char_size > 16 ? ", NULL" : "",
               strs_name,
               char_size < 32 ? ", NULL" : "", char_size < 16 ? ", NULL" : "",
	       strs_name);
} else
  fputs(", NULL, NULL, NULL, 0", pnf);
@

<<Plain table name>>=
t == UNI_PROP_TYPE_STR ? "arr" :
@

The lookup functions return offset and length.  An additional lookup
function fills in a string given the offset and length.  This is so
that the exact name of the special value for Hangul Syllables is
encoded in these functions, rather than everywhere lookups are used.

<<Unicode property exports>>=
/** Look up a decomposition property value for \p cp in \p tab.
  * The result is a string descriptor in \p off and \p len; both are zero
  * if lookup failed.  If \p compat is not NULL, it is set to a flag
  * which is non-zero if the lookup returns a compatibility decomposition.
  * If and only if the \p h flag is true, synthetic (Hangul syllables)
  * are considered valid as, well, and are indicated by a negative \p off */
void uni_x_dec(uint32_t cp, const uint32_t *tab, int32_t *off, uint16_t *len,
               uint8_t *compat, int h);
@

<<Unicode property exports>>=
/** Look up full canonical decomposition for \p cp.
  * Returns string descriptor in \p off and \p len.  Finish the lookup
  * using \ref uni_get_decomp.  This is a preprocessor macro */
void uni_find_canon_decomp(uint32_t cp, int32_t *off, uint16_t *len);
#define uni_find_canon_decomp(cp, off, len) \
  uni_x_dec(cp, uni_canon_decomp_mtab, off, len, NULL, 1)
/** Look up full compatibility decomposition for \p cp.
  * Returns string descriptor in \p off and \p len.  If non-NULL,
  * \p compat is set to a flag which is false if this is a purely canonical
  * decomposition.  Finish the lookup with \ref uni_get_decomp.  This is a
  * preprocessor macro */
void uni_find_compat_decomp(uint32_t cp, int32_t *off, uint16_t *len,
                            uint8_t *compat);
#define uni_find_compat_decomp(cp, off, len, compat) \
  uni_x_dec(cp, uni_compat_decomp_mtab, off, len, compat, 1)
/** Look up raw decomposition property for \p cp.
  * Returns string descriptor in \p off and \p len.  If non-NULL,
  * \p compat is set to a flag which is false if this is a canonical
  * decomposition.  Finish the lookup with \ref uni_get_decomp.  This is a
  * preprocessor macro */
void uni_find_dm(uint32_t cp, int32_t *off, uint16_t *len, uint8_t *compat);
#define uni_find_dm(cp, off, len, compat) \
  uni_x_dec(cp, uni_dm_mtab, off, len, compat, 0)
/** Finish decomposition property lookup
  * Write the results of decomposition from \ref uni_find_canon_decomp,
  * \ref uni_find_compat_decomp or \ref uni_find_dm into \p buf.  The
  * \p cp, \p off and \p len parameters are the same as for the first
  * part of the lookup (although \p off and \p len are not changed).  You
  * must ensure enough space in \p buf (currently at least 18 words) */
void uni_get_decomp(uint32_t cp, uint32_t *buf, int32_t *off, uint16_t *len);
#define uni_get_decomp(cp, buf, off, len) do { \
  if((off) == -1) \
    uni_hangul_syllable_decomp(cp, buf, (len) >= 3); \
  else \
    cpybuf(buf, uni_dm_strs + (off), len); \
} while(0)
@

<<Unicode property functions>>=
void uni_x_dec(uint32_t cp, const uint32_t *tab, int32_t *off, uint16_t *len,
               uint8_t *compat, int h)
{
  const uint8_t *mr;
  uni_multi_tab_lookup(tab, cp * 4, &mr, 0);
  if(!mr) {
    *off = 0;
    *len = 0;
    if(compat)
      *compat = 0;
    return;
  } else {
    uni_str_ptr_t *v = (void *)mr;
    if(h >= 0 && !v->len && v->off) {
      *off = -1;
      *len = h && (cp - 0xAC00) % 28 ? 3 : 2;
      if(compat)
        *compat = 0;
    } else {
      *off = v->off & 0x7ffff;
      *len = v->len;
      if(compat)
        *compat = v->off >> 19;
    }
  }
}
@

\subsection{Testing}

To test the table implementations, a different, but nearly identical,
set of routines is used compared to the ones one for booleans.

<<Functions to help test generated tables>>=
#define str(x) doit_str(#x, uni_##x##_arr, uni_##x##_arr_len, uni_##x##_mtab)

static void doit_str(const char *name, const uni_str_arr_t *rng, uint32_t nent,
                     const uint32_t *mtab)
{
    uint32_t i;

    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    const uni_str_ptr_t *ms, *rs;
    static const uni_str_ptr_t z = {0};
    uni_str_arr_t *r;
    for(i = 0; i < 0x110000; i++) {
      r = bsearch(&i, rng, nent, sizeof(*rng), uni_cmp_cp);
      // bit field offsets unavailable, so make this unportable:
      // rs = r ? (uni_str_ptr_t *)&r->off : &z;
      rs = r ? (uni_str_ptr_t *)((uint32_t *)r + 1) : &z;
      ms = uni_x_str_of(i, mtab);
      if(cmpbuf(ms, rs, 1)) {
        fprintf(stderr, "mismatch %s@%d %d/%d/%d %d/%d/%d\n", name, i,
	                (int)rs->off & 0x7ffff, (int)rs->len, (int)rs->off >> 19,
			(int)ms->off & 0x7ffff, (int)ms->len, (int)ms->off >> 19);
	exit(1);
      }
    }
    /* check performance */
    int j;
    double tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        r = bsearch(&i, rng, nent, sizeof(*rng), uni_cmp_cp);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        uni_x_str_of(i, mtab);
    tt = tend();
    printf("  r%.0f t%.0f %.2fx\n", tr, tt, tr / tt);
}
@

<<Unicode property exports>>=
/** Look up string pointer associated with \p cp in multi-level table \p tab.
  * The default is zeroes, so an offset and length of zero generaly
  * indicate lookup failure */
const uni_str_ptr_t *uni_x_str_of(uint32_t cp, const uint32_t *tab);
@

<<Unicode property functions>>=
const uni_str_ptr_t *uni_x_str_of(uint32_t cp, const uint32_t *tab)
{
  const uni_str_ptr_t *ret;
  static const uni_str_ptr_t z = {0};
  uni_multi_tab_lookup(tab, cp * 4, (const uint8_t **)&ret, 0);
  return ret ? ret : &z;
}
@

<<Dump character information as C code>>=
fputs("str(canon_decomp);\n"
      "str(compat_decomp);\n"
      "str(dm);\n", tstf);
@

\subsection{Parsing the UCD -- DUCET Decompositions}

An alternate decomposition table is that provided by
\texttt{decomps.txt}.  This file is informative: no official algorithm
uses this, but it can be handy for examining the DUCET.  Some DUCET
mappings automatically decompose the character they are mapping.
These include canonical compositions, compatibility compositions, and
``other'' compositions defined by this text file.  The format of the
file is similar to \texttt{UnicodeData.txt}: semicolon-separated
fileds, the first of which is a single code point.  The second field
is similar to the dt portion of the decomposition mapping field, but
is separate from the dm-equivalent field.  The third field is the
dm-equivalent field, but without dt information.  A special
decomposition type, \texttt{sort}, is used to indicate mappings which
are only in this file.  Rather than create a new enumeration (or add
\texttt{sort} to the dt enumeration), this is encoded as compat.  The
way to distinguish real compat mappings from \texttt{sort} mappings is
to look it up in the regular dt table.

While the \texttt{sort} value is documented, decomps also adds
\texttt{circlekata} and \texttt{smallnarrow}, which are translated to
\texttt{circle} and \texttt{narrow}, repsectively, for now.  No great
effort will be put into solving this problem; I used this table when
analyzing the DUCET, and see little other value for it.

<<Initialize UCD files>>=
decl_str(DUCET_dm);
decl_enum(DUCET_dt, "None");
@

<<Parse UCD files>>=
prop_DUCET_dt = add_prop("DUCET_dt");
parsed_props[prop_DUCET_dt].def = parsed_props[prop_dt].def;
/* copy enum decls from dt for lookup */
enum_vals[prop_DUCET_dt] = enum_vals[prop_dt];
enum_vals_len[prop_DUCET_dt] = enum_vals_len[prop_dt];
open_f("decomps.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  char *dt_val;
  split_line(lbuf);
  if(num_fields != 3) /* e.g. comment lines */
   continue;
  low = high = strtol(fields[0], NULL, 16);
  dt_val = fields[1];
  if(*dt_val) {
    if(*dt_val++ != '<') {
      perror("decomps");
      exit(1);
    }
    for(s = dt_val; *s && *s != '>'; s++);
    if(*s != '>' || s[1]) {
      perror("decomps");
      exit(1);
    }
    *s = 0;
    if(!strcmp(dt_val, "sort"))
      dt_val = (char *)"compat";
    else if(!strcmp(dt_val, "circlekata"))
      dt_val[6] = 0;
    else if(!strcmp(dt_val, "smallnarrow"))
      dt_val += 5;
  }
  if(*dt_val)
    add_enum(DUCET_dt, dt_val);
  else
    add_enum(DUCET_dt, "can");
  add_str(DUCET_dm, fields[2]);
}
/* remove enum info so new enum not created */
enum_vals[prop_DUCET_dt] = NULL;
enum_vals_len[prop_DUCET_dt] = 0;
@

Unfortunately, some of the dm strings have multiple entries for one
index.  Luckily, the dt values do not have the same problem.  To fix
the dm strings, the result is sorted, and any duplicate indices are
removed by merging the two entries, separated by a zero.  Nothing ever
maps to zero, so there should be no problem distinguishing the
entries.  No attempt is made to sort the entries; there are at most
two anyway, so searching is no great effort.

<<Parse UCD files>>=
prop_t *prop_ddm = &parsed_props[prop_DUCET_dm];
qsort(prop_ddm->str_arr, prop_ddm->len, sizeof(raw_cp_str_t), uni_cmp_cp);
for(i = prop_ddm->len - 1; i > 0; i--) {
  for(j = i; j > 0; j--)
    if(prop_ddm->str_arr[j - 1].cp != prop_ddm->str_arr[i].cp)
      break;
  if(j == i)
    continue;
  low = j;
  int len = 0;
  for(j = low; j <= i; j++)
    len += prop_ddm->str_arr[j].len + 1;
  check_size(prop_ddm->strs, prop_ddm->max_strs, prop_ddm->strs_len + len);
  for(j = low, len = 0; j <= i; j++) {
    movebuf(prop_ddm->strs + prop_ddm->strs_len + len,
            prop_ddm->strs + prop_ddm->str_arr[j].off,
	    prop_ddm->str_arr[j].len);
    len += prop_ddm->str_arr[j].len + 1;
    prop_ddm->strs[prop_ddm->strs_len + len - 1] = 0;
  }
  len--;  /* strip final 0 */
  prop_ddm->str_arr[low].off = prop_ddm->strs_len;
  prop_ddm->str_arr[low].len = len;
  prop_ddm->strs_len += len;
  movebuf(prop_ddm->str_arr + low + 1, prop_ddm->str_arr + i + 1,
          prop_ddm->len - i - 1);
  prop_ddm->len -= i - low;
  if(!(i = low))
    break;
}
@

To dump the tables, the generic routines can be relied upon.  However,
there is not yet a generic routine to dump string tables.  In order to
avoid stomping on the work done for the decomposition tables, the
generic routine for strings needs to be called manually.

<<Dump character information as C code>>=
dump_str_tabs(prop_ddm, "DUCET Decomposition_Mapping", gen_h, tstf);
@

<<UCD parser local functions>>=
static void dump_str_arr(prop_t *prop, const char *lname, FILE *gen_h, FILE *tstf)
{
  uint32_t i;
  char nbuf[64];
  uni_str_arr_t *short_str = NULL;
  if(prop->mt)
    inisize(short_str, prop->len);
  sprintf(nbuf, "uni_%s_arr.gen.c", prop->name);
  open_wf(rt, nbuf);
  fprintf(rt, "#include \"uni_prop.h\"\n\n"
              "const uni_str_arr_t uni_%s_arr[] = {\n\t", prop->name);
  for(i = 0; i < prop->len; i++) {
    const raw_cp_str_t *ent = &prop->str_arr[i];
    if(prop->mt) {
      short_str[i].cp = ent->cp;
      short_str[i].off = ent->off;
      short_str[i].len = ent->len;
    }
    fprintf(rt, "{ 0x%04X, %d, %d}%s", (int)ent->cp, (int)ent->off, 
                                       (int)ent->len,
				       i < prop->len - 1 ? ",\n\t" : "\n};\n");
  }
  fclose(rt);
  fprintf(gen_h, "/** Sorted string pointer array for %s property */\n"
                 "extern const uni_str_arr_t uni_%s_arr[];\n"
		 "/** Length of \\ref uni_%s_arr */\n"
	         "#define uni_%s_arr_len %d /* %d lookups max */\n",
	         lname, prop->name, prop->name, prop->name, i, lg2(i + 1));
  if(prop->mt) {
    uint32_t ml_len, *tmt = uni_cp_val_to_multi((uni_cp_val_t *)short_str, i, &ml_len);
    free(short_str);
    print_mtab(prop->name, lname, tmt, gen_h);
    free(tmt);
    /* for direct lookup */
    fprintf(gen_h, "/** Returns string pointer associated with \\p cp for %s property */\n"
		   "#define uni_%s_of(cp) uni_x_str_of(cp, uni_%s_mtab)\n",
                   lname, prop->name, prop->name);
    /* test requires mtab as well */
    fprintf(tstf, "str(%s);\n", prop->name);
  }
}
@

<<UCD parser local functions>>=
static void dump_str_tabs(prop_t *prop, const char *lname, FILE *gen_h, FILE *tstf)
{
  dump_strs(prop, lname, gen_h);
  /* dump_strs "un-sorts" str_arr */
  qsort(prop->str_arr, prop->len, sizeof(*prop->str_arr), uni_cmp_cp);
  dump_str_arr(prop, lname, gen_h, tstf);
}
#define enable_str_mt(p) (p)->mt = (uint32_t *)~0
#define enable_str_mt_p(p) parsed_props[p].mt = (uint32_t *)~0
@

<<Parse UCD files>>=
enable_str_mt(prop_ddm);
@

\subsection{Parsing the UCD -- Composition}

As mentioned earlier, composition takes place on pairs.  The canonical
composition of A and B is C if the plain dm value for C is A B, and dt
for C is canonical, and C does not have the Comp\_Ex property.  Given
these conditions, B is removed and A is replaced by C.  After
replacement, C becomes A for a further pass at composition.  It is not
possible to make a ``fully composed'' table that, like the full
decomposition table, iterates thorugh all possible compositions. So,
the composition table is basically a table returning C (or nothing)
given A and B.  

There are a number of possible strategies for storing these values for
efficient retrieval. For sorted arrays, the pair of A and B could
simply be used as the key. When both A and B are known, this is the
most effective form of the sorted array.  However, real normalization
takes place by knowing A, and then scanning all potential candidates
for B.  For the sorted array, this is still not too terrible: the
first and last occurrence of A can be found relatively quickly, and
that limited subset can be searched for B.  The search for the last
and first occurence can be removed by instead having A return a table
(generally an address and a table length) indexed on B.  The B-indexed
table then returns C.  This arrangement can be used regardless of the
actual A-indexed table structure.

Rather than creating a new structure to store these B-indexed tables,
they are stored as if they were strings themselves: A is used to look
up a string, which is actually a code point/value table to look up C
given B.  As with the dm table, the Hangul Syllables are given special
treatment. The A-indexed entries for all LV and L characters point to
zero-length tables, indicating that the [[uni_hangul_syllable_comp]]
function should be used to compose instead.

After converting the string to UTF-16, the binary search no longer
works properly, because the size of each entry is not always exactly
two words.  In order to still support binary searching, all entries
are forced to become 32 bits by padding any non-32-bit entries with
trailing zeroes.  It is easy to detect this on lookup: if the second
word of the string is either a surrogate or a zero, 32-bit lookups are
required.

<<Initialize UCD files>>=
decl_str(cm);
@

<<Parse UCD files>>=
prop_cm = add_prop("canon_comp");
prop_t *fce_prop = &parsed_props[add_prop("Comp_Ex")];
prop_t *cm_prop = &parsed_props[prop_cm];
for(i = 0; i < dm_prop->len; i++) {
  uint32_t str[2];
  uint32_t cp = dm_prop->str_arr[i].cp; /* cp == C */
  if(dm_prop->str_arr[i].len != 2 ||
     uni_chrrng_dat8(cp, dt_prop->rng_dat8, dt_prop->len, none_val) !=
         can_val ||
     uni_is_cp_chrrng(cp, fce_prop->rng, fce_prop->len))
    continue; /* skip if not canonical composition */
  str[0] = dmf_prop->strs[dm_prop->str_arr[i].off + 1]; /* B */
  str[1] = cp;
  cp = dmf_prop->strs[dm_prop->str_arr[i].off]; /* A */
  /* just add; tables are merged later */
  add_str_rng(cm_prop, cp, cp, str, 2);
}
/* add entries for Hangul */
add_str_rng(cm_prop, 0x1100, 0x1112, NULL, 0); /* L */
for(i = 0xAC00; i <= 0xD7A3; i += 28) /* LV */
  add_str_rng(cm_prop, i, i, NULL, 0);
/* merge B->C tables, and add padding zeroes if needed */
qsort(cm_prop->str_arr, cm_prop->len, sizeof(*cm_prop->str_arr), uni_cmp_cp);
for(i = cm_prop->len - 1; i > 0; i--) {
  int off, k, is_32 = 0;;
  j = i;
  while(j > 0 && cm_prop->str_arr[i].cp == cm_prop->str_arr[j - 1].cp)
    j--;
  if(j == i && !cm_prop->str_arr[i].len)
    continue;
  uint32_t jb = cm_prop->strs[cm_prop->str_arr[j].off];
  uint32_t jc = cm_prop->strs[cm_prop->str_arr[j].off + 1];
  if(j == i && ((jb < 0x10000 && jc < 0x10000) ||
                (jb > 0xFFFF  && jc > 0xFFFF)))
    continue; /* note: in UCD 6.2, this path always taken if j == i */
  /* leave room for 2* (B+C) and possible padding zeroes (B+C again) */
  check_size(cm_prop->strs, cm_prop->max_strs,
             cm_prop->strs_len + 4 * (i - j + 1));
  uint32_t *dest = cm_prop->strs + cm_prop->strs_len;
  for(k = j, off = 0; k <= i; k++) {
    if((dest[off++] = cm_prop->strs[cm_prop->str_arr[k].off]) > 0xFFFF)
      is_32 = 1;
    if((dest[off++] = cm_prop->strs[cm_prop->str_arr[k].off + 1]) > 0xFFFF)
      is_32 = 1;
  }
  /* sort B->C table by B */
  qsort(dest, off / 2, 2 * sizeof(*cm_prop->strs), uni_cmp_cp);
  /* insert zeroes in string if 32-bit needed */
  /* NOTE: UCD 6.2 never touches this code (all are 1-entry, b & c 32-bit) */
  if(is_32) {
    for(k = off - 1; k >= 0; k--) {
      if(cm_prop->strs[cm_prop->strs_len + k] < 0x10000) {
        movebuf(dest + k + 2, dest + k + 1, off - k - 1);
	dest[k + 1] = 0;
	off++;
      }
    }
  }
  /* finish entry */
  cm_prop->str_arr[j].off = cm_prop->strs_len;
  cm_prop->str_arr[j].len = off;
  cm_prop->strs_len += off;
  /* remove excess entries */
  movebuf(cm_prop->str_arr + j + 1, cm_prop->str_arr + i + 1,
          cm_prop->len - i - 1);
  cm_prop->len -= i - j;
  if(!(i = j))
    break;
}
@

<<Unicode property exports>>=
/** Compose two characters into a Hangul syllable
  * Composes L and V into LV or LV and T into LVT.  Returns zero if
  * \p a and \p b are not one of those two combinations */
uint32_t uni_hangul_syllable_comp(uint32_t a, uint32_t b);
@

<<Unicode property functions>>=
uint32_t uni_hangul_syllable_comp(uint32_t a, uint32_t b)
{
  if(a >= 0x1100 && a <= 0x1112) /* L */
    if(b >= 0x1161 && b <= 0x1175) /* V */
      return 0xAC00 + 28 * ((a - 0x1100) * 21 + (b - 0x1161)); /* LV */
  if(b > 0x11A7 && b <= 0x11C2) /* T */
    if(a >= 0xAC00 && a <= 0xD7A3 && !((a - 0xAC00) % 28)) /* LV */
      return a + (b - 0x11A7); /* LVT */
  return 0;
}
@

There is probably little value in converting the B-indexed lookup
tables to multi-level tables, so the string table can be printed as is
now. The [[dump_strs]] function is the most convenient method to do
so. The fucntion will pointlessly try to compress the table, but
that's better than writing a dumper function just for this table.

<<Dump character information as C code>>=
int max_len = 0;
for(i = 0; i < cm_prop->len; i++)
  if(cm_prop->str_arr[i].len > max_len)
    max_len = cm_prop->str_arr[i].len;
max_len /= 2;
fprintf(gen_h, "/* %d max B->C len (%d lookups) */\n", max_len, lg2(max_len + 1));
dump_strs(cm_prop, "composition mapping", gen_h);
@

The A-indexed plain table can now be sorted, converted to a
multi-level table, and printed.

<<Dump character information as C code>>=
qsort(cm_prop->str_arr, cm_prop->len, sizeof(*cm_prop->str_arr), uni_cmp_cp);
dump_str_arr(cm_prop, "composition mapping", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt(cm_prop);
@

The same lookup function as was used for decomposition lookups mostly
works for the A-indexed level as well.  For the B-indexed level, a
binary search is performed.  This is hand-coded for speed and to
easily adjust for 32-bit entries.

<<Unicode property exports>>=
/** Find potential canonical compositions beginning with \p cp.
  * Returns string descriptor in \p off and \p len; finish lookup with
  * \ref uni_canon_comp.  This is a preprocessor macro */
void uni_find_canon_comp(uint32_t cp, int16_t *off, uint8_t *len);
#define uni_find_canon_comp(cp, off, len) \
  uni_x_dec(cp, uni_canon_comp_mtab, off, len, NULL, -1)
/** Find canonical composition of \p cpa with \p cpb.
  * The string descriptor \p off and \p len are from \ref uni_find_canon_comp.
  * The return value is zero if no composition is possible.  Otherwise, it
  * is the result of composition.  Note that multiple secondary lookups
  * can be performed after a single \ref uni_find_canon_comp lookup.
  * This is a preprocessor macro */
uint32_t uni_canon_comp(uint32_t cpa, uint32_t cpb, uint32_t off, uint8_t len);
#define uni_canon_comp(cpa, cpb, off, len) \
  ((len) > 0 ? uni_lookup_compent(cpb, uni_canon_comp_strs + (off), len) : \
               uni_hangul_syllable_comp(cpa, cpb))
@

<<Unicode property exports for generator>>=
/** Find composition with \p cp within potential compositions.
  * Given a string of canonical composition candidates (\p tab, \p len), find
  * one which matches \p cp and return it.  Zero is returned if there is no
  * such entry.  This does not take into account synthetic compositions;
  * use \ref uni_canon_comp instead */
uint32_t uni_lookup_compent(uint32_t cp, const uint16_t *tab, uint32_t len);
@

<<Unicode property functions for generator>>=
uint32_t uni_lookup_compent(uint32_t cp, const uint16_t *tab, uint32_t len)
{
  if(tab[1] && !uni_is_surrogate(tab[1])) {
    int32_t l = 0, h = len / 2 - 1;
    while(h >= l) {
      uint32_t m = (l + h) / 2;
      int32_t c = (int32_t)tab[m * 2] - (int32_t)cp;
      if(!c)
        return tab[m * 2 + 1];
      else if(c < 0)
        l = m + 1;
      else
        h = m - 1;
    }
  } else {
    int32_t l = 0, h = len / 4 - 1;
    while(h >= l) {
      uint32_t m = (l + h) / 2;
      uint32_t mc = uni_int_utf16_decode(tab + m * 4, NULL);
      int32_t c = (int32_t)mc - (int32_t)cp;
      if(!c) {
        mc = uni_int_utf16_decode(tab + m * 4 + 2, NULL);
	return mc;
      } else if(c < 0)
        l = m + 1;
      else
        h = m - 1;
    }
  }
  return 0;
}
@

\subsection{Parsing the UCD -- Case Conversion}

The next strings are the various case conversion tables.
[[UnicodeData.txt]] contains three of them: the simple case mappings
(lower, upper, and title case).  These are always just one character
long, so they are added as a simple value, instead.

It might make sense to make a sort of combined case conversion table,
but it's easier to just dump the three as separate tables.  The only
savings is to suppress redundant entries:  any which match the
character (which should never occur, but it doesn't hurt to check just
in case) or any title case entries which match the upper case entries
(which does occur).

<<Initialize UCD files>>=
decl_num(slc);
decl_num(suc);
decl_num(stc);
@

<<Process a line of [[UnicodeData.txt]]>>=
{
  const char *slcs = fields[13], *sucs = fields[12], *stcs = fields[14];
  uint32_t slc = strtol(slcs, NULL, 16);
  uint32_t suc = strtol(sucs, NULL, 16);
  uint32_t stc = strtol(stcs, NULL, 16);

  if(*slcs && slc != low)
    add_num(slc, slc, 1);
  if(*sucs && suc != low)
    add_num(suc, suc, 1);
  if(*stcs && stc != low && (!*sucs || stc != suc))
    add_num(stc, stc, 1);
}
@

[[SpecialCasing.txt]] lists the exceptions to the above case mappings.
The above values are always either zero or one character (technically
always one character, since the default value is the unmodified code
point); the ones from this file may be more than one character, or may
require more than one character as input, or may only apply under
certain conditions.  The mappings with conditions override the simple
mappings above when the condition is in effect.  It would be nice to
only require one lookup to do case conversion, but combining the above
tables with this one is not that simple.  Instead, this table needs to
always be consulted first, followed by the above tables.

The file is short enough that it can be encoded in a relatively
inefficient format, which simply directly encodes the contents of the
file.  Three tables are generated, corresponding to the desired case
conversion.  If the conversion is already in one of the above tables,
it is simply skipped.  Otherwise, it is encoded as a condition word,
followed by the result string.  Since the condition flags can be
encoded in fewer than 16 bits, the lower 5 bits also encode the length of
the conversion.  That way, multiple conversions with different
conditions can simply be appended to the string.  Unfortunately,
multiple entries with the same key are not always consecutive, so this
merging is done after they have all been gathered.

The string table generator converts 32-bit values to UTF-16, but
assumes that they are valid (i.e., not surrogates themselves).  This
assumption works to our advantage here, in that the flag/length field
is just a plain 16-bit value, and will not be mangled by the
conversion.  However, the length field needs to be adjusted for any
values that encode into two 16-bit values.

<<Special casing conditions>>=
/* all known conditions from SpecialCasing.txt, version 6.2 */
/* future revisions may need more flags/parsing */
/** \addtogroup uni_prop_sc Special Casing Conditions
  * Flags used by case-related functions and special casing properties
  * for casing context and locale.  In the descriptions, ccs stands
  * for combining character sequence (i.e., grapheme cluster).
  * @{ */
#define UNI_SC_FL_LT                (1<<5)  /**< lt locale */
#define UNI_SC_FL_AZ                (1<<6)  /**< az locale */
#define UNI_SC_FL_TR                (1<<7)  /**< tr locale */
#define UNI_SC_FL_LOCALE            (0x07 << 5)   /**< Mask: any locale flags */

#define UNI_SC_FL_NOT               (1<<8)  /**< Negates non-locale conditions */
#define UNI_SC_FL_AFTER_I           (1<<9)  /**< Capital I in same ccs */
#define UNI_SC_FL_AFTER_SOFT_DOTTED (1<<10) /**< Lower-case i in same ccs */
#define UNI_SC_FL_BEFORE_DOT        (1<<11) /**< Combining dot above in same ccs */
#define UNI_SC_FL_MORE_ABOVE        (1<<12) /**< Accent above glyph in same ccs */
#define UNI_SC_FL_FINAL_SIGMA       (1<<13) /**< No following letters in same word */
#define UNI_SC_FL_CONTEXT           (0x1f << 9) /**< Mask: any context flags */

#define UNI_SC_FL_LEN_MASK          (0x1f) /**< Mask: length in raw storage */
/** @} */
@

<<Unicode property exports for generator>>=
<<Special casing conditions>>
@

<<Initialize UCD files>>=
decl_str(lc);
decl_str(uc);
decl_str(tc);
@

<<Parse UCD files>>=
open_f("SpecialCasing.txt");
#define add_sc(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  uint32_t str[64]; /* max known value == 30, so 64 should be enough */ \
  uint32_t len, len16; \
  for(s = v, len = len16 = 0; *s; len++, len16++) { \
    str[len + 1] = strtol(s, &s, 16); \
    if(str[len + 1] > 0xFFFF) \
      len16++; \
  } \
  str[0] = flg | len16; \
  if(len == 1 && low == high) { \
    int32_t num; \
    uint8_t den; \
    prop_t *sc = &parsed_props[prop_s##n]; \
    uni_chrrng_val(low, sc->rng_dat32, sc->len, &num, &den); \
    if(den && num == str[1]) \
      break; \
  } \
  add_str_rng(&parsed_props[prop_##n], low, high, str, len + 1); \
} while(0)
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  uint16_t flg = 0;
  if(num_fields > 4) {
    s = fields[4];
    /* slow parse, but at least accurate */
    if(!strncasecmp(s, "lt", 2)) {
      flg |= UNI_SC_FL_LT;
      s += 2;
      if(*s)
        ++s;
    } else if(!strncasecmp(s, "az", 2)) {
      flg |= UNI_SC_FL_AZ;
      s += 2;
      if(*s)
        ++s;
    } else if(!strncasecmp(s, "tr", 2)) {
      flg |= UNI_SC_FL_TR;
      s += 2;
      if(*s)
        ++s;
    }
    if(!strncasecmp(s, "Not_", 4)) {
      flg |= UNI_SC_FL_NOT;
      s += 4;
    }
    if(!strcasecmp(s, "After_I"))
      flg |= UNI_SC_FL_AFTER_I;
    else if(!strcasecmp(s, "After_Soft_Dotted"))
      flg |= UNI_SC_FL_AFTER_SOFT_DOTTED;
    else if(!strcasecmp(s, "Before_Dot"))
      flg |= UNI_SC_FL_BEFORE_DOT;
    else if(!strcasecmp(s, "More_Above"))
      flg |= UNI_SC_FL_MORE_ABOVE;
    else if(!strcasecmp(s, "Final_Sigma"))
      flg |= UNI_SC_FL_FINAL_SIGMA;
    else if(*s) {
      fprintf(stderr, "Unknown case condition: %s\n", s);
      exit(1);
    }
  }
  add_sc(lc, fields[1]);
  add_sc(uc, fields[3]);
  /* note that this condition may be too simple */
  /* it does not account for excess ignorable whitespace */
  /* it does not check if it ends up changing stc */
  /* if(strcmp(fields[3], fields[2]) */
    add_sc(tc, fields[2]);
}
fclose(f);
@

<<Parse UCD files>>=
<<Postprocess simple casing for [[lc]]>>
<<Postprocess simple casing for [[uc]]>>
<<Postprocess simple casing for [[tc]]>>
@

<<Postprocess simple casing for (@c)>>=
{
  prop_t *prop = &parsed_props[prop_<<@c>>];
  qsort(prop->str_arr, prop->len, sizeof(*prop->str_arr), uni_cmp_cp);
  /* pass 1: merge */
  for(i = 1; i < prop->len; i++) {
    raw_cp_str_t *prev = &prop->str_arr[i - 1], *cur = &prop->str_arr[i];
    if(cur->cp == prev->cp) {
      uint32_t newstr[cur->len + prev->len];
      cpybuf(newstr, prop->strs + prev->off, prev->len);
      cpybuf(newstr + prev->len, prop->strs + cur->off, cur->len);
      prev->cp = 0;
      add_str_rng(prop, cur->cp, cur->cp, newstr, cur->len + prev->len);
      cur->off = prop->str_arr[prop->len - 1].off;
      cur->len += prev->len;
      prop->len--;
    }
  }
  /* pass 2: remove merged */
  /* doing it one at a time is inefficient, but simple and good enough */
  for(i = 0; i < prop->len; i++)
    if(!prop->str_arr[i].cp) {
      movebuf(&prop->str_arr[i], &prop->str_arr[i + 1], prop->len - i);
      prop->len--;
      i--;
    }
}
@

<<Additional parse-ucd C files>>=
btricks.h \
@

<<Additional parse-ucd includes>>=
#include "btricks.h"
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_lc], "Lower_Case", gen_h, tstf);
dump_str_tabs(&parsed_props[prop_uc], "Upper_Case", gen_h, tstf);
dump_str_tabs(&parsed_props[prop_tc], "Title_Case", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_lc);
enable_str_mt_p(prop_uc);
enable_str_mt_p(prop_tc);
@

While the properties were named after the final result, these tables
only provide a small part of it.  To get the final result, helper
functions are provided, along with macros to use those functions.  A
generic overall helper function returns a pointer to the raw results,
while the more specific functions convert the results into one of the
three supported output formats.

These helper functions are stored in their own file, as they are not
of general utility, and are used by the data-expensive string
converters below.

\lstset{language=txt}
<<Library [[uni]] Members>>=
case.o
@

\lstset{language=C}
<<case.c>>=
<<Common C Header>>
#include "uni_prop.h"
@

<<Unicode property exports>>=
<<[[uni_case_convert]][[32]] Prototype>>;
<<[[uni_case_convert]][[16]] Prototype>>;
<<[[uni_case_convert]][[8]] Prototype>>;
@

<<[[uni_case_convert]](@sz) Prototype>>=
/** Generic case conversion with context.
  * Convert \p cp to another case given context and locale \p cond and
  * case conversion tables \p simple and \p special/\p strs.  The result
  * is returned in \p buf, \p off and \p buf_len; see e.g.
  * \ref uni_return32_buf<<@sz>> for details on how these three parameters work.
  * This function is not meant to be called directly; see e.g. \ref uni_lc<<@sz>>. */
int uni_case_convert<<@sz>>(uint32_t cp, uint32_t cond,
                       <<Buffer return parameters for UTF-[[<<@sz>>]]>>,
                       const uint32_t *simple, const uint32_t *special,
		       const uint16_t *strs)
@

<<case.c>>=
/* return code is similar to above, except when ret == NULL, positive */
/*  return is return code point */
/* rather than filling a buffer, ret is the return string */
static int32_t uni_case_convert(uint32_t cp, uint32_t cond,
                                const uint16_t **ret, const uint32_t *simple,
				const uint32_t *special, const uint16_t *strs)
{
  const uint8_t *mr;
  uni_multi_tab_lookup(special, cp * 4, &mr, 0);
  if(!mr) {
    uni_multi_tab_lookup(simple, cp * 4, &mr, 0);
    *ret = NULL;
    return mr && ((uni_frac_t *)mr)->num ? ((uni_frac_t *)mr)->num : cp;
  }
  const uni_str_ptr_t *v = (const void *)mr;
  const uint16_t *str = strs + v->off;
  if(!(*str & ~UNI_SC_FL_LEN_MASK) && v->len == (*str & UNI_SC_FL_LEN_MASK) + 1) {
    /* no conditions */
    *ret = str + 1;
    return v->len - 1;
  }
  if(cond == (uint32_t)~0)
    return -1;
  uint32_t rem = v->len;
  const uint16_t *ncond = NULL;
  while(rem > 0) {
    if(!(*str & UNI_SC_FL_LOCALE) || (*str & UNI_SC_FL_LOCALE & cond)) {
      if(!(*str & UNI_SC_FL_CONTEXT)) {
        /* locale condition is stronger than no condition at all */
        if(!ncond || (*str & UNI_SC_FL_LOCALE))
	  ncond = str;
      } else if(!(*str & cond & UNI_SC_FL_CONTEXT) == !!(*str & UNI_SC_FL_NOT)) {
        /* condition true: return result (assumes no other cond could be true) */
	ncond = str;
	break;
      }
    }
    rem -= (*str & UNI_SC_FL_LEN_MASK) + 1;
    str += (*str & UNI_SC_FL_LEN_MASK) + 1;
  }
  if(!ncond) {
    uni_multi_tab_lookup(simple, cp * 4, &mr, 0);
    *ret = NULL;
    return mr && ((uni_frac_t *)mr)->num ? ((uni_frac_t *)mr)->num : cp;
  }
  *ret = ncond + 1;
  return *ncond & UNI_SC_FL_LEN_MASK;
}
@

<<case.c>>=
<<[[uni_case_convert]][[32]]>>
<<[[uni_case_convert]][[16]]>>
<<[[uni_case_convert]][[8]]>>
@

<<[[uni_case_convert]](@sz)>>=
<<[[uni_case_convert]][[<<@sz>>]] Prototype>>
{
  const uint16_t *rbuf;
  int32_t ret = uni_case_convert(cp, cond, &rbuf, simple, special, strs);
  if(ret < 0)
    return ret;
  if(!rbuf)
    return uni_return32_buf<<@sz>>((uint32_t *)&ret, 1, buf, off, buf_len);
  return uni_return16_buf<<@sz>>(rbuf, ret, buf, off, buf_len);
}
@

<<[[uni_case_convert]] support for (@type)>>=
<<[[uni_case_convert]][[32]] support>>
<<[[uni_case_convert]][[16]] support>>
<<[[uni_case_convert]][[8]] support>>
@

<<Doxydoc for [[uni_]](@type)>>=
/** Unicode conversion to [<<@type>>].
  * Convert \p cp to [<<@type>>] given context and locale flags \p cond.
  * If \p cond is ~0, return -1 if a condition may affect the results.
  * Otherwise, return the length and use the \p buf, \p off and \p buf_len
  * parameters as described for \ref uni_return32_buf<<@sz>>.  This is a
  * preprocessor macro */
@

\lstset{language=make}
<<makefile.vars>>=
uni_prop.h: NOTANGLE_POSTPROC+=|sed -e \
   's/\[lc\]/lower-case/;s/\[uc\]/upper-case/;s/\[tc\]/title-case/'
@

\lstset{language=C}
<<[[uni_case_convert]](@sz) support>>=
<<Doxydoc for [[uni_]][[<<@type>>]]>>
int uni_<<@type>><<@sz>>(uint32_t cp, uint32_t cond, <<Buffer return parameters for UTF-[[<<@sz>>]]>>);
#define uni_<<@type>><<@sz>>(cp, cond, buf, off, buf_len) \
  uni_case_convert<<@sz>>(cp, cond, buf, off, buf_len, \
                     uni_s<<@type>>_mtab, uni_<<@type>>_mtab, uni_<<@type>>_strs)
@

<<Doxydoc for [[uni_]][[tc]]>>=
/** Unicode conversion to [<<@type>>].
  * Convert \p cp to [<<@type>>] given context and locale flags \p cond.
  * If \p cond is ~0, return -1 if a condition may affect the results.
  * Otherwise, return the length and use the \p buf, \p off and \p buf_len
  * parameters as described for \ref uni_return32_buf<<@sz>>.  If the result
  * of this function call is just \p cp, convert using \ref uni_uc<<@sz>>
  * instead.  This is a preprocessor macro */
@

<<Unicode property exports>>=
<<[[uni_case_convert]] support for [[lc]]>>
<<[[uni_case_convert]] support for [[uc]]>>
<<[[uni_case_convert]] support for [[tc]]>>
@

These only convert; they don't check to see if a character \emph{is} a
certain case.  One way to do this is to check the gc property; in
fact, this I recommend this, since that returns \emph{which} case the
character is, rather than requiring three tests.  However, Unicode has
a different definition in mind: checking if the result of conversion
matches the input.

<<Unicode property exports>>=
/** Check if \p cp is cased
  * Checks if \p cp is cased given locale and context \p cond, and
  * case lookup tables \p simple and \p special/\p strs.  This
  * function is not meant to be called directly; see e.g. \ref uni_is_lc
  * instead */
int uni_case_check(uint32_t cp, uint32_t cond, const uint32_t *simple,
                   const uint32_t *special, const uint16_t *strs);
@

<<[[uni_case_convert]] support for (@type)>>=
/** Check if \p cp is [<<@type>>].
  * Convert \p cp to [<<@type>>] given context and locale flags \p cond.
  * If \p cond is ~0, return -1 if a condition may affect the results.
  * Otherwise, return 0 if the result does not match \p cp, and 1 if it
  * does */
int uni_is_<<@type>>(uint32_t cp, uint32_t cond);
<<[[uni_is_]][[<<@type>>]] macro>>
@

<<[[uni_is_]](@type) macro>>=
#define uni_is_<<@type>>(cp, cond) \
  uni_case_check(cp, cond, uni_s<<@type>>_mtab, uni_<<@type>>_mtab, \
                  uni_<<@type>>_strs)
@

<<[[uni_is_]][[tc]] macro>>=
#define uni_is_tc(cp, cond) \
  (uni_case_check(cp, cond, uni_stc_mtab, uni_tc_mtab, uni_tc_strs) || \
   uni_case_check(cp, cond, uni_suc_mtab, uni_uc_mtab, uni_uc_strs))
@

<<case.c>>=
int uni_case_check(uint32_t cp, uint32_t cond, const uint32_t *simple,
                   const uint32_t *special, const uint16_t *strs)
{
  const uint16_t *rbuf;
  int32_t ret = uni_case_convert(cp, cond, &rbuf, simple, special, strs);
  if(ret < 0)
    return ret;
  if(!rbuf)
    return ret == cp;
  if(ret == 1 || (ret == 2 && uni_is_surrogate(*rbuf)))
    return cp == uni_int_utf16_decode(rbuf, NULL);
  return 1;
}
@

Unicode also provides a generic ``cased'' condition; this means that
some case conversion may succeed.  Naturally, this pulls in all three
conversion tables.

<<Unicode property exports>>=
/** Check if \p cp is cased.
  * Convert \p cp to lc, uc, and tc given context and locale flags \p
  * cond.  If \p cond is ~0, return -1 if a condition may affect the
  * results.  Otherwise, return 1 if any result does not match \p cp,
  * and 0 if all results match \p cp.  This is a preprocessor macro */
int uni_is_cased(uint32_t cp, uint32_t cond);
#define uni_is_cased(cp, cond) \
  (!uni_case_check(cp, cond, uni_stc_mtab, uni_tc_mtab, uni_tc_strs) && \
   !uni_case_check(cp, cond, uni_suc_mtab, uni_uc_mtab, uni_uc_strs) && \
   !uni_case_check(cp, cond, uni_suc_mtab, uni_uc_mtab, uni_lc_strs))
@

These provide support for single-character conversion and detection. 
Normally, this is sufficient.  For users too lazy to loop over a
string themselves, here are some string case conversion and detection
functions. There are actually some additional functions which can be
performed when doing a full string conversion.  All of the non-locale
context flags can be resolved, for example.  Title case always
requires word boundary detection, as well.  In fact, regular context
requries word boundary detection as well.  Boundary detection pulls in
a table.  Title case and full case checking also pull in all tables.

While the lower-case and upper-case conversion functions are identical
except for the tables, the title-case conversion function is only
mostly identical.  Rather than duplicate the code, a single function
is used for both upper and lower case, whereas macro tricks are used
to get the title case converter.

Also, most of the logic of checking case is identical to the logic of
converting case, so the code is shared for that, as well.

<<Unicode property exports>>=
<<[[uni_str_case_convert]][[32]] Prototype>>;
<<[[uni_str_case_convert]][[16]] Prototype>>;
<<[[uni_str_case_convert]][[8]] Prototype>>;
<<[[uni_str_case_check]][[32]] Prototype>>;
<<[[uni_str_case_check]][[16]] Prototype>>;
<<[[uni_str_case_check]][[8]] Prototype>>;
@

<<[[uni_str_case_convert]](@sz) Prototype>>=
/** Convert case of entire UTF-<<@sz>> string \p s/\p slen.
  * If \p slen is less than zero, \p s is zero-terminated.
  * The return buffer parameters \p buf, \p off and \p buf_len are
  * described in \ref uni_return<<@sz>>_buf<<@sz>>.  The case conversion
  * is determined by which tables are used.  Set \p simple to the simple
  * case conversion lookup table and \p special to the special case lookup
  * table whose strings are in \p strs.  Note that word boundaries are not
  * taken into consideration, so this cannot be used for title casing.
  * Only the locale-related flags in \p cond are recognized */
int uni_str_case_convert<<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                            uint32_t cond, <<Buffer return parameters for UTF-[[<<@sz>>]]>>,
                            const uint32_t *simple, const uint32_t *special,
		            const uint16_t *strs)
@

<<[[uni_str_case_check]](@sz) Prototype>>=
/** Check case of entire UTF-<<@sz>> string \p s/\p slen.
  * If \p slen is less than zero, \p s is zero-terminated.
  * The routine converts the entire string's case and compares the results
  * with the original, returning true if they match.  The case conversion
  * is determined by which tables are used.  Set \p simple to the simple
  * case conversion lookup table and \p special to the special case lookup
  * table whose strings are in \p strs.  Note that word boundaries are not
  * taken into consideration, so this cannot be used for title casing.
  * Only the locale-related flags in \p cond are recognized */
int uni_str_case_check<<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                          uint32_t cond,
                          const uint32_t *simple, const uint32_t *special,
		          const uint16_t *strs)
@

<<[[uni_str_]](@type)(@sz) Prototype>>=
/** Convert entire UTF-<<@sz>> string \p s/\p slen to [<<@type>>].
  * If \p slen is less than zero, \p s is zero-terminated.
  * The return buffer parameters \p buf, \p off and \p buf_len are
  * described in \ref uni_return<<@sz>>_buf<<@sz>>.
  * Only the locale-related flags in \p cond are recognized */
int uni_str_<<@type>><<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                         uint32_t cond, <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<[[uni_str_is_]](@type)(@sz) Prototype>>=
/** Test if entire UTF-<<@sz>> string \p s/\p slen is [<<@type>>].
  * If \p slen is less than zero, \p s is zero-terminated.
  * Only the locale-related flags in \p cond are recognized.  */
int uni_str_is_<<@type>><<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                            uint32_t cond)
@

<<[[uni_case_convert]](@sz) support>>=
<<[[uni_str_case_convert_check]] support for [[<<@type>>]]>>
@

<<[[uni_str_case_convert_check]] support for (@type)>>=
<<[[uni_str_]][[<<@type>>]][[<<@sz>>]] Prototype>>;
<<[[uni_str_is_]][[<<@type>>]][[<<@sz>>]] Prototype>>;
<<Generic [[str_case_convert_check]] for [[<<@type>>]]>>
@

<<Unicode property exports>>=
<<[[uni_str_case_convert]][[32]] Prototype>>;
<<[[uni_str_case_convert]][[16]] Prototype>>;
<<[[uni_str_case_convert]][[8]] Prototype>>;
<<[[uni_str_is_]][[cased]][[32]] Prototype>>;
<<[[uni_str_is_]][[cased]][[16]] Prototype>>;
<<[[uni_str_is_]][[cased]][[8]] Prototype>>;
@

<<Generic [[str_case_convert_check]] for (@type)>>=
#define uni_str_<<@type>><<@sz>>(s, slen, cond, buf, off, buf_len) \
  uni_str_case_convert<<@sz>>(s, slen, cond, buf, off, buf_len, \
                     uni_s<<@type>>_mtab, uni_<<@type>>_mtab, uni_<<@type>>_strs)
#define uni_str_is_<<@type>><<@sz>>(s, slen, cond) \
  uni_str_case_check<<@sz>>(s, slen, cond, \
                   uni_s<<@type>>_mtab, uni_<<@type>>_mtab, uni_<<@type>>_strs)
@

<<Generic [[str_case_convert_check]] for [[tc]]>>=
@

<<case.c>>=
<<[[Generic]] [[uni_str_case_convert_check]][[32]]>>
<<[[Generic]] [[uni_str_case_convert_check]][[16]]>>
<<[[Generic]] [[uni_str_case_convert_check]][[8]]>>
<<[[Titlecase]] [[uni_str_case_convert_check]][[32]]>>
<<[[Titlecase]] [[uni_str_case_convert_check]][[16]]>>
<<[[Titlecase]] [[uni_str_case_convert_check]][[8]]>>
<<[[Cased]] [[uni_str_case_check]][[32]]>>
<<[[Cased]] [[uni_str_case_check]][[16]]>>
<<[[Cased]] [[uni_str_case_check]][[8]]>>
@

<<(@type) [[uni_str_case_convert]] Prototype>>=
<<[[uni_str_case_convert]][[<<@sz>>]] Prototype>>
@

<<[[Titlecase]] [[uni_str_case_convert]] Prototype>>=
<<[[uni_str_]][[tc]][[<<@sz>>]] Prototype>>
@

<<(@type) [[uni_str_case_convert_check]](@sz)>>=
<<[[<<@type>>]] [[uni_str_case_convert]] Prototype>>
{
  unsigned int rlen = 0;
  <<Initialize string case conversion>>
  
  while(slen) {
    <<Convert a character's case into output, updating [[rlen]]>>
  }
  <<Clean up after string case conversion>>
  return rlen;
}
@

<<(@type) [[uni_str_case_check]] Prototype>>=
<<[[uni_str_case_check]][[<<@sz>>]] Prototype>>
@

<<[[Titlecase]] [[uni_str_case_check]] Prototype>>=
<<[[uni_str_is_]][[tc]][[<<@sz>>]] Prototype>>
@

<<[[Cased]] [[uni_str_case_check]] Prototype>>=
<<[[uni_str_is_]][[cased]][[<<@sz>>]] Prototype>>
@

<<(@type) [[uni_str_case_convert_check]](@sz)>>=
<<[[<<@type>>]] [[uni_str_case_check]][[<<@sz>>]]>>
@

<<(@type) [[uni_str_case_check]](@sz)>>=
<<[[<<@type>>]] [[uni_str_case_check]] Prototype>>
{
  <<Initialize string case checking>>
  
  while(slen) {
    <<Convert a character's case, returning 1 if changed>>
  }
  <<Return default case check for [[<<@type>>]]>>
}
@

<<Return default case check for (@type)>>=
return 1;
@

<<Return default case check for [[Cased]]>>=
return 0;
@

In order to avoid having to declare separate noweb macros for every
case, the C preprocessor is used to select which code to include.

<<Initialize string case checking>>=
#undef USE_CHECK_CODE
#define USE_CHECK_CODE 2
<<Initialize string case conversion>>
@

<<Convert a character's case, returning 1 if changed>>=
<<Convert a character's case into output, updating [[rlen]]>>
@

<<Initialize string case conversion>>=
#if USE_CHECK_CODE == 2
#undef USE_CHECK_CODE
#define USE_CHECK_CODE 1
#else
#undef USE_CHECK_CODE
#endif
<<Use code for [[<<@type>>]] case conversion>>
@

<<Use code for (@type) case conversion>>=
#undef USE_TC_CODE
#undef USE_ALL_CODE
@

<<Use code for [[Titlecase]] case conversion>>=
#undef USE_TC_CODE
#define USE_TC_CODE 1
#undef USE_ALL_CODE
@

<<Use code for [[Cased]] case conversion>>=
<<Use code for [[Titlecase]] case conversion>>
#define USE_ALL_CODE 1
@

Empty input produces empty output (length zero).  For the case
checkers, empty strings always return false (zero as well).  This is
the only degenerate case.

<<Initialize string case conversion>>=
if(!slen)
  return 0;
@

No check is made to ensure that the input string does not overlap the
output buffer; the input string will simply be clobbered, possibly
corrupting the result.  However, one special case is permitted:  If
the input starts at the start of the output buffer, [[NULL]] can be
used for [[s]], meaning that care will be taken to not corrupt the
output if the input gets overwritten.

<<Initialize string case conversion>>=
#ifndef USE_CHECK_CODE
uint<<@sz>>_t *stmp = NULL;

if(!s) {
  /* FIXME: only do this if output will overwrite input */
  inisize(stmp, slen);
  cpybuf(stmp, *buf + (off > 0 ? off : 0), slen);
  s = stmp;
}
#endif
@

<<Clean up after string case conversion>>=
if(stmp)
  free(stmp);
@

While producing output, the output buffer parameters need to be
adjusted.  For dynamic buffers, this amounts to adding to the offset.
For static buffers, this requires managing a separate buffer pointer
and length.

<<Initialize string case conversion>>=
#ifndef USE_CHECK_CODE
uint<<@sz>>_t *sbuf = *buf;
unsigned int sblen = *buf_len;

if(off < 0) {
  buf = &sbuf;
  buf_len = &sblen;
}
#endif
@

Since many of the case conversion flags require knowledge of a
complete grapheme cluster, the loop operates one cluster at a time.
Since UTF conversion isn't too expensive, only the first character of
the next cluster is saved.  The grapheme cluster boundary is
determined by the character count to the next start.

<<Initialize string case conversion>>=
int gc_res = 0;
int next_gc = 0;
unsigned int clen, next_cp_len;
/* warning: _int_ may over-read buffer with invalid UTF input */
uint32_t next_cp = uni_int_utf<<@sz>>_decode(s, &clen);
next_cp_len = clen;
@

<<Convert a character's case into output, updating [[rlen]]>>=
/* warning: _int_ may over-read buffer with invalid UTF input */
uint32_t cp = next_gc ? uni_int_utf<<@sz>>_decode(s, &clen) : next_cp;
if(!next_gc)
  clen = next_cp_len;
if(clen > slen) /* should never happen with valid UTF input */
  slen = 0;
else {
  slen -= clen;
  s += clen;
}
if(!next_gc) {
  <<Find next grapheme cluster boundary for case conversion>>
} else
  next_gc -= clen;
@

While finding the next cluster boundary, context flags are adjusted as
well.  The final sigma flag is supposed to be set on the last letter
of a word.  However, the word boundary determination algorithm only
flags changes at the start of words, so a simplified ``anything but
alphabetic'' end-of-word boundary is determined, instead.

Note that the only user-supplied conditional flags respected are the
locale ones.  Unlike the single-character conversion functions, no
facility is provided to check if setting them would result in
different output.

<<Find next grapheme cluster boundary for case conversion>>=
cond &= UNI_SC_FL_LOCALE;
while(1) {
  <<Set casing condition flags for [[next_cp]]>>
  if(next_gc == slen) {
    cond |= UNI_SC_FL_FINAL_SIGMA;
    break;
  }
  /* warning: _int_ may over-read buffer with invalid UTF input */
  uint32_t ncp = uni_int_utf<<@sz>>_decode(s + next_gc, &clen);
  gc_res = uni_gc_brk(next_cp, ncp, gc_res, 0);
#if 0
  /* finding word break is pointless, since it happens too late */
  word_res = uni_word_brk(next_cp, ncp, word_res, NULL);
  /* FIXME: no backtracking */
  if(!word_res)
    cond |= UNI_SC_FL_FINAL_SIGMA;
#else
  /* instead, eoword is defined as last before non-alpha or end */
  if(gc_res > 0) {
    uni_gc_t gc = uni_gc_of(ncp);
    if(!((1ULL << gc) & uni_gc_trans[UNI_gc_L]))
      cond |= UNI_SC_FL_FINAL_SIGMA;
  }
#endif
  next_cp = ncp;
  next_cp_len = clen;
  if(gc_res > 0)
    break;
  next_gc += clen;
  if(next_gc > slen)
    next_gc = slen; /* should never happen with valid UTF input */
}
@

<<Set casing condition flags for [[next_cp]]>>=
if(next_cp == 'I')
  cond |= UNI_SC_FL_AFTER_I;
else if(next_cp == 'i')
  cond |= UNI_SC_FL_AFTER_SOFT_DOTTED;
else if(next_cp == 0x0307) /* COMBINING DOT ABOVE */
  cond |= UNI_SC_FL_BEFORE_DOT;
/* note: this is supposed to be more generic */
/* "any combining mark above" */
/* but it's just for Lithuanian, and I'm using their list */
else if(next_cp == 0x0300 || /* COMBINING GRAVE ACCENT */
        next_cp == 0x0301 || /* COMBINING ACUTE ACCENT */
	next_cp == 0x0303 || /* COMBINING TILDE */
	/* unsure about these two */
	/* but I doubt a Lithuanian will ever complain */
	next_cp == 0x0328 || /* COMBINING OGONEK */
	next_cp == 0x1DCE) /* COMBINING OGONEK ABOVE */
  cond |= UNI_SC_FL_MORE_ABOVE;
@

However, title casing does need to know the start of a new word.  For
this, the official algorithm is used to set a flag.  This algorithm
requires locale knowledge, so the word break table needs to be passed
in, as well.  Since it technically applies to the next character, two
flags are used.  Also, since grapheme cluster extensions are generally
ignored, only the two cluster start characters are compared.

<<[[uni_str_]][[tc]](@sz) Prototype>>=
/** Convert entire UTF-<<@sz>> string \p s/\p slen to [<<@type>>].
  * If \p slen is less than zero, \p s is zero-terminated.
  * The return buffer parameters \p buf, \p off and \p buf_len are
  * described in \ref uni_return<<@sz>>_buf<<@sz>>.
  * Only the locale-related flags in \p cond are recognized.
  * Additional locale customization can be done by overriding the
  * WB property table (\p WB_tab) used for word boundary detection;
  * the default is to use \ref uni_WBp_mtab */
int uni_str_tc<<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                         uint32_t cond, const uint32_t *WB_tab,
			 <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<[[uni_str_is_]][[tc]](@sz) Prototype>>=
/** Test if entire UTF-<<@sz>> string \p s/\p slen is [tc].
  * If \p slen is less than zero, \p s is zero-terminated.
  * Only the locale-related flags in \p cond are recognized.
  * Additional locale customization can be done by overriding the
  * WB property table (\p WB_tab) used for word boundary detection;
  * the default is to use \ref uni_WBp_mtab */
int uni_str_is_tc<<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                            uint32_t cond, const uint32_t *WB_tab)
@

<<[[uni_str_is_]][[cased]](@sz) Prototype>>=
/** Test if entire UTF-<<@sz>> string \p s/\p slen is cased.
  * If \p slen is less than zero, \p s is zero-terminated.
  * The string is converted to lower-case, upper-case, and title-case,
  * and true is returned only if any of the three are not equal to \p s.
  * Only the locale-related flags in \p cond are recognized.
  * Additional locale customization can be done by overriding the
  * WB property table (\p WB_tab) used for word boundary detection;
  * the default is to use \ref uni_WBp_mtab */
int uni_str_is_cased<<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                            uint32_t cond, const uint32_t *WB_tab)
@

<<Initialize string case conversion>>=
#ifdef USE_TC_CODE
int prev_word = 1, next_word = 0;
int word_res = 0;
#endif
@

<<Find next grapheme cluster boundary for case conversion>>=
#ifdef USE_TC_CODE
next_word = prev_word;
if(next_gc != slen) {
  word_res = uni_word_brk(cp, next_cp, word_res, WB_tab);
  /* FIXME: no backtracking */
  prev_word = !word_res;
}
#endif
@

Once the flags have been determined, all characters can be converted
into the output buffer.  For regular casing, fallback is to the
original character.   Title casing chooses title or lower case
conversion depending on whether or not the character is at the start
of a word.  Fallback for title casing is upper casing, and then the
original character.

<<Convert a character's case into output, updating [[rlen]]>>=
int32_t rlen1;
const uint16_t *rbuf;
#ifndef USE_TC_CODE /* also always defined when USE_ALL_CODE */
rlen1 = uni_case_convert(cp, cond, &rbuf, simple, special, strs);
#else
#ifdef USE_ALL_CODE
int did_uc = 0, did_lc = 0;
/* doing tc below, even if tc not explicitly used */
#endif
if(next_word) {
  next_word = 0;
  rlen1 = uni_case_convert(cp, cond, &rbuf, uni_stc_mtab, uni_tc_mtab,
                            uni_tc_strs);
  if(!rbuf && rlen1 == cp) {
#ifdef USE_ALL_CODE
    did_uc = 1;
#endif
    rlen1 = uni_case_convert(cp, cond, &rbuf, uni_suc_mtab, uni_uc_mtab,
                              uni_uc_strs);
  }
} else {
#ifdef USE_ALL_CODE
  did_lc = 1;
#endif
  rlen1 = uni_case_convert(cp, cond, &rbuf, uni_slc_mtab, uni_lc_mtab,
                            uni_lc_strs);
}
#endif
@

<<Convert a character's case into output, updating [[rlen]]>>=
#ifndef USE_CHECK_CODE
if(rbuf)
  rlen1 = uni_return16_buf<<@sz>>(rbuf, rlen1, buf, off, buf_len);
else
  rlen1 = uni_return32_buf<<@sz>>((uint32_t *)&rlen1, 1, buf, off, buf_len);
if(off < 0) {
  if(sblen > rlen1) {
    sblen -= rlen1;
    sbuf += rlen1;
  } else
    sblen = 0;
  } else {
    if(!*buf) {
      /* continue gathering lengths, but final return will be NULL */
      sblen = 0;
      buf_len = &sblen;
      off = -1;
    }
  off += rlen1;
}
rlen += rlen1;
#endif
@

<<Convert a character's case into output, updating [[rlen]]>>=
#ifdef USE_CHECK_CODE
<<Check [[rbuf]]/[[rlen1]] for case conversion>>
#ifdef USE_ALL_CODE
if(!did_lc) {
  rlen1 = uni_case_convert(cp, cond, &rbuf, uni_slc_mtab, uni_lc_mtab,
                            uni_lc_strs);
  <<Check [[rbuf]]/[[rlen1]] for case conversion>>
}
if(!did_uc) {
  rlen1 = uni_case_convert(cp, cond, &rbuf, uni_suc_mtab, uni_uc_mtab,
                            uni_uc_strs);
  <<Check [[rbuf]]/[[rlen1]] for case conversion>>
}
#endif
#endif
@

<<Check [[rbuf]]/[[rlen1]] for case conversion>>=
if((!rbuf && rlen1 != cp) ||
   (rbuf && ((rlen1 != 1 && (rlen1 != 2 || !uni_is_surrogate(*rbuf))) ||
             cp != uni_int_utf16_decode(rbuf, NULL))))
#ifdef USE_ALL_CODE
  return 1;
#else
  return 0;
#endif
@

<<FIXME>>=
provide test data & driver
  current test is done externally, with simple driver to call all str
  functions (so char functions are completely untested).  Need test
  text to test conditional flags better and test locale flags
@

For more case transformation fun, there is [[CaseFolding.txt]].  It
provides four different case folding (lower-case) transformations:
common, simple, full, and Turkic special cases.  The common and simple
cases are guaranteed to always have length one.  There are few enough
full and Turkic cases that it makes sense to just encode them using
the same method as the full case conversion tables above.  Note that
the official cf property ignores the Turkic cases.

<<Initialize UCD files>>=
decl_num(scf);
decl_str(cf);
@

<<Parse UCD files>>=
open_f("CaseFolding.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  uint32_t flg;
  uint32_t scf = strtol(fields[2], NULL, 16);
  switch(fields[1][0]) {
    case 'C': /* default; value == cp */
      add_num(scf, scf, 1);
      break;
    case 'S': /* scf == C+S */
      add_num(scf, scf, 1);
      break;
    case 'F': /* cf == C+F */
      flg = 0;
      add_sc(cf, fields[2]);
      break;
    case 'T': /* tr, az lang only; not used by anything, technically */
      flg = UNI_SC_FL_AZ | UNI_SC_FL_TR;
      add_sc(cf, fields[2]);
      break;
  }
}
fclose(f);
<<Postprocess simple casing for [[cf]]>>
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_cf], "Case_Folding", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_cf);
@

Technically, any entry which is in the S class but not in the F class
should have no translation.  However, I believe that there is no such
case.  Like with the case conversions, a set of macros is provided for
cf property lookup.  Since there is no condition input, special macros
are provided for Turkic locales as well.

<<[[uni_case_convert]] cf support for (@type) using condition (@cond)>>=
<<[[uni_case_convert]][[32]] cf support>>
<<[[uni_case_convert]][[16]] cf support>>
<<[[uni_case_convert]][[8]] cf support>>
/** Check if \p cp is case-folded.
  * Returns true if case-folding \p cp does not change \p cp.
  * Special casing uses the condition <<@cond>>. */
int uni_is_<<@type>>(uint32_t cp);
#define uni_is_<<@type>>(cp) \
  uni_case_check(cp, <<@cond>>, uni_scf_mtab, uni_cf_mtab, uni_cf_strs)
@

<<[[uni_case_convert]](@sz) cf support>>=
/** Case-fold \p cp.
  * Return parameters are described with \ref uni_return<<@sz>>_buf<<@sz>>.
  * Special casing uses the condition <<@cond>>. */
int uni_<<@type>><<@sz>>(uint32_t cp, <<Buffer return parameters for UTF-[[<<@sz>>]]>>);
#define uni_<<@type>><<@sz>>(cp, buf, off, buf_len) \
  uni_case_convert<<@sz>>(cp, <<@cond>>, buf, off, buf_len, \
                     uni_return16_buf<<@sz>>, uni_return32_buf<<@sz>>, \
                     uni_scf_mtab, uni_cf_mtab, uni_cf_strs)
/** Case-fold UTF-<<@sz>> string \p s/\p slen.
  * If \p slen is negative, \p s is zero-terminated.
  * Return parameters are described with \ref uni_return<<@sz>>_buf<<@sz>>.
  * Special casing uses the condition <<@cond>>. */
int uni_str_<<@type>><<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                           <<Buffer return parameters for UTF-[[<<@sz>>]]>>);
#define uni_str_<<@type>><<@sz>>(s, slen, buf, off, buf_len) \
  uni_str_case_convert<<@sz>>(s, slen, <<@cond>>, buf, off, buf_len, \
                            uni_scf_mtab, uni_cf_mtab, uni_cf_strs)
/** Check if string \p s/\p slen is case-folded.
  * If \p slen is negative, \p s is zero-terminated.
  * Returns true if case-folding \p s does not change \p s.
  * Special casing uses the condition <<@cond>>. */
int uni_str_is_<<@type>><<@sz>>(const uint<<@sz>>_t *s, unsigned int slen);
#define uni_str_is_<<@type>><<@sz>>(s, slen) \
  uni_str_case_check<<@sz>>(s, slen, <<@cond>>, uni_scf_mtab, uni_cf_mtab, \
                          uni_cf_strs)
@

<<Unicode property exports>>=
<<[[uni_case_convert]] cf support for [[cf]] using condition [[0]]>>
<<[[uni_case_convert]] cf support for [[tcf]] using condition [[UNI_SC_FL_TR]]>>
@

<<FIXME>>=
provide test data & driver (see other uni_case_convert stuff)
@

Once case folding is done, the obvious thing to do is compare the
results.   The following function combines the two steps in a way that
does not require intermediate storage.

<<[[uni_case_convert]](@sz) cf support>>=
/** Case-fold and compare two UTF-<<@sz>> strings without intermediate storage.
  * Case-folds \p a/\p alen and \p b/\p blen and compares the results by
  * code point, then length.  Returns -1 if the case-folded \p a is less
  * than the case-folded \p b, 1 if it is greater, and zero if they are equal.
  * The special casing uses the condition flag <<@cond>>. */
int uni_<<@type>>_strcmp<<@sz>>(const uint<<@sz>>_t *a, unsigned int alen,
                            const uint<<@sz>>_t *b, unsigned int blen);
#define uni_<<@type>>_strcmp<<@sz>>(a, alen, b, blen) \
  uni_either_cf_strcmp<<@sz>>(a, alen, b, blen, <<@cond>>)
@

<<Unicode property exports>>=
<<[[uni_either_cf_strcmp]][[32]] Prototype>>;
<<[[uni_either_cf_strcmp]][[16]] Prototype>>;
<<[[uni_either_cf_strcmp]][[8]] Prototype>>;
@

<<[[uni_either_cf_strcmp]](@sz) Prototype>>=
/** Case-fold and compare two UTF-<<@sz>> strings without intermediate storage.
  * Case-folds \p a/\p alen and \p b/\p blen and compares the results by
  * code point, then length.  Returns -1 if the case-folded \p a is less
  * than the case-folded \p b, 1 if it is greater, and zero if they are equal.
  * The special casing uses the condition flag \p cond, which is expected
  * to either be zero or UNI_SC_FLAG_TR. */
int uni_either_cf_strcmp<<@sz>>(const uint<<@sz>>_t *a, unsigned int alen,
                              const uint<<@sz>>_t *b, unsigned int blen,
			      uint32_t cond)
@

<<case.c>>=
<<[[uni_either_cf_strcmp]][[32]]>>
<<[[uni_either_cf_strcmp]][[16]]>>
<<[[uni_either_cf_strcmp]][[8]]>>
@

<<[[uni_either_cf_strcmp]](@sz)>>=
<<[[uni_either_cf_strcmp]][[<<@sz>>]] Prototype>>
{
  const uint16_t *arbuf = NULL, *brbuf = NULL;
  uint32_t arlen = 0, brlen = 0;
  uint32_t ac, bc;
  while((alen || arlen) && (blen || brlen)) {
    <<Get next case folded character from [[a]] using [[uni_cf]]>>
    <<Get next case folded character from [[b]] using [[uni_cf]]>>
    if(ac > bc)
      return 1;
    if(ac < bc)
      return -1;
  }
  return blen ? -1 : alen > 0;
}
@

<<Get next case folded character from (@ab) using (@stab)>>=
if(!<<@ab>>rlen) {
  unsigned int clen;
  <<Get case conversion result for next character using [[<<@stab>>]]>>
  if(clen > <<@ab>>len)
#pragma GCC diagnostic push /* STFU GCC */
#pragma GCC diagnostic ignored "-Wtautological-compare"
    return a == <<@ab>> ? 2 : -2; /* invalid input always mismatches */
#pragma GCC diagnostic pop
  <<@ab>>len -= clen;
  <<@ab>> += clen;
}
if(!<<@ab>>rbuf) {
  <<@ab>>c = <<@ab>>rlen;
  <<@ab>>rlen = 0;
} else {
  unsigned int clen;
  <<@ab>>c = uni_int_utf16_decode(<<@ab>>rbuf, &clen);
  <<@ab>>rbuf += clen;
  <<@ab>>rlen -= clen;
}
@

<<Get case conversion result for next character using (@stab)>>=
<<@ab>>rlen = uni_case_convert(uni_int_utf<<@sz>>_decode(<<@ab>>, &clen), cond,
                             &<<@ab>>rbuf, uni_scf_mtab, <<@stab>>_mtab,
			     <<@stab>>_strs);
@

If the inputs are not stored in a form for which the case folding
transformation is valid, normalization needs to be done first.  In
fact, the Unicode standard expects normalization to performed both
before \emph{and after} case folding.  The simpler of these two
requires canonical normalization before and after, and the more
complex requires canonical, followed by case folding, followed by
compatibility, followed by case folding, followed by compatibility
again.  In order to reduce the complexity of these ridiculous
transformations, a few observations are made:

\begin{itemize}
\item Case folding applies one character at a time, so canonical
ordering does not affect case folding.  Likewise, decomposition
applies one character at a time, so canonical ordering does not affect
decomposition.  Thus, rather than decomposing, ordering, folding,
decomposing, and then ordering again, the first ordering can be
removed.  Similarly, for compatibility comparison, both inner
decompositions' ordering step can be skipped.
\item Since the result for any particular character of the entire
sequence of decompositions and case folding is independent of context,
a table can be pre-built with the results.  A similar table has
already been provided by the UCD for NFKC\_CF transformation; this
would just provide tables for NFD\_CF and NFKD\_CF transformations.
Note that the UCD-provided NFKC\_CF table also removes default
ignorable code points.  If it were not for that, the NFKC\_CF table
would in fact provide exactly what's needed for NFKD\_CF (this table
is misnamed; it does the decomposition and case folding, but does not
perform the final ordering and composition steps needed for NFKC\_CF).
\end{itemize}

In other words, it is possible to create a pair of new string
properties which perform all steps of the transformation except for
canonical ordering.  The first of these to generate is the canonical
decomposition form.  This performs NFD, case folding, and NFD again.
For the first step, the NFD decomposition table is scanned.  Any
decomposition is then case folded one character at a time.  For those
characters which do not decompose, the case folding tables are scanned
at the same time, so those entries are added to the table as well.
Technically the latter could be skipped, and a failure to look up a
code point in the generated table would fall back to normal case
folding.

The format of the table is similar to the long-form case conversion
tables: length words with flags, followed by strings.  However, to
save a little space, if there is only one string, with no flags, of
length one, no length word is stored.  Even if that string ends up as
two 16-bit words, the surrogate initiators cannot be confused with
length words if explicitly checked for.

Note that decompositions which are not explicitly stored cannot be
case converted.  Currently, this is the set of Hangul syllable
characters.  Since none of the L, V, and T elements have case
translations, this should be fine.  However, the lookup routine needs
to be aware that zero means that Hangul syllable decomposition needs
to happen.

<<Initialize UCD files>>=
decl_str(NFD_CF);
@

<<Parse UCD files>>=
prop_NFD_CF = add_prop("NFD_CF");
prop_t *nfd_cf = &parsed_props[prop_NFD_CF];
prop_t *cf = &parsed_props[prop_cf];
prop_t *scf = &parsed_props[prop_scf];
nfd_cf->max_len = dmf_prop->max_len;
inisize(nfd_cf->str_arr, nfd_cf->max_len);
nfd_cf->max_strs = cf->max_strs;
inisize(nfd_cf->strs, nfd_cf->max_strs);
/* prepare for merging and searching; may already be in order */
qsort(dmf_prop->str_arr, dmf_prop->len, sizeof(*dmf_prop->str_arr), cmp_cp_flg);
{
  unsigned int nfdi, cfi, scfi;
  uint32_t prev_cp = 0;

  for(nfdi = 0; dmf_prop->str_arr[nfdi].flags; nfdi++);
  for(cfi = scfi = 0; nfdi < dmf_prop->len || cfi < cf->len || scfi < scf->len; ) {
    check_size(nfd_cf->str_arr, nfd_cf->max_len, nfd_cf->len + 1);
    raw_cp_str_t *nent = &nfd_cf->str_arr[nfd_cf->len++];
    nent->off = nfd_cf->strs_len;
    /* prefer nfd over all */
    if(nfdi < dmf_prop->len &&
       (cfi == cf->len ||
        dmf_prop->str_arr[nfdi].cp <= cf->str_arr[cfi].cp) &&
       (scfi == scf->len ||
        dmf_prop->str_arr[nfdi].cp <= scf->rng_dat32[scfi].low +
	                              scf->rng_dat32[scfi].len)) {
      <<Copy and case-fold decomposition [[nfdi]] into [[nent]]/[[nfd_cf]]>>
      if(cfi < cf->len && dmf_prop->str_arr[nfdi].cp == cf->str_arr[cfi].cp)
        cfi++;
      if(scfi < scf->len &&
         dmf_prop->str_arr[nfdi].cp == scf->rng_dat32[scfi].low +
                                       scf->rng_dat32[scfi].len)
        scfi++;
      while(++nfdi < dmf_prop->len && dmf_prop->str_arr[nfdi].flags);
    /* prefer cf over scf */
    } else if(cfi < cf->len &&
              (scfi == scf->len ||
 	       cf->str_arr[cfi].cp <= scf->rng_dat32[scfi].low +
	                              scf->rng_dat32[scfi].len)) {
      <<Copy full case fold entry into [[nent]]/[[nfd_cf]]>>
      if(scfi < scf->len && cf->str_arr[cfi].cp == scf->rng_dat32[scfi].low +
                                                   scf->rng_dat32[scfi].len)
        scfi++;
      cfi++;
    /* fall back to scf */
    } else {
      <<Copy simple case fold entry into [[nent]]/[[nfd_cf]]>>
      if(cp == scf->rng_dat32[scfi].low + scf->rng_dat32[scfi].len)
        scfi++;
    }
    prev_cp = nent->cp;
  }
}
@

<<Copy and case-fold decomposition (@deci) into [[nent]]/(@prop)>>=
/* + 1 to allow for len/flag word */
unsigned int out_len = dmf_prop->str_arr[<<@deci>>].len + 1;
/* * 2 to allow room for tcf */
out_len = out_len * 2 + <<@prop>>->strs_len;
check_size(<<@prop>>->strs, <<@prop>>->max_strs, out_len);
nent->cp = dmf_prop->str_arr[<<@deci>>].cp;
/* plain */
unsigned int off = <<@prop>>->strs_len++; /* where to put len */
int has_turk = 0;
const uint32_t *str = dmf_prop->strs + dmf_prop->str_arr[<<@deci>>].off;
for(i = 0; i < dmf_prop->str_arr[<<@deci>>].len; i++) {
  <<Case fold [[str + i]] into [[<<@prop>>]] for non-Turkic>>
}
unsigned int slen = <<@prop>>->strs_len - off - 1;
for(i = slen; i > 0; i--)
  if(<<@prop>>->strs[off + i] > 0xffff)
    slen++;
if(slen > UNI_SC_FL_LEN_MASK) {
  fprintf(stderr, "<<@prop>> overflow %04X %d\n", nent->cp, slen);
  exit(1);
}
<<@prop>>->strs[off] = slen;
/* turkic */
if(has_turk) {
  off = <<@prop>>->strs_len++; /* where to put len & flags */
  for(i = 0; i < dmf_prop->str_arr[<<@deci>>].len; i++) {
    <<Case fold [[str + i]] into [[<<@prop>>]] for Turkic>>
  }
  slen = <<@prop>>->strs_len - off - 1;
  for(i = slen; i > 0; i--)
    if(<<@prop>>->strs[off + i] > 0xffff)
      slen++;
  if(slen > UNI_SC_FL_LEN_MASK) {
    fprintf(stderr, "<<@prop>> overflow %04X %d\n", nent->cp, slen);
    exit(1);
  }
  <<@prop>>->strs[off] = slen | UNI_SC_FL_AZ | UNI_SC_FL_TR;
}
if(<<@prop>>->strs_len == nent->off + 2 &&
   !(<<@prop>>->strs[nent->off] & UNI_SC_FL_TR)) {
  <<@prop>>->strs_len--;
  <<@prop>>->strs[nent->off] = <<@prop>>->strs[nent->off + 1];
}
nent->len = <<@prop>>->strs_len - nent->off;
@

<<Case fold (@char pointer) into (@prop) for non-Turkic>>=
raw_cp_str_t *cfs = bsearch(<<@char pointer>>, cf->str_arr, cf->len,
	                    sizeof(*cf->str_arr), uni_cmp_cp);
if(cfs) {
  const uint32_t *cfstr = cf->strs + cfs->off;
  unsigned int cflen = cfs->len;
  if(*cfstr & UNI_SC_FL_TR) {
    cflen -= (*cfstr & UNI_SC_FL_LEN_MASK) + 1;
    cfstr += (*cfstr & UNI_SC_FL_LEN_MASK) + 1;
    has_turk = 1;
  }
  if(cflen) {
    if(cflen != (*cfstr & UNI_SC_FL_LEN_MASK) + 1)
      has_turk = 1;
    cflen = *cfstr & UNI_SC_FL_LEN_MASK;
    cfstr++;
    if(cflen > 1) {
      out_len += cflen - 1;
      check_size(<<@prop>>->strs, <<@prop>>->max_strs, out_len);
    }
    while(cflen) {
      <<@prop>>->strs[<<@prop>>->strs_len++] = *cfstr++;
      cflen--;
    }
    continue;
  }
  /* else drop through and try scf */
}
uni_chrrng_dat32_t *res, scp;
scp.low = *(<<@char pointer>>);
scp.len = 0;
res = bsearch(&scp, scf->rng_dat32, scf->len, sizeof(scp), uni_cmprng_dat32);
<<@prop>>->strs[<<@prop>>->strs_len++] =
  res ? num_of(res->dat) : *(<<@char pointer>>);
@

<<UCD parser local functions>>=
static uint32_t num_of(uint32_t d32)
{
  /* wtf, C?  You used to be so cool.. */
  /* this removes gcc's "type punned pointer" warning when casting directly */
  /* gcc doesn't even like (void *)&d32; it needs an intermediary like p */
  /* if this compiles to more than a single instruction, the compiler is stupid */
  void *p = &d32;

  return ((uni_frac_t *)p)->num;
}
@

<<Case fold (@char pointer) into (@prop) for Turkic>>=
raw_cp_str_t *cfs = bsearch(<<@char pointer>>, cf->str_arr, cf->len,
	                    sizeof(*cf->str_arr), uni_cmp_cp);
if(cfs) {
  const uint32_t *cfstr = cf->strs + cfs->off;
  unsigned int cflen = cfs->len;
  if(!(*cfstr & UNI_SC_FL_TR) &&
     cflen > (*cfstr & UNI_SC_FL_LEN_MASK) + 1) {
    cflen -= (*cfstr & UNI_SC_FL_LEN_MASK) + 1;
    cfstr += (*cfstr & UNI_SC_FL_LEN_MASK) + 1;
  }
  cflen = *cfstr & UNI_SC_FL_LEN_MASK;
  cfstr++;
  if(cflen > 1) {
    out_len += cflen - 1;
    check_size(<<@prop>>->strs, <<@prop>>->max_strs, out_len);
  }
  while(cflen) {
    <<@prop>>->strs[<<@prop>>->strs_len++] = *cfstr++;
    cflen--;
  }
} else {
  uni_chrrng_dat32_t *res, scp;
  scp.low = *(<<@char pointer>>);
  scp.len = 0;
  res = bsearch(&scp, scf->rng_dat32, scf->len, sizeof(scp), uni_cmprng_dat32);
  <<@prop>>->strs[<<@prop>>->strs_len++] =
    res ? num_of(res->dat) : *(<<@char pointer>>);
}
@

<<Copy full case fold entry into [[nent]]/[[nfd_cf]]>>=
nent->cp = cf->str_arr[cfi].cp;
check_size(nfd_cf->strs, nfd_cf->max_strs,
           cf->str_arr[cfi].len + nfd_cf->strs_len);
if(cf->str_arr[cfi].len == 2 &&
   !(cf->strs[cf->str_arr[cfi].off] & UNI_SC_FL_TR)) {
  nfd_cf->strs[nfd_cf->strs_len] = cf->strs[cf->str_arr[cfi].off + 1];
  nent->len = 1;
} else {
  cpybuf(nfd_cf->strs + nfd_cf->strs_len, cf->strs + cf->str_arr[cfi].off,
         cf->str_arr[cfi].len);
  nent->len = cf->str_arr[cfi].len;
}
nfd_cf->strs_len += nent->len;
/* if turkic only, add scf */
if(nfd_cf->strs[nent->off] == (UNI_SC_FL_TR | UNI_SC_FL_AZ | (nent->len - 1)) &&
   scfi < scf->len && cf->str_arr[cfi].cp >= scf->rng_dat32[scfi].low) {
  check_size(nfd_cf->strs, nfd_cf->max_strs, nfd_cf->strs_len + 2);
  nfd_cf->strs[nfd_cf->strs_len++] = 1;
  nfd_cf->strs[nfd_cf->strs_len++] = num_of(scf->rng_dat32[scfi].dat);
  nent->len += 2;
}
@

<<Copy simple case fold entry into [[nent]]/[[nfd_cf]]>>=
uint32_t cp = prev_cp + 1;
if(cp < scf->rng_dat32[scfi].low)
  cp = scf->rng_dat32[scfi].low;
check_size(nfd_cf->strs, nfd_cf->max_strs, nfd_cf->strs_len + 1);
nent->cp = cp;
nent->len = 1;
nfd_cf->strs[nfd_cf->strs_len++] = num_of(scf->rng_dat32[scfi].dat);
@

The final NFD pass consists of NFD decomposition followed by canonical
ordering.  The canonical ordering needs to be done externally.  The
decomposition can be skipped if the above table already produces
normalized output.  The following code verifies that this is, in fact,
the case.  If the UCD ever fails this test, all changed entries will
have to be reallocated and fixed.

<<Parse UCD files>>=
for(i = 0; i < nfd_cf->len; i++) {
  const uint32_t *str = nfd_cf->strs + nfd_cf->str_arr[i].off;
  unsigned int tlen = nfd_cf->str_arr[i].len - 1, slen;
  
  <<Check if [[str]] is canonically decomposed>>
  if(!tlen)
    continue;
  <<Check if [[str]] is canonically decomposed>>
}
@

<<Check if [[str]] is canonically decomposed>>=
slen = tlen ? *str++ & UNI_SC_FL_LEN_MASK : *str > 0xffff ? 2 : 1;
if(!tlen)
  tlen++;
for(; slen; slen--, tlen--, str++) {
  if(*str > 0xffff)
    slen--;
  raw_cp_str_t *dec = bsearch(str, dmf_prop->str_arr, dmf_prop->len,
                              sizeof(*dmf_prop->str_arr), uni_cmp_cp);
  if(dec) {
    while(dec > dmf_prop->str_arr && dec[-1].cp == dec->cp)
      dec--;
    if(!dec->flags) {
      fprintf(stderr, "FIXME: nfd_cf %04X %04X\n", nfd_cf->str_arr[i].cp, *str);
      exit(1);
    }
  }
}
@

The second of these to generate is the compatibility decomposition
form.  This performs NFD, followed by case folding, followed by NFKD,
followed by case folding again, followed by NFKD again.  The first two
steps have already been performed to generate the above table, so that
is mostly copied into the result while applying NFKD and case folding.
At the same time, the NFKD table is used to fill in any gaps.  Using
the tables generated above means that it is unnecessary to fill in
gaps using the case folding tables (although, again, technically this
could just fall back to the case folding tables and not store them
explicitly anyway).

<<Initialize UCD files>>=
decl_str(NFKD_CF);
@

<<Parse UCD files>>=
prop_NFKD_CF = add_prop("NFKD_CF");
prop_t *nfkd_cf = &parsed_props[prop_NFKD_CF];
nfkd_cf->max_len = nfd_cf->max_len;
inisize(nfkd_cf->str_arr, nfkd_cf->max_len);
nfkd_cf->max_strs = cf->max_strs;
inisize(nfkd_cf->strs, nfkd_cf->max_strs);
{
  unsigned int nfdi, nfkdi;

  for(nfkdi = 0; dmf_prop->str_arr[nfkdi].flags != 1; nfkdi++);
  for(nfdi = 0; nfkdi < dmf_prop->len || nfdi < nfd_cf->len; ) {
    check_size(nfkd_cf->str_arr, nfkd_cf->max_len, nfkd_cf->len + 1);
    raw_cp_str_t *nent = &nfkd_cf->str_arr[nfkd_cf->len++];
    /* FIXME: share string table with nfd_cf? */
    nent->off = nfkd_cf->strs_len;
    /* start with nfd_cf */
    if(nfdi < nfd_cf->len &&
       (nfkdi == dmf_prop->len ||
        nfd_cf->str_arr[nfdi].cp <= dmf_prop->str_arr[nfkdi].cp)) {
      <<Copy, compatibility decompose and case fold [[nfd_cf]] entry into [[nent]]/[[nfkd_cf]]>>
      if(nfkdi != dmf_prop->len &&
         dmf_prop->str_arr[nfkdi].cp == nfd_cf->str_arr[nfdi].cp)
        while(++nfkdi < dmf_prop->len && dmf_prop->str_arr[nfkdi].flags != 1);
      nfdi++;
    /* othewise, there's a plain nfkd entry waiting */
    } else {
      <<Copy and case-fold decomposition [[nfkdi]] into [[nent]]/[[nfkd_cf]]>>
      while(++nfkdi < dmf_prop->len && dmf_prop->str_arr[nfkdi].flags != 1);
    }
  }
}
@

<<Copy, compatibility decompose and case fold [[nfd_cf]] entry into [[nent]]/[[nfkd_cf]]>>=
unsigned int out_len = nfd_cf->str_arr[nfdi].len + nfkd_cf->strs_len + 1;
check_size(nfkd_cf->strs, nfkd_cf->max_strs, out_len);
nent->cp = nfd_cf->str_arr[nfdi].cp;
/* plain */
int has_turk = 0;
const uint32_t *str = nfd_cf->strs + nfd_cf->str_arr[nfdi].off;
unsigned int slen = nfd_cf->str_arr[nfdi].len;
if(slen != 1) {
  if(*str & UNI_SC_FL_TR) {
    has_turk = 1;
    slen -= (*str & UNI_SC_FL_LEN_MASK) + 1;
    str += (*str & UNI_SC_FL_LEN_MASK) + 1;
  }
  if(slen)
    slen = *str++ & UNI_SC_FL_LEN_MASK;
}
unsigned int off;
int no_plain = !slen; /* needs nfkc(X) entry if no plain */
if(no_plain)
  fprintf(stderr, "%04X\n", nent->cp);
if(!no_plain) {
#undef DO_TURKIC
  <<Compatibility decompose and case fold [[str]]>>
}
/* turkic */
if(has_turk) {
#define DO_TURKIC 1
  <<Compatibility decompose and case fold [[str]]>>
#undef DO_TURKIC
}
nent->len = nfkd_cf->strs_len - nent->off;
if(nent->len == 2 && !(nfkd_cf->strs[nent->off] & UNI_SC_FL_TR)) {
  nfkd_cf->strs_len--;
  nent->len--;
  nfkd_cf->strs[nent->off] = nfkd_cf->strs[nent->off + 1];
}
@

<<Compatibility decompose and case fold [[str]]>>=
off = nfkd_cf->strs_len++; /* where to put len */
for(i = 0; i < slen; i++) {
  raw_cp_str_t *dm = bsearch(str + i, dmf_prop->str_arr, dmf_prop->len,
	                     sizeof(*dmf_prop->str_arr), uni_cmp_cp);
  if(dm) {
    /* only look for compatibility decomps */
    /* there shouldn't be any canon decomps anyway */
    while(dm > dmf_prop->str_arr && dm[-1].cp == dm->cp)
      dm--;
    if(!dm->flags)
      dm++;
    if(dm->flags != 1)
      dm = NULL;
  }
  if(dm) {
    const uint32_t *dmstr = dmf_prop->strs + dm->off;
    if(dm->len > 1) {
      out_len += dm->len - 1;
      check_size(nfkd_cf->strs, nfkd_cf->max_strs, out_len);
    }
    for(j = 0; j < dm->len; j++) {
#ifdef DO_TURKIC
      <<Case fold [[dmstr + j]] into [[nfkd_cf]] for Turkic>>
#else
      <<Case fold [[dmstr + j]] into [[nfkd_cf]] for non-Turkic>>
#endif
    }
  } else
    nfkd_cf->strs[nfkd_cf->strs_len++] = str[i];
}
slen = nfkd_cf->strs_len - off - 1;
for(i = slen; i > 0; i--)
  if(nfkd_cf->strs[off + i] > 0xffff)
    slen++;
if(slen > UNI_SC_FL_LEN_MASK) {
  fprintf(stderr, "nfkd_cf overflow %04X %d\n", nent->cp, slen);
  exit(1);
}
#ifdef DO_TURKIC
slen |= UNI_SC_FL_TR | UNI_SC_FL_AZ;
#endif
nfkd_cf->strs[off] = slen;
@

Once again, the final NFKD decomposition step can be skipped if the
result is already fully decomposed.  The following code verifies this.

<<Parse UCD files>>=
for(i = 0; i < nfkd_cf->len; i++) {
  const uint32_t *str = nfkd_cf->strs + nfkd_cf->str_arr[i].off;
  unsigned int tlen = nfkd_cf->str_arr[i].len - 1, slen;
  
  <<Check if [[str]] is compatibility decomposed>>
  if(!tlen)
    continue;
  <<Check if [[str]] is compatibility decomposed>>
}
@

<<Check if [[str]] is compatibility decomposed>>=
slen = tlen ? *str++ & UNI_SC_FL_LEN_MASK : *str > 0xffff ? 2 : 1;
if(!tlen)
  tlen++;
for(; slen; slen--, tlen--, str++) {
  if(*str > 0xffff)
    slen--;
  raw_cp_str_t *dec = bsearch(str, dmf_prop->str_arr, dmf_prop->len,
                              sizeof(*dmf_prop->str_arr), uni_cmp_cp);
  if(dec) {
    while(dec > dmf_prop->str_arr && dec[-1].cp == dec->cp)
      dec--;
    if(!dec->flags || dec->flags == 1) {
      fprintf(stderr, "FIXME: nfkd_cf %04X %04X\n", nfkd_cf->str_arr[i].cp, *str);
      exit(1);
    }
  }
}
@

Once finished, they can be dumped.

<<Dump character information as C code>>=
dump_str_tabs(nfd_cf, "NFD_CF", gen_h, tstf);
dump_str_tabs(nfkd_cf, "NFKD_CF", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_NFD_CF);
enable_str_mt_p(prop_NFKD_CF);
@

<<FIXME>>=
share string tables with NFKD_CF or other _decs?
@

Given the above properties, string comparison is pretty simple.  The
main complication is that any character with non-zero canonical
combining class requires accumulation of any adjacent characters with
non-zero ccc, followed by the canonical ordering step.  The only time
this can be skipped is if the entire group matches exactly.

<<[[uni_case_convert]](@sz) cf support>>=
<<[[uni_case_convert]] cf support using [[NFD]]>>
<<[[uni_case_convert]] cf support using [[NFKD]]>>
@

<<[[uni_case_convert]] cf support using (@norm)>>=
/** Case-fold, normalize and compare two UTF-<<@sz>> strings.
  * Normalizes and case-folds \p a/\p alen and \p b/\p blen and compares
  * the results by code point, then length.  Returns -1 if the
  * normalized, case-folded \p a is less than the normalized, case-folded
  * \p b, 1 if it is greater, and zero if they are equal.
  * The strings are normalized to <<@norm>>, and the special casing uses the
  * condition flag <<@cond>>. */
int uni_<<@norm>>_<<@type>>_strcmp<<@sz>>(const uint<<@sz>>_t *a, unsigned int alen,
                                   const uint<<@sz>>_t *b, unsigned int blen);
#define uni_<<@norm>>_<<@type>>_strcmp<<@sz>>(a, alen, b, blen) \
  uni_norm_cf_strcmp<<@sz>>(a, alen, b, blen, <<@cond>>, \
                            uni_<<@norm>>_CF_mtab, uni_<<@norm>>_CF_strs)
@

<<Unicode property exports>>=
<<[[uni_norm_cf_strcmp]][[32]] Prototype>>;
<<[[uni_norm_cf_strcmp]][[16]] Prototype>>;
<<[[uni_norm_cf_strcmp]][[8]] Prototype>>;
@

<<[[uni_norm_cf_strcmp]](@sz) Prototype>>=
/** Case-fold, normalize and compare two UTF-<<@sz>> strings.
  * Normalizes and case-folds \p a/\p alen and \p b/\p blen and compares
  * the results by code point, then length.  Returns -1 if the
  * normalized, case-folded \p a is less than the normalized, case-folded
  * \p b, 1 if it is greater, and zero if they are equal.
  * The strings are normalized using the property values \p ncf_mtab
  * and \p ncf_strs, and the special casing uses the condition flag \p cond. */
int uni_norm_cf_strcmp<<@sz>>(const uint<<@sz>>_t *a, unsigned int alen,
                            const uint<<@sz>>_t *b, unsigned int blen,
			    uint32_t cond,
			    const uint32_t *ncf_mtab, const uint16_t *ncf_strs)
@

<<case.c>>=
<<[[uni_norm_cf_strcmp]][[32]]>>
<<[[uni_norm_cf_strcmp]][[16]]>>
<<[[uni_norm_cf_strcmp]][[8]]>>
@

<<[[uni_norm_cf_strcmp]](@sz)>>=
<<[[uni_norm_cf_strcmp]][[<<@sz>>]] Prototype>>
{
  const uint16_t *arbuf = NULL, *brbuf = NULL;
  uint32_t arlen = 0, brlen = 0;
  uint32_t ac, bc;
  uint32_t abuf[4], *abuf_more = NULL, bbuf[4], *bbuf_more = NULL;
  /* 8 bits for ccc, 24 bits for index into buf */
  uint32_t accc[4], *accc_more = NULL, bccc[4], *bccc_more = NULL;
  unsigned int abuf_len = 0, bbuf_len = 0, abuf_ptr = 0, bbuf_ptr = 0,
               abuf_max = 0, bbuf_max = 0;
  while((alen || arlen) && (blen || brlen)) {
    <<Get next normalized, case folded character from [[a]] using [[ncf]]>>
    <<Get next normalized, case folded character from [[b]] using [[ncf]]>>
    if(ac > bc) {
      <<Free normalization buffers, if allocated>>
      return 1;
    }
    if(ac < bc) {
      <<Free normalization buffers, if allocated>>
      return -1;
    }
  }
  <<Free normalization buffers, if allocated>>
  return blen ? -1 : alen > 0;
}
@

<<Free normalization buffers, if allocated>>=
if(abuf_more)
  free(abuf_more);
if(bbuf_more)
  free(bbuf_more);
if(accc_more)
  free(accc_more);
if(bccc_more)
  free(bccc_more);
@

<<Get next normalized, case folded character from (@ab) using (@stab)>>=
if(<<@ab>>buf_ptr == <<@ab>>buf_len) {
  <<Get next case folded character from [[<<@ab>>]] using [[<<@stab>>]]>>
  uni_ccc_t ccc = uni_ccc_of(<<@ab>>c);
  if(ccc) {
    <<@ab>>ccc[0] = ccc;
    <<@ab>>buf[0] = <<@ab>>c;
    <<@ab>>buf_len = 1;
    while(<<@ab>>len || <<@ab>>rlen) {
      if(<<@ab>>buf_len == 4) {
        <<@ab>>buf_more = malloc(8 * sizeof(*<<@ab>>buf_more));
	<<@ab>>ccc_more = malloc(8 * sizeof(*<<@ab>>ccc_more));
	<<@ab>>buf_max = 8;
	if(!<<@ab>>buf_more || !<<@ab>>ccc_more) {
	  /* no way to indicate mem errors; just return mismatch */
	  <<Free normalization buffers, if allocated>>
#pragma GCC diagnostic push /* STFU GCC */
#pragma GCC diagnostic ignored "-Wtautological-compare"
	  return a == <<@ab>> ? 3 : -3;
	}
	memcpy(<<@ab>>buf_more, <<@ab>>buf, sizeof(<<@ab>>buf));
	memcpy(<<@ab>>ccc_more, <<@ab>>ccc, sizeof(<<@ab>>ccc));
      } else if(<<@ab>>buf_len == <<@ab>>buf_max) {
        <<@ab>>buf_max *= 2;
	uint32_t *t = realloc(<<@ab>>buf_more, <<@ab>>buf_max * sizeof(*t));
	if(!t) {
	  <<Free normalization buffers, if allocated>>
	  return a == <<@ab>> ? 3 : -3;
	}
	<<@ab>>buf_more = t;
	t = realloc(<<@ab>>ccc_more, <<@ab>>buf_max * sizeof(*t));
	if(!t) {
	  <<Free normalization buffers, if allocated>>
	  return a == <<@ab>> ? 3 : -3;
#pragma GCC diagnostic pop
	}
	<<@ab>>ccc_more = t;
      }
      <<Get next case folded character from [[<<@ab>>]] using [[<<@stab>>]]>>
      ccc = uni_ccc_of(<<@ab>>c);
      if(<<@ab>>buf_len < 4) {
        <<@ab>>buf[<<@ab>>buf_len] = <<@ab>>c;
	<<@ab>>ccc[<<@ab>>buf_len] = ccc;
      } else {
        <<@ab>>buf_more[<<@ab>>buf_len] = <<@ab>>c;
	<<@ab>>ccc_more[<<@ab>>buf_len] = ccc;
      }
      <<@ab>>buf_len++;
      if(!ccc)
        break;
    }
    uint32_t *cccp = <<@ab>>buf_len > 4 ? <<@ab>>ccc_more : <<@ab>>ccc;
    unsigned int i;
    /* set back pointer for after sort */
    /* also makes sort stable */
    for(i = 0; i < <<@ab>>buf_len; i++) {
      if(!cccp[i]) /* last one may be 0 */
        break;
      /* << 23 instead of 24 to allow signed compares */
      cccp[i] = (cccp[i] << 23) | i; /* sort by ccc first, then index */
    }
    qsort(cccp, i, sizeof(*cccp), uni_cmp_cp);
    <<@ab>>buf_ptr = 0;
  }
}
if(<<@ab>>buf_ptr != <<@ab>>buf_len) {
  uint32_t *cccp = <<@ab>>buf_len > 4 ? <<@ab>>ccc_more : <<@ab>>ccc;
  uint32_t *bp = <<@ab>>buf_len > 4 ? <<@ab>>buf_more : <<@ab>>buf;
  
  if(cccp[<<@ab>>buf_ptr])
    <<@ab>>c = bp[cccp[<<@ab>>buf_ptr] & ((1 << 23) - 1)];
  else
    <<@ab>>c = bp[<<@ab>>buf_ptr];
  <<@ab>>buf_ptr++;
}
@

[[DerivedNormalizationProps.txt]] contains the final case folding
method%
\footnote{The FC\_NFKC property is related to case folding, but is
deprecated since 6.0.0 and therefore not provided as a property.}%
.  The main difference between it and the other methods is that
it applies multiple transformations simultaneously.  In addition to
case folding, it also removes default ignorable code points and
performs compatibility decomposition on the results repeatedly until
the result is stable.

<<Initialize UCD files>>=
decl_str(NFKC_CF);
@

<<Process a line of [[DerivedNormalizationProps.txt]]>>=
if(num_fields == 3 && !strcmp(fields[1], "NFKC_CF"))
  add_str(NFKC_CF, fields[2]);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_NFKC_CF], "NFKC_CF", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_NFKC_CF);
@

\subsection{Additional Support for Normalization}

The raw property lookup functions are not meant to be used directly. 
Instead, here are some more high-level functions.  They are stored in
their own object file, even though the actual amount of code is tiny.
That way, the tables they reference will not be pulled in for all
properties.

<<Library [[uni]] Members>>=
uni_norm.o
@

\lstset{language=C}
<<Library [[uni]] headers>>=
#include "uni_norm.h"
@

<<uni_norm.h>>=
<<Common C Warning>>
#ifndef UNI_NORM_H
#define UNI_NORM_H
/** \file uni_norm.h High-level normalization support */

#include "uni_prop.h"
/** \addtogroup uni_norm High-level Normalization */
/** @{ */
<<Unicode normalization support exports>>
/** @} */
#endif /* UNI_NORM_H */
@

<<Headers to Install>>=
uni/uni_norm.h \
@

<<uni_norm.c>>=
<<Common C Header>>
#include "uni_all.h"

<<Unicode normalization support local definitions>>

<<Unicode normalization support functions>>
@

The first step in any normalization is to decompose.  The only
difference in the three function groups is what table they use, so
only one group of functions exists; each group just calls the helper
with the appropriate tables.  This is one case where Ada or C++
generics would be very handy; each group of functions is virtually
triplicated with minor changes due to character type.  noweb makes
things additionally complicated by disallowing chunk references inside
preprocessor macro definitions; otherwise, the definition of each
group could have been made using a single preprocessor call.

There are a number of ways a function which modifies a Unicode string
might operate.  For example, it might use separate parameters to
specify input and output strings, or it might always allocate space
for its return value from memory, or it might use the input buffer as
an output buffer as well.  The original versions of these functions
only supported the latter.  However, they assumed that enough room was
available for the output, and moved the tail end of the string around
every time a character's transformed length was not one.  In order to
support checking lengths and resizable output buffers, the input
buffer is provided separately.  The only case where the input buffer
and output buffer are safe to be the same is when the input
buffer length is one character (which may result in [[ilen]] greater
than one in UTF-16 or UTF-8).  No check will be made to ensure that
they do not overlap, though.  To convert an entire string, the user
should duplicate the input string, first.  As a special convenience,
if [[iptr]] is [[NULL]], this is done internally.

<<Unicode normalization support exports>>=
/* wrappers */

/* decomp functions can operate on any number of chars, including 1 */
/* decomp functions return final length */
<<[[uni_X_dec]] Prototypes for [[NFD_dec]]>>
<<[[uni_X_dec]] Prototypes for [[NFKD_dec]]>>
/* NFKC_Casefold assumes NFD has already been run */
<<[[uni_X_dec]] Prototypes for [[NFKC_Casefold]]>>
<<[[do_any_dec]][[32]] Prototype>>;
<<[[do_any_dec]][[16]] Prototype>>;
<<[[do_any_dec]][[8]] Prototype>>;
@

<<[[uni_X_dec]] Prototypes for (@type)>>=
<<[[uni_X_dec]][[32]] Prototype>>;
<<[[uni_X_dec]][[16]] Prototype>>;
<<[[uni_X_dec]][[8]] Prototype>>;
@

<<[[uni_X_dec]](@sz) Prototype>>=
/** Perform full <<@type>> decomposition on UTF-<<@sz>> string.
  * The string \p ibuf/\p ilen is decomposed to <<@type>> form, but without
  * canonical ordering.  If \p ibuf is NULL, the input starts at the same
  * place as the output (i.e., \p *buf or \p *buf + \p off).  The
  * return string parameters are described with \ref uni_return<<@sz>>_buf<<@sz>>. */
int uni_<<@type>><<@sz>>(const uint<<@sz>>_t *ibuf, unsigned int ilen,
                 <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<Unicode normalization support functions>>=
<<[[do_any_dec]][[32]]>>
<<[[do_any_dec]][[16]]>>
<<[[do_any_dec]][[8]]>>
@

<<[[do_any_dec]](@sz) Prototype>>=
/** Perform full generic decomposition on UTF-<<@sz>> string.
  * The string \p ibuf/\p ilen is decomposed using decomposition table
  * \p tab/\p str, but without canonical ordering.  If \p ibuf is NULL,
  * the input starts at the same place as the output (i.e., \p *buf or
  * \p *buf + \p off).  The return string parameters are described with
  * \ref uni_return<<@sz>>_buf<<@sz>>. */
int uni_do_any_dec<<@sz>>(const uint<<@sz>>_t *ibuf, unsigned int ilen,
                       <<Buffer return parameters for UTF-[[<<@sz>>]]>>,
		       const uint32_t *tab, const uint16_t *str)
@

<<[[do_any_dec]](@sz)>>=
<<[[do_any_dec]](@sz) Prototype>>
{
  int i;
  int rlen = 0;
  unsigned int blen = off > 0 ? ~0 : buf_len ? *buf_len : 0, clen = 0;
  uint<<@sz>>_t *bptr = off < 0 && buf ? *buf : NULL;
  if(!ilen)
    return 0;
  uint<<@sz>>_t *ibuftmp = NULL;
  if(!ibuf) {
    /* FIXME: ideally, only duplicate input when output pointer exceeds input */
    /* but for now, just do it always */
    inisize(ibuftmp, ilen);
    cpybuf(ibuftmp, *buf + (off > 0 ? off : 0), ilen);
    ibuf = ibuftmp;
  }
  for(i = 0; i < ilen; i += clen) {
    int32_t doff;
    uint16_t len;
    uint8_t compat;
    const uint32_t *rptr32 = NULL;
    const uint16_t *rptr16 = NULL;
    uint32_t hbuf[3], cp = uni_int_utf<<@sz>>_decode(ibuf + i, &clen);
    uni_x_dec(cp, tab, &doff, &len, &compat, 1);
    if(!len && !doff) {
      len = 1;
      rptr32 = &cp;
    } else if(doff < 0) {
      uni_hangul_syllable_decomp(cp, hbuf, 1);
      rptr32 = hbuf;
    } else
      rptr16 = str + doff;
    len = rptr16 ? uni_return16_buf<<@sz>>(rptr16, len, off < 0 ? &bptr : buf,
	                                   off, off < 0 ? &blen : buf_len)
                 : uni_return32_buf<<@sz>>(rptr32, len, off < 0 ? &bptr : buf,
	                                   off, off < 0 ? &blen : buf_len);
    rlen += len;
    blen -= blen > len ? len : blen;
    if(off >= 0)
      off += len;
    else
      bptr += len;
  }
  if(ibuftmp)
    free(ibuftmp);
  return rlen;
}
@

<<[[uni_dec_any]] for (@type) using (@tab) and (@str)>>=
<<[[uni_dec_any]][[32]]>>
<<[[uni_dec_any]][[16]]>>
<<[[uni_dec_any]][[8]]>>
@

<<[[uni_dec_any]](@sz)>>=
#define uni_<<@type>><<@sz>>(ibuf, ilen, buf, off, buf_len) \
  uni_do_any_dec<<@sz>>(ibuf, ilen, buf, off, buf_len, <<@tab>>, <<@str>>)
@

<<Unicode normalization support exports>>=
<<[[uni_dec_any]] for [[NFD_dec]] using [[uni_canon_decomp_mtab]] and [[uni_dm_strs]]>>
<<[[uni_dec_any]] for [[NFKD_dec]] using [[uni_compat_decomp_mtab]] and [[uni_dm_strs]]>>
<<[[uni_dec_any]] for [[NFKC_Casefold]] using [[uni_NFKC_CF_mtab]] and [[uni_NFKC_CF_strs]]>>
@

Canonical ordering is the second step in any normalization.  This
requires that any consecutive characters with non-zero canonical
combining class be ordered by their canonial combining class.  The
procedure is described as a bubble sort, so the sort must be stable.
These two requirements indicate the need for a side array for sorting,
containing the original position of each character along with its ccc
value.  The code point itself is stored as well, so that no effort is
needed to write out the sorted array without clobbering the old string.
If the last character passed in has a non-zero ccc, there may be more,
so a continuation is requested.  Any characters which have already
been processed, and could not possibly be reordered, are returned as
output.

Since this is just a reordering of characters, there is no need to
provide resizable output or separate input and output buffers.
Instead, only an in-place sorter is provided.

<<Unicode normalization support exports>>=
<<[[canon_order]][[32]] Prototype>>;
<<[[canon_order]][[16]] Prototype>>;
<<[[canon_order]][[8]] Prototype>>;
@

<<[[canon_order]](@sz) Prototype>>=
/** Perform canonical ordering step of normalization on a UTF-<<@sz>> string.
  * This performs in-place ordering of \p buf of length \p ilen.  It may
  * be used to order partial strings, in which case \p last should be set
  * to true when no more characters are available.  It returns the number
  * of code points processed; if it is less than \p ilen, then more
  * code points are needed before ordering can be performed on the remaining
  * text (which can only happen if \p last is false). */
int uni_Canon_Order<<@sz>>(uint<<@sz>>_t *buf, unsigned int ilen, int last)
@

<<Unicode normalization support functions>>=
<<[[canon_order]][[32]]>>
<<[[canon_order]][[16]]>>
<<[[canon_order]][[8]]>>
@

<<Unicode normalization support local definitions>>=
/* for stable sort */
struct ccs {
  uint32_t cp /* :24 */, ccc /* :8 */;
  unsigned int opos;
};

static int cmpcc(const void *a, const void *b)
{
  const struct ccs *p1 = (const struct ccs *)a;
  const struct ccs *p2 = (const struct ccs *)b;
  if(p1->ccc != p2->ccc)
    return (int32_t)p1->ccc - (int32_t)p2->ccc;
  return (int)p1->opos - (int)p2->opos;
}
@

<<[[canon_order]](@sz)>>=
<<[[canon_order]][[<<@sz>>]] Prototype>>
{
  int i;
  for(i = 0; i < ilen; i += uni_utf<<@sz>>_nextc(buf + i)) {
    uint32_t cp = uni_int_utf<<@sz>>_decode(buf + i, NULL);
    int cc = uni_ccc_of(cp);
    if(cc) {
      int ni = i + uni_utf<<@sz>>_nextc(buf + i);
      int j, l;
      for(l = 1, j = ni; j < ilen; j += uni_utf<<@sz>>_nextc(buf + j), l++) {
        uint32_t cp2 = uni_int_utf<<@sz>>_decode(buf + j, NULL);
	int cc2 = uni_ccc_of(cp2);
	if(!cc2)
	  break;
      }
      if(j == ilen && !last)
        return i;
      struct ccs ccbuf[l];
      ccbuf[0].ccc = cc;
      ccbuf[0].cp = cp;
      ccbuf[0].opos = i;
      int k;
      for(l = 1, k = ni; k < j; k += uni_utf<<@sz>>_nextc(buf + k), l++) {
        uint32_t cp2 = uni_int_utf<<@sz>>_decode(buf + k, NULL);
	ccbuf[l].ccc = uni_ccc_of(cp2);
	ccbuf[l].cp = cp2;
	ccbuf[l].opos = k;
      }
      qsort(ccbuf, l, sizeof(*ccbuf), cmpcc);
      for(k = 0, j = i; k < l; k++, j += uni_utf<<@sz>>_nextc(buf + j))
        if(ccbuf[k].opos != j)
	  (void)uni_int_utf<<@sz>>_encode(buf + j, ccbuf[k].cp);
    }
  }
  return ilen;
}
@

The last step in any composition normalization is canonical
composition.  Two characers may be combined if the first character has
a canonical combining class of zero and all intervening characters
have a non-zero canonical combining class less than the canonical
combining class of the second character.  This requires a continuation
flag, just like canonical ordering.  However, since this is once again
a function which can transform its input, more thought is required.
Unlike decomposition, it is impossible for a string to expand in
length.  This means that in-place conversion is possible.  In fact,
just like canonical ordering, only in-place transformation is supported.

<<Unicode normalization support exports>>=
<<[[uni_NFC_comp]][[32]] Prototype>>;
<<[[uni_NFC_comp]][[16]] Prototype>>;
<<[[uni_NFC_comp]][[8]] Prototype>>;
@

<<[[uni_NFC_comp]](@sz) Prototype>>=
/** Perform canonical composition step of normalization for UTF-<<@sz>> string.
  * Performs composition in-place on \p buf of length \p ilen.  The length
  * of the result is returned.  This can be performed on partial
  * strings; set \p nok to a non-NULL pointer to indicate that more data
  * may be available.  The number of code points in the result which are
  * fully processed is returned in \p *nok.  If this is less than the
  * function's return value, the remaining code points in the result
  * have not yet been processed, and must be passed in again along with
  * additional data. */
int uni_NFC_comp<<@sz>>(uint<<@sz>>_t *buf, unsigned int ilen, unsigned int *nok)
@

<<Unicode normalization support functions>>=
<<[[uni_NFC_comp]][[32]]>>
<<[[uni_NFC_comp]][[16]]>>
<<[[uni_NFC_comp]][[8]]>>
@

<<[[uni_NFC_comp]](@sz)>>=
<<[[uni_NFC_comp]][[<<@sz>>]] Prototype>>
{
  int i, last0 = -1;
  for(i = 0; i < ilen; i += uni_utf<<@sz>>_nextc(buf + i)) {
    uint32_t cp = uni_int_utf<<@sz>>_decode(buf + i, NULL);
    int ccc = uni_ccc_of(cp);
    /* first char must be ccc=0 */
    if(ccc)
      continue;
    int32_t off;
    uint16_t len;
    uni_find_canon_comp(cp, &off, &len);
    if(!len && !off)
      continue; /* no compositions for this char */
    last0 = i;
    int lastccc = 0;
    int j;
    for(j = i + uni_utf<<@sz>>_nextc(buf + i); j < ilen;
        j += uni_utf<<@sz>>_nextc(buf + j)) {
      uint32_t cp2 = uni_int_utf<<@sz>>_decode(buf + j, NULL);
      ccc = uni_ccc_of(cp2);
      /* second char must have no intervening equal ccc */
      /* this assumes canonical ordering has been done */
      if(ccc && lastccc && ccc == lastccc)
        continue;
      /* or greater ccc; this only happens if back down to 0 */
      if(!ccc && lastccc) {
        i = j - 1;
	break;
      }
      lastccc = ccc;
      int ccp = uni_canon_comp(cp, cp2, off, len);
      if(ccp > 0) {
        /* composition replaces char 1 and deletes char 2 */
	int clen = uni_utf<<@sz>>_enclen(ccp);
	int len1 = uni_utf<<@sz>>_nextc(buf + i), len2 = uni_utf<<@sz>>_nextc(buf + j);
	/* make room for ccp, if necessary */
	if(len1 != clen) {
	  /* WARNING: this may cause buffer overrun if */
	  /*  clen > len1 + len2 */
	  movebuf(buf + i + clen, buf + i + len1, j - i - len1);
	  len2 += len1 - clen;
	}
	(void)uni_int_utf<<@sz>>_encode(buf + i, ccp);
	if(len2 > 0 && ilen - j - len2)
	  movebuf(buf + j, buf + j + len2, ilen - j - len2);
        ilen -= len2;
	/* start over with new start char */
	cp = ccp;
	uni_find_canon_comp(cp, &off, &len);
	if(!len && !off)
	  break; /* no compositions for this char */
        j = i;
	lastccc = 0;
	continue;
      }
      /* if no match and hit a ccc=0, try next 1st char */
      if(!ccc) {
        i = j + uni_utf<<@sz>>_prevc(buf + j);
	break;
      }
    }
  }
  if(nok) {
    /* need more if we got a potential start char */
    /* but don't need to look at anything before that char */
    if(last0 < 0)
      *nok = ilen;
    else {
      int32_t off;
      uint16_t len;
      uni_find_canon_comp(uni_int_utf<<@sz>>_decode(buf + last0, NULL), &off, &len);
      *nok = len || off ? last0 : ilen;
    }
  }
  return ilen;
}
@

The above routine may cause buffer overruns if the composition causes
the string to grow.  This happens when the composition result's word
length exceeds the sum of the composed characters' word lengths.  This
can never happen with 32-bit words, because all lengths are one.  This
can never happen with 16-bit words, because the maximum composed
character length is two, and the minimum lengths of the components are
one.  However, with 8-bit words, the maximum result length is 4, and
the minimum component lengths are one.  This leaves plenty of room for
growth.  To ensure this never happens, a check is made while reading
the data.  The only data which does not need to be checked is Hangul
syllable composition: all three are always of length 3.  For the
versions against which this library was built, the check has
succeeded.  If it ever fails, the composition code will need to be
rewritten to support resizable buffers.

<<Parse UCD files>>=
for(i = 0; i < cm_prop->len; i++) {
  const uint32_t *bctab = cm_prop->strs + cm_prop->str_arr[i].off;
  int len1 = uni_utf8_enclen(cm_prop->str_arr[i].cp);
  for(j = 0; j < cm_prop->str_arr[i].len; j += 2) {
    int len2 = uni_utf8_enclen(bctab[j]);
    int clen = uni_utf8_enclen(bctab[j + 1]);
    if(clen > len1 + len2) {
      fprintf(stderr, "Compose UTF-8 overflow: %04X(%d) + %04X(%d) < %04X(%d)\n",
                      (int)cm_prop->str_arr[i].cp, len1, (int)bctab[j], len2,
		      (int)bctab[j + 1], clen);
      exit(1);
    }
  }
}
@

<<Additional parse-ucd includes>>=
#include "uni_io.h" /* uni_utf8_enclen() */
@

<<Additional parse-ucd C files>>=
uni_io.h \
@

To demonstrate proper normalization, here are some functions which do the
full procedure, either in-place or using a separate input buffer.

<<Unicode normalization support exports>>=
<<[[full_dec]] Prototypes for [[NFD]]>>
<<[[full_dec]] Prototypes for [[NFKD]]>>
@

<<[[full_dec]] Prototypes for (@type)>>=
<<[[full_dec]][[32]] Prototype>>;
<<[[full_dec]][[16]] Prototype>>;
<<[[full_dec]][[8]] Prototype>>;
@

<<[[full_dec]](@sz) Prototype>>=
<<[[uni_dec]] Prototype>>
@

<<[[uni_dec]] Prototype>>=
/** Perform full <<@type>> normalization on a UTF-<<@sz>> string.
  * The string \p ibuf/\p ilen is normalized to <<@type>> form.
  * If \p ibuf is NULL, the input starts at the same place as the output
  * (i.e., \p *buf or \p *buf + \p off).  The return string parameters are
  * described with \ref uni_return<<@sz>>_buf<<@sz>>. */
int uni_<<@type>><<@sz>>(uint<<@sz>>_t *ibuf, unsigned int ilen,
                 <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<Unicode normalization support functions>>=
<<[[full_dec]] for [[NFKD]]>>
<<[[full_dec]] for [[NFD]]>>
@

<<[[full_dec]] for (@type)>>=
<<[[full_dec]][[32]]>>
<<[[full_dec]][[16]]>>
<<[[full_dec]][[8]]>>
@

<<[[full_dec]](@sz)>>=
<<[[uni_dec]] Prototype>>
{
  <<Decompose using [[<<@type>>]]>>
  return ret;
}
@

<<Decompose using (@type)>>=
if(!ilen)
  return 0;
int ret = uni_<<@type>>_dec<<@sz>>(ibuf, ilen, buf, off, buf_len);
if(!*buf || (off < 0 && ret > *buf_len))
  return ret;
ret = uni_Canon_Order<<@sz>>(*buf + (off > 0 ? off : 0), ret, 1);
if(ret <= 0)
  return ret;
@

<<Unicode normalization support exports>>=
<<[[full_dec]] Prototypes for [[NFC]]>>
<<[[full_dec]] Prototypes for [[NFKC]]>>
@

<<Unicode normalization support functions>>=
<<[[full_comp]] for [[NFKC]] using [[NFKD]]>>
<<[[full_comp]] for [[NFC]] using [[NFD]]>>
@

<<[[full_comp]] for (@type) using (@dec)>>=
<<[[full_comp]][[32]]>>
<<[[full_comp]][[16]]>>
<<[[full_comp]][[8]]>>
@

<<[[full_comp]](@sz)>>=
<<[[uni_dec]] Prototype>>
{
  <<Decompose using [[<<@dec>>]]>>
  return uni_NFC_comp<<@sz>>(*buf + (off > 0 ? off : 0), ret, NULL);
}
@

Note that [[NFKC_Casefold]] is not a simple in-place function, so the
intermediate result is always duplicated in the last step.  This
should probably be fixed to be less expensive.

<<Unicode normalization support exports>>=
<<[[full_dec]] Prototypes for [[NFKC_CF]]>>
@

<<Unicode normalization support functions>>=
<<[[NFKC_CF]] normalizer using UTF-[[32]]>>
<<[[NFKC_CF]] normalizer using UTF-[[16]]>>
<<[[NFKC_CF]] normalizer using UTF-[[8]]>>
@

<<(@type) normalizer using UTF-(@sz)>>=
<<[[uni_dec]] Prototype>>
{
  <<Decompose using [[NFD]]>>
  return uni_NFKC_Casefold<<@sz>>(NULL, ret, buf, off, buf_len);
}
@

String comparison using this method should be supported as well.  While
this method technically requires canonical composition as its last
step and canonical decomposition as its first step, both of these can
be skipped when simply using the method for string comparison. Here
are the steps, in detail:

\begin{enumerate}
\item Decomposition using the canonical decomposition tables.
\item Canonical ordering.
\item Decomposition, default ignorable removal, and case folding using
the NFKC\_CF table
\item Decomposition using the canonical decomposition tables.
\item Canonical ordering.
\item Canonical composition.
\end{enumerate}

The second step does not affect the third step, since decomposition
applies to individual characters without context.  It can be dropped,
since the fifth step repeats the process.  The first step does not
affect the third step, because the third step performs the same
transformation, and more.  The fourth step does not have any effect,
because the third step produces fully decomposed text.  The last step
is losslessly reversible using decomposition, so for comparison, there
is no need to compose, either.  That leaves only two steps:  applying
the NFKC\_CF table, and applying canonical ordering.  That means that
the comparison function using NFKC\_CF is identical to the normalizing
case folding functions above, except in how to use the tables.

<<Unicode property exports>>=
<<[[uni_NFKC_CF_strcmp]][[32]] Prototype>>;
<<[[uni_NFKC_CF_strcmp]][[16]] Prototype>>;
<<[[uni_NFKC_CF_strcmp]][[8]] Prototype>>;
@

<<[[uni_NFKC_CF_strcmp]](@sz) Prototype>>=
/** Case-fold, normalize and compare two UTF-<<@sz>> strings.
  * Normalizes and case-folds \p a/\p alen and \p b/\p blen using NFKC_CF
  * and compares the results by code point, then length.  Returns -1 if the
  * normalized, case-folded \p a is less than the normalized, case-folded
  * \p b, 1 if it is greater, and zero if they are equal.  */
int uni_NFKC_CF_strcmp<<@sz>>(const uint<<@sz>>_t *a, unsigned int alen,
                             const uint<<@sz>>_t *b, unsigned int blen)
@

<<case.c>>=
<<[[uni_NFKC_CF_strcmp]][[32]]>>
<<[[uni_NFKC_CF_strcmp]][[16]]>>
<<[[uni_NFKC_CF_strcmp]][[8]]>>
@

<<[[uni_NFKC_CF_strcmp]](@sz)>>=
<<[[uni_NFKC_CF_strcmp]][[<<@sz>>]] Prototype>>
{
  const uint16_t *arbuf = NULL, *brbuf = NULL;
  uint32_t arlen = 0, brlen = 0;
  uint32_t ac, bc;
  uint32_t abuf[4], *abuf_more = NULL, bbuf[4], *bbuf_more = NULL;
  /* 8 bits for ccc, 24 bits for index into buf */
  uint32_t accc[4], *accc_more = NULL, bccc[4], *bccc_more = NULL;
  unsigned int abuf_len = 0, bbuf_len = 0, abuf_ptr = 0, bbuf_ptr = 0,
               abuf_max = 0, bbuf_max = 0;
  while((alen || arlen) && (blen || brlen)) {
    <<Get next normalized, case folded character from [[a]] using [[uni_NFKC_CF]]>>
    <<Get next normalized, case folded character from [[b]] using [[uni_NFKC_CF]]>>
    if(ac > bc) {
      <<Free normalization buffers, if allocated>>
      return 1;
    }
    if(ac < bc) {
      <<Free normalization buffers, if allocated>>
      return -1;
    }
  }
  <<Free normalization buffers, if allocated>>
  return blen ? -1 : alen > 0;
}
@

<<Get case conversion result for next character using [[uni_NFKC_CF]]>>=
uint32_t lucp = uni_int_utf<<@sz>>_decode(<<@ab>>, &clen);
const uni_str_ptr_t *p = uni_NFKC_CF_of(lucp);
if(!p->off && !p->len) {
  <<@ab>>rbuf = NULL;
  <<@ab>>rlen = lucp;
} else {
  <<@ab>>rbuf = uni_NFKC_CF_strs + p->off;
  <<@ab>>rlen = p->len;
}
@

<<FIXME>>=
add NFD_CF and NFKD_CF as generic normalization routines?
What about NFC_CF and NFKC_CF w/o ignorable removal?
Add strchr and strstr equivalents, or return special values if at
  end-of-string (since I crop to 1/-1, just return 10/-10 at end-of-string)
    thus an inefficient strcasestr could be something like:
      while(1) {
        int ret = uni_NFKC_CF_strcmp@sz(a, alen, b, blen);
	if(ret == -10)
	  return NULL;
	if(ret == 10 || !ret)
	  return a;
	/* advance to next fjkc_cf char in a & adjust alen */
      }
@

\subsection{Generic Normalized Unicode Input}

To make things even easier, we can make a function which returns a
normalized character from an input stream.  Any normalization type we
have code for, we support.

<<Unicode normalization support exports>>=
/** Normalization types for \ref uni_fgetc_norm */
typedef enum {
  UNI_NORM_NONE, /**< No normalization */
  UNI_NORM_NFD, /**< Canonical Decomposition normalization */
  UNI_NORM_NFKD, /**< Compatibility Decomposition normalization */
  UNI_NORM_NFC, /**< Canonical Composition normalization */
  UNI_NORM_NFKC, /**< Compatibility Composition normalization */
  UNI_NORM_NFKC_CF /**< Compatibility Composition normalization
                     * with Case Folding and Ignorable removal */
} uni_normtype_t;
@

<<Known Data Types>>=
uni_normtype_t,%
@

<<Unicode normalization support exports>>=
#include "uni_io.h"

/** Read a code point from a file after normalization.
  * The contents of the underlying file are transparently normalized
  * according to \p nt, and the next code point is returned.  The entire
  * file must be read using this function in order for it to work
  * correctly.  Changing \p nt mid-stream is also not supported, since
  * any normalization in progress will not be affected.  Errors are
  * ignored; invalid Unicode in the file is converted to U+FFFD, REPLACEMENT
  * CHARACTER. */
int32_t uni_fgetc_norm(uni_file_t *uf, uni_normtype_t nt);
@

<<Unicode normalization support functions>>=
int32_t uni_fgetc_norm(uni_file_t *uf, uni_normtype_t nt)
{
  <<Get normalized character>>
}
@

<<FIXME>>=
add NFD_CF and NFKD_CF to generic normalization routines?
What about NFC_CF and NFKC_CF w/o ignorable removal?
@

A simple function which returns a normalized character requires that
the current decomposition and composition state be kept.  It would be
ideal to extend the [[uni_file_t]] structure with the extra information
rather than allocating yet another wrapper, but for now I would prefer
[[uni_file_t]] to stay isolated to unnormalized I/O.

In order to make this transparent, the extra support structure is
automatically created and placed on a linked list.  It is assumed that
the file will not be closed and reopened without an intervening EOF.
It is also assumed that the function will not be called any more after
it returns EOF.

<<Unicode normalization support local definitions>>=
typedef struct unifile_norm {
  struct unifile_norm *next;
  uni_file_t *uf;
  <<Normalized input buffer state>>
} unifile_norm_t;
static unifile_norm_t *norm_files = NULL;
@

<<Known Data Types>>=
unifile_norm_t,%
@

<<Get normalized character>>=
unifile_norm_t *ufn = norm_files;

while(ufn && ufn->uf != uf)
  ufn = ufn->next;
if(!ufn) {
  inisize(ufn, 1);
  clearbuf(ufn, 1);
  ufn->next = norm_files;
  norm_files = ufn;
}

<<Return normalized character if not EOF>>

unifile_norm_t **bufp;
for(bufp = &norm_files; *bufp != ufn; bufp = &(*bufp)->next);
*bufp = ufn->next;
free(ufn);

return -1;
@

For support, we'll need to store at least the maximal decomposition of
a character.

<<Normalized input buffer state>>=
uint32_t *buf;
unsigned int buf_len;
@

We'll also need to know how many characters are in the buffer, and how
many of those are good to go.  Since more than one may be good to go,
a pointer to the next character to go is kept as well.  Since the last
good-to-go character may be the last in the file, a flag is added to
detect this.

<<Normalized input buffer state>>=
int len, oklen, okptr, eof;
@

To return a single character, we'll need to normalize until at least
one character could not possibly be modified any more (i.e., [[oklen]]
is non-zero).

<<Return normalized character if not EOF>>=
unsigned int oklen = ufn->oklen;
while(!oklen && !ufn->eof) {
  /* read a char into end of buf, if needed & possible */
  <<Read and normalize a character>>
}
if(oklen) {
  /* now we have oklen chars in buf that are ready to return */
  int c = ufn->buf[ufn->okptr++];
  if(ufn->okptr == oklen) {
    ufn->okptr = ufn->oklen = 0;
    if(ufn->len > oklen)
      movebuf(ufn->buf, ufn->buf + oklen, ufn->len - oklen);
    ufn->len -= oklen;
  } else
   ufn->oklen = oklen;
  return c;
} else if(ufn->eof)
  return -1;
@

To add more characters, the decomposition of the next read character
is appended to the buffer.  The total buffer length after
decomposition is stored in [[bp]].  The Unicode standard states that
case folding requires an additional NFD step beforehand, but reading
of the data suggests that instead, it needs canonical ordering and
composition afterwards, just like the NFC and NFKC forms.  This makes
case folding just another decomposition method.

Switching forms mid-stream is not really supportable.  Any characters
in the buffer have already been processed, at least using the
decomposition method which was in effect before the switch.  The only
compromise is to switch as soon as possible, performing no additional
work on the buffered characters.  A custom routine should be used if
mid-stream switching is desired.

<<Read and normalize a character>>=
int32_t c;
int bp = ufn->len;
if(bp && nt == UNI_NORM_NONE) {
  oklen = 1;
  break;
}
c = uni_fgetc(uf);
if(c == -1)
  ufn->eof = 1;
if(c < -1)
  c = 0xFFFD; /* recommended REPLACEMENT CHARACTER */
if(nt == UNI_NORM_NONE)
  return c; /* don't bother allocating buf */
if(c != -1)
  switch(nt) {
    case UNI_NORM_NONE: /* here to remove warning */
    case UNI_NORM_NFD:
    case UNI_NORM_NFC:
      bp += uni_NFD_dec32((uint32_t *)&c, 1, &ufn->buf, bp, &ufn->buf_len);
      break;
    case UNI_NORM_NFKD:
    case UNI_NORM_NFKC:
      bp += uni_NFKD_dec32((uint32_t *)&c, 1, &ufn->buf, bp, &ufn->buf_len);
      break;
    case UNI_NORM_NFKC_CF:
      bp += uni_NFKC_Casefold32((uint32_t *)&c, 1, &ufn->buf, bp, &ufn->buf_len);
  }
@

Then, the entire set of characters already read in can be ordered.  The
number of ordered characters is saved in [[oblen]].  If no characters
can be ordered, try to read more right away.

<<Read and normalize a character>>=
int oblen = uni_Canon_Order32(ufn->buf, bp, ufn->eof);
if(!oblen) {
  ufn->len = bp;
  continue;
}
@

Then, all characters which have been ordered can be composed.  If
composition might need more characters, the next trip around the loop
will get them.

<<Read and normalize a character>>=
switch(nt) {
  case UNI_NORM_NONE: /* here to remove warning */
  case UNI_NORM_NFD:
  case UNI_NORM_NFKD:
    /* no composition */
    oklen = oblen;
    break;
  case UNI_NORM_NFC:
  case UNI_NORM_NFKC:
  case UNI_NORM_NFKC_CF: {
    int cblen = uni_NFC_comp32(ufn->buf, oblen, ufn->eof ? NULL : &oklen);
    if(ufn->eof)
      oklen = cblen;
    /* move unordered characters down if composition removed chars */
    if(cblen < oblen) {
      if(bp > oblen)
        movebuf(ufn->buf + cblen, ufn->buf + oblen, bp - oblen);
      bp -= oblen - cblen;
    }
    break;
  }
}
ufn->len = bp;
@

\subsection{Testing - Normalization}

To fully demonstrate that the normalization data is correct, the
official test suite is run.  This is provided by the UCD in
NormalizationTest.txt.  This time, rather than compiling the file's
location in, it is simply fed into standard input.

\lstset{language=make}
<<C Test Support Executables>>=
tstnorm \
@

<<Additional Tests>>=
./tstnorm <$(UCD_LOC)/NormalizationTest.txt
@

\lstset{language=C}
<<tstnorm.c>>=
<<Common C Header>>
#include "uni_all.h"

/* longest line == 587 chars */
char lbuf[1024];

/* longest string == 18 chars */
uint32_t ibuf[5][128], ilen[5];
uint32_t obuf[128];
uint32_t * /* const */ obptr = obuf;
/* const */ unsigned int oblen = 128;

int main(void)
{
  int ret = 0;
  <<Read and process NormalizationTest.txt>>
  return ret;
}
@

First, we need to track when we're in part 1, and mark every code
point encountered there as having been processed.  According to the
test, after exiting part 1, we can run all four standard
normalizations on any code point not encountered and get no effect.

In general, when entering a new section, print the section name.
Otherwise, print a dot after every 50 tests.

<<Read and process NormalizationTest.txt>>=
char *didnorm;
inisize(didnorm, 0x110000);
clearbuf(didnorm, 0x110000);
int inpart1 = 0;
int ntests = 0;
while(fgets(lbuf, sizeof(lbuf), stdin)) {
  if(*lbuf == '#')
    continue;
  if(*lbuf == '@') {
    if(ntests)
      putchar('\n');
    if(lbuf[5] == '1')
      inpart1 = 1;
    else if(inpart1) {
      inpart1 = 0;
      int i;
      for(i = 0; i < 0x110000; i++)
        if(!didnorm[i]) {
	  <<Test normalization does not affect [[i]]>>
	}
    }
    fputs(lbuf, stdout);
    continue;
  }
  if(inpart1) {
    int cp = strtol(lbuf, NULL, 16);
    didnorm[cp] = 1;
  }
  <<Run normalization test for [[lbuf]]>>
  if(!(++ntests % 50)) {
    putchar('.');
    fflush(stdout);
  }
}
putchar('\n');
@

First, let's parse a line into the input buffers.  Each line has five
semicolon-separated fields, terminated by a semicolon.  Each field has
space-separated hexadecimal numbers.

<<Run normalization test for [[lbuf]]>>=
int i;
char *s = lbuf;
for(i = 0; i < 5; i++) {
  int l = 0;
  while(1) {
    ibuf[i][l++] = strtol(s, &s, 16);
    while(isspace(*s))
      s++;
    if(*s == ';')
      break;
  }
  ilen[i] = l;
  if(*s == ';')
    s++;
  else {
    fprintf(stderr, "bad line %s\n", lbuf);
    exit(1);
  }
}
@

Then, run the conformance tests.

<<Run normalization (@type) on fields (@i0) through (@il) to produce field (@r)>>=
for(i = <<@i0>>; i < <<@il>>; i++) {
  cpybuf(obuf, ibuf[i], ilen[i]);
  int l = uni_<<@type>>32(NULL, ilen[i], &obptr, -1, &oblen);
  if(l != ilen[<<@r>>] || cmpbuf(obuf, ibuf[<<@r>>], l)) {
    ret = 1;
    fprintf(stderr, "Failed <<@type>> test %d/%d on %s\nGot: ",
                    <<@r>> + 1, i + 1, lbuf);
    int j;
    for(j = 0; j < l; j++)
      fprintf(stderr, " %04X", obuf[j]);
    fputs("\nExpected: ", stderr);
    for(j = 0; j < ilen[<<@r>>]; j++)
      fprintf(stderr, " %04X", ibuf[<<@r>>][j]);
    fputc('\n', stderr);
    continue;
  }
}
@

<<Run normalization test for [[lbuf]]>>=
<<Run normalization [[NFC]] on fields [[0]] through [[3]] to produce field [[1]]>>
<<Run normalization [[NFC]] on fields [[3]] through [[5]] to produce field [[3]]>>
@

<<Run normalization test for [[lbuf]]>>=
<<Run normalization [[NFD]] on fields [[0]] through [[3]] to produce field [[2]]>>
<<Run normalization [[NFD]] on fields [[3]] through [[5]] to produce field [[4]]>>
@

<<Run normalization test for [[lbuf]]>>=
<<Run normalization [[NFKC]] on fields [[0]] through [[5]] to produce field [[3]]>>
@

<<Run normalization test for [[lbuf]]>>=
<<Run normalization [[NFKD]] on fields [[0]] through [[5]] to produce field [[4]]>>
@

<<Test normalization does not affect [[i]]>>=
obuf[0] = i;
unsigned int l1 = 1;
if(uni_NFC32(obuf, 1, &obptr, -1, &l1) != 1 || obuf[0] != i ||
   uni_NFD32(obuf, 1, &obptr, -1, &l1) != 1 || obuf[0] != i ||
   uni_NFKC32(obuf, 1, &obptr, -1, &l1) != 1 || obuf[0] != i ||
   uni_NFKD32(obuf, 1, &obptr, -1, &l1) != 1 || obuf[0] != i) {
  fprintf(stderr, "Failed Part1 end check at %04x\n", i);
  ret = 1;
}
@

\subsection{Parsing the UCD -- Names}

An ``Other'' property which can be treated as a string for now is
the na property, the last property to be retrieved from
[[UnicodeData.txt]]%
\footnote{Fields 11 and 12 (isc and na1) are obsolete and informative,
so they will not be read in as a property unless I find a use for
them.  na1 was useful prior to 6.2, since it contained names for
control characters, but those are now officially aliases.}%
.  This includes the Name\_Alias property (from [[NameAliases.txt]]%
\footnote{The third field of [[NameAliases.txt]] (Type) is
informative, and will not be read in as a property unless I find a use.}%
), as well.

Since these names are always pure ASCII, storing one character per
32-bit word is even more wasteful than above:  more than 25 bits are
always wasted on each character.  For this reason, the string table
will be dumped as 8-bit characters.  The prerequisite format is to
cast the 32-bit string pointer to an 8-bit string pointer, and pad
with zeroes to the next 32-bit boundary.  In fact, since they are
always upper-case letters, digits, spaces, and hyphens, they could be
represented as 6 bits per word, but that amount of compression could
just end up being too expensive to extract.

Some characters' names are generated by appending the hexadecimal code
point to a range name.  The Hangul syllable characters also have
specially generated names.  In both cases, UnicodeData.txt simply
indicates a range.  Since code point tables are not good at storing
ranges, these are stored in a separate property, na\_rng.  This
includes all of the like-named <control> characters.  In addition,
since the CJK COMPATIBILITY IDEOGRAPH characters (U+F900-U+FAD9,
U+2F800-U+2FA1D) are named as though they were numbered ranges, they
are stored in that property as well.  For now, the na\_rng property is
stored as a plain string property, with each end point in the range
table.  The even-numbered table entries are always the low end of the
range, and the next entry the high end.

The actual names used in UnicodeData.txt are not always relevant.  In
fact, only the CJK ideographs and Hangul syllables have real names;
the rest have ``labels.'' To distinguish real names from labels, the
label has a leading <, which is meant to be terminated by a closing >
after the code point is appended.  The extensions have group names (A,
B, C, D) that need to be stripped.  All surrogate types are actually
just named surrogate, and all other private use types are just named
private-use.

<<UCD parser local definitions>>=
#define add_str8(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[64]; \
    uint32_t len = strlen(v); \
    str[len / 4] = 0; \
    memcpy(str, v, len); \
    len = (len + 3) / 4; \
    add_str_rng(&parsed_props[prop_##n], low, high, str, len); \
    parsed_props[prop_##n].strs_char_size = 8; \
  } \
} while(0)
@

<<Initialize UCD files>>=
decl_str(na);
decl_str(Name_Alias);
decl_str(na_rng);
@

<<Process a line of [[UnicodeData.txt]]>>=
if(fields[1][0] != '<' && (high < 0xF900 || low > 0xFAD9) &&
   (high < 0x2F800 || low > 0x2FA1D))
  add_str8(na, fields[1]);
else {
  uint32_t h = high;
  high = low;
  if(fields[1][0] == '<' && (s = strchr(fields[1], ','))) {
    *s = 0;
    /* standard claims all names are upper-case, but ranges have lower-case */
    for(s = fields[1]; *s; s++)
      if(islower(*s))
        *s = toupper(*s);
    if(strstr(fields[1] + 1, "CJK IDEOGRAPH")) {
      if((s = strstr(fields[1] + 1, "EXTENSION")))
        s[9] = 0;
    } else if(strstr(fields[1] + 1, "SURROGATE"))
      strcpy(fields[1] + 1, "<surrogate");
    else if(strstr(fields[1] + 1, "PRIVATE USE"))
      strcpy(fields[1] + 1, "<private-use");
    add_str8(na_rng, fields[1] + 1);
    low = high = h;
    add_str8(na_rng, fields[1] + 1);
  } else if(low == 0xF900 || low == 0x2F800) {
    *strchr(fields[1], '-') = 0;
    add_str8(na_rng, fields[1]);
    high = low = low == 0xF900 ? 0xFAD9 : 0x2FA1D;
    add_str8(na_rng, fields[1]);
  } else if(!strcmp(fields[1], "<control>")) {
    prop_t *na_rng = prop_na_rng < 0 ? NULL : &parsed_props[prop_na_rng];
    if(na_rng && na_rng->len &&
       na_rng->str_arr[na_rng->len - 1].cp == h - 1)
      na_rng->str_arr[na_rng->len - 1].cp = h;
    else {
      low = high = h;
      fields[1][8] = 0;
      add_str8(na_rng, fields[1]);
      add_str8(na_rng, fields[1]);
    }
  } /* else CJK COMPATIBILITY other than low or some unknown crap */
}
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_na], "Name", gen_h, tstf);
dump_str_tabs(&parsed_props[prop_na_rng], "Name synthetic range", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_na);
@

In addition, the Hangul syllable names require the name of each part,
also known as the Jamo\_Short\_Name (JSN) property.  It is extracted
from [[Jamo.txt]].  It has one blank entry, requiring a bypass of the
standard string adder.

<<Initialize UCD files>>=
decl_str(JSN);
@

<<Parse UCD files>>=
open_f("Jamo.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_str8(JSN, fields[1]);
  if(!*fields[1])
    add_str_rng(&parsed_props[prop_JSN], low, high, NULL, 0);
}
fclose(f);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_JSN], "Jamo_Short_name", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_JSN);
@

Unfortunately, the Name\_Alias property may have multiple values per
code point.  To fix this, all aliases are combined into a single
string, prefixing each with its length.

<<Parse UCD files>>=
open_f("NameAliases.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  char nabuf[128];
  strcpy(nabuf + 1, fields[1]);
  nabuf[0] = strlen(fields[1]) - 1;
  add_str8(Name_Alias, nabuf);
}
fclose(f);
prop_t *na_alias = &parsed_props[prop_Name_Alias];
@

<<Post-process property data>>=
qsort(na_alias->str_arr, na_alias->len, sizeof(*na_alias->str_arr), uni_cmp_cp);
for(i = na_alias->len - 1; i > 0; i--) {
  unsigned int nlen = na_alias->str_arr[i].len;
  for(low = i; low > 0; low--) {
    if(na_alias->str_arr[low - 1].cp != na_alias->str_arr[i].cp)
      break;
    nlen += na_alias->str_arr[low - 1].len;
  }
  if(low == i)
    continue;
  check_size(na_alias->strs, na_alias->max_strs, na_alias->strs_len + nlen);
  uint8_t *strs8 = (uint8_t *)(na_alias->strs + na_alias->strs_len);
  unsigned int strs8_len = 0;
  for(j = low; j <= i; j++) {
    uint8_t *p8 = (uint8_t *)(na_alias->strs + na_alias->str_arr[j].off);
    unsigned int len8 = *p8 + 2;
    memcpy(strs8 + strs8_len, p8, len8);
    strs8_len += len8;
  }
  while(strs8_len % 4)
    strs8[strs8_len++] = 0;
  movebuf(na_alias->str_arr + low + 1, na_alias->str_arr + i + 1,
          na_alias->len - (i + 1));
  na_alias->str_arr[low].off = na_alias->strs_len;
  na_alias->str_arr[low].len = strs8_len / 4;
  na_alias->strs_len += strs8_len / 4;
  na_alias->len -= i - low;
  if(!(i = low))
    break;
}
@

<<Dump character information as C code>>=
dump_str_tabs(na_alias, "Name_Alias", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt(na_alias);
@

A related table which is not strictly a per-character property is the
Named Sequences table (from [[NamedSequences.txt]]%
\footnote{There is another file, [[NamedSequencesProv.txt]], which
contains provisional named sequence names.  These will not be
supported.  Insted, wait for them to become official.}%
).  This associates names with character sequences, rather than just
single characters. This is read in as well, and stored in the
pseudo-property na\_seq.  The names are indexed by the first character
of the sequence, and both the name and the additional index characters
are stored in the string value.

The index needs to be word-aligned in order to use the simple decoder.
If the index is at the end of the entry, the last entry might be
stripped (in 8-bit mode) if its last byte is zero.  If it is at the
beginning, finding its length may be hard.  Placing it one up in order
to insert a length byte wastes two bytes, so it is placed at the
beginning, but the length is placed at the end of the string.  The
name then occupies the middle.  Multiple concatenated entries are
word-aligned by padding with a zero.  In fact, the entire table needs
to be word-aligned.  One way to do that would be to use 16-bit mode,
but I prefer being able to sort of read the generated table, so it
uses 8-bit mode and always pads the name (rather than the whole
string) to force the full length to be word-aligned.

<<Initialize UCD files>>=
decl_str(na_seq);
@

<<UCD parser local definitions>>=
#define add_strseq(n, seq, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[64]; \
    uint16_t *p16 = (uint16_t *)str; \
    low = strtol(seq, &s, 16); \
    uint32_t seqlen = 0; \
    while(*s) { \
      high = strtol(s + 1, &s, 16); \
      int clen = uni_int_utf16_encode(p16, high); \
      p16 += clen; \
      seqlen += clen; \
    } \
    uint8_t *p8 = (uint8_t *)p16; \
    strcpy((char *)p8, v); \
    int len = strlen((char *)p8); \
    p8 += len; \
    if(!(len & 1)) \
      *p8++ = 0; \
    *p8++ = (len - 1) | ((seqlen - 1) << 6); \
    /* current UCD has max strlen 57 and max seqlen 4 */ \
    if(seqlen > 4 || len > 64) { \
      fprintf(stderr, "Unparsable sequence file %s (%d/%d)\n", lbuf, len, seqlen); \
      exit(1); \
    } \
    len = p8 - (uint8_t *)str; \
    while(len % 4) { \
      *p8++ = 0; \
      len++; \
    } \
    add_str_rng(&parsed_props[prop_##n], low, low, str, len / 4); \
    parsed_props[prop_##n].strs_char_size = 8; \
  } \
} while(0)
@

<<Parse UCD files>>=
open_f("NamedSequences.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  split_line(lbuf);
  if(num_fields < 2 || !isxdigit(fields[1][0]))
    continue;
  add_strseq(na_seq, fields[1], fields[0]);
}
@

<<Post-process property data>>=
prop_t *na_seq = &parsed_props[prop_na_seq];
qsort(na_seq->str_arr, na_seq->len, sizeof(*na_seq->str_arr), uni_cmp_cp);
for(i = na_seq->len - 1; i > 0; i--) {
  while(i > 0 && na_seq->str_arr[i].cp == na_seq->str_arr[i - 1].cp) {
    int jlen = na_seq->str_arr[i - 1].len, ilen = na_seq->str_arr[i].len;
    check_size(na_seq->strs, na_seq->max_strs, na_seq->strs_len + ilen + jlen);
    uint16_t *newseq = (uint16_t *)(na_seq->strs + na_seq->strs_len);
    jlen *= 2;
    cpybuf(newseq, na_seq->strs + na_seq->str_arr[i - 1].off, jlen);
    if(!newseq[jlen - 1])
      jlen--;
    ilen *= 2;
    cpybuf(newseq + jlen, na_seq->strs + na_seq->str_arr[i].off, ilen);
    if(!newseq[jlen + ilen - 1])
      ilen--;
    if((ilen + jlen) % 2)
      newseq[ilen++ + jlen] = 0;
    na_seq->str_arr[i - 1].len = (ilen + jlen) / 2;
    na_seq->str_arr[i - 1].off = na_seq->strs_len;
    na_seq->strs_len += (ilen + jlen) / 2;
    movebuf(na_seq->str_arr + i, na_seq->str_arr + i + 1, na_seq->len - (i + 1));
    na_seq->len--;
    i--;
  }
}
@

<<Dump character information as C code>>=
dump_str_tabs(na_seq, "named sequences", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_na_seq);
@

<<FIXME>>=
emoji-variation-sequences.txt = strseq
emoji-sequences.txt, emoji-zwj-sequences.txt - non-UCD strseq
@

The string tables are much too large for 16-bit offsets (509,515 bytes
for the na property alone).  One possible fix would be to use 8-bit
lengths and 24-bit offsets, instead.  However, since a function needs
to be used to retrieve the name anyway due to the synthesized names, a
more complex format could be used.  One way to reduce the string table
is to remove common substrings.  However, common substring removal is
rather expensive, so instead words are split out.  Each word of more
than two characters is placed into its own array, and replaced by a
two-byte pointer into that array.  The first byte of the pointer has
its upper bit set to distinguish from other characters.

While the na property is the only one which really needs this
compression, the same will be applied to the na\_seq, na\_rng, and
Name\_Alias properties.  In order to only have to manage pointers into
one string array during the word building phase, all names from the
other properties are temporarily added to na's strings.

<<UCD parser local functions>>=
#define NT_NAME  0
#define NT_SEQ   1
#define NT_ALIAS 2
static void collect_words(prop_t *words, prop_t *na, uint32_t *strs,
                          unsigned int off, int na_type)
{
  unsigned int i;

  if(off)
    cpybuf(strs + off, na->strs, na->strs_len);
  for(i = 0; i < na->len; i++) {
    <<For each word [[ws]] .. [[we]]>>
      if((we - ws) > 2) {
        check_size(words->str_arr, words->max_len, words->len + 1);
        words->str_arr[words->len].cp = 0; /* avoid valgrind warning */
        words->str_arr[words->len].off = ws - (uint8_t *)strs;
        words->str_arr[words->len].len = we - ws;
        words->len++;
      }
    <<End for each word [[ws]] .. [[we]]>>
  }
}
@

<<For each word [[ws]] .. [[we]]>>=
uint8_t *ws, *s, *n, *ne;
s = (uint8_t *)(strs + na->str_arr[i].off + off);
ne = (uint8_t *)(strs + na->str_arr[i].off + off + na->str_arr[i].len);
while(!ne[-1])
  ne--;
n = ne;
while(n != s) {
  uint8_t *e;
  if(na_type == NT_SEQ) {
    uint8_t wlen = *--n;
    if(!n[-1])
      n--;
    e = n;
    n -= (wlen & 0x3f) + 1;
    ws = n;
    n -=  2 * ((wlen >> 6) + 1);
  } else if(na_type == NT_ALIAS) {
    for(e = s; e < ne; e += *e + 2)
      if(n == e + *e + 2)
        break;
    n = e;
    ws = e + 1;
    e += *e + 2;
  } else {
    e = n;
    ws = n = s;
  }
  while(ws < e) {
    uint8_t *we;
    for(we = ws + 1; we < e; we++)
      if(*we == ' ' || *we == '-')
        break;
@

<<End for each word [[ws]] .. [[we]]>>=
    ws = we + 1;
    while(ws < e && (*ws == '-' || *ws == ' '))
      ws++;
    ws--; /* include leading char (space/-) in word */
  }
}
@

<<Post-process property data>>=
prop_t *na = &parsed_props[prop_na], *na_rng = &parsed_props[prop_na_rng];
prop_t words;
clearbuf(&words, 1);
/* collect all words as pointers into na_strs */
check_size(na->strs, na->max_strs, na->strs_len + na_seq->strs_len +
                                   na_alias->strs_len + na_rng->strs_len);
inisize(words.str_arr, words.max_len = 100);
collect_words(&words, na, na->strs, 0, NT_NAME);
collect_words(&words, na_seq, na->strs, na->strs_len, NT_SEQ);
collect_words(&words, na_alias, na->strs, na->strs_len + na_seq->strs_len, NT_ALIAS);
collect_words(&words, na_rng, na->strs,
              na->strs_len + na_seq->strs_len + na_alias->strs_len, NT_NAME);
/* prepare for merge */
inisize(words.strs, words.max_strs = words.len);
sort_strs = na->strs;
qsort(words.str_arr, words.len, sizeof(*words.str_arr), cmp_words);
@

<<UCD parser local functions>>=
static int cmp_words(const void *_a, const void *_b)
{
  const raw_cp_str_t *a = _a, *b = _b;
  const char *s = (const char *)sort_strs;
  int len = a->len < b->len ? a->len : b->len;
  int c = memcmp(s + a->off, s + b->off, len);
  if(c)
    return c;
  if(len < a->len)
    return 1;
  if(len < b->len)
    return -1;
  return 0;
}
@

<<Post-process property data>>=
/* fix words array */
uint8_t *na_strs = (uint8_t *)na->strs;
for(i = 0; i < words.len; i++) {
  /* merge duplicates */
  for(j = i + 1; j < words.len; j++)
    if(words.str_arr[j].len != words.str_arr[i].len ||
       memcmp(na_strs + words.str_arr[i].off, na_strs + words.str_arr[j].off,
              words.str_arr[i].len))
      break;
  /* copy unique word into words_strs and adjust pointers */
  int wlen = (words.str_arr[i].len + 3) / 4;
  check_size(words.strs, words.max_strs, words.strs_len + wlen);
  words.strs[words.strs_len + wlen - 1] = 0;
  memcpy(words.strs + words.strs_len, na_strs + words.str_arr[i].off,
         words.str_arr[i].len);
 words.str_arr[i].cp = i;
 words.str_arr[i].off = words.strs_len;
  words.strs_len += wlen;
 /* remove dups */
 movebuf(words.str_arr + i + 1, words.str_arr + j, words.len - j);
 words.len -= j - (i + 1);
}
@

<<Post-process property data>>=
/* replace words in name strings by pointers */
set_words(&words, na, NT_NAME);
set_words(&words, na_seq, NT_SEQ);
set_words(&words, na_alias, NT_ALIAS);
set_words(&words, na_rng, NT_NAME);
@

<<UCD parser local functions>>=
static void set_words(prop_t *words, prop_t *na, int na_type)
{
  unsigned int i;

  const uint32_t *strs = na->strs;
  int off = 0;
  for(i = 0; i < na->len; i++) {
    <<For each word [[ws]] .. [[we]]>>
      if((we - ws) > 2) {
        int l = 0, h = words->len - 1, m;
        while(l <= h) {
          m = (l + h) / 2;
	  int len = words->str_arr[m].len;
	  if(len > we - ws)
	    len = we - ws;
	  int cmp = memcmp(words->strs + words->str_arr[m].off, ws, len);
	  if(!cmp)
	    cmp = (int)words->str_arr[m].len - (int)(we - ws);
	  if(cmp < 0)
	    l = m + 1;
	  else if(cmp > 0)
	    h = m - 1;
	  else
	    break;
        }
        if(l <= h) {
	  if(na_type != NT_SEQ || !((ne - we) % 2))
            memmove(ws + 2, we, ne - we);
	  else {
	    /* need to keep things word-aligned */
	    memmove(ws + 2, we, e - we);
	    if(*e) {
	      ws[2 + (e - we)] = 0;
	      memmove(ws + 2 + (e - we) + 1, e, ne - e);
	      ne++;
	    } else {
	      memmove(ws + 2 + (e - we), e + 1, ne - (e + 1));
	      ne--;
	    }
	  }
	  ws[0] = (words->str_arr[m].cp >> 8) | 0x80;
	  ws[1] = words->str_arr[m].cp & 0xff;
	  e -= we - (ws + 2);
	  ne -= we - (ws + 2);
	  /* adjust name length byte */
	  if(na_type == NT_SEQ) {
	    if(*e)
	      *e -= we - (ws + 2);
	    else
	      e[1] -= we - (ws + 2);
	  } else if(na_type == NT_ALIAS)
	    *n -= we - (ws + 2);
	  we -= we - (ws + 2);
        }
      }
    <<End for each word [[ws]] .. [[we]]>>
    int slen = ne - (uint8_t *)(na->strs + na->str_arr[i].off);
    while(slen % 4) {
      *ne++ = 0;
      slen++;
    }
    na->str_arr[i].len = slen / 4;
  }
}
@

<<Post-process property data>>=
/* adjust word lens for dump_strtab */
for(i = 0; i < words.len; i++)
  words.str_arr[i].len = (words.str_arr[i].len + 3) / 4;
@

<<Dump character information as C code>>=
/* compress & dump strings */
words.strs_char_size = 8;
words.name = "na_words";
dump_str_ptr_arr(&words, "name compression (words)", gen_h);
free(words.strs);
free(words.str_arr);
@

<<UCD parser local functions>>=
static void dump_str_ptr_arr(prop_t *p, const char *lname, FILE *gen_h)
{
  unsigned int i;

  dump_strs(p, lname, gen_h);
  /* sort by word # */
  qsort(p->str_arr, p->len, sizeof(*p->str_arr), uni_cmp_cp);
  /* dump table */
  char fn[64];
  sprintf(fn, "uni_%s_arr.gen.c", p->name);
  open_wf(af, fn);
  fprintf(af, "#include \"uni_prop.h\"\n\n"
              "const uni_str_ptr_t uni_%s_arr[] = {\n", p->name);
  for(i = 0; i < p->len; i++) {
    fprintf(af, "\t{ %5u, %5u }", p->str_arr[i].off, p->str_arr[i].len);
    if(i < p->len - 1)
      putc(',', af);
    putc('\n', af);
  }
  fputs("};\n", af);
  fclose(af);
  /* dump header code */
  fprintf(gen_h, "/** Sorted string pointer array for %s property */\n"
                 "extern const uni_str_ptr_t uni_%s_arr[];\n"
		 "/** Length of \\ref uni_%s_arr */\n"
                 "#define uni_%s_arr_len %d\n",
		 lname, p->name, p->name, p->name, p->len);
}
@

This reduces the name table by well over half, to 179,282 bytes.  That
is still too long, but one easy way around it is to continue to retain
the 32-bit encoding.  The number of 32-bit words is only 54,286, which
adds a lot of padding (37,862 bytes), but allows 16-bit offsets.

<<Post-process property data>>=
na->strs_char_size = 32;
@

Since the format of the name table is not very simple, and there are
algorithmically generated names, a function is provided for name
lookup.  This function should also support looking up sequence names,
so the code point is passed in as a buffer pointer and length.  The
return format is similar to the UTF-8 static/dynamic return buffer
mechanism used by other string functions, plus a parameter to indicate
if passing more code points might result in a different name.  Since
the name table is rather large, this function is placed in a separate
object file.

<<Library [[uni]] Members>>=
cp_to_name.o
@

<<cp_to_name.c>>=
<<Common C Header>>
#include "uni_prop.h"

@

<<Unicode property exports>>=
/** Look up the Unicode name of code point(s).
 * Looks up name of \p len code points starting at \p cp.  If non-NULL,
 * the number of code points consumed to produce the name is returned
 * in the absolute value of \p *seq_len.  If \p *seq_len is less than zero,
 * using a longer input may produce a different name.  If non-NULL,
 * \p *alias can be used to select an alternate name.  If zero is passed in,
 * the canonical name is returned, and \p *alias is updated to the number
 * of aliases.  If \p *alias is a valid alias index, that alias is returned,
 * and \p *alias is decremented.  If \p *alias is not a valid alias index
 * or zero, it is updated to -1 and no name is returned.  If \p alias is
 * NULL, only the canonical name is returned.  The returned string is
 * described in \ref uni_return8_buf8.   An empty return string indicates
 * an invalid code point or nonexistent alias.  Otherwise, if the code point
 * has no name, the name is a synthetic one enclosed in angle brackets
 * (\<\e range-\e hex_cp\>).  Note that some code points have no name, but
 * do have aliases */
int uni_cp_to_name(const uint32_t *cp, unsigned int len, int *seq_len,
                   int *alias, <<Buffer return parameters for UTF-[[8]]>>);
@

<<cp_to_name.c>>=
int uni_cp_to_name(const uint32_t *cp, unsigned int len, int *seq_len,
                   int *alias, <<Buffer return parameters for UTF-[[8]]>>)
{
  if(!len) {
    if(seq_len)
      *seq_len = 0;
    if(alias && *alias)
      *alias = -1;
    return 0;
  }
  if(alias && *alias < 0) {
    if(seq_len)
      *seq_len = 0;
    return 0;
  }
  <<Return name for [[*cp]]>>
}
@

The first thing to do is look up the code point in the [[na_seq]] table.
If present, the longest possible match is taken, and if there is a
longer match beyond the passed-in length, [[*seq_len]] is negated as
well.  Since there may not be an exact match at all, this code may
just set [[*seq_len]] to $-1$ and fall through to the single-character
case.  Even if an exact match is found, every possible sequence is
checked, since there might be a longer sequence that matches.  Note
that there are no sequence aliases, so any attempt to get an alias is
denied.

<<Return name for [[*cp]]>>=
if(seq_len)
  *seq_len = 1;
const uni_str_ptr_t *lu = len > 1 || seq_len ? uni_na_seq_of(*cp) : NULL;
if(lu && lu->len && (len == 1 || (alias && *alias))) {
  if(seq_len)
    *seq_len = -1;
} else if(lu && lu->len) {
  const uint8_t *found_na = NULL;
  unsigned int found_nalen = 0, found_seqlen = 0, seqlen, nalen;
  const uint8_t *ep = uni_na_seq_strs + lu->off, *p = ep + lu->len;
  const uint16_t *seqptr;
  const uint32_t *cp2;
  while(p > ep) {
    nalen = *--p;
    seqlen = (nalen >> 6) + 1;
    nalen = (nalen & 0x3f) + 1;
    if(!p[-1])
      p--;
    p -= nalen + seqlen * 2;
    seqptr = (const uint16_t *)p;
    int sp = 0;
    for(cp2 = cp + 1; cp2 - cp < len; cp2++) {
      unsigned int clen;
      if(*cp2 != uni_int_utf16_decode(seqptr + sp, &clen))
        break;
      sp += clen;
      if(sp == seqlen) {
        if(seqlen > found_seqlen) {
          found_na = p + seqlen * 2;
	  found_nalen = nalen;
	  found_seqlen = seqlen;
	}
	break;
      }
    }
    /* if at end of string, mark as possibly longer */
    if(seq_len && sp < seqlen && cp2 - cp == len)
      *seq_len = -1;
  }
  if(found_na) {
    if(seq_len)
      *seq_len *= found_seqlen + 1; /* 1 or -1 */
    return uni_na_expand_words(found_na, found_nalen, buf, off, buf_len);
  }
}
@

In order to support raw lookup by the user as well, a function is
provided (and used by the main lookup function) to interpret at least
the word-encoded portion.

<<Unicode property exports>>=
/** Expand word-compressed string table entry.
  * Expands word-compressed string table entry \p na_raw/\p na_raw_len
  * using the word lookup table \p words/\p words_strs.  The return
  * value is described with \ref uni_return8_buf8. */
int uni_naX_expand_words(const uint8_t *na_raw, unsigned int na_raw_len,
                         const uni_str_ptr_t *words, const uint8_t *words_strs,
			 <<Buffer return parameters for UTF-[[8]]>>);
/** Expand compressed na(me) or na(med)_seq(uence) string table entry.
  * Expands compressed name string table entry \p na_raw/\p na_raw_len.
  * The return value is described with \ref uni_return8_buf8. */
int uni_na_expand_words(const uint8_t *na_raw, unsigned int na_raw_len,
                        <<Buffer return parameters for UTF-[[8]]>>);
#define uni_na_expand_words(na_raw, na_raw_len, buf, off, buf_len) \
  uni_naX_expand_words(na_raw, na_raw_len, uni_na_words_arr, uni_na_words_strs, \
                       buf, off, buf_len)
@

<<Unicode property functions>>=
int uni_naX_expand_words(const uint8_t *na_raw, unsigned int na_raw_len,
                         const uni_str_ptr_t *words, const uint8_t *words_strs,
			 <<Buffer return parameters for UTF-[[8]]>>)
{
  unsigned int i;
  unsigned int na_len = na_raw_len;

  for(i = 0; i < na_raw_len; i++)
    if(na_raw[i] & 0x80) {
      unsigned int wno = (((unsigned int)na_raw[i] & 0x7f) << 8);
      if(i < na_raw_len - 1)
	wno += na_raw[++i];
      else
        na_len++;
      na_len += words[wno].len - 2;
    }
  /* shortcut for just computing length */
  if(off < 0 && (!buf_len || !*buf_len))
    return na_len;
  /* otherwise build name on stack & return the usual way */
  uint8_t na[na_len], *np = na;
  for(i = 0; i < na_raw_len; i++) {
    if(na_raw[i] & 0x80) {
      unsigned int wno = (((unsigned int)na_raw[i] & 0x7f) << 8);
      if(i < na_raw_len - 1)
	wno += na_raw[++i];
      memcpy(np, words_strs + words[wno].off, words[wno].len);
      np += words[wno].len;
    } else
      *np++ = na_raw[i];
  }
  return uni_return8_buf8(na, na_len, buf, off, buf_len);
}
@

For the single code point case, the easiest thing to look up first is
the alias, if an alias index is provided.  The return value is either
that alias or an empty string to indicate an invalid alias index.  If
no alias was requested, the total alias count is returned in [[*alias]].

<<Return name for [[*cp]]>>=
if(alias) {
  int alias_no = *alias;
  const uni_str_ptr_t *ap = uni_Name_Alias_of(*cp);
  int alen = ap->len;
  if(!alen && alias_no) {
    if(seq_len)
      *seq_len = 0;
    *alias = -1;
    return 0;
  }
  if(alen) {
    const uint8_t *np = uni_Name_Alias_strs + ap->off;
    while(alen) {
      unsigned int nlen = *np + 1;
      if(nlen > alen - 1) /* in case last word# was cut off */
        nlen = alen - 1;
      np++;
      if(!--alias_no) {
        --*alias;
	if(seq_len)
	  *seq_len = 1;
	return uni_na_expand_words(np, nlen, buf, off, buf_len);
      }
      alen -= nlen + 1;
      np += nlen;
    }
    if(alias_no > 0) {
      if(seq_len)
        *seq_len = 0;
      *alias = -1;
      return 0;
    }
    *alias = -alias_no;
  }
}
@

For the single code point non-alias case, the first step is once again
lookup in the name table.  A successful match can sort of be returned
immediately, but the word pointers need to be replaced with actual
words first.

<<Return name for [[*cp]]>>=
lu = uni_na_of(*cp);
if(lu->len) {
  const uint8_t *na_raw = (const uint8_t *)(uni_na_strs + lu->off);
  unsigned int na_raw_len = lu->len * 4;
  while(!(na_raw[na_raw_len - 1]))
    na_raw_len--;
  return uni_na_expand_words(na_raw, na_raw_len, buf, off, buf_len);
}
@

Failure to look up a name may be because it is an invalid code point.
In that case, return an empty string.

<<Return name for [[*cp]]>>=
if(*cp > UNI_MAX_CP)
  return 0;
@

Otherwise, it must be a generated name.  All generated names consist
of the na\_rng string for the code point's range followed by a generated
suffix, except for unnamed code points, which map to either
``<noncharacter-'' or ``<reserved-'' followed by a generated suffix.
Note that all generated names with a leading < actually map to a blank
na property value.

<<Return name for [[*cp]]>>=
const uni_str_arr_t *rng = uni_na_rng_of(*cp);
unsigned int nalen;
int is_label = 0;
if(!rng) {
#define uni_return_const(str, sz) \
  uni_return8_buf##sz((uint8_t *)str, sizeof(str) - 1, buf, off, buf_len)
  if(uni_is_NChar(*cp))
    nalen = uni_return_const("<noncharacter", 8);
  else
    nalen = uni_return_const("<reserved", 8);
  is_label = 1;
} else {
  /* always extract at least 1st char of range name to determine if < */
  uint8_t c, *p = &c;
  unsigned int l = 1;
  if(!buf_len && off < 0) {
    nalen = uni_na_expand_words(uni_na_rng_strs + rng->off, rng->len,
                                &p, off, &l);
    is_label = c == '<';
  } else {
    nalen = uni_na_expand_words(uni_na_rng_strs + rng->off, rng->len,
                                buf, off, buf_len);
    is_label = **buf == '<';
  }
}
unsigned int suflen;
<<Compute [[suflen]] for [[*cp]]'s suffix>>
suflen += is_label;
/* shortcut for just computing length or if suffix won't fit */
if(off < 0 && (!buf_len || !*buf_len || *buf_len <= nalen))
  return nalen + suflen;
/* otherwise, build suffix from stack */
/* name was already built into output buffer by expand_words() */
char suffix[suflen + 1];
<<Build [[suffix]] from [[*cp]]>>
if(is_label)
  suffix[suflen - 1] = '>';
if(off < 0) {
  unsigned int blen = *buf_len - nalen;
  uint8_t *bptr = *buf + nalen;
  uni_return8_buf8((uint8_t *)suffix, suflen, &bptr, off, &blen);
} else if(off >= 0)
  uni_return8_buf8((uint8_t *)suffix, suflen, buf, off + nalen, buf_len);
return nalen + suflen;
@

<<Unicode property exports>>=
/** Determine synthetic name range for \p cp.
  * Returns the name string pointer of the synthetic range \p cp
  * belongs to, or NULL if it doesn't belong to any.  The string
  * pointer is for \ref uni_na_rng_strs, which is word-compressed
  * like a standard name. */
const uni_str_arr_t *uni_na_rng_of(uint32_t cp);
@

<<cp_to_name.c>>=
const uni_str_arr_t *uni_na_rng_of(uint32_t cp)
{
  int l = 0, h = uni_na_rng_arr_len / 2 - 1, m;
  while(l <= h) {
    m = (l + h) / 2;
    if(uni_na_rng_arr[m * 2].cp > cp)
      h = m - 1;
    else if(uni_na_rng_arr[m * 2 + 1].cp < cp)
      l = m + 1;
    else
      return &uni_na_rng_arr[m * 2];
  }
  return NULL;
}
@

The suffix for most ranges is just a hyphen, followed by the code point
in hex.  For Hangul syllables, though, the suffix is a space, followed
by the broken down compoents' names.

<<Compute [[suflen]] for [[*cp]]'s suffix>>=
<<Prepare to compute [[suflen]] for Hangul syllable>>
if(rng && rng->cp == 0xAC00) {
  <<Compute [[suflen]] for Hangul syllable>>
} else {
  suflen = 1 /* - */ + (lg2(*cp + 1) + 3) / 4 /* XXXX */;
  if(suflen < 5)
    suflen = 5;
}
@

<<Build [[suffix]] from [[*cp]]>>=
if(rng && rng->cp == 0xAC00) {
  <<Compute [[suffix]] for Hangul syllable>>
} else
  sprintf(suffix, "-%04X", (int)*cp);
@

For Hangul syllables, the Jamo\_Short\_Name property contains the text
to append to the name.  First the syllable is broken down into L, V,
and T, and then the string corresponding to each is looked up.

<<Prepare to compute [[suflen]] for Hangul syllable>>=
const uni_str_ptr_t *ls = NULL, *vs = NULL, *ts = NULL; /* shut gcc up */
@

<<Compute [[suflen]] for Hangul syllable>>=
unsigned int jt = (*cp - 0xAC00) % 28;
unsigned int jv = (*cp - 0xAC00) / 28;
unsigned int jl = jv / 21;
jv %= 21;
ls = uni_JSN_of(0x1100 + jl);
vs = uni_JSN_of(0x1161 + jv);
ts = jt ? uni_JSN_of(0x11A7 + jt) : NULL;
suflen = 1 + ls->len + vs->len + (ts ? ts->len : 0);
@

The suffix itself is just a space, followed by the L, V, and T short
names.

<<Compute [[suffix]] for Hangul syllable>>=
suffix[0] = ' ';
memcpy(suffix + 1, uni_JSN_strs + ls->off, ls->len);
memcpy(suffix + 1 + ls->len, uni_JSN_strs + vs->off, vs->len);
if(ts)
  memcpy(suffix + 1 + ls->len + vs->len, uni_JSN_strs + ts->off, ts->len);
@

To test this, a simple program just generates every single name.  No
verification is done; this is meant to be verified manually.

<<C Test Support Executables>>=
tstcp_na \
@

<<tstcp_na.c>>=
<<Common C Header>>

#include "uni_prop.h"

int main(void)
{
  uint32_t cp;
  uint8_t *buf = NULL;
  unsigned int buf_len = 0, clen;
  int has_more, aliases;
  for(cp = 0; cp <= UNI_MAX_CP + 1; cp++) {
    aliases = 0;
    clen = uni_cp_to_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
    while(!clen && aliases > 0)
      clen = uni_cp_to_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
    if(!clen)
      continue;
    printf("%04X %d %.*s\n", cp, has_more, clen, buf);
    while(aliases > 0) {
      clen = uni_cp_to_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
      printf("%04X %d &%.*s\n", cp, has_more, clen, buf);
    }
  }
  /* all sequences */
  int i;
  uint32_t seq[10];
  for(i = 0; i < uni_na_seq_arr_len; i++) {
    const uint8_t *ep = uni_na_seq_strs + uni_na_seq_arr[i].off,
                  *p = ep + uni_na_seq_arr[i].len;
    unsigned int seqlen, nalen;
    while(p > ep) {
      uint32_t *sp = seq;
      *sp = uni_na_seq_arr[i].cp;
      printf("%04X", (int)*sp++);
      nalen = *--p;
      if(!p[-1])
        p--;
      seqlen = (nalen >> 6) + 1;
      nalen = (nalen & 0x3f) + 1;
      p -= nalen + seqlen * 2;
      const uint16_t *seqp = (const uint16_t *)p;
      while(seqlen > 0) {
        *sp = uni_int_utf16_decode(seqp, &clen);
	printf(":%04X", (int)*sp++);
	seqp += clen;
	seqlen -= clen;
      }
      clen = uni_cp_to_name(seq, (int)(sp - seq), &has_more, NULL, &buf, 0, &buf_len);
      printf(" %d %.*s\n", has_more, clen, buf);
    }
  }
  return 0;
}
@

The name tables are not really that useful as is:  few programs
display a Unicode code point's name.  However, there are many file
parsers which allow Unicode code points to be specified by name.  Thus
a reverse lookup table is required.  There is no perfect reverse
lookup method; it depends on the application.  It is expected that a
program using the name tables will dump a custom version of both
forward and reverse lookups as application-specific C code.

The obvious thing to provide is an explicit name lookup function, using
either a range list sorted by name instead of code point, or a hash
table, or something similar.  Matching loosely needs to be supported as
well, by stripping spaces and hyphens for the lookup.

For the hash table, either some random hash function could be
provided, or a perfect hash function could be generated using
[[gperf]] or a similar tool.  My initial implementation used
[[gperf]], but generating a perfect hash for the full Unicode set took
two hours on my machine, and an additional hour to compile.  In
comparison, a simple hash used by a master symbol table in my compiler
took less than a second to generate and compile.

The first thing to do is generate a name table that is preprocessed
for lookup.  The lookup algorithm is always loose.  UAX44-LM2 states
that lookups should ignore case, whitespace, underscore, and medial
hyphens except in U+1180.  There are 15 names with non-medial hyphens,
but only two are actually ambiguous.  UTS~\#18 (Regular Expressions),
section 2.5.1 ("Individually Named Characters") acknowledges this by
requiring hyphen checks only for U+1180, U+0F60, and U+0FB0.  While
the algorithm I use violates the original loose matching algorithm, I
will go ahead and only restrict those three.  The code actually
derives this list by checking for duplicates, so it is guaranteed to
be safe for the current UCD.

The lookup table contains all possible names:  the na property, the
Name\_Alias property, and the na\_seq property.  The na\_rng property
is stored separately, because it needs to be searched as a prefix
rather than the whole name.

Since all names refer to the word table, the easiest way to preprocess
most of the text is to use a different word table.  The old table is
copied while stripping spaces and hyphens and converting to
lower-case.

<<Post-process property data>>=
prop_t revwords;
clearbuf(&revwords,1);
inisize(revwords.strs, revwords.max_strs = revwords.strs_len = words.strs_len);
uint8_t *rwstr = (uint8_t *)revwords.strs;
unsigned int rwstr_len = 0;
inisize(revwords.str_arr, revwords.len = revwords.max_len = words.len);
for(i = 0; i < revwords.len; i++) {
  const uint8_t *ws = (uint8_t *)(words.strs + words.str_arr[i].off);
  unsigned int wl = words.str_arr[i].len * 4;
  while(!ws[wl -1])
    wl--;
  /* strip all spaces and hypens in this array */
  while(*ws == ' ' || *ws == '-') {
    ws++;
    wl--;
  }
  revwords.str_arr[i].off = rwstr_len / 4;
  revwords.str_arr[i].cp = words.str_arr[i].cp;
  revwords.str_arr[i].flags = 0;
  for(j = 0; j < wl; j++)
    rwstr[rwstr_len++] = isupper(ws[j]) ? tolower(ws[j]) : ws[j];
  while(rwstr_len % 4)
    rwstr[rwstr_len++] = 0;
  revwords.str_arr[i].len = rwstr_len / 4 - revwords.str_arr[i].off;
}
@

The main name table is then converted, ignoring word pointers and
otherwise stripping spaces and hyphens and converting to lower-case.
While this does introduce the duplicates mentioned earlier, they are
not processed until the whole table has been filled.

<<Post-process property data>>=
int prop_rev_na = add_prop("rev_na");
prop_t *rev_na = &parsed_props[prop_rev_na];
rev_na->max_strs = na->strs_len + na_seq->strs_len + na_alias->strs_len;
inisize(rev_na->strs, rev_na->max_strs);
uint8_t *dstp = (uint8_t *)rev_na->strs;
unsigned int dstlen = 0;
rev_na->max_len = na->len + na_seq->len * 2 + na_alias->len * 2;
inisize(rev_na->str_arr, rev_na->max_len);
for(i = 0; i < na->len; i++) {
  uint8_t *src = (uint8_t *)(na->strs + na->str_arr[i].off);
  unsigned int srclen = na->str_arr[i].len * 4;
  while(!src[srclen - 1])
    srclen--;
  rev_na->str_arr[i].off = dstlen / 4;
  rev_na->str_arr[i].cp = na->str_arr[i].cp;
  rev_na->str_arr[i].flags = 0;
  for(j = 0; j < srclen; j++) {
    <<Preprocess [[src[j]]]] into [[dstp[dstlen]]]>>
  }
  while(dstlen % 4)
    dstp[dstlen++] = 0;
  rev_na->str_arr[i].len = dstlen / 4 - rev_na->str_arr[i].off;
}
rev_na->len = na->len;
rev_na->strs_char_size = 32; /* almost as large as na, so reduce same way */
@

<<Preprocess [[src[j]]]] into [[dstp[dstlen]]]>>=
if(src[j] & 0x80) {
  dstp[dstlen++] = src[j++];
  if(j < srclen)
    dstp[dstlen++] = src[j];
} else if(isupper(src[j]))
  dstp[dstlen++] = tolower(src[j]);
else if(src[j] != ' ' && src[j] != '-')
  dstp[dstlen++] = src[j];
@

The aliases must be split up into individual names.  The table being
built is indexed on name, so multiple entries can safely have the same
code point.

<<Post-process property data>>=
for(i = 0; i < na_alias->len; i++) {
  uint8_t *src = (uint8_t *)(na_alias->strs + na_alias->str_arr[i].off);
  unsigned int srclen = na_alias->str_arr[i].len * 4;
  while(!src[srclen - 1])
    srclen--;
  for(low = 0; low < srclen; low += src[low] + 2) {
    check_size(rev_na->str_arr, rev_na->max_len, rev_na->len + 1);
    rev_na->str_arr[rev_na->len].off = dstlen / 4;
    rev_na->str_arr[rev_na->len].cp = na_alias->str_arr[i].cp;
    rev_na->str_arr[rev_na->len].flags = 0;
    unsigned int elen = src[low] + 1;
    for(j = low + 1; j < low + 1 + elen; j++) {
      <<Preprocess [[src[j]]]] into [[dstp[dstlen]]]>>
    }
    while(dstlen % 4)
      dstp[dstlen++] = 0;
    rev_na->str_arr[rev_na->len].len = dstlen / 4 - rev_na->str_arr[rev_na->len].off;
    rev_na->len++;
  }
}
@

The name sequences must be split up as well.  The returned value
should be the code point, but sequences require special handling.  In
this case, each sequence is assigned a number, and that number is
added to [[UNI_MAX_CP]] to obtain the code point.  A separate table
([[na_seq_id]]) maps this to an actual sequence.

<<Post-process property data>>=
int prop_na_seq_id = add_prop("na_seq_id");
prop_t *na_seq_id = &parsed_props[prop_na_seq_id];
inisize(na_seq_id->str_arr, na_seq_id->max_len = na_seq->len * 2);
inisize(na_seq_id->strs, na_seq_id->max_strs = na_seq->max_strs);
for(i = 0; i < na_seq->len; i++) {
  uint8_t *src = (uint8_t *)(na_seq->strs + na_seq->str_arr[i].off);
  unsigned int srclen = na_seq->str_arr[i].len * 4;
  while(!src[srclen - 1])
    srclen--;
  while(srclen > 0) {
    low = srclen - 1;
    unsigned int elen = src[low];
    unsigned int seqlen = (elen >> 6) + 1;
    elen = (elen & 0x3f) + 1;
    if(!src[low - 1])
      low--;
    low -= elen;
    check_size(rev_na->str_arr, rev_na->max_len, rev_na->len + 1);
    rev_na->str_arr[rev_na->len].off = dstlen / 4;
    rev_na->str_arr[rev_na->len].flags = 0;
    <<Add [[src[i]]] sequence from [[na_seq]] at [[low[seqlen]]] to [[na_seq_id]]>>
    rev_na->str_arr[rev_na->len].cp = na_seq_id->len + UNI_MAX_CP;
    for(j = low; j < low + elen; j++) {
      <<Preprocess [[src[j]]]] into [[dstp[dstlen]]]>>
    }
    while(dstlen % 4)
      dstp[dstlen++] = 0;
    rev_na->str_arr[rev_na->len].len = dstlen / 4 - rev_na->str_arr[rev_na->len].off;
    rev_na->len++;
    srclen = low - seqlen * 2;
  }
}
@

<<Add [[src[i]]] sequence from (@seq) at [[low[seqlen]]] to (@id)>>=
check_size(<<@id>>->str_arr, <<@id>>->max_len, <<@id>>->len + 1);
check_size(<<@id>>->strs, <<@id>>->max_strs, <<@id>>->strs_len + 1 + seqlen);
<<@id>>->str_arr[<<@id>>->len].cp = <<@id>>->len;
<<@id>>->str_arr[<<@id>>->len].off = <<@id>>->strs_len;
<<@id>>->str_arr[<<@id>>->len].flags = 0;
<<@id>>->strs[<<@id>>->strs_len++] = <<@seq>>->str_arr[i].cp;
for(j = 0; j < seqlen;) {
  unsigned int clen;
  <<@id>>->strs[<<@id>>->strs_len++] = 
            uni_int_utf16_decode((uint16_t *)(src + low) - seqlen + j, &clen);
  j += clen;
}
<<@id>>->str_arr[<<@id>>->len].len = <<@id>>->strs_len - <<@id>>->str_arr[<<@id>>->len].off;
<<@id>>->len++;
@

<<Dump character information as C code>>=
dump_str_ptr_arr(na_seq_id, "named sequence mapping", gen_h);
@

Now that all constituent name lists have been collected into
[[rev_na]], it's time to detect and remove duplicates.  This is done by
simply sorting the array by name, and comparing consecutive elements
for equality.  When a pair matches, the location of the first hyphen
in one but not the other is chosen to distinguish the code points.
Since the hyphens have already been stripped out, this comparison is
done using the original [[na]] property.  In the current UCD, this is
safe, but it may be possible that this needs rewriting in the future
to ignore medial hyphens in everything but U+1180.  It may also become
necessary to check the name aliases or sequence names.

<<Post-process property data>>=
sort_words = &revwords;
sort_strs = rev_na->strs;
qsort(rev_na->str_arr, rev_na->len, sizeof(*rev_na->str_arr), cmp_wordstr);
int prop_na_hyphen = add_prop("na_hyphen");
prop_t *na_hyphen = &parsed_props[prop_na_hyphen];
inisize(na_hyphen->rng_dat32, na_hyphen->max_len = 10);
for(i = rev_na->len - 1; i > 0; i--)
  if(!cmp_wordstr(&rev_na->str_arr[i], &rev_na->str_arr[i - 1])) {
    /* this isn't really what the standard says, but it works with current data */
    /* just find the first mismatched char; if it's a -, store - location */
    /* also, this only searches na, not na_alias or na_seq */
    const raw_cp_str_t *a = bsearch(&rev_na->str_arr[i].cp, na->str_arr,
                                    na->len, sizeof(*na->str_arr), uni_cmp_cp),
                       *b = bsearch(&rev_na->str_arr[i - 1].cp, na->str_arr,
                                    na->len, sizeof(*na->str_arr), uni_cmp_cp);
    const uint8_t *astr = (const uint8_t *)(na->strs + a->off),
                  *bstr = (const uint8_t *)(na->strs + b->off);
    unsigned int alen = a->len * 4, blen = b->len * 4, hloc = 0;
    while(!astr[alen - 1])
      alen--;
    while(!bstr[blen - 1])
      blen--;
    const uint8_t *aword = NULL, *bword = NULL;
    unsigned int aword_len = 0, bword_len = 0;
    uint8_t ac = 0, bc; /* init to shut gcc up */
    sort_words = &words;
    while((alen || aword_len) && (blen || bword_len)) {
      <<Get next char from word-encoded string [[a]] during parse>>
      <<Get next char from word-encoded string [[b]] during parse>>
      if(ac != bc)
        break;
      if(ac != ' ' && ac != '-')
        hloc++;
    }
    sort_words = &revwords;
    check_size(na_hyphen->rng_dat32, na_hyphen->max_len, na_hyphen->len + 1);
    na_hyphen->rng_dat32[na_hyphen->len].dat = hloc;
    na_hyphen->rng_dat32[na_hyphen->len].len = 0;
    if(ac == '-') {
      na_hyphen->rng_dat32[na_hyphen->len].low = rev_na->str_arr[i - 1].cp;
      na_hyphen->rng_dat32[na_hyphen->len].dat |= rev_na->str_arr[i].cp << 8;
    } else {
      na_hyphen->rng_dat32[na_hyphen->len].low = rev_na->str_arr[i].cp;
      na_hyphen->rng_dat32[na_hyphen->len].dat |= rev_na->str_arr[i - 1].cp << 8;
      rev_na->str_arr[i - 1].cp = rev_na->str_arr[i].cp;
    }
    na_hyphen->len++;
    movebuf(rev_na->str_arr + i, rev_na->str_arr + i + 1, rev_na->len - (i + 1));
    rev_na->len--;
    if(!--i)
      break;
  }
qsort(na_hyphen->rng_dat32, na_hyphen->len, sizeof(*na_hyphen->rng_dat32), uni_cmp_cp);
@

<<UCD parser local functions>>=
static const prop_t *sort_words;
static int cmp_wordstr(const void *_a, const void *_b)
{
  const raw_cp_str_t *a = _a, *b = _b;
  const uint8_t *astr = (const uint8_t *)(sort_strs + a->off),
                *bstr = (const uint8_t *)(sort_strs + b->off);
  unsigned int alen = a->len * 4, blen = b->len * 4;
  while(alen && !astr[alen - 1])
    alen--;
  while(blen && !bstr[blen - 1])
    blen--;
  const uint8_t *aword = NULL, *bword = NULL;
  unsigned int aword_len = 0, bword_len = 0;
  while((alen || aword_len) && (blen || bword_len)) {
    uint8_t ac, bc;
    <<Get next char from word-encoded string [[a]] during parse>>
    <<Get next char from word-encoded string [[b]] during parse>>
    if(ac != bc)
      return (int)ac - (int)bc;
  }
  if(alen || aword_len)
    return 1;
  else if(blen || bword_len)
    return -1;
  return 0;
}
@

<<Get next char from word-encoded string (@ab) during parse>>=
if(<<@ab>>word_len) {
  <<@ab>>c = *<<@ab>>word++;
  <<@ab>>word_len--;
} else if(*<<@ab>>str & 0x80) {
  unsigned int wno = ((unsigned int)*<<@ab>>str++ & 0x7f) << 8;
  <<@ab>>len--;
  if(<<@ab>>len) {
    wno += *<<@ab>>str++;
    <<@ab>>len--;
  }
  <<@ab>>word = (const uint8_t *)(sort_words->strs + sort_words->str_arr[wno].off);
  <<@ab>>word_len = sort_words->str_arr[wno].len * 4;
  while(!<<@ab>>word[<<@ab>>word_len - 1])
    <<@ab>>word_len--;
  <<@ab>>c = *<<@ab>>word++;
  <<@ab>>word_len--;
} else {
  <<@ab>>c = *<<@ab>>str++;
  <<@ab>>len--;
}
@

While the generated [[na_hyphen]] table is sufficient to check for
whether or not hyphens are needed, this check needs to be performed on
every single lookup.  To make that go a little faster, a macro is
generated to test against all known potentially duplicate name matches.

<<Dump character information as C code>>=
const char *nd_next = "/** True if hyphens must be retained for loose matching "
                      "for name of code point \\p x */\n"
		      "int uni_rev_na_needs_hyphen(uint32_t cp);\n"
		      "#define uni_rev_na_needs_hyphen(x) (";
for(i = 0; i < na_hyphen->len; i++) {
  fprintf(gen_h, "%s(x) == 0x%04X", nd_next, na_hyphen->rng_dat32[i].low);
  nd_next = " || ";
}
fputs(")\n", gen_h);
@

The final table to generate is for range lookups.  As mentioned above,
it needs to be separate because its usage is different.  The other
tables are for direct searching, whereas this one is for prefix
searching.  Technically, a binary search could permit both; a special
indicator at the end of the name could be used to detect that this is
a prefix.  In fact, some of my old code used to do just that.  For
now, I prefer keeping this separate, though.  Stripping is performed
the same way as with the main name tables.

<<Post-process property data>>=
int prop_rev_na_rng = add_prop("rev_na_rng");
prop_t *rev_na_rng = &parsed_props[prop_rev_na_rng];
inisize(rev_na_rng->str_arr, rev_na_rng->max_len = rev_na_rng->len = na_rng->len);
inisize(rev_na_rng->strs, rev_na_rng->max_strs = na_rng->strs_len);
dstp = (uint8_t *)rev_na_rng->strs;
dstlen = 0;
for(i = 0; i < na_rng->len; i++) {
  uint8_t *src = (uint8_t *)(na_rng->strs + na_rng->str_arr[i].off);
  unsigned int srclen = na_rng->str_arr[i].len * 4;
  while(!src[srclen - 1])
    srclen--;
  rev_na_rng->str_arr[i].off = dstlen / 4;
  rev_na_rng->str_arr[i].cp = na_rng->str_arr[i].cp;
  rev_na_rng->str_arr[i].flags = 0;
  for(j = 0; j < srclen; j++) {
    <<Preprocess [[src[j]]]] into [[dstp[dstlen]]]>>
  }
  while(dstlen % 4)
    dstp[dstlen++] = 0;
  rev_na_rng->str_arr[i].len = dstlen / 4 - rev_na_rng->str_arr[i].off;
}
rev_na_rng->strs_char_size = 8;
@

Now, all of the tables can be dumped.  Since the default dumper sorts
by code point, it is rewritten here to sort by name instead.  In order
to sort, the offsets and lengths need to be accurate for the current
in-memory representation.  The [[dump_strs]] function changes these to
match the generated representation, though.  For this reason, dumping
the word table must be delayed until sorting (which needs the words in
memmory) is complete.

<<Dump character information as C code>>=
sort_words = &revwords;
dump_wordord_strs(rev_na, "reverse Name lookup", gen_h);
dump_wordord_strs(rev_na_rng, "reverse synthetic Name lookup", gen_h);
revwords.strs_char_size = 8;
revwords.name = "rev_na_words";
dump_str_ptr_arr(&revwords, "reverse Name lookup compression support", gen_h);
free(revwords.strs);
free(revwords.str_arr);
@

In fact, the [[dump_strs]] function sorts its input as well, which
screws up the table order.  In order to compensate for this, a copy is
made of the string array before dumping, so that the original offsets
and lengths are retained, and then the copy is sorted instead.  The
code point field is used process to keep the arrays matched up.  While
generating strings, the original's [[cp]] points to the array entry in
the copy, and then the copy's [[cp]] is switched to point to the
original.  Then, after sorting the copy, the copy is dumped in order
by dumping the original array entry corresponding to the [[cp]].

<<UCD parser local functions>>=
static void dump_wordord_strs(prop_t *p, const char *lname, FILE *gen_h)
{
  unsigned int i, j;

  /* dump_strs mangles str_arr, so save it and restore it afterwards */
  raw_cp_str_t *ostr_arr;
  inisize(ostr_arr, p->len);
  cpybuf(ostr_arr, p->str_arr, p->len);
  for(i = 0; i < p->len; i++)
    p->str_arr[i].cp = i;
  dump_strs(p, lname, gen_h);
  for(i = 0; i < p->len; i++) {
    j = p->str_arr[i].cp;
    p->str_arr[i].cp = ostr_arr[j].cp;
    ostr_arr[j].cp = i;
  }
  sort_strs = p->strs;
  qsort(ostr_arr, p->len, sizeof(*ostr_arr), cmp_wordstr);
  char nbuf[64];
  sprintf(nbuf, "uni_%s_arr.gen.c", p->name);
  open_wf(rt, nbuf);
  fprintf(rt, "#include \"uni_prop.h\"\n\n"
              "const uni_str_arr_t uni_%s_arr[] = {\n\t", p->name);
  for(i = 0; i < p->len; i++) {
    const raw_cp_str_t *ent = &p->str_arr[ostr_arr[i].cp];
    fprintf(rt, "{ 0x%04X, %d, %d}%s", (int)ent->cp, (int)ent->off, 
                                       (int)ent->len,
				       i < p->len - 1 ? ",\n\t" : "\n};\n");
  }
  free(ostr_arr);
  fclose(rt);
  fprintf(gen_h, "/** Sorted string pointer array for %s property */\n"
		 "extern const uni_str_arr_t uni_%s_arr[];\n"
		 "/** Length of \\ref uni_%s_arr */\n"
	         "#define uni_%s_arr_len %d /* %d lookups max */\n",
	         lname, p->name, p->name, p->name, i, lg2(i + 1));
}
@

It is expected that the user will take the tables as is and dump them
in a more suitable format.  Since they are meant to be handled raw, a
function is provided to interpret the word pointers, at least.

<<Unicode property exports>>=
/** Expand compressed rev(erse)_na(me) string table entry.
  * Expands compressed string table entry \p na_raw/\p na_raw_len.
  * The return value is described with \ref uni_return8_buf8. */
int uni_rev_na_expand_words(const uint8_t *na_raw, unsigned int na_raw_len,
                           const uni_str_ptr_t *words, const uint8_t *words_strs);
#define uni_rev_na_expand_words(na_raw, na_raw_len, buf, off, buf_len) \
  uni_naX_expand_words(na_raw, na_raw_len, uni_rev_na_words_arr, uni_rev_na_words_strs, \
                       buf, off, buf_len)
@

For raw comparison, the compressed form can be retained, if a special
string comparison function is used.

<<Unicode property exports>>=
/** Compare two strings, either one of which may be word-compressed.
  * Compares \p stra/\p lena to \p strb/\p lenb by code point, then length.
  * Returns -1 if \p stra is less than \p strb, 1 if greater, and 0 if equal.
  * If either string is word-compressed, it will be uncompressed without
  * explicit storage during comparison using the \p words/\p words_strs
  * word table.  Also, if \p eqlen is non-NULL, it is updated to the
  * uncompressed length of initial matching text */
int uni_cmp_word_str(const uint8_t *stra, unsigned int lena,
                     const uint8_t *strb, unsigned int lenb,
		     unsigned int *eqlen,
		     const uni_str_ptr_t *words, const uint8_t *words_strs);
/** Compare two strings, either one of which may be compressed
  * rev(erse)_na(me) strings.
  * Compares \p stra/\p lena to \p strb/\p lenb by code point, then length.
  * Returns -1 if \p stra is less than \p strb, 1 if greater, and 0 if equal.
  * If either string is word-compressed, it will be uncompressed without
  * explicit storage during comparison like a rev_na table entry.  Also, if
  * \p eqlen is non-NULL, it is updated to the uncompressed length of
  * initial matching text */
int uni_cmp_rev_na_str(const uint8_t *stra, unsigned int lena,
                      const uint8_t *strb, unsigned int lenb,
		      unsigned int *eqlen);
#define uni_cmp_rev_na_str(stra, lena, strb, lenb, eqlen) \
  uni_cmp_word_str(stra, lena, strb, lenb, eqlen, uni_rev_na_words_arr, \
                   uni_rev_na_words_strs)
@

<<Unicode property functions>>=
int uni_cmp_word_str(const uint8_t *astr, unsigned int alen,
                     const uint8_t *bstr, unsigned int blen,
		     unsigned int *eqlen,
		     const uni_str_ptr_t *words, const uint8_t *words_strs)
{
  const uint8_t *aword = NULL, *bword = NULL;
  unsigned int aword_len = 0, bword_len = 0;
  int eq = 0;
  while((alen || aword_len) && (blen || bword_len)) {
    uint8_t ac, bc;
    <<Get next char from word-encoded string [[a]]>>
    <<Get next char from word-encoded string [[b]]>>
    if(ac != bc) {
      if(eqlen)
        *eqlen = eq;
      return (int)ac - (int)bc;
    }
    eq++;
  }
  if(eqlen)
    *eqlen = eq;
  if(alen || aword_len)
    return 1;
  else if(blen || bword_len)
    return -1;
  return 0;
}
@

Fetching a character is slightly different than during parsing because
the table formats have been simplified.

<<Get next char from word-encoded string (@ab)>>=
if(<<@ab>>word_len) {
  <<@ab>>c = *<<@ab>>word++;
  <<@ab>>word_len--;
} else if(*<<@ab>>str & 0x80) {
  unsigned int wno = ((unsigned int)*<<@ab>>str++ & 0x7f) << 8;
  <<@ab>>len--;
  if(<<@ab>>len) {
    wno += *<<@ab>>str++;
    <<@ab>>len--;
  }
  <<@ab>>word = words_strs + words[wno].off;
  <<@ab>>word_len = words[wno].len;
  <<@ab>>c = *<<@ab>>word++;
  <<@ab>>word_len--;
} else {
  <<@ab>>c = *<<@ab>>str++;
  <<@ab>>len--;
}
@

Knowning the match length does little good without also knowing the
length of each string.

<<Unicode property exports>>=
/** Find uncompressed length of a word-compressed string.
  * Computes uncompressed length of \p str/\p len compressed using
  * word table \p words/words_strs */
int uni_word_strlen(const uint8_t *str, unsigned int len,
		    const uni_str_ptr_t *words, const uint8_t *words_strs);
/** Find uncompressed length of a rev(erse)_na(me) table string.
  * Computes uncompressed length of \p str/\p len as if it were a
  * rev_na table string  */
int uni_rev_na_strlen(const uint8_t *str, unsigned int len);
#define uni_rev_na_strlen(str, len) \
  uni_word_strlen(str, len, uni_rev_na_words_arr, uni_rev_na_words_strs)
@

<<Unicode property functions>>=
int uni_word_strlen(const uint8_t *str, unsigned int len,
		    const uni_str_ptr_t *words, const uint8_t *words_strs)
{
  unsigned int rlen = 0;
  while(len) {
    if(*str & 0x80) {
      unsigned int wno = ((unsigned int)*str++ & 0x7f) << 8;
      len--;
      if(len) {
        wno += *str++;
	len--;
      }
      rlen += words[wno].len;
    } else {
      rlen++;
      len--;
    }
  }
  return rlen;
}
@

The range lookup function is not just a simple lookup, so it is
provided as a separate function.  It might be possible to use this as
a fallback while using another mechanism to look up unsynthesized
names.  However, its usage is rare enough and not associated with the
other name functions that it is stored in a separate object file to be
ignored if needed.

<<Library [[uni]] Members>>=
narev_rng_bin.o
@

<<Unicode property exports>>=
/** Look up synthetic Unicode code point name.
  * Look up \p name/\p len, assuming it maps to one of the synthetic
  * ranges.  If \p len is negative, \p name is zero-terminated.
  * \p name must already be normalized and case-folded.
  * This function can also interpret angle-bracket-enclosed labels
  * (i.e. \<\e range-\e hex_cp\>).  If the return value is negative,
  * \p name does not map to a synthetic range.  */
int32_t uni_name_rng_to_cp(const char *name, int len);
@

<<narev_rng_bin.c>>=
<<Common C Header>>
#include "uni_prop.h"

int32_t uni_name_rng_to_cp(const char *name, int len)
{
  <<Convert synthetic [[name]] to code point>>
}
@

The first thing to do is to strip the name.  By requring normalized,
case-folded input, most of the work is already done.  The only thing
left is to strip all spaces and hyphens.  The code assumes that no
range will ever have an ambiguous hyphen.  Since no synthesized name
in the current UCD is longer than 127 characters (actually 83
characters in the current UCD), an input that long is automatically
rejected.  That also keeps invalid input from overflowing the stack,
where the stripped name is being built.  While labels are not
technically names, they can be looked up with this function as well.
It is up to the caller to ensure that the name does not start with a
less-than if no labels are desired.
 
<<Convert synthetic [[name]] to code point>>=
const char *n = name;
if(len < 0)
  len = strlen(name);
unsigned int olen = len;
if(olen > 127)
  olen = 127;
char oname[olen + 1];
olen = 0;
for(; len > 0; n++, len--) {
  if(isspace(*n) || *n == '_' || *n == '-')
    continue;
  if(olen == 127)
    return -1;
  if(!isdigit(*n) && !islower(*n) && *n != '<' && *n != '>')
    return -1;
  oname[olen++] = *n;
}
oname[olen] = 0;
@

Next, we do a simple binary search.  An exact match is rejected:
synthesized names always have extra characters.  The search will end
with [[h]] being the correct entry, if there is one.

<<Convert synthetic [[name]] to code point>>=
int l = 0, h = uni_rev_na_rng_arr_len / 2 - 1, m;
unsigned int heq = 0, meq;
while(l <= h) {
  m = l + h;
  int c = uni_cmp_rev_na_str((uint8_t *)oname, olen,
                             uni_rev_na_rng_strs + uni_rev_na_rng_arr[m].off,
			     uni_rev_na_rng_arr[m].len, &meq);
  if(l == h)
    heq = meq;
  if(!c)
    return -1;
  if(c > 0)
    l = m / 2 + 1;
  else {
    h = m / 2 - 1;
    heq = 0;
  }
}
@

To test if [[h]] is a valid match, it must first be a valid array
index.  It must also be a prefix of [[oname]]: the match length must
be equal to its length.  If it is not a valid match, it may still be a
reserved or non-character.

<<Convert synthetic [[name]] to code point>>=
if(h >= 0) {
  h *= 2;
  if(!heq)
    uni_cmp_rev_na_str((uint8_t *)oname, olen,
                       uni_rev_na_rng_strs + uni_rev_na_rng_arr[h].off,
		       uni_rev_na_rng_arr[h].len, &heq);
  if(heq != uni_rev_na_strlen(uni_rev_na_rng_strs + uni_rev_na_rng_arr[h].off,
		              uni_rev_na_rng_arr[h].len))
    h = -1;
}
if(h < 0) {
  if(!strncmp(oname, "<reserved", sizeof("<reserved") - 1)) {
    h = -1;
    heq = sizeof("<reserved") - 1;
  } else if(!strncmp(oname, "<noncharacter", sizeof("<noncharacter") - 1)) {
    h = -2;
    heq = sizeof("<noncharacter") - 1;
  } else
    return -1;
}
@

The next thing to check is the extension itself.  For all but Hangul
syllables, this is a hexadecimal digit string of four or more
characters, with no excess leading zeroes, possibly followed by a
greater-than symbol.  There is at least one range with the same name
but two different numerical ranges, so a name match is checked for all
others with the exact same name as well.

<<Convert synthetic [[name]] to code point>>=
char *s = oname + heq;
if(h >= 0 && uni_rev_na_rng_arr[h].cp == 0xAC00) {
  <<Parse Hangul syllable extension>>
} else {
  uint32_t ret;
  if((*s == '0' && (olen - heq) > 4 && s[4] != '>') ||
     olen - heq < 4 || (olen - heq > 8 && s[8] != '>'))
    return -1;
  ret = strtoul(s, &s, 16);
  if(oname[0] == '<')
    if(*s++ != '>')
      return -1;
  if(*s)
    return -1;
  if(h >= 0) {
    if(ret >= uni_rev_na_rng_arr[h].cp && ret <= uni_rev_na_rng_arr[h + 1].cp)
      return ret;
    m = h;
    while(m > 0 && uni_rev_na_rng_arr[m - 1].off == uni_rev_na_rng_arr[m].off &&
          uni_rev_na_rng_arr[m - 1].len == uni_rev_na_rng_arr[m].len) {
      m -= 2;
      if(ret >= uni_rev_na_rng_arr[m].cp && ret <= uni_rev_na_rng_arr[m + 1].cp)
        return ret;
    }
    m = h;
    while(m < uni_rev_na_rng_arr_len - 2 &&
          uni_rev_na_rng_arr[m + 2].off == uni_rev_na_rng_arr[m].off &&
          uni_rev_na_rng_arr[m + 2].len == uni_rev_na_rng_arr[m].len) {
      m += 2;
      if(ret >= uni_rev_na_rng_arr[m].cp && ret <= uni_rev_na_rng_arr[m + 1].cp)
        return ret;
    }
  } else {
    if(h == -2 && uni_is_NChar(ret)) /* nonchar */
      return ret;
    /* Name lookup, but at least it doesn't pull in name strings */
    /* Probably ought to turn this into a boolean property */
    else if(h == -1 && !uni_na_of(ret)->len && !uni_na_rng_of(ret))
      return ret;
  }
  return -1;
}
@

Parsing the Hangul syllables is a bit harder, requiring
character-by-character parsing.  The suffix is a JSN of hst L,
followed by a JSN of hst V, optionally followed by a JSN of hst T.  It
might be possible to come up with a regular expression to do the
match, but that would not help with returning the correct code point.
Instead, the JSN property is modified in a manner similar to the
names, but split by hst, to assist in reverse lookups.  The simplest
way to keep things separated by hst is to prefix each name with its
hst.  No strings are more than 3 characters, so this does not affect
the string table length.  Rather than look up hst, the first 19 are
marked as Choseong (L), the next 21 as Jungseong (V), and the rest as
Jongseong (T).  In fact, the enumeration values are not used at all,
but instead just 0, 1, and 2, respectively.  The code points
themselves are somewhat useless, so they are changed to an offset from
the first entry in each group.

<<Post-process property data>>=
int prop_rev_JSN = add_prop("rev_JSN");
prop_t *rev_JSN = &parsed_props[prop_rev_JSN];
prop_t *JSN = &parsed_props[prop_JSN];
rev_JSN->max_strs = rev_JSN->strs_len = JSN->strs_len;
inisize(rev_JSN->strs, rev_JSN->max_strs);
dstp = (uint8_t *)rev_JSN->strs;
dstlen = 0;
rev_JSN->max_len = rev_JSN->len = JSN->len;
inisize(rev_JSN->str_arr, rev_JSN->max_len);
int lvtno = 0;
for(i = 0; i < JSN->len; i++) {
  uint8_t *src = (uint8_t *)(JSN->strs + JSN->str_arr[i].off);
  unsigned int srclen = JSN->str_arr[i].len * 4;
  while(srclen && !src[srclen - 1])
    srclen--;
  rev_JSN->str_arr[i].off = dstlen / 4;
  if(i == 19 || i == 19 + 21)
    lvtno = 0;
  rev_JSN->str_arr[i].cp = lvtno++;
  rev_JSN->str_arr[i].flags = 0;
  if(srclen)
    dstp[dstlen++] = i < 19 ? 0 /* L */ : i < 19 + 21 ? 1 /* V */ : 2 /* T */;
  for(j = 0; j < srclen; j++) {
    <<Preprocess [[src[j]]]] into [[dstp[dstlen]]]>>
  }
  while(dstlen % 4)
    dstp[dstlen++] = 0;
  rev_JSN->str_arr[i].len = dstlen / 4 - rev_JSN->str_arr[i].off;
}
rev_JSN->strs_char_size = 8;
@

<<Dump character information as C code>>=
dump_wordord_strs(rev_JSN, "Jamo_Short_Name reverse lookup", gen_h);
@

The L is always either a single character, no character, or a doubled
character.  It is always followed by a V, which is always non-blank
and never begins with the same character as an L.  This means that the
first character (or two if the next is the same) can be looked up for
L.  A lookup failure is interpreted as a blank L.  

<<Parse Hangul syllable extension>>=
int slen = *s == s[1] ? 2 : 1;
l = 1; /* skip 0, since it's blank */
h = 18;
<<Search reverse JSN for [[s]] of length [[slen]]>>
int L = uni_rev_JSN_arr[l <= h ? m : 0].cp;
if(l <= h)
  s += slen;
@

<<Search reverse JSN for [[s]] of length [[slen]]>>=
while(l <= h) {
  m = (l + h) / 2;
  len = uni_rev_JSN_arr[m].len - 1;
  if(len > slen)
    len = slen;
  int c = memcmp(s, uni_rev_JSN_arr[m].off + uni_rev_JSN_strs + 1, len);
  if(!c && slen == uni_rev_JSN_arr[m].len - 1)
    break;
  else if(c < 0 || (!c && slen == len))
    h = m - 1;
  else
    l = m + 1;
}
@

The V is always a single character, followed by optional vowels (A,
E, I, O, or U).  Since T never starts with a vowel, V can be extracted
and looked up.  A lookup failure is an invalid suffix.

<<Parse Hangul syllable extension>>=
slen = 1;
while(s[slen] == 'a' || s[slen] == 'e' || s[slen] == 'i' ||
      s[slen] == 'o' || s[slen] == 'u')
  slen++;
l = 19;
h = l + 21 - 1;
<<Search reverse JSN for [[s]] of length [[slen]]>>
if(l > h)
  return -1;
int V = uni_rev_JSN_arr[m].cp;
s += slen;
@

Finally, the T part is what's left over, if anything.  A blank is a
missing T, and otherwise a lookup failure is an invalid suffix.

<<Parse Hangul syllable extension>>=
slen = strlen(s);
int T;
if(!slen)
  T = 0;
else {
  l = 19 + 21;
  h = uni_rev_JSN_arr_len - 1;
  <<Search reverse JSN for [[s]] of length [[slen]]>>
  if(l > h)
    return -1;
  T = uni_rev_JSN_arr[m].cp + 1;
}
@

A successful lookup on all three means that the return value can be
computed and returned.

<<Parse Hangul syllable extension>>=
return 0xAC00 + T + 28 * (L * 21 + V);
@

In addition, a binary search function for the main name table is
provided, although it is not intended to be used for serious
applications.  Since the range lookup function pulls in a smaller
table, it is placed in a separate object file from the main lookup
function.

<<Unicode property exports>>=
/** Look up Unicode code point name.
  * Look up \p name/\p len using binary searching and loose matching.  If
  * \p len is negative, \p name is zero-terminated.
  * This function can also interpret angle-bracket-enclosed labels
  * (i.e. \<\e range-\e hex_cp\>).  If the return value is negative,
  * \p name does not loosely match any Unicode code point name, sequence
  * name, or label.  If the return value is greather than \ref UNI_MAX_CP,
  * it is the sequence \ref uni_na_seq_id_arr[\e N - \ref UNI_MAX_CP - 1],
  * where \e N is the return value from this function.  */
int32_t uni_name_to_cp(const char *name, int len);
@

<<Library [[uni]] Members>>=
narev_bin.o
@

<<narev_bin.c>>=
<<Common C Header>>
#include "uni_prop.h"

int32_t uni_name_to_cp(const char *name, int len)
{
  unsigned int nlen = len < 0 ? len : strlen(name);
  <<Strip name before lookup>>
  <<Look up name and return if present>>
  return uni_name_rng_to_cp(name, nlen);
}
@

Unlike with the name ranges, dash locations must be tracked.  This is
done using a flag in a side array.

<<Strip name before lookup>>=
const char *n = name;
unsigned int olen = nlen;
if(olen > 127)
  olen = 127;
char oname[olen + 1], hyphen[olen];
olen = 0;
int had_hyphen = 0;
for(; len > 0; n++, len--) {
  if(isspace(*n) || *n == '_')
    continue;
  if(*n == '-') {
    had_hyphen = 1;
    continue;
  }
  if(olen == 127)
    return -1;
  if(!isdigit(*n) && !islower(*n) && *n != '<' && *n != '>')
    return -1;
  hyphen[olen] = had_hyphen;
  oname[olen++] = *n;
  had_hyphen = 0;
}
oname[olen] = 0;
@

Like with ranges, the next step is a binary search.  Unlike ranges, an
exact match is the only thing that matters.

<<Look up name and return if present>>=
int l = 0, h = uni_rev_na_arr_len - 1, m;
while(l <= h) {
  m = (l + h) / 2;
  const uint8_t *rev_na = (const uint8_t *)(uni_rev_na_strs + uni_rev_na_arr[m].off);
  len = uni_rev_na_arr[m].len * 4;
  while(!rev_na[len - 1])
    len--;
  int c = uni_cmp_rev_na_str((uint8_t *)oname, olen, rev_na, len, NULL);
  if(!c)
    break;
  if(c > 0)
    l = m + 1;
  else
    h = m - 1;
}
@

An exact match does not include hyphens, so once that's checked, a
value can be returned.

<<Look up name and return if present>>=
if(l <= h) {
  uint32_t ret = uni_rev_na_arr[m].cp;
  <<Return code point [[ret]] unless a hyphen overrides it>>
}
@

<<Return code point [[ret]] unless a hyphen overrides it>>=
if(!uni_rev_na_needs_hyphen(ret))
  return ret;
for(l = 0; l < uni_na_hyphen_rng_len; l++)
  if(ret == uni_na_hyphen_rng[l].low) {
    if(hyphen[uni_na_hyphen_rng[l].dat & 0xff])
      return uni_na_hyphen_rng[l].dat >> 8;
    else
      return ret;
  }
@

Testing all of this can be difficult.  Instead of a thorough test, the
following program takes the output of [[tstcp_na]] above, and looks up
the name.  If the looked up code points do not match the advertised
ones, an error is returned.

\lstset{language=make}
<<Additional Tests>>=
./tstcp_na | ./tstna_cp >/dev/null
@

<<C Test Support Executables>>=
tstna_cp \
@

\lstset{language=C}
<<tstna_cp.c>>=
<<Common C Header>>

#include "uni_prop.h"

char buf[256];

int main(void)
{
  int lno;
  for(lno = 0; fgets(buf, 256, stdin); lno++) {
    <<Set [[e]] to start of name to check>>
    for(s = e; *s; s++)
      if(isupper(*s))
        *s = tolower(*s);
    uint32_t ret = uni_name_to_cp(e, (int)(s - e));
    <<Print and check [[name_to_cp]] results using [[na_seq_id]]>>
  }
  fprintf(stderr, "Successfully looked up %d entries\n", lno);
  return 0;
}
@

<<Set [[e]] to start of name to check>>=
char *s, *e;
if(!(e = strchr(buf, ' ')))
  continue;
*e++ = 0;
if(!(e = strchr(e, ' ')))
  continue;
e++;
if(*e == '&')
  e++;
s = strchr(e, '\n');
if(s)
  *s = 0;
@

<<Print and check [[name_to_cp]] results using (@seqid)>>=
if(!strchr(buf, ':')) {
  printf("%04X %s\n", ret, e);
  if(ret != strtoul(buf, &s, 16) || *s)
    exit(1);
} else if(ret <= UNI_MAX_CP)
  exit(1);
else {
  const uni_str_ptr_t *seq = &uni_<<@seqid>>_arr[ret - UNI_MAX_CP - 1];
  const uint16_t *retp = seq->off + uni_<<@seqid>>_strs;
  int len = seq->len;
  int eq = 1;
  for(s = buf; len > 0; ) {
    unsigned int clen;
    uint32_t next = uni_int_utf16_decode(retp, &clen);
    if(strtoul(s, &s, 16) != next)
      eq = 0;
    printf("%04X ", (int)next);
    len -= clen;
    retp += clen;
    if(*s)
      s++;
  }
  puts(e);
  if(!eq)
    exit(1);
}
@

A binary search is not very effective with such a large table.  At
least a sample hash table implementation should be provided as well.
As menioned above, a perfect hash generated by [[gperf]] is
impractical.  However, while trying to find a faster [[gperf]], I came
across a program called [[cmph]]%
\footnote{\url{http://sourceforge.net/projects/cmph}}%
.  This generates minimal perfect hash functions in linear time (sub
second time for the entire Unicode name set!).  However, it is not
very good for inclusion in a library, because it generates the hash
table in an external file or in memory (but not as a single savable
memory block) rather than as compilable C code.  This means that the
hash table would need to be stored somewhere and be parsed every time
the hash table is first used in a program.  I have made changes to
[[cmph]] to allow generation of C code instead%
\footnote{\url{http://sourceforge.net/tracker/?func=detail\&aid=3590339\&group_id=126608\&atid=706189}
and \url{http://sourceforge.net/u/darktjm/cmph}}% .  While these
changes were submitted upstream, they were rejected by the authors as
``not being general enough.''   Their recommended workaround of
encoding the file as a static array is too much trouble (because there
is no function to load such an array, you'd have to write a program
which loads the file, converts it to a packed blob, and use that
instead, which is slower to use at run-time as well).  The following
assumes you use my method instead.

The [[cmph]] support is optional, and must be enabled with a
configuration option.  Since using it requires linking to its library
([[-lcmph]]) as well, an extra step is taken:  the hash table and its
support functions are not just in a separate object file, they are in
an entirely different library.

\lstset{language=make}
<<Library [[uni_cmph]] Members>>=
cmph_rev_na.o
cmph_na_supt.o
@

<<makefile.config>>=
# Path to cmph binary; set to enable cmph support
#CMPH=cmph

# Generation options for cmph table
CMPH_GEN=-a bdz_ph -b 3

# Include directory for cmph headers
#CMPH_INCLUDE=/usr/include

# Library flags for cmph library
#CMPH_LDFLAGS=-lcmph

@

<<makefile.rules>>=
cmph_in_na.gen: cmph_in_na_gen
	./cmph_in_na_gen >$@

cmph_in_na.gen.mph: cmph_in_na.gen
	$(CMPH) -g $(CMPH_GEN) cmph_in_na.gen

ifneq ($(CMPH),)
cmph_rev_na.c: cmph_in_na.gen.mph
	$(CMPH) -C uni_rev_na_mph -o $@ cmph_in_na.gen
else
cmph_rev_na.c: uni_prop.gen.h
	echo >$@
endif

# needed for cproto.h, so may as well include everywhere
ifneq ($(CMPH),)
EXTRA_CFLAGS += -DUSE_CMPH
ifneq ($(CMPH_INCLUDE),)
EXTRA_CFLAGS += -I$(CMPH_INCLUDE)
endif
else
# don't offer CMPH if it's not supported in binary
NOTANGLE_POSTPROC+=|sed -e 's/ifdef USE_CMPH/if 0/'
endif
@

<<Clean temporary files>>=
rm -f cmph_in_na.gen{,.mph} cmph_rev_{,x}na.c
@

\lstset{language=C}
<<cmph_na_supt.c>>=
#ifdef USE_CMPH
<<Common C Header>>

#include "uni_prop.h"

<<[[cmph]] name lookup support>>
#endif
@

<<Unicode property exports>>=
#ifdef USE_CMPH
#include <cmph.h>
<<Unicode property exports using [[cmph]]>>
#endif
@

<<Unicode property exports using [[cmph]]>>=
/** Pre-generated cmph perfect hash table for looking up names */
extern const cmph_t uni_rev_na_mph;
@

The input to [[cmph]] is a list of names.  The output function returns
an index into that list, which must be verified by comparing directly
to that list element.  Since the name list must be expanded, the
original na array cannot be used.  So, the same rev\_na list built above
is still needed.

\lstset{language=make}
<<C Build Executables>>=
cmph_in_na_gen \
@

<<makefile.rules>>=
cmph_in_na_gen: libuni.a cmph_in_na_gen.o
	$(CC) -o $@ cmph_in_na_gen.o -L. -luni
@

\lstset{language=C}
<<cmph_in_na_gen.c>>=
<<Common C Header>>

#include "uni_prop.h"

int main(void)
{
  unsigned int i;
  uint8_t *buf = NULL;
  unsigned int buf_len = 0;
  
  for(i = 0; i < uni_rev_na_arr_len; i++) {
    const uint8_t *p = (const uint8_t *)(uni_rev_na_strs + uni_rev_na_arr[i].off);
    unsigned int l = uni_rev_na_arr[i].len * 4;
    while(!p[l - 1])
      l--;
    l = uni_rev_na_expand_words(p, l, &buf, 0, &buf_len);
    printf("%.*s\n", l, (char *)buf);
  }
  return 0;
}
@

The value returned by the lookup function is arbitrary.  The only way
to map this number back to a name is to look up the names, and
generate a translation table from return value to name array index.
This translation is unnecessary if the algorithm is CHM, so a separate
preprocessor define activates this table (but it is always generated,
anyway).  On all but two algorithms, the return value is within range
of the input table size, so a blind direct lookup is possible.
However, for a few types, the values are sparse, so the return value
must be checked against the larger table size rather than the name
table size.  Thus the table size is exported (always) as well.

\lstset{language=make}
<<makefile.config>>=
# Set to non-blank if perfect hash function is not order-preserving
# at time of writing, this means any algorithm other than chm (the default)
CMPH_XLATE=y

# Set to non-blank if the perfect hash function's results are not within
# the range 0 - <len>-1
# at time of writing, this means bdz_ph and chd_ph
CMPH_SPARSE=y

@

<<makefile.rules>>=
# needed for cproto.h, so may as well include everywhere
ifneq ($(CMPH),)
ifneq ($(CMPH_XLATE),)
EXTRA_CFLAGS += -DCMPH_XLATE $(if $(CMPH_SPARSE),-DCMPH_SPARSE)
endif
endif
@

\lstset{language=C}
<<Unicode property exports using [[cmph]]>>=
/** Translate return from cmph name lookup return to a \ref uni_rev_na_arr
  * array index.  If the return value is ~0, the lookup failed.  */
extern const uint32_t uni_rev_na_mph_xlate[];
/** The maximum valid value returned by a cmph name lookup. */
extern const uint32_t uni_rev_na_max_mph_res;
@

\lstset{language=make}
<<makefile.rules>>=
cmph_na_xlate.gen.c: cmph_in_na.gen.mph
	$(call gen_xlate,na) >$@
@

<<makefile.vars>>=
gen_xlate = ( \
  echo "\#include <stdint.h>"; \
  echo "const uint32_t uni_rev_$1_mph_xlate[] = {"; \
  $(CMPH) -v -m cmph_in_$1.gen.mph cmph_in_$1.gen | sed 's/.* -> //' | \
  ( i=0; while read n; do echo $$n $$i; i=$$((i+1)); done ) | sort -n | \
  ( i=0; while read x n; do \
           while [ $$i -lt $$x ]; do echo $$i '~0'; i=$$((i+1)); done; \
	   echo $$x $$n; i=$$((i+1)); \
	 done ) | \
  ( \
    i=0; tr='\t'; \
    while read x n; do \
      i=$$((i+1)); \
      printf "$$tr$$n"; \
      if [ $$i -lt 8 ]; then tr=', '; else i=0; tr=',\n\t'; fi; \
    done); \
  echo; \
  echo "};"; \
  echo $(if $(CMPH_SPARSE), \
        "const uint32_t uni_rev_$1_max_mph_res = " \
        "sizeof(uni_rev_$1_mph_xlate)/sizeof(uint32_t);") )
@

<<Library [[uni_cmph]] Members>>=
cmph_na_xlate.gen.o
@

<<Clean temporary files>>=
rm -f cmph_na_xlate.gen.c
@

The lookup function itself is identical to the one using binary
searching, except for the primary name lookup.  That means that binary
searching is still used for the range lookups; there is no easy way to
convert that to a hash lookup.  A prefix tree (trie) might be more
effective, but that is left as an exercise for the end user.

\lstset{language=C}
<<Unicode property exports using [[cmph]]>>=
/** Look up Unicode code point name.
  * Look up \p name/\p len using a cmph perfefct hash function and loose
  * matching.  If \p len is negative, \p name is zero-terminated.
  * This function can also interpret angle-bracket-enclosed labels
  * (i.e. \<\e range-\e hex_cp\>).  If the return value is negative,
  * \p name does not loosely match any Unicode code point name, sequence
  * name, or label.  If the return value is greather than \ref UNI_MAX_CP,
  * it is the sequence \ref uni_na_seq_id_arr[\e N - \ref UNI_MAX_CP - 1],
  * where \e N is the return value from this function.  */
int32_t uni_cmph_name_to_cp(const char *name, int len);
@

<<[[cmph]] name lookup support>>=
int32_t uni_cmph_name_to_cp(const char *name, int len)
{
  unsigned int nlen = len < 0 ? len : strlen(name);
  <<Strip name before lookup>>
  <<Look up name using [[cmph]] and return if present>>
  return uni_name_rng_to_cp(name, nlen);
}
@

The primary name lookup requires a hash lookup, followed by an
explicit comparison.  A successful comparison is followed by a hyphen
check, just like with the binary searching routine.

<<Look up name using [[cmph]] and return if present>>=
uint32_t ret = cmph_search((cmph_t *)&uni_rev_na_mph, oname, olen);
#if defined(CMPH_SPARSE) && defined(CMPH_XLATE)
if(ret < uni_rev_na_max_mph_res)
  ret = uni_rev_na_mph_xlate[ret];
#endif
if(ret < uni_rev_na_arr_len) {
#if defined(CMPH_XLATE) && !defined(CMPH_SPARSE)
  ret = uni_rev_na_mph_xlate[ret];
#endif
  const uint8_t *rev_na = (const uint8_t *)(uni_rev_na_strs + uni_rev_na_arr[ret].off);
  len = uni_rev_na_arr[ret].len * 4;
  while(!rev_na[len - 1])
    len--;
  if(!uni_cmp_rev_na_str((uint8_t *)oname, olen, rev_na, len, NULL)) {
    ret = uni_rev_na_arr[ret].cp;
    int l; /* for hyphen check */
    <<Return code point [[ret]] unless a hyphen overrides it>>
  }
}
@

Testing this is mostly identical to testing the binary search
routines, but timings are also taken to verify that the hash is
actually faster.

\lstset{language=make}
<<Additional Tests>>=
$(if $(CMPH),./tstcp_na | ./tstna_cp_cmph >/dev/null)
@

<<C Test Support Executables>>=
$(if $(CMPH),tstna_cp_cmph) \
@

<<makefile.rules>>=
tstna_cp_cmph: LDFLAGS += $(if $(CMPH_LDFLAGS),$(CMPH_LDFLAGS),-lcmph)
@

\lstset{language=C}
<<tstna_cp_cmph.c>>=
#ifdef USE_CMPH
<<Common C Header>>

#include "uni_prop.h"

<<POSIX timing support>>

char buf[256];

int main(void)
{
  double tb = 0, th = 0, tbr = 0, thr = 0, tr = 0;
  unsigned long nn = 0, nr = 0;
  while(fgets(buf, 256, stdin)) {
    <<Set [[e]] to start of name to check>>
    for(s = e; *s; s++)
      if(isupper(*s))
        *s = tolower(*s);
    int32_t ret = uni_cmph_name_to_cp(e, (int)(s - e));
    <<Print and check [[name_to_cp]] results using [[na_seq_id]]>>
    /* time lookup failures (synthetic names) and lookup successes separately */
    int isr = ret < 0 || (ret <= UNI_MAX_CP && !uni_na_of(ret)->len);
    if(isr)
      nr++;
    else
      nn++;
    int j;
    tstart();
    for(j = 0; j < 25; j++)
      ret = uni_name_to_cp(e, (int)(s - e));
    if(isr)
      tbr += tend();
    else
      tb += tend();
    tstart();
    for(j = 0; j < 25; j++)
      ret = uni_cmph_name_to_cp(e, (int)(s - e));
    if(isr)
      thr += tend();
    else
      th += tend();
    if(isr) {
      /* also time pure synthetic name speed w/o lookup failure overhead */
      tstart();
      for(j = 0; j < 25; j++)
        ret = uni_name_rng_to_cp(e, (int)(s - e));
      tr += tend();
    }
  }
  /* report as ops per second rather than total seconds */
  tb = nn * 25000000.0 / tb;
  th = nn * 25000000.0 / th;
  fprintf(stderr, "norm %lu b%.0f/s h%.0f/s %.2fx\n", nn, tb, th, th/tb);
  tr = nr * 25000000.0 / tr;
  tbr = nr * 25000000.0 / tbr;
  thr = nr * 25000000.0 / thr;
  fprintf(stderr, "syn %lu r%.0f/s b%.0f/s h%.0f/s %.2fx\n", nr, tr, tbr, thr, thr/tbr);
  return 0;
}
#endif
@

There may be better tuning options, but my experiments show that the
hash function is not, in fact, sufficiently faster to justify its use.
For nearly all inputs, the hash function is within a few percent of
the performance of the binary search, and in fact it is sometimes
slower.  The fastest result I found was bdz\_ph with the lowest
possible [[-b]] setting, which is about 16\%-20\% faster.

\lstset{language=txt}
<<FIXME>>=
Document lack of NamesList.txt (informative, harder to parse, mostly useless)
@

\subsection{Parsing the UCD -- DUCET}

The most complex optional string value is derived from the Default
Unicode Collation Element Table
(\url{http://unicode.org/Public/UCA/}).  It associates many strings
(collation elements, which are the index to the table) with explicit
sort keys (the data).  This includes several multi-character collation
elements (called contractions) and multi-entry keys (called
expansions).  Each sort key entry consists of a number for each
supported sort level, plus a flag.  Currently, the DUCET supports four
sort levels, although the standard states that more may be added in a
future version (although recent versions have actually reduced the
number of levels to three for most keys).  The flag indicates
so-called variable entries.  The flag does not actually need to be
stored, since it is always true for entries with a non-zero first
level up to a certain top.  This top is stored separately as a single
integer.

Rather than make this reader as generic as possible, given the
definitions, it is tailored to the actual data present in the UCA
versions available at the time of this writing.  That means that
recoding may be necessary at a future date.  The first constraint is
that exactly four levels are supported.  Even though versions 6.3 and
later actually only have three levels, the fourth is still filled in
with the number zero.

While reading, each entry is indexed on the first character of the
collation element.  The raw string value is a four-word sequence for
every sort key entry, followed by the remainder of the collation
element index.  Since no multi-character index has more than four
characters, there is no problem distinguishing the character groups in
the value: the number of key entries is the length divided by four,
and the number of extra characters in the index is the remainder.
Since several multi-character collation elements start with the same
character (and in fact the well-formedness rules require this),
multiple entries with the same [[cp]] are created, but the raw string
table storage method does not care.

\lstset{language=C}
<<Initialize UCD files>>=
decl_str(DUCET);
@

<<Parse UCD files>>=
uint32_t ducet_vartop;
ducet_vartop = parse_ducet("allkeys.txt", (prop_DUCET = add_prop("DUCET")));
if(!ducet_vartop) {
  perror("allkeys.txt");
  exit(1);
}
parsed_props[prop_DUCET].strs_char_size = 32;
@

<<UCD parser local functions>>=
<<DUCET parser globals>>
static uint32_t parse_ducet(const char *fname, int propn)
{
  <<Parser common variables>>
  prop_t *prop = &parsed_props[propn];
  uint32_t vartop = 0;
  if(!(f = fopen(fname, "r")))
    return 0;
  while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    uint32_t str[20*4]; /* current max: 18 * 4 */
    uint32_t len = 0;
    int is_var;

    split_line(lbuf);
    if(!num_fields || !isxdigit(*lbuf))
      continue;
    low = high = strtol(lbuf, &s, 16);
    for(s = fields[1]; s; s = strchr(s, '[')) {
      is_var = s[1] == '*';
      str[len++] = strtol(s + 2, &s, 16);
      if(is_var && str[len - 1] > vartop)
        vartop = str[len - 1];
      str[len++] = strtol(s + 1, &s, 16);
      str[len++] = strtol(s + 1, &s, 16);
      str[len++] = strtol(s + 1, &s, 16);
    }
    /* this assumes that fields[0] will never have more than 4 chars */
    /* that way, len % 4 == extra chars in fields[0]; len / 4 = # of keys */
    if((s = strchr(fields[0], ' '))) {
      do {
        str[len++] = strtol(s, &s, 16);
	if(!(len % 4)) {
	  fputs("DUCET contraction too long\n", stderr);
	  exit(1);
        }
      } while(*s);
    }
    /* 6.2: 120007 words (6212 saved) */
    /* 6.2-CLDR: 120493 words (5724 saved) */
    /* 7.0: 117482 words (17796 saved) */
    /* 7.0-CLDR: 117534 words (17688 saved) */
    add_str_rng(prop, low, high, str, len);
  }
  fclose(f);
  if(lbuf)
    free(lbuf);
  <<Post-process DUCET>>
  return vartop;
}
@

The raw data in the Unicode 6.2 DUCET comes to 126,219 words, of which
6,212 can be removed due to redundancy.  This is too much data for the
16-bit offset used by [[uni_str_arr_t]] type, so some adjustment needs
to be made.

A little space can be saved while storing this by making some strings
identical when they would normally not be, thereby increasing the
removals due to redundancy.  Level 4 is usually equal to the code
point, and is never 1, so that case can be stored as 1 to indicate
that it is equal to the code point.  This saves some space in 6.2, but
in versions 6.3 and later the level 4 field is dropped almost
entirely, making this a less useful optimization.  In fact, the data
becomes slightly larger as a result (I'm not sure why, and don't care
enough to investigate).  However, this transformation becomes more
useful later, so I'll leave it in.

<<Post-process DUCET>>=
<<Prepare for DUCET post-processing>>
for(i = 0; i < prop->len; i++) {
  uint32_t len = prop->str_arr[i].len;
  uint32_t *str = prop->strs + prop->str_arr[i].off;
  uint32_t cp = prop->str_arr[i].cp;
  <<Reduce DUCET entry>>
}
@

<<Reduce DUCET entry>>=
for(j = 3; j < len; j += 4)
  if(str[j] == cp)
    str[j] = 1;
@

One common expansion is for the sort key to be the concatenation of
the sort keys for each character in the canonical decomposition of the
collation element.  Since the official algorithm requires canonical
decomposition, these entries are removed, at least for
single-character collation elements.

The removal of this data requires decomposition lookup during DUCET
lookup.  However, the UCA algorithm specifies NFD form for its input
strings; in that case, the decomposition will already be done, and no
extra work is needed in the lookup function.  However, a special case
(FCD; see below) allows for unnormalized data inputs, where this
lookup would be necessary.  That is probably why these redundant
entries were present in the first place, but even they were not
complete:  they did not include synthetic compositions (i.e., the
Hangul syllables).  In any case, limiting it to single-character
collation elements means that at least the sorting part of NFD can be
skippped.

Note that the standard specifies a much more liberal decomposition
policy for generating expansions.  It uses compatibility
decomposition instead of canonical decomposition, and replaces level 3
of the resulting keys with a translation of the dt property value. 
The amount of additional work at run-time required to support this
isn't that great, and it might reduce the table a lot, but it affects
many characters even after NFD decomposition, so for now, entries
which use this technique are not suppressed.

% tertiary weight table (note: can is impossible):
%  none     def 02
%  wide     def 03
%  compat   def 04
%  font     def 05
%  circle   def 06
%  can/none uc  08
%  wide     uc  09
%  compat   uc  0A
%  font     uc  0B
%  circle   uc  0C
%  small    sh  0D
%  none     nh  0E
%  small    sk  0F
%  narrow   snk 10
%  none     nk  11
%  narrow   nkh 12
%  circle   ck  13
%  super    def 14
%  sub      def 15
%  vertical def 16
%  initial  def 17
%  medial   def 18
%  final    def 19
%  isolated def 1A
%  noBreak  def 1B
%  square   def 1C
%  square/super/sub uc 1D
%  fraction def 1E
%  MAX      --  1F
%
% uc == Uppercase, sh == small hira, nh == non-small hira,
% sk == small kata, nk == non-small kata, snk == small narrow kata,
% nkh == narrow katakana/hangul, ck == circled kata
% -- == def for last char in decomp; also used for disambiguation

% l4 weight:
%   l3-ignorable and Cc, Cf, or "variation selector":  0
%   otherwise:  cp (but what about contractions & expansions?)
% however, l4=0 weight rule is violated on 12 entries:
%  0600 0601 0602 0603 0604 0605 06DD 2061 2062 2063 2064 110BD
%   all of these are Cc or Cf, but have l4 == cp
% 6.3 simply drops l4 from allkeys.txt.

<<Prepare for DUCET post-processing>>=
prop_t *dmf_prop = &parsed_props[add_prop("dm_full")];
/* sort ducet for component lookup */
qsort(prop->str_arr, prop->len, sizeof(*prop->str_arr), uni_cmp_cp);
@

<<Reduce DUCET entry>>=
/* if it's pre-decomposed, then strip it */
if(str[3] != 1 && !(len % 4)) {
  raw_cp_str_t *dec = bsearch(&cp, dmf_prop->str_arr,
                              dmf_prop->len,
			      sizeof(raw_cp_str_t), uni_cmp_cp);
  if(dec) {
    while(dec->flags && dec > dmf_prop->str_arr && dec->cp == cp)
      dec--; /* select canonical full decomp only */
    if(dec->flags || dec->cp != cp)
      dec = NULL;
  }
  if(dec && dec->len * 4 <= len) { /* may be longer if synthetics present */
    int k;
    for(j = k = 0; j < dec->len && k < len; j++, k += 4) {
      uint32_t dc = dmf_prop->strs[dec->off + j];
      <<Skip if synthetic key for [[dc]] present>>
      raw_cp_str_t *cd;
      uint32_t ocp = str[3 + k];
      if(ocp && ocp != dc)
        break;
      cd = bsearch(&dc, prop->str_arr, prop->len, sizeof(raw_cp_str_t),
		   uni_cmp_cp);
      if(!cd)
        break;
      /* scan for single-element entry */
      /* some may have dc as 1st char of multi-char entry */
      while(cd->len != 4 && cd > prop->str_arr && cd[-1].cp == dc)
        cd--;
      while(cd->len != 4 && cd < prop->str_arr + prop->len - 1 && 
            cd[1].cp == dc)
	cd++;
      if(cd->len != 4 ||
         /* need to compare against dc in case not converted to 1 yet */
	 (prop->strs[cd->off + 3] && prop->strs[cd->off + 3] != 1 &&
	 prop->strs[cd->off + 3] != dc) ||
	 memcmp(prop->strs + cd->off, str + 4 * j, 12))
        break;
    }
    if(j == dec->len && k == len) {
      movebuf(prop->str_arr + i, prop->str_arr + i + 1, prop->len - i - 1);
      --i;
      --prop->len;
      continue;
    }
  }
}
@

While sort keys present in the table can simply be looked up, some of
the characters may require synthetic keys.  If the lookup fails, a
key is synthesized.  Note that synthesis requires the UIdeo and blk
properties as specified, although this could be modified to use
explicit range checks instead.  The UIdeo property is complicated
enough and yet small enough (12 ranges/300 bytes) that conversion
would not save much, but the blk checks are just single ranges each
(and the blk table is much larger, at 220 ranges/8032 bytes).  This
assumes that the ranges will never change in the future, which is
actually a pretty safe bet.

Unicode 9.0.0 introduces an entry in [[allkeys.txt]] for all but the
UIdeo ones:  [[@implicitweights]].  This would allow new synthesis
entries without having to recode.  However, they only include the
value for the first synthetic element, not the second (which is OK if
the second just subtracts the start of the range, but that is not the
case for Tangut Supplement, added in 13.0.0).

The UCA standard used to give no hints on how to set the secondary
value; presumably any non-zero number will work, because the primaries
will always differ.  The most common value (0020) is chosen for this.
The tertiary value has a table, but since no decomposition is
involved, and all special cases are already covered in the DUCET, all
characters get the default (0002) assigned to them.   Note that in
recent standards, 0020 and 0002 are the recomended values.

For the fourth level, the UCA standard states that any non-zero value
should be used on the first key element, and zero on the second.
However, actual 6.2 DUCET values which duplicate synthesized values
(due to decomposition) show a non-zero level four on the second key
element as well, and in fact both use the code point.  For
consistency, that is what is generated.  In 6.3 and later, zero is
generated for both level 4 values in the tables.

<<Skip if synthetic key for [[dc]] present>>=
if(k < len - 4 && str[k] == uca_synth1(dc) && str[k + 1] == 0x20 &&
  str[k + 2] == 2 && (!str[k + 3] || str[k + 3] == 1 || str[k + 3] == dc) &&
  str[k + 4] == uca_synth2(dc) && !str[k + 5] &&
  !str[k + 6] && (!str[k + 7] || str[k + 7] == 1 || str[k + 7] == dc)) {
 k += 4;
 continue;
}
@

<<Synthetic UCA key support>>=
static uint32_t uca_synth1(uint32_t c)
{
  uint32_t ret = c >> 15;
#if 0
  uni_blk_t blk = uni_blk_of(c);
#endif

#if 0
  if(blk == UNI_blk_Tangut || blk == UNI_blk_Tangut_Components ||
     blk == UNI_blk_Tangut_Sup)
#else
  if((c >= 0x17000 && c <= 0x18AFF) || (c >= 0x18D00 && c <= 0x18D8F))
#endif
    return 0xFB00;
#if 0
  if(blk == UNI_blk_Nushu)
#else
  if(c >= 0x1B170 && c <= 0x1B2FF)
#endif
    return 0xFB01;
#if 0
  if(blk == UNI_blk_Khitan_Small_Script)
#else
  if(c >= 0x18B00 && c <= 0x18CFF)
#endif
    return 0xFB02;
  if(uni_is_UIdeo(c)) {
#if 0
    if(blk == UNI_blk_CJK || blk == UNI_blk_CJK_Compat_Ideographs)
#else
    if((c >= 0x4E00 && c <= 0x9FFF) || (c >= 0xF900 && c <= 0xFAFF))
#endif
      ret += 0xFB40;
    else
      ret += 0xFB80;
  } else
    ret += 0xFBC0;
  return ret;
}
#define uca_synth2(c) \
  (((c) >= 0x17000 && (c) <= 0x18AFF) || ((c) >= 0x18D00 && (c) <= 0x18D8F) ? \
    (c) - 0x17000 + 0x8000 : \
   (c) >= 0x1B170 && (c) <= 0x1B2FF ? \
    (c) - 0x1B170 + 0x8000 : \
   (c) >= 0x18B00 && (c) <= 0x18CFF ? \
    (c) - 0x18B00 + 0x8000 : \
    (((c) & 0x7fff) | 0x8000))
@

<<DUCET parser globals>>=
static int uni_is_UIdeo(uint32_t c)
{
  static int UIdeo = -1;
  if(UIdeo < 0)
    UIdeo = add_prop("UIdeo");
  uni_chrrng_t rng = {c, c};
  return bsearch(&rng, parsed_props[UIdeo].rng, parsed_props[UIdeo].len,
                 sizeof(rng), uni_cmprng) != NULL;
}
<<Synthetic UCA key support>>
@

Similarly, some contractions are the canonical decomposition of a
single character, and the sort key is the sort key for that character.
That case could be encoded by a zero-length key, but retaining the
extra collation element characters as its string. However, not much is
saved (about 415 words in 6.2, although surprisingly many more for the
CLDR table), so this extra lookup complication is not imposed.  Unlike
the decomposition entries above, the composition entries are
legitimate collation elements, even after NFD transformation.  In
fact, if anything, the entry for the character they represent should be
removed, but that would save even less space.

One other redundancy I've noticed in some of the tables is the
presence of raw synthetic keys.  There is no need to store something
which can be generated by code at no real extra expense.

<<Reduce DUCET entry>>=
if(len == 8 && str[0] == uca_synth1(cp) && str[1] == 0x20 && str[2] == 2 &&
   (!str[3] || str[3] == 1) && str[4] == uca_synth2(cp) && !str[5] && !str[6] &&
   (!str[7] || str[7] == 1)) {
  movebuf(prop->str_arr + i, prop->str_arr + i + 1, prop->len - i - 1);
  --prop->len;
  --i;
  continue;
}
@

While there is a pattern to most of the rest of the DUCET, it is
difficult to encode, and encoding would make lookup much more
expensive than it probably already is.  The only other features of the
numbers which can be easily taken advantage of are the ranges.  The
first three levels are never more than four digits, or 16 bits.  In
fact, they are quite a bit smaller.  Rather than relying on a single
revision's ranges, the ranges are extracted in the first pass.  This
is mainly to provide a sanity check on the only characteristic that is
likely to change rapidly.

<<Prepare for DUCET post-processing>>=
uint32_t max_0 = 0, max_1 = 0, max_2 = 0;
@

<<Reduce DUCET entry>>=
for(j = 0; j + 3 < len; j += 4) {
  if(str[j] > max_0)
    max_0 = str[j];
  if(str[j + 1] > max_1)
    max_1 = str[j + 1];
  if(str[j + 2] > max_2)
    max_2 = str[j + 2];
}
@

In practice, the first level can be stored in 16 bits, the second in 9
bits, and the third in 5 bits.  In other words, all three levels fit
easily in a single 32-bit word.  While the second and third could be
stored in more bits to allow for more wiggle room, storing minimally
leaves two extra bits for other purposes.  No allowance is made for
larger required field sizes; recoding would be needed in several
places.

<<Post-process DUCET>>=
if(lg2(max_0) > 16 || lg2(max_1) > 9 || lg2(max_2) > 5) {
  fputs("FIXME: Can't reduce DUCET\n", stderr);
  exit(1);
}
@

The extra two bits can be used to encode certain common cases,
possibly eliminating the need to store a full 32-bit word for the
fourth level:

\begin{itemize}
\item 0: Level 4 is 0
\item 1: Level 4 is the index cp (encoded above as 1)
\item 2: The next word is level 4
\end{itemize}

When combining like this, each entry drops in length to 2 or fewer
words.  This means that if there are more than two characters in the
index string, it can no longer be detected by length alone.  Since
every character can be encoded in fewer than 30 bits, the extra index
characters are simply shifted left two, leaving the lower two bits for
special encoding as above:

\begin{itemize}
\item 3: This word is actually part of the collation element (index)
\end{itemize}

This also means that locating the extra index characters is more
difficult:  the entire string would need to be scanned rather than
just computing the length of the key and skipping it.  To prevent the
need for scanning, the extra index characters are moved to the front
of the string as well.

For consistency, the explicitly stored level 4 values are also shifted
left by two.  This way, the only characters in the string with both of
the lower two bits set are the index characters.

After collapsing the keys like this, over two thirds of the required
space is removed.

<<Post-process DUCET>>=
for(i = 0; i < prop->len; i++) {
  int k;
  uint32_t len = prop->str_arr[i].len;
  uint32_t extra[3], extra_len = len % 4;
  uint32_t *str = prop->strs + prop->str_arr[i].off;
  for(j = 0; j < extra_len; j++)
    extra[j] = str[len - extra_len + j];
  for(j = 0, k = extra_len; j + 3 < len; j += 4, k++) {
    str[k] = (str[j] << 16) + (str[j + 1] << 7) + (str[j + 2] << 2);
    if(str[j + 3] == 1)
      str[k] |= prop->str_arr[i].cp != 0; /* special case: 0 is 0 */
    else if(str[j + 3]) {
      str[k] |= 2;
      str[++k] = str[j + 3] << 2;
    }
  }
  prop->str_arr[i].len = k;
  for(k = 0; j < len; j++, k++)
    str[k] = (extra[k] << 2) | 3;
}
@

The final problem to deal with is the possibility of multiple
contractions with the same starting character, which results in
several array entries with the same index.  Because of this, the
string table cannot be converted to a multi-level table.  After the
last encoding, though, it is easy to simply concatenate all entries
with the same starting character (index) into a single string.
Searching for entries after the first is not too efficient, but at
least it only needs to be done for relatively few first index
characters.  The encoding allows limited binary searching, as well:
the nearest index extension can be located by checking the lower two
bits for 3.

Before merging, though, a copy is made of the raw data for further
processing without having to extract individual elements from the
merged data.  Only the entry pointers are copied:  the strings
cannot be merged in-place anyway (since they might overlap), so
merging always appends new strings and leaves the old ones alone.

<<Post-process DUCET>>=
raw_cp_str_t *raw_ents;
uint32_t raw_ents_size;
inisize(raw_ents, raw_ents_size = prop->len);
cpybuf(raw_ents, prop->str_arr, raw_ents_size);
@

<<Post-process DUCET>>=
ducet_strs = prop->strs; /* for sort & abbrev */
for(i = prop->len - 1; i > 0; i--) {
  uint32_t cp = prop->str_arr[i].cp;
  for(j = i; j > 0 && prop->str_arr[j - 1].cp == cp; j--);
  if(i == j)
    continue;
  /* sort by index extension */
  low = j;
  qsort(prop->str_arr + low, i - low + 1, sizeof(*prop->str_arr), cmp_ducet_idx);
  if(prop->str_arr[low].len && (ducet_strs[prop->str_arr[low].off] & 3) == 3) {
    fprintf(stderr, "Error: no DUCET entry for %04X\n", (int)cp);
    exit(1);
  }
  uint32_t len = 0;
  for(j = low; j <= i; j++)
    len += prop->str_arr[j].len;
  check_size(prop->strs, prop->max_strs, prop->strs_len + len);
  ducet_strs = prop->strs; /* for sort & abbrev */
  cpybuf(ducet_strs + prop->strs_len,
         ducet_strs + prop->str_arr[low].off,
	 prop->str_arr[low].len);
  prop->str_arr[low].off = prop->strs_len;
  prop->strs_len += prop->str_arr[low].len;
  prop->str_arr[low].len = len;
  for(j = low + 1; j <= i; j++) {
    uint32_t *str = ducet_strs + prop->str_arr[j].off;
    len = prop->str_arr[j].len;
    cpybuf(ducet_strs + prop->strs_len, str, len);
    prop->strs_len += len;
  }
  movebuf(prop->str_arr + low + 1, prop->str_arr + i + 1,
          prop->len - i - 1);
  prop->len -= i - low;
  if(!(i = low))
    break;
}
/* 6.2: 27009 words (2850 saved) */
/* 6.2-CLDR: 25858 words (2756 saved) */
/* 7.0: 26739 words (3408 saved) */
/* 7.0-CLDR: 26650 words (3421 saved) */
@

<<DUCET parser globals>>=
static uint32_t *ducet_strs;
static int cmp_ducet_idx(const void *a, const void *b)
{
  const raw_cp_str_t *_a = a, *_b = b;
  uint32_t *stra = ducet_strs + _a->off, *strb = ducet_strs + _b->off;
  uint32_t lena = _a->len, lenb = _b->len;

  /* assume all indices are unique, so equality will never be returned */
  while(lena > 0 && lenb > 0) {
    if((*stra & 3) != 3)
      return -1;
    if((*strb & 3) != 3)
      return 1;
    if(*stra != *strb)
      return *stra > *strb ? 1 : -1;
    lena--;
    lenb--;
    stra++;
    strb++;
  }
  return lena ? 1 : -1;
}
@

Unsurprisingly, combining entries reduces redundancy a bit (gaining 2
words).  Thus the original 126,219 words are reduced to 27,009 words.
This allows the use of 16-bit offsets, as supported by the
[[uni_str_arr_t]] type.  Note that this is just the string table; the
lookup tables add additional overhead (about 200kb for the code point
table and 100kb for the multi-level table).  Finally, the raw DUCET is
ready to be dumped.  The collected default variable top is dumped to
the header as well.

<<Dump character information as C code>>=
fprintf(gen_h, "/** Default variable top property for the DUCET */\n"
               "#define uni_DUCET_var_top %d\n", (int)ducet_vartop);
dump_str_tabs(&parsed_props[prop_DUCET],
              "Default Unicode Collation Element Table", gen_h, tstf);
@

<<Post-process DUCET>>=
enable_str_mt(prop);
@

In order to compensate for some of the compression above, a special
lookup function is provided.  It is intended for piece-wise lookup of
a full string's key sequence.  This is \emph{not} a key string as
defined by the UCA.  It is ordered by level first, and then key
element sequence, and includes zeroes at some levels.  The only thing
needed to convert this to a UCA key is to reorder the results by key
element sequence first, and then by level, and to remove zeroes.  The
format of the returned values is similar to the raw format stored in
they table.  Also, since the table is so large, the related functions
are stored in their own object file.

<<Library [[uni]] Members>>=
ducet_lookup.o
@

<<ducet_lookup.c>>=
<<Common C Header>>
#include "uni_prop.h"
// static_proto

<<DUCET lookup functions>>
@

<<DUCET lookup format defs>>=
#define UNI_DUCET_LEV1_MASK  0xffff0000  /**< Level 1 component */
#define UNI_DUCET_LEV1_SHIFT 16          /**< Location of level 1 component */
#define UNI_DUCET_LEV2_MASK  0x0000ff80  /**< Level 2 component */
#define UNI_DUCET_LEV2_SHIFT 7           /**< Location of level 2 component */
#define UNI_DUCET_LEV3_MASK  0x0000007c  /**< Level 3 component */
#define UNI_DUCET_LEV3_SHIFT 2           /**< Location of level 3 component */
@

<<Unicode property exports>>=
/** \addtogroup uni_prop_uca_lookup DUCET lookup return values.
  * The low two bits of the level123 value are always zero.  */
/** @{ */
<<[[uni_uca_lookup]] parameters>>
/** @} */
@

<<[[uni_uca_lookup]] parameters>>=
<<DUCET lookup format defs>>
@

In order to make it piece-wise, only one character is passed in at a
time, along with a running state.  Either a key element is returned,
or not.  Either the next character must be passed in next, or nothing.
It makes no sense for nothing to be returned, and no new character to
be required from the caller, so only three cases are consisdered:
returning a value, and requiring a new input next time ([[OK]]),
returning a value, and requiring no new input ([[AGAIN]]), and
returning no value, and requiring new input next time ([[NONE]]).  At
the end of input (flagged with a special character, [[END]]), the
final returned value is indicated by [[OK]] or [[NONE]].

<<[[uni_uca_lookup]] parameters>>=
/**  \ref uni_uca_lookup return value.
  *  \p lev123 and \p lev4 are valid; pass in the next code point (in \p c)
  *  next time. That is, unless input \p c was \ref UNI_UCA_LOOKUP_END,
  *  in which case \p lev123 and \p lev4 are the last ones to be returned. */
#define UNI_UCA_LOOKUP_OK    0
/**  \ref uni_uca_lookup return value.
  * \p lev123 and lev4 are valid; call again for more values without
  * passing in the next code point.  In other words, the next time,
  * the \p c parameter will be ignored. */
#define UNI_UCA_LOOKUP_AGAIN 1
/**  \ref uni_uca_lookup return value.
  * \p lev123 and lev4 were not updated; call again with the next code point.
  * That is, unless input was \ref UNI_UCA_LOOKUP_END, in which case
  * there will never be any more return values.  */
#define UNI_UCA_LOOKUP_NONE -1

/** \ref uni_uca_lookup input indicating end-of-input */
#define UNI_UCA_LOOKUP_END 0xffffffff

/** Opaque state tracker used by \ref uni_uca_lookup */
typedef struct uni_uca_lookup_state_t uni_uca_lookup_state_t;
@

<<Known Data Types>>=
uni_uca_lookup_state_t,%
@

<<Unicode property exports>>=
/** Look up DUCET values.
  * Pass in first character for \p c; see \ref uni_prop_uca_lookup for
  * details on return values.  Point \p state to a local pointer initialized
  * to NULL.  It will be automatically allocated when needed, and freed
  * at the same time a return indicates no more values will be returned */
int uni_uca_lookup(uint32_t c, uint32_t *lev123, uint32_t *lev4,
                   uni_uca_lookup_state_t **state);
@

<<DUCET lookup functions>>=
<<Private UCA lookup definitions>>
struct uni_uca_lookup_state_t {
  <<UCA lookup state members>>
};
<<Private UCA lookup globals>>
int uni_uca_lookup(uint32_t c, uint32_t *lev123, uint32_t *lev4,
                   uni_uca_lookup_state_t **state)
{
  if(!*state && c == UNI_UCA_LOOKUP_END)
    return UNI_UCA_LOOKUP_NONE;
  <<Initialize single DUCET element lookup>>
  <<Look up and return a single key element from DUCET>>
}
@

Rather than just look up raw values from the DUCET, some other
optional UCA-related processing may be done.  To select the processing
options, a separate structure and function are provided to set the
options.   The main reason for providing a separate function is to
ensure that options do not change mid-lookup.

<<[[uni_uca_opts_t]]>>=
/** Options for routines related to the Unicode Collation Algorithm */
typedef struct {
  <<Unicode UCA function options>>
} uni_uca_opts_t;
@

<<Known Data Types>>=
uni_uca_opts_t,%
@

<<Unicode property exports>>=
<<[[uni_uca_opts_t]]>>
/** Set options for \ref uni_uca_lookup.
  * Call this with a NULL \p *state to set options in \p *state from
  * \p opts.  Must be called at most once before the first call to
  * \ref uni_uca_lookup.  */
/* set options for UCA lookups; call before lookup() */
void uni_uca_lookup_opts(uni_uca_lookup_state_t **state,
                         const uni_uca_opts_t *opts);
@

<<DUCET lookup functions>>=
void uni_uca_lookup_opts(uni_uca_lookup_state_t **state,
                           const uni_uca_opts_t *opts)
{
  if(*state)
    return; /* error */
  inisize(*state, 1);
  clearbuf(*state, 1);
  uni_uca_lookup_state_t *st = *state;
  if(opts) {
    <<Copy UCA lookup options from [[opts]] to [[st]]>>
  }
  <<Set default UCA lookup options>>
}
@

<<Initialize single DUCET element lookup>>=
/* if(!state) exit(1); */
uni_uca_lookup_state_t *st = *state;
if(!st) {
  uni_uca_lookup_opts(state, NULL);
  st = *state;
}
@

First of all, there are two DUCET tables that could be used for
lookup.  One way to implement locales would be to load tables from
files, or to construct tables on the fly, so other tables may be used
as well.  The default table to use is the plain DUCET, but other
tables may be specified.

<<Unicode UCA function options>>=
const uint32_t *tab, /**< DUCET multi-level table (default: \ref uni_DUCET_mtab) */
               *strs; /**< DUCET strings (default: \ref uni_DUCET_strs) */
@

<<UCA lookup state members>>=
const uint32_t *tab, *strs;
@

<<Copy UCA lookup options from [[opts]] to [[st]]>>=
st->tab = opts->tab;
st->strs = opts->strs;
@

<<Set default UCA lookup options>>=
if(!st->tab) {
  st->tab = uni_DUCET_mtab;
  st->strs = uni_DUCET_strs;
}
@

One other easily implemented option is to select the comparison
strength, which is a level number above which other levels are zeroed
out.  This may reduce the number of returned elements as well, by
increasing the number of all-zero elements.  The first three levels
are most easily zeored using a mask, so that is stored in the state as
well.

<<Unicode UCA function options>>=
uint8_t max_level; /**< Comparison strength (default: 3) */
@

<<UCA lookup state members>>=
uint32_t mask123;
uint8_t max_level;
@

<<Copy UCA lookup options from [[opts]] to [[st]]>>=
st->max_level = opts->max_level;
@

<<Set default UCA lookup options>>=
if(!st->max_level)
  st->max_level = 3;
if(st->max_level == 1)
  st->mask123 = UNI_DUCET_LEV1_MASK;
else if(st->max_level == 2)
  st->mask123 = UNI_DUCET_LEV1_MASK | UNI_DUCET_LEV2_MASK;
else
  st->mask123 = ~3;
@

In order to avoid constantly referring to a pointed-to value, the
return value is built in [[l123]] and [[l4]] rather than [[*lev123]]
and [[*lev4]].

<<Initialize single DUCET element lookup>>=
uint32_t l123, l4;
@

<<Mangle DUCET return value>>=
l123 &= st->mask123;
if(st->max_level < 4)
  l4 = 0;
@

Another option which immediately modifies results is the variable key
element processing.  Doing this requires the table's variable top and
a processing mode setting.  While the UCA apparently uses shifted mode
by default, this routine sets the default to minimal processing, which
is done with the non-ignorable mode.  Coincidentallly, non-ignorable
is also the default mode for CLDR.

<<Unicode UCA function options>>=
uint8_t var_mode; /**< Mode for procssing variable key elements (default: */
                  /**< NON_IGNORABLE -- the UCA default of SHIFTED must be */
		  /**< selected manually) */
uint32_t var_top; /**< Last variable key element in DUCET (default: \ref uni_DUCET_var_top) */
@

<<UCA lookup state members>>=
uint8_t var_mode;
uint32_t var_top;
int in_var;
@

<<Copy UCA lookup options from [[opts]] to [[st]]>>=
st->var_mode = opts->var_mode;
/* to allow direct <= comparison w/ lev123 */
if(opts->var_top)
  st->var_top = (opts->var_top << 16) | 0xffff;
@

<<Set default UCA lookup options>>=
if(!st->var_top)
  /* to allow direct <= comparison w/ lev123 */
  st->var_top = (uni_DUCET_var_top << 16) | 0xffff;
@

<<Unicode property exports>>=
<<UCA variable modes>>
@

<<UCA variable modes>>=
#define UNI_UCA_VAR_MODE_NON_IGNORABLE 0 /**< Variable key processing mode */
#define UNI_UCA_VAR_MODE_BLANKED       1 /**< Variable key processing mode */
#define UNI_UCA_VAR_MODE_SHIFTED       2 /**< Variable key processing mode */
#define UNI_UCA_VAR_MODE_IGNORE_SP     3 /**< Variable key processing mode */
#define UNI_UCA_VAR_MODE_SHIFT_TRIMMED 4 /**< Variable key processing mode */
@

<<Initialize single DUCET element lookup>>=
uint8_t var_mode = st->var_mode;
int shifted = var_mode >= UNI_UCA_VAR_MODE_SHIFTED,
    blanked = var_mode == UNI_UCA_VAR_MODE_BLANKED;
uint32_t var_top = st->var_top;
@

<<Mangle DUCET return value>>=
/* UCA variable mode modifies variable entry & anything following */
/* plus some other random ignorables */
if(shifted) {
  if(!l123)
    l4 = 0; /* L1, L2, L3 = 0 */
  else if(!(l123 & UNI_DUCET_LEV1_MASK)) { /* L1 = 0, L3  0 */
    if(st->in_var)
      l4 = l123 = 0; /* following a variable */
    else
      l4 = 0xffff; /* not following a variable */
  } else { /* L1  0 */
    st->in_var = 0;
    l4 = 0xffff; /* set to L1 below if SHIFTED */
    /* actually, 2nd synth should always be 0 */
    /* if(did_synth2) */
    /* but impossible to detect if synth2 explicitly in DUCET */
    /* maybe just detect l2, l3 = 0?  Seems to pass tests. */
    if(!(l123 & (UNI_DUCET_LEV2_MASK | UNI_DUCET_LEV3_MASK)))
      l4 = 0;
  }
} else if(blanked && st->in_var) {
  if(!(l123 & UNI_DUCET_LEV1_MASK))
    l123 = l4 = 0;
  else
    st->in_var = 0;
}
/* due to modification of var_top above: */
/*    same as lev1 <= var_top && lev1 > 0 */
if(var_mode > 0 && l123 <= var_top && l123 > 0xffff) {
  st->in_var = 1;
  switch(var_mode) {
    /* UNI_UCA_VAR_MODE_NON_IGNORABLE does nothing */
    case UNI_UCA_VAR_MODE_BLANKED:
      l123 = l4 = 0;
      break;
    case UNI_UCA_VAR_MODE_SHIFTED:
      l4 = l123 >> UNI_DUCET_LEV1_SHIFT;
      /* !!! Undocumented CLDR behavior (required to pass test): !!! */
      /* UCA says lev1 > 0 and lev1 <= var_top is variable */
      /*  but CLDR defines one entry with lev1 == 1, but it is not var */
      /*  yet in shifted mode, it sets lev4 == lev1 as above */
      /*  GRRRR.... */
      /*  at least UCA data files never use lev1 < 0x0200, so */
      /*  just testing for 1 is safe */
      /* who knows what IgnoreSP and Trimmed are supposed to work like */
      /* IgnoreSP ws removed in 6.3.0, anyway */
      if(l4 > 1)
        l123 = 0;
      else
        st->in_var = 0;
      break;
    case UNI_UCA_VAR_MODE_IGNORE_SP: /* obsolete; removed in 6.3.0 */
      if((uni_gc_trans[UNI_gc_P] & (1 << uni_gc_of(c))) ||
         uni_is_WSpace(c)) {
        l4 = l123 >> UNI_DUCET_LEV1_SHIFT;
	/* undocumented behavior copied from SHIFTED */
	if(l4 > 1)
	  l123 = 0;
	else
	  st->in_var = 0;
      } else
        st->in_var = 0;
      break;
    case UNI_UCA_VAR_MODE_SHIFT_TRIMMED:
      l4 = l123 >> UNI_DUCET_LEV1_SHIFT;
      /* undocumented behavior copied from SHIFTED */
      if(l4 > 1)
        l123 = 0;
      else
        st->in_var = 0;
      break;
  }
}
@

Stripping trailing level 4 [[0xFFFF]] values, as required for
shift-trimmed mode, requires delaying any [[0xFFFF]] output until
either a non-[[0xFFFF]] output is found, or the final value is being
returned.  That could be best accomplished by the caller, by simply
accumulating the entire result and trimming the trailing [[0xFFFF]]
values.  However, it may be difficult to document that this mode, and
only this mode, is only partially implemented.  So, levels 1--3 of any
output whose level 4 is [[0xFFFF]] are saved in an expandable buffer.
Once a non-[[0xFFFF]] level 4 is encountered, all accumulated entries
need to be output before the key elements with a non-[[0xFFFF]] level
4, followed by that key element.  Once the end is encountered, all
accumulated entries need to be output, but with a level 4 of zero.
For the former case, levels 1--3 of the non-[[0xFFFF]] element can be
saved in place of the first saved element to be output, but level 4
needs to be saved separately.

Note that the caller can still prevent this accumulation by using
shifted mode, and trimming manually.

<<UCA lookup state members>>=
uint32_t *l4_ffff;
unsigned int l4_ffff_len, l4_ffff_max, l4_ffff_ptr;
uint32_t non_ffff_l4;
@

<<Free UCA lookup state members>>=
if(st->l4_ffff)
  free(st->l4_ffff);
@

<<Mangle DUCET return value>>=
if(var_mode == UNI_UCA_VAR_MODE_SHIFT_TRIMMED) {
  if(l4 == 0xffff) {
    /* FIXME: return error instead of exiting */
    check_size(st->l4_ffff, st->l4_ffff_max, st->l4_ffff_len + 1);
    st->l4_ffff[st->l4_ffff_len++] = l123;
    <<Skip [[l4_ffff]] member>>
  } else if(st->l4_ffff_len) {
    uint32_t l123s = l123;
    l123 = st->l4_ffff[0];
    st->l4_ffff[0] = l123s;
    st->non_ffff_l4 = l4;
    l4 = 0xffff;
    st->l4_ffff_ptr = 1;
    <<Return [[l4_ffff]] members>>
  }
}
@

<<Check for [[l4_ffff]] members>>=
if(st->l4_ffff_ptr) {
  <<Maybe check for [[l4_ffff]] members at end>>
  if(st->l4_ffff_ptr < st->l4_ffff_len) {
    l123 = st->l4_ffff[st->l4_ffff_ptr++];
    l4 = 0xffff;
    <<Return [[l4_ffff]] members>>
  } else {
    l123 = st->l4_ffff[0];
    l4 = st->non_ffff_l4;
    st->l4_ffff_len = st->l4_ffff_ptr = 0;
    <<Return post-[[l4_ffff]] members>>
  }
}
@

<<Check for [[l4_ffff]] members at end>>=
while(st->l4_ffff_len) {
  l123 = st->l4_ffff[st->l4_ffff_ptr++];
  l4 = 0;
  if(st->l4_ffff_ptr == st->l4_ffff_len) {
    st->l4_ffff_ptr = st->l4_ffff_len = 0;
    <<Return last [[l4_ffff]] member>>
  } else if(l123) {
    <<Return [[l4_ffff]] members at end>>
  }
}
@

There is one other UCA parameter: reversed level 2.  That is, level 2
values are returned in reverse order.  To do this, the entire string
would have be be accumulated before generating the first output.  Since
some routines may be more efficient if results are returned one at a
time, this could be a problem.  Instead, it is the responsibility of
the caller to implement this option.

There are additional CLDR-specific parameters which could affect the
results of this function.  These may be implemented at a future date.
Adding shift mode processing does not affect complexity much, but
adding DUCET reordering, additional pseudo-levels, numeric
accumulation and parsing, and other CLDR options will make this
function very complex.

Some operations, such as looking up multi-character collation
elements, require at least one character of lookahead.  In fact, the
reordering requirement requires arbitrary lookahead.  To support this,
a buffer may be filled with lookahead characters.  Any time a NONE
return would happen, if there are buffer characters, the routine
should instead restart itself.  Any time an OK return would happen, an
AGAIN should be returned instead.  Any time an AGAIN return would
happen, the character consumed from the buffer is reinserted, so that
the next time the function is called, that character is consumed again
instead of the next character in the buffer.  If there are no buffered
characters, this is not necessary as no character will be consumed.

This is also the appropriate place to mask out the unneeded levels of
the return value, and to alter the return code as needed.  An OK
return is treated as a NONE return, and an AGAIN return loops without
returning a value.

\lstset{language=C}
<<UCA lookup state members>>=
uint32_t *bt;
int32_t btp, num_bt, max_bt;
@

<<Look up and return a single key element from DUCET>>=
if(c != UNI_UCA_LOOKUP_END && c > UNI_MAX_CP)
  /* FIXME: actually, this should replace any length of illegal chars */
  /* including surrogates as well, at least */
  c = 0xFFFD; /* recommended REPLACEMENT CHARACTER */
<<Check for [[l4_ffff]] members>>
restart:
  if(st->num_bt) {
    if(st->btp >= 0)
      c = st->bt[st->btp++];
    else
      st->btp++;
    if(st->btp == st->num_bt)
      st->btp = st->num_bt = 0;
  }
@

<<Free UCA lookup state members>>=
if(st->bt)
  free(st->bt);
@

<<Private UCA lookup globals>>=
static void add_bt(uint32_t c, uni_uca_lookup_state_t *st)
{
  /* FIXME: return error instead of exiting */
  check_size(st->bt, st->max_bt, st->num_bt + 1);
  st->bt[st->num_bt++] = c;
}

static void add_bt_buf(const uint32_t *buf, uint32_t len,
                       uni_uca_lookup_state_t *st)
{
  /* FIXME: return error instead of exiting */
  check_size(st->bt, st->max_bt, st->num_bt + len);
  cpybuf(st->bt + st->num_bt, buf, len);
  st->num_bt += len;
}
@

<<Skip [[l4_ffff]] member>>=
<<Return no DUCET values>>
@

<<Return post-[[l4_ffff]] members>>=
*lev123 = l123;
*lev4 = l4;
/* assume that non-default state needed AGAIN */
return st->state ? UNI_UCA_LOOKUP_AGAIN : UNI_UCA_LOOKUP_OK;
@

<<Return [[l4_ffff]] members>>=
*lev123 = l123;
*lev4 = l4;
return UNI_UCA_LOOKUP_AGAIN;
@

<<Return no DUCET values>>=
if(st->num_bt)
  goto restart;
else
  return UNI_UCA_LOOKUP_NONE;
@

<<Return one DUCET value>>=
<<Mangle DUCET return value>>
if(!l123 && !l4) {
  <<Return no DUCET values>>
}
*lev123 = l123;
*lev4 = l4;
return st->num_bt ? UNI_UCA_LOOKUP_AGAIN : UNI_UCA_LOOKUP_OK;
@

<<Return more than one DUCET value>>=
if(st->num_bt)
  st->btp--;
<<Mangle DUCET return value>>
if(!l123 && !l4)
  goto restart;
*lev123 = l123;
*lev4 = l4;
return UNI_UCA_LOOKUP_AGAIN;
@

A private enumeration type is used to represent the current lookup
progress.

<<Private UCA lookup definitions>>=
typedef enum {
  ST_uca_lookup_start
  <<UCA lookup states>>
} uca_lookup_state_t;
@

<<Known Data Types>>=
uca_lookup_state_t,%
@

<<UCA lookup state members>>=
uca_lookup_state_t state;
@

<<Look up and return a single key element from DUCET>>=
switch(st->state) {
  case ST_uca_lookup_start:
    <<Look up in DUCET given no state>>
  <<Look up in DUCET given state>>
}
@

The first thing to do is check for end-of-string.  At end-of string,
the state is freed, and nothing is returned, unless there are
accumulated [[l4_ffff]] members.

<<UCA lookup state members>>=
int eoi;
@

<<Look up in DUCET given no state>>=
if(st->eoi || c == UNI_UCA_LOOKUP_END) {
  <<Check for [[l4_ffff]] members at end>>
  <<Free UCA lookup state members>>
  free(st);
  *state = NULL;
  return UNI_UCA_LOOKUP_NONE; /* can't have any more bt */
}
@

<<Return last [[l4_ffff]] member>>=
if(l123 || l4) {
  *lev123 = l123;
  *lev4 = l4;
  <<Free UCA lookup state members>>
  free(st);
  *state = NULL;
  return UNI_UCA_LOOKUP_OK;
} else {
  <<Free UCA lookup state members>>
  free(st);
  *state = NULL;
  return UNI_UCA_LOOKUP_NONE;
}
@

<<Return [[l4_ffff]] members at end>>=
st->eoi = 1;
*lev123 = l123;
*lev4 = l4;
return UNI_UCA_LOOKUP_AGAIN;
@

<<Maybe check for [[l4_ffff]] members at end>>=
if(st->eoi) {
  <<Check for [[l4_ffff]] members at end>>
}
@

Otherwise, a lookup is performed in the table.  If it succeeds, the
results are returned appropriately.  Otherwise, it might be a
canonically decomposable character.  The automatic single-character
decomposition performed by the original DUCET was stripped above, so
it must be done here, by pushing the decomposition onto the
backtracking buffer and retrying.  If it is neither in the table nor a
canonical composition, a key is synthesized.

<<UCA lookup state members>>=
uni_str_ptr_t v;
uint32_t c;
@

<<Private UCA lookup definitions>>=
<<Synthetic UCA key support>>
@

<<Look up in DUCET given no state>>=
const uni_str_ptr_t *v;
uni_multi_tab_lookup(st->tab, c * 4, (const uint8_t **)&v, 0);
if(!v || !v->len) {
  <<Set [[v]] from canonical decomposition>>
}
if(v && v->len) {
  st->v = *v;
  <<Process DUCET initial lookup>>
} else {
  /* synthesized elements are 2-parters; prepare for 2nd part */
  st->c = c;
  st->state = ST_uca_lookup_synth;
  l123 = (uca_synth1(c) << 16) + (0x0020 << 7) + (0x0002 << 2);
  l4 = c; /* actual value does not matter, as long as it's non-0 */
  <<Return more than one DUCET value>>
}
@

<<UCA lookup states>>=
,ST_uca_lookup_synth
@

<<Look up in DUCET given state>>=
case ST_uca_lookup_synth:
  l123 = uca_synth2(st->c) << 16;
  l4 = st->c; /* should be 0, but sample data uses c, so I will as well */
  st->state = 0;
  <<Return one DUCET value>>
@

Two sources of single-character decomposition exist:  table entries
and Hangul syllable decomposition.  While it may be more efficient to
store the decomposition results as a pointer and length, since it's a
static buffer, it is instead pushed onto the regular backtracking
buffer in order to avoid having to duplicate processing code.  The
input to the function should be canonically decomposed anyway, so this
will not happen often.

<<UCA lookup state members>>=
const uint16_t *multi_ret16;
uint32_t multi_ret_len;
@

<<Set [[v]] from canonical decomposition>>=
int32_t coff;
uint16_t clen;
uni_find_canon_decomp(c, &coff, &clen);
if(coff >= 0 && clen > 0) {
  const uint16_t *str = uni_canon_decomp_strs + coff;
  unsigned int cl;
  c = uni_int_utf16_decode(str, &cl);
  uni_multi_tab_lookup(st->tab, c * 4, (const uint8_t **)&v, 0);
  while(clen -= cl) {
    str += cl;
    add_bt(uni_int_utf16_decode(str, &cl), st);
  }
} else if(coff < 0) {
  uint32_t hbuf[3];
  int hlen = uni_hangul_syllable_decomp(c, hbuf, 1);
  if(hlen > 0) {
    c = hbuf[0];
    uni_multi_tab_lookup(st->tab, c * 4, (const uint8_t **)&v, 0);
    add_bt(hbuf[1], st);
    if(hlen > 2)
      add_bt(hbuf[2], st);
  }
}
@

For a successful lookup, the response for a single-character index is
extracted.  If multi-character indices are available as well, more
characters are requested after saving the work done so far in the
state.  Otherwise, the single-character reponse is returned.

<<UCA lookup state members>>=
uint32_t curoff;
@

<<Process DUCET initial lookup>>=
const uint32_t *str = st->strs + st->v.off;
uint32_t len = st->v.len, len1;
for(len1 = 0; len1 < len; len1++)
  if((str[len1] & 3) == 3) {
    <<Accumulate and check more collation element characters>>
    <<Return no DUCET values>>
  }
<<Return DUCET entry for single-character index>>
@

<<Accumulate and check more collation element characters>>=
st->state = ST_uca_lookup_gotfirst;
st->curoff = len1;
st->c = c;
@

<<UCA lookup states>>=
,ST_uca_lookup_gotfirst
@

If the next character is end-of-string, the match so far is returned.
Otherwise, it is searched for in the subindex strings.   A match
increases the total [[matchlen]], which can be skipped.  Only exact
matches (i.e. only one character after [[matchlen]], and it must match
[[c]]) are considered successful.

<<Look up in DUCET given state>>=
case ST_uca_lookup_gotfirst:
  if(c == UNI_UCA_LOOKUP_END) {
    <<Return DUCET entry for single or multi-character index>>
  } else {
    int32_t l, h, m;
    const uint32_t *str = st->strs + st->v.off;
    m = ducet_lookup_subindex(c, &l, &h, st);
    <<Process index extension search results>>
  }
@

<<UCA lookup state members>>=
uint32_t matchlen;
@

<<Accumulate and check more collation element characters>>=
st->matchlen = 0;
@

<<Private UCA lookup globals>>=
static int ducet_lookup_subindex(uint32_t c, int32_t *_l, int32_t *_h,
                                 const uni_uca_lookup_state_t *st)
{
  uint32_t l = st->curoff, h = st->v.len, m;
  const uint32_t *str = st->strs + st->v.off;
  while(l <= h) {
    <<Find start of middle index extension>>
    m += st->matchlen;
    uint32_t mc = str[m] >> 2;
    if(mc < c) {
      l = m + 1;
      <<Skip to next index extension>>
    } else if(mc > c ||
              /* also greater if longer than one char */
	      (h > m && mc == c &&
	      (str[m + 1] & 3) == 3 && str[m + 1] != 3)) {
      h = m - st->matchlen - 1;
    } else {
      *_l = l;
      *_h = h;
      return m - st->matchlen;
    }
  }
  *_l = l;
  *_h = h;
  return 0;
}
@

<<Find start of middle index extension>>=
m = (l + h) / 2;
while(m > l && ((str[m] & 3) != 3 || str[m] == 3))
  m--;
while(m > l && ((str[m - 1] & 3) == 3 && str[m - 1] != 3))
  m--;
@

<<Skip to next index extension>>=
while(l <= h && (str[l] & 3) == 3 && str[l] != 3)
  l++;
if(l <= h && str[l] == 3)
  l++;
else
  while(l <= h && (str[l] & 3) != 3)
    l++;
@

A successful match is returned if there can be no further extension of
the index.  Otherwise, the search results are narrowed to the
possibilities collected so far, and more input is requested.

<<Process index extension search results>>=
if(l <= h) {
  /* advance search result to match */
  st->v.off += m;
  st->v.len -= m;
  str += m;
  /* find end of match entry */
  l = 1;
  h = st->v.len - 1;
  <<Skip to next index extension>>
  ducet_adjust_search(c, l, st);
  if(st->curoff == st->v.len) {
    /* there are no more possible matches, so just return */
    st->state = ST_uca_lookup_start;
    <<Return DUCET entry for multi-character index>>
  } else {
    st->matchlen++;
    <<Return no DUCET values>>
  }
}
@

<<Private UCA lookup globals>>=
static void ducet_adjust_search(uint32_t c, int32_t l,
                                uni_uca_lookup_state_t *st)
{
  int32_t h = st->v.len - 1, m;
  const uint32_t *str = st->strs + st->v.off;
  st->curoff = l;
  while(l <= h) {
    <<Find start of middle index extension>>
    if((str[m + st->matchlen] >> 2) != c)
      h = m - 1;
    else {
      l = m + 1;
      <<Skip to next index extension>>
    }
  }
  st->v.len = h + 1;
}
@

When a mismatch occurs, however, it is not guaranteed that this will
never match.  There are two cases where a future character might still
cause a match.  If a longer string is required for a match due to a
DUCET which does not have entries for all intervening-length strings
(the standard DUCET does not), [[h]] always points just below at least
one such string.  If more data is required because a reordering might
cause a match, the ccc of [[c]] and any future characters must be
non-zero and not match the ccc of any potential candidates for
reordering.%
\footnote{But see above:  maybe what should be checked is start-ccc
and end-ccc rather than ccc.}

<<Process index extension search results>>=
else {
  uint32_t ccc;
  /* if c in str, but not only member of str, ill-formed DUCET */
  /* h always points just below longer index */
  h += st->matchlen + 1;
  if(h < st->v.len && str[h] == (c << 2) + 3) {
    <<Enable possible longer lookup>>
  /* check if possible match due to reordering rule */
  } else if((ccc = uni_ccc_of(c))) {
    <<Enable reordering extension>>
  } else {
    /* no possible longer matches, so go ahead and return */
    add_bt(c, st);
    <<Return DUCET entry for single or multi-character index>>
  }
}
@

The first case is easy to detect and manage.  At most one intermediate
is allowed to be missing, so the next character determines if a match
is made.  if so, processing continues just as above.  Otherwise, the
two characters that were not processed are stored, and special states
are entered to feed these back, one at a time.  They will never
collide with each other, because the second character is the first one
which could cause the extension state to reappear, and by then, the
stored characters are done being processed.

Rather than modify the code to loop and retrieve characters from
buffers, a recursive call is used.  Again, this should happen rarely,
and does not affect performance much.

<<UCA lookup state members>>=
uint32_t long_off;
@

<<UCA lookup states>>=
,ST_uca_lookup_longer
@

<<Enable possible longer lookup>>=
st->state = ST_uca_lookup_longer;
st->long_off = st->curoff;
st->c = c;
ducet_adjust_search(c, l, st);
<<Return no DUCET values>>
@

<<Look up in DUCET given state>>=
case ST_uca_lookup_longer:
  {
    int32_t m, l, h;
    const uint32_t *str = st->strs + st->v.off;
    st->matchlen++;
    m = ducet_lookup_subindex(c, &l, &h, st);
    if(l > h) { /* assume no even longer matches possible.. */
      add_bt(st->c, st);
      add_bt(c, st);
      st->curoff = st->long_off;
      st->state = ST_uca_lookup_start;
      <<Return DUCET entry for single or multi-character index>>
    } else {
      st->v.off += m;
      st->v.len -= m;
      l = 1;
      h = st->v.len - 1;
      <<Skip to next index extension>>
      st->curoff = l;
      <<Return DUCET entry for multi-character index>>
    }
  }
@

\label{doc:uca-ccc-reorder}The second case requires checking the ccc
of all subsequent characters; if a zero is detected, no further
reordering is possible. Otherwise, as long as a character does not
match the ccc of one of the previously accumulated characters, it is
looked up and if it matches, a reordering can take place.%
\footnote{But see above:  maybe what should be checked is start-ccc
rather than ccc.}


By the time the decision is made, at least one character, and possibly
more, have been accumulated.  A buffer (separate from [[bt]]) needs to
be used to accumulate these and eventually transfer them into [[bt]]. 

<<UCA lookup state members>>=
uint32_t *buf, buflen, maxbuf;
uint32_t *ccc_buf;
@

<<UCA lookup states>>=
,ST_uca_lookup_reorder
@

<<Enable reordering extension>>=
if(!st->buf) {
  inisize(st->buf, (st->maxbuf = 10));
  inisize(st->ccc_buf, st->maxbuf);
}
st->buf[0] = c;
st->ccc_buf[0] = ccc;
st->buflen = 1;
st->state = ST_uca_lookup_reorder;
<<Return no DUCET values>>
@

<<Look up in DUCET given state>>=
case ST_uca_lookup_reorder:
  {
    uint32_t ccc = c == UNI_UCA_LOOKUP_END ? 0 : uni_ccc_of(c);
    uint32_t i;
    if(st->maxbuf == st->buflen) {
      /* FIXME: return error instead of exiting */
      check_size(st->buf, st->maxbuf, st->buflen + 1);
      resize(st->ccc_buf, st->maxbuf);
    }
    st->buf[st->buflen] = c;
    st->ccc_buf[st->buflen++] = ccc;
    if(!ccc) {
      st->state = ST_uca_lookup_start;
      add_bt_buf(st->buf, st->buflen, st);
      <<Return DUCET entry for single or multi-character index>>
    }
    for(i = 0; i < st->buflen - 1; i++)
      if(st->ccc_buf[i] == ccc) {
        <<Return no DUCET values>>
      }
    int32_t l, h, m;
    m = ducet_lookup_subindex(c, &l, &h, st);
    if(l <= h) {
      /* 6.3.0: must fail if not exact match */
      /* otherwise, 0FB2 0334 0F81 sorts incorrectly */
      const uint32_t *str = st->strs + st->v.off + m;
      if((str[st->matchlen + 1] & 3) == 3 && str[st->matchlen + 1] != 3) {
        fprintf(stderr, "no ext for %04X %04X\n", st->c, c);
        <<Return no DUCET values>>
      }
      /* advance search result to match */
      st->v.off += m;
      st->v.len -= m;
      /* copy rest of characters accumulated to bt */
      add_bt_buf(st->buf, st->buflen - 1, st);
      /* find end of match entry */
      l = 1;
      h = st->v.len - 1;
      <<Skip to next index extension>>
      if(l > h) {
        /* there are no more possible matches, so just return */
        st->curoff = l;
	st->state = ST_uca_lookup_start;
	<<Return DUCET entry for multi-character index>>
      } else {
        st->state = ST_uca_lookup_gotfirst;
	ducet_adjust_search(c, l, st);
	st->matchlen++;
	<<Return no DUCET values>>
      }
    } else {
      <<Return no DUCET values>>
    }
  }
@

<<Free UCA lookup state members>>=
if(st->buf)
  free(st->buf);
@

Rather than reproduce the code for returning the results everywhere,
it is placed after the switch.  Thus a [[break]] indicates that a
result should be returned.  The only difference between them is that
[[c]] needs to be retrieved from the state for multi-character indices.
Also, [[curoff]] is assumed to be the end of the entry to return, but
it is never assigned for the single-character index case.

<<Return DUCET entry for single or multi-character index>>=
c = st->c;
break;
@

<<Return DUCET entry for multi-character index>>=
c = st->c;
break;
@

<<Return DUCET entry for single-character index>>=
st->curoff = v->len;
break;
@

Due to advancing the pointers above, [[v]] points to the desired result
to return.  The result only requires work if there is more than one
key entry.  If so, it is returned later.

<<UCA lookup state members>>=
const uint32_t *multi_ret32;
@

<<Look up and return a single key element from DUCET>>=
int i;
const uint32_t *str = st->strs + st->v.off;
for(i = 0; i < st->curoff; i++)
  if((str[i] & 3) != 3)
    break;
str += i;
st->multi_ret32 = str;
st->curoff -= i;
<<Return DUCET value for [[str]]/[[c]]>>
str++;
st->multi_ret_len = st->curoff - (int)(str - st->multi_ret32);
if(!st->multi_ret_len) {
  st->state = ST_uca_lookup_start;
  <<Return one DUCET value>>
}
st->multi_ret32 = str;
st->state = ST_uca_lookup_multikey;
<<Return more than one DUCET value>>
@

<<UCA lookup states>>=
,ST_uca_lookup_multikey
@

<<Return DUCET value for [[str]]/[[c]]>>=
l123 = *str & ~3;
switch(*str & 3) {
  case 0:
    l4 = 0;
    break;
  case 1:
    l4 = c;
    break;
  default:
    l4 = *++str >> 2;
    break;
}
@

<<Look up in DUCET given state>>=
case ST_uca_lookup_multikey:
  {
    const uint32_t *str = st->multi_ret32;
    c = st->c;
    <<Return DUCET value for [[str]]/[[c]]>>
    str++;
    st->multi_ret_len -= str - st->multi_ret32;
    if(st->multi_ret_len) {
      st->multi_ret32 = str;
      <<Return more than one DUCET value>>
    } else {
      st->state = ST_uca_lookup_start;
      <<Return one DUCET value>>
    }
  }
@

Note that the multi-level comparison algorithm requires that more than
one pass be made over the keys, unless the strings being compared
happen to generate keys of the same ignorable level.  This can be done
by either calling [[uni_uca_lookup]] as many times as there are
levels for each character, or by storing the results of the lookup in
dynamic arrays, to be processed in later passes.  The only entries
which do not need to be stored are all-zero entries.  A simple lookup
function is provided to do the lookups, and return the results as well
as the number of entries in each level (if requested).  While it would
be possible to implement level 2 reversal and literal string appends
(the as-yet unimplemented UCA options) in this function, they are left
for an even higher level function.  Since this is only a partial UCA
implementation, I have named it [[uca_raw]].

<<Unicode property exports>>=
<<[[uni_str_uca_raw]][[32]] Prototype>>;
<<[[uni_str_uca_raw]][[16]] Prototype>>;
<<[[uni_str_uca_raw]][[8]] Prototype>>;
@

<<[[uni_str_uca_raw]](@sz) Prototype>>=
/** Obtain DUCET values for an entire UTF-<<@sz>> string.
  * Returns the DUCET lookups for all memebers of \p str/\p slen.  If
  * \p slen is negative, \p str is zero-terminated.  Look options may
  * be specified in \p opts.  This does \e not return the UCA sort key,
  * but a raw concatenation of DUCET lookups.  It is allocated from
  * memory if non-empty, and has length \p *rlen.  If \p llen is not
  * NULL, it is a four-integer array which is filled in with the
  * number of non-zero elements at each of the four levels, in order. */
uint32_t *uni_str_uca_raw<<@sz>>(const uint<<@sz>>_t *str, int slen,
                        const uni_uca_opts_t *opts, unsigned int *rlen,
			unsigned int *llen)
@

<<DUCET lookup functions>>=
<<[[uni_str_uca_raw]][[32]]>>
<<[[uni_str_uca_raw]][[16]]>>
<<[[uni_str_uca_raw]][[8]]>>
@

<<[[uni_str_uca_raw]](@sz)>>=
<<[[uni_str_uca_raw]][[<<@sz>>]] Prototype>>
{
  uint32_t *res, res_max, nres = 0;
  uni_uca_lookup_state_t *st = NULL;
  int use_lev4 = opts && opts->max_level > 3 ? 1 : 0;
  int ret = UNI_UCA_LOOKUP_OK;

  if(!str || !slen) {
    *rlen = 0;
    return NULL;
  }
  inisize(res, (res_max = 10));
  if(opts)
    uni_uca_lookup_opts(&st, opts);
  if(llen)
    llen[0] = llen[1] = llen[2] = llen[3] = 0;
  while(1) {
    uint32_t lev123, lev4;
    uint32_t cp;
    if(!slen)
      cp = UNI_UCA_LOOKUP_END;
    else if(ret == UNI_UCA_LOOKUP_AGAIN)
      cp = 0; /* any old random value */
    else {
      unsigned int clen;
      cp = uni_int_utf<<@sz>>_decode(str, &clen);
      str += clen;
      if(slen > 0)
        slen -= clen; /* underflow if slen was wrong, but I don't care */
      else if(!cp) {
        cp = UNI_UCA_LOOKUP_END;
	slen = 0;
      }
    }
    ret = uni_uca_lookup(cp, &lev123, &lev4, &st);
    if(ret == UNI_UCA_LOOKUP_NONE) {
      if(cp == UNI_UCA_LOOKUP_END)
        break;
      else
        continue;
    }
    if(lev123 || lev4) {
      /* FIXME: return error instead of exiting */
      check_size(res, res_max, nres + 2);
      res[nres++] = lev123;
      if(use_lev4)
        res[nres++] = lev4;
      if(llen) {
        if(lev123 & UNI_DUCET_LEV1_MASK)
          llen[0]++;
        if(lev123 & UNI_DUCET_LEV2_MASK)
          llen[1]++;
        if(lev123 & UNI_DUCET_LEV3_MASK)
          llen[2]++;
        if(lev4)
          llen[3]++;
      }
    }
    if(ret == UNI_UCA_LOOKUP_OK && cp == UNI_UCA_LOOKUP_END)
      break;
  }
  *rlen = nres;
  return res;
}
@

The above result could be considered the UCA key, albeit in a
completely different format than recommended by the UCA.  However, the
options need to be passed along with it, in order to determine if
level 4 is present or not, and also to implement any UCA options that
were deferred to a higher level.  A function is provided to perform
the key comparison, but it only adds one UCA option:  reversal of
level 2.  The final option, literal string comparison, is deferred to
an even higher level.

<<Unicode UCA function options>>=
/** Reverse order of level 2 key elements.
  * Ignored by \ref uni_uca_lookup, but honored by higher-level routines. */
uint8_t reverse_lev2;
@

<<Unicode property exports>>=
/** Compre raw DUCET lookup strings.
  * Compares two results from \ref uni_str_uca_raw32 and friends. Results
  * are indeterminate if \p opts is different than what was used to generate
  * the keys.  */
int uni_uca_raw_cmp(const uint32_t *key1, unsigned int len1,
                    const uint32_t *key2, unsigned int len2,
		    const uni_uca_opts_t *opts);
@

<<DUCET lookup functions>>=
static int uni_uca_raw_cmp123(uint32_t mask,
                              const uint32_t *key1, unsigned int len1,
			      const uint32_t *key2, unsigned int len2,
			      int l4, int rev)
{
  uint32_t k1 = 0, k2; /* stupid gcc complains about k1 uninitialized */

  while(1) {
    while(len1 > 0 && !(k1 = *key1 & mask)) {
      key1 += 1 + l4;
      len1 -= 1 + l4;
    }
    while(len2 > 0 && !(k2 = *key2 & mask)) {
      key2 += 1 + l4;
      len2 -= 1 + l4;
    }
    if(!len2)
      return len1 > 0;
    if(!len1)
      return -1;
    if(k1 < k2)
      return -1;
    if(k2 < k1)
      return 1;
    key1 += 1 + l4;
    key2 += 1 + l4;
    len1 -= 1 + l4;
    len2 -= 1 + l4;
  }
}

int uni_uca_raw_cmp(const uint32_t *key1, unsigned int len1,
                    const uint32_t *key2, unsigned int len2,
		    const uni_uca_opts_t *opts)
{
  int lev = opts && opts->max_level ? opts->max_level : 3;
  int l4 = lev > 3;
  int c;
  c = uni_uca_raw_cmp123(UNI_DUCET_LEV1_MASK, key1, len1, key2, len2, l4, 0);
  if(c || lev == 1)
    return c;
  c = uni_uca_raw_cmp123(UNI_DUCET_LEV2_MASK, key1, len1, key2, len2, l4,
                           opts && opts->reverse_lev2);
  if(c || lev == 2)
    return c;
  c = uni_uca_raw_cmp123(UNI_DUCET_LEV3_MASK, key1, len1, key2, len2, l4, 0);
  if(c || lev == 3)
    return c;
  c = uni_uca_raw_cmp123(~0, key1 + 1, len1, key2 + 1, len2, 1, 0);
  return c;
}
@

Implementing the final step when literal comparison is to be made is
supported by a simple [[strcmp]]-like function.  Conversion to valid NFD
form is expected to have been done before the call.  The string length
can either be negative, indicating zero termination, or positive,
indicating an absolute length.

<<Unicode I/O Exports>>=
/* "int"ernal strcmp; assumes a & b are valid UTF */
<<[[uni_int_utf-]][[32]][[_strcmp]] Prototype>>;
<<[[uni_int_utf-]][[16]][[_strcmp]] Prototype>>;
<<[[uni_int_utf-]][[8]][[_strcmp]] Prototype>>;
@

<<[[uni_int_utf-]](@sz)[[_strcmp]] Prototype>>=
/** Compare two seqences of Unicode code points.
  * If \p alen is less than zero, \p a is zero-terminated.  Likewise, if
  * \p blen is less than zero, \p b is zero-terminated.  Encoding is
  * native UTF-<<@sz>>.  Ordering is by code point, then length; -1 is
  * returned if \p a is less than \p b, 1 if it is greater, and 0 if it
  * is equal to \p b.  Results are undefined if either string is not valid
  * native UTF-<<@sz>>. */
int uni_int_utf<<@sz>>_strcmp(const uint<<@sz>>_t *a, int alen, const uint<<@sz>>_t *b, int blen)
@

<<Unicode I/O functions>>=
<<[[uni_int_utf-]][[32]][[_strcmp]]>>
<<[[uni_int_utf-]][[16]][[_strcmp]]>>
<<[[uni_int_utf-]][[8]][[_strcmp]]>>
@

<<[[uni_int_utf-]](@sz)[[_strcmp]]>>=
<<[[uni_int_utf-]][[<<@sz>>]][[_strcmp]] Prototype>>
{
  while(alen && blen) {
    unsigned int aclen, bclen;
    if(alen < 0 && !*a)
      return blen < 0 && !*b ? 0 : -1;
    if(blen < 0 && !*b)
      return 1;
    uint32_t ac = uni_int_utf<<@sz>>_decode(a, &aclen);
    uint32_t bc = uni_int_utf<<@sz>>_decode(b, &bclen);
    if(ac < bc)
      return -1;
    if(ac > bc)
      return 1;
    a += aclen;
    b += bclen;
    alen -= aclen; /* underflows if alen was wrong, but don't care */
    blen -= bclen; /* likewise with blen */
  }
  return alen > 0 || (alen < 0 && *a) ? 1 :
         blen > 0 || (blen < 0 && *b) ? -1 : 0;
}
@

Another option would be to support [[memcmp]], [[strcmp]] or something
similar, by building a key more like the UCA format.  This might make
storing keys for later use much easier.  However, other than storage
efficiency, I see no advantage, and will not implement this.

For direct UCA string comparison, the above functions could be
combined by generating both keys, comparing them, and freeing them.
However, direct string comparison can also be performed by only
storing levels 2-4, and aborting the comparison if a level 1 mismatch
occurs.  The original string pointers need to be retained as well, in
case a literal comparison needs to be made for the last step.  In
order to avoid passing the literal comparison flag separately, this is
added to the options structure.  While the standard indicates that
literal comparison is selected with [[max_level]] of 5 or higher, in
fact the test data suggests that literal comparison should be possible
with lower levels as well.  If [[max_level]] is 5 or higher, or the
explicit [[do_literal]] flag is set, literal comparison will be made.

<<Unicode UCA function options>>=
/** Do a literal comparison as a final step.   Ignored by
  * \ref uni_uca_lookup and \ref uni_uca_raw_cmp, but honored by
  * \ref uni_uca_strcmp32 and friends.  Set automatically if \ref max_level
  * is greater than four.  */
int8_t do_literal;
@

<<Unicode property exports>>=
<<[[uni_uca_strcmp]][[32]] Prototype>>;
<<[[uni_uca_strcmp]][[16]] Prototype>>;
<<[[uni_uca_strcmp]][[8]] Prototype>>;
@

<<[[uni_uca_strcmp]](@sz) Prototype>>=
/** Compare two strings using the Unicode Collation Algorithm.
  * Strings \p a/\p alen and \p b/\p blen are converted to UCA sort keys.
  * Negative \p alen or \p blen indicate zero-terminated \p a or \p b,
  * respectively.  If the sort keys differ, -1 is returned if \p a's key
  * is less than \p b's, and 1 is returned if \p b's key is less than \p a's.
  * If the keys are equal, and the \p do_literal option is not set, zero
  * is returned.  Otherwise, the result of a raw code point comparison is
  * returned. */
int uni_uca_strcmp<<@sz>>(const uint<<@sz>>_t *a, int alen,
                     const uint<<@sz>>_t *b, int blen,
                     const uni_uca_opts_t *opts)
@

<<DUCET lookup functions>>=
<<[[uni_uca_strcmp]][[32]]>>
<<[[uni_uca_strcmp]][[16]]>>
<<[[uni_uca_strcmp]][[8]]>>
@

<<[[uni_uca_strcmp]](@sz)>>=
<<[[uni_uca_strcmp]][[<<@sz>>]] Prototype>>
{
  const uint<<@sz>>_t *rawa = a, *rawb = b;
  int raw_alen = alen, raw_blen = blen;
  uint32_t *duca, *ducb;
  unsigned int max_duca = 0, max_ducb = 0, duc_lena = 0, duc_lenb = 0;
  int do_literal = opts && opts->do_literal;
  int lev = opts && opts->max_level ? opts->max_level : 3;
  uni_uca_lookup_state_t *sta = NULL, *stb = NULL;
  int has_l4 = lev > 3;

  if(lev > 4) {
    do_literal = 1;
    lev = 4;
  }
  if(opts) {
    uni_uca_lookup_opts(&sta, opts);
    uni_uca_lookup_opts(&stb, opts);
  }
  uint32_t lev123a, lev123b, lev4a, lev4b;
  int reta = UNI_UCA_LOOKUP_OK, retb = UNI_UCA_LOOKUP_OK;

  <<Get next key element for string [[a]]>>
  <<Get next key element for string [[b]]>>
  if(reta == UNI_UCA_LOOKUP_NONE && retb == UNI_UCA_LOOKUP_NONE)
    return do_literal ? uni_int_utf<<@sz>>_strcmp(rawa, raw_alen, rawb, raw_blen) : 0;
  inisize(duca, max_duca = 10);
  inisize(ducb, max_ducb = 10);
  while(1) {
    <<Scan to next non-zero level 1 in string [[a]]>>
    <<Scan to next non-zero level 1 in string [[b]]>>
    if(reta == UNI_UCA_LOOKUP_NONE && retb == UNI_UCA_LOOKUP_NONE) {
      int c = 0;
      if(lev > 1)
        c = uni_uca_raw_cmp123(UNI_DUCET_LEV2_MASK, duca, duc_lena,
                                                      ducb, duc_lenb, has_l4,
						      opts && opts->reverse_lev2);
      if(lev > 2 && !c)
        c = uni_uca_raw_cmp123(UNI_DUCET_LEV3_MASK, duca, duc_lena,
                                                      ducb, duc_lenb, has_l4, 0);
      if(lev > 3 && !c)
        c = uni_uca_raw_cmp123(~0, duca + 1, duc_lena,
                                     ducb + 1, duc_lenb, 1, 0);
      free(duca);
      free(ducb);
      return c ? c : do_literal ? uni_int_utf<<@sz>>_strcmp(rawa, raw_alen,
                                                          rawb, raw_blen) : 0;
    }
    if(reta == UNI_UCA_LOOKUP_NONE) {
      free(duca);
      free(ducb);
      return -1;
    }
    if(retb == UNI_UCA_LOOKUP_NONE) {
      free(duca);
      free(ducb);
      return 1;
    }
    uint32_t l1a = lev123a & UNI_DUCET_LEV1_MASK,
             l1b = lev123b & UNI_DUCET_LEV1_MASK;
    if(l1a != l1b) {
      free(duca);
      free(ducb);
      return l1a < l1b ? -1 : 1;
    }
    lev123a &= ~UNI_DUCET_LEV1_MASK;
    lev123b &= ~UNI_DUCET_LEV1_MASK;
  }
}
@

<<Scan to next non-zero level 1 in string (@ab)>>=
while(ret<<@ab>> != UNI_UCA_LOOKUP_NONE && !(lev123<<@ab>> & UNI_DUCET_LEV1_MASK)) {
  /* save l2+ keys obtained so far if non-zero */
  if(lev4<<@ab>> || lev123<<@ab>> & ~(UNI_DUCET_LEV1_MASK | 3)) {
    /* FIXME: return error instead of exiting */
    check_size(duc<<@ab>>, max_duc<<@ab>>, duc_len<<@ab>> + 2);
    duc<<@ab>>[duc_len<<@ab>>++] = lev123<<@ab>>;
    if(has_l4)
      duc<<@ab>>[duc_len<<@ab>>++] = lev4<<@ab>>;
  }
  <<Get next key element for string [[<<@ab>>]]>>
}
@

<<Get next key element for string (@ab)>>=
while(1) {
  uint32_t cp;
  if(!<<@ab>>len)
    cp = UNI_UCA_LOOKUP_END;
  else if(ret<<@ab>> == UNI_UCA_LOOKUP_AGAIN)
    cp = 0; /* any old random value */
  else {
    unsigned int clen;
    cp = uni_int_utf<<@sz>>_decode(<<@ab>>, &clen);
    <<@ab>> += clen;
    if(<<@ab>>len > 0)
      <<@ab>>len -= clen; /* underflow if <<@ab>>len wrong, but don't care */
    else if(!cp) {
      cp = UNI_UCA_LOOKUP_END;
      <<@ab>>len = 0;
    }
  }
  ret<<@ab>> = uni_uca_lookup(cp, &lev123<<@ab>>, &lev4<<@ab>>, &st<<@ab>>);
  /* only ever return NONE if this was the last */
  if(ret<<@ab>> != UNI_UCA_LOOKUP_NONE || cp == UNI_UCA_LOOKUP_END)
    break;
}
@

The above code expects input strings to be in NFD normalization form.
This requires a single decomposition lookup, followed by multuple ccc
lookups and a sort.  However, most strings are already close enough to
being normalized that the DUCET lookup would work anyway.  In
particular, the standard allows a so-called Fast C or D form (FCD)
test%
\footnote{<sigh> the CLDR DUCET in 6.2 at least has additional
problems with this.  Specifically, U+0F73 decomposes to U+0F71 U+0F72.
U+0F73 is a starter (ccc=0), and the other two are non-starters
(ccc>0).  This means that the latter two will be reordered to combine
with U+0FB3, but U+0F73 will not.  Having the string in FCD form makes
no difference; UCA simply disallows moving starters around.  I don't
know if this is a bug in the UCD (maybe U+0F73 shouldn't be a starter,
since it decomposes into two non-starters) or a bug in the claim that
the UCA can be applied as-is (maybe it needs changes to support FCD)
or a bug in the CLDR DUCET or what.}%
.  This requires only the ccc of the first and last character of
each full canonical decomposition.  Since this property is not
provided directly in any other way, it is generated here.

<<Initialize UCD files>>=
decl_num(FCD);
@

<<Parse UCD files>>=
qsort(ccc_prop->rng_dat8, ccc_prop->len, sizeof(*ccc_prop->rng_dat8), uni_cmp_cp);
prop_t *FCD_prop = &parsed_props[prop_FCD = add_prop("FCD")];
FCD_prop->len = FCD_prop->max_len = ccc_prop->len;
inisize(FCD_prop->rng_dat16, FCD_prop->len);
for(i = 0; i < FCD_prop->len; i++) {
  FCD_prop->rng_dat16[i].low = ccc_prop->rng_dat8[i].low;
  FCD_prop->rng_dat16[i].high = ccc_prop->rng_dat8[i].high;
  FCD_prop->rng_dat16[i].dath = FCD_prop->rng_dat16[i].datl = ccc_prop->rng_dat8[i].dat;
}
for(i = 0; i < dmf_prop->len; i++) {
  uint8_t sccc, eccc;
  int l, h, m;
  uint32_t cp = dmf_prop->str_arr[i].cp;
  if(dmf_prop->str_arr[i].flags)
    continue;
  sccc = uni_chrrng_dat8(dmf_prop->strs[dmf_prop->str_arr[i].off],
                         ccc_prop->rng_dat8, ccc_prop->len, ccc_prop->def);
  eccc = uni_chrrng_dat8(dmf_prop->strs[dmf_prop->str_arr[i].off + 
                                        dmf_prop->str_arr[i].len - 1],
		         ccc_prop->rng_dat8, ccc_prop->len, ccc_prop->def);
  for(l = 0, h = FCD_prop->len - 1; l <= h; ) {
    m = (l + h) / 2;
    if(FCD_prop->rng_dat16[m].high < cp)
      l = m + 1;
    else if(FCD_prop->rng_dat16[m].low > cp)
      h = m - 1;
    else
      break;
  }
  if(l <= h) {
    if(sccc == FCD_prop->rng_dat16[m].dath &&
       eccc == FCD_prop->rng_dat16[m].datl)
      continue;
    if(FCD_prop->rng_dat16[m].low == FCD_prop->rng_dat16[m].high) {
      FCD_prop->rng_dat16[m].dath = sccc;
      FCD_prop->rng_dat16[m].datl = eccc;
      continue;
    }
    /* always ensure room for 2, even if only 1 needed */
    check_size(FCD_prop->rng_dat16, FCD_prop->max_len, FCD_prop->len + 2);
    if(FCD_prop->rng_dat16[m].low == cp) {
      movebuf(FCD_prop->rng_dat16 + m + 1, FCD_prop->rng_dat16 + m,
              FCD_prop->len - m);
      FCD_prop->len++;
      FCD_prop->rng_dat16[m + 1].low++;
    } else if(FCD_prop->rng_dat16[m].high == cp) {
      movebuf(FCD_prop->rng_dat16 + m + 2, FCD_prop->rng_dat16 + m + 1,
              FCD_prop->len - m - 1);
      FCD_prop->len++;
      FCD_prop->rng_dat16[m++].high--;
    } else {
      movebuf(FCD_prop->rng_dat16 + m + 2, FCD_prop->rng_dat16 + m,
              FCD_prop->len - m);
      FCD_prop->len += 2;
      FCD_prop->rng_dat16[m++].high = cp - 1;
      FCD_prop->rng_dat16[m + 1].low = cp + 1;
    }
    FCD_prop->rng_dat16[m].low = FCD_prop->rng_dat16[m].high = cp;
    FCD_prop->rng_dat16[m].dath = sccc;
    FCD_prop->rng_dat16[m].datl = eccc;
  } else if(sccc || eccc) {
    check_size(FCD_prop->rng_dat16, FCD_prop->max_len, FCD_prop->len + 1);
    movebuf(FCD_prop->rng_dat16 + l + 1, FCD_prop->rng_dat16 + l,
            FCD_prop->len - l);
    FCD_prop->len++;
    FCD_prop->rng_dat16[l].low = FCD_prop->rng_dat16[l].high = cp;
    FCD_prop->rng_dat16[l].dath = sccc;
    FCD_prop->rng_dat16[l].datl = eccc;
  }
}
@

The tables were already dumped above.  The table testing program can
ensure that the two representations are equal, as well.

<<Functions to help test generated tables>>=
<<[[dat]][[16]] test>>
@

<<Additional property type names>>=
/** 16-bit integer data: range table is \ref uni_chrrng_dat16, and
 ** multi-level table is 16 bits per code point */
UNI_PROP_TYPE_DAT16,
@

<<Set prop type for export>>=
if(parsed_props[i].rng_dat16)
  t = UNI_PROP_TYPE_DAT16;
@

It would probably be possible to skip normalization up to (but not
including) the last zero start or end ccc, and even to just do one
normalization step and continue the FCD test for the rest of the
string, but that is left as an exercise for the future.  For now, the
FCD algorithm is copied directly from the technical note, and is
therefore full-string only.

<<Unicode normalization support exports>>=
<<[[uni_is_FCD]][[32]] Prototype>>;
<<[[uni_is_FCD]][[16]] Prototype>>;
<<[[uni_is_FCD]][[8]] Prototype>>;
@

<<[[uni_is_FCD]](@sz) Prototype>>=
/** Check if UTF-<<@sz>> string is in Fast C or D form.
  * Returns true if \p str/\p len is in FCD form.  This may
  * mean that full canonical decomposition need not be performed for
  * algorithms which would other require it. */
int uni_is_FCD<<@sz>>(const uint<<@sz>>_t *str, unsigned int len)
@

<<Unicode normalization support functions>>=
<<[[uni_is_FCD]][[32]]>>
<<[[uni_is_FCD]][[16]]>>
<<[[uni_is_FCD]][[8]]>>
@

<<[[uni_is_FCD]](@sz)>>=
<<[[uni_is_FCD]][[<<@sz>>]] Prototype>>
{
  uint8_t prev = 0, curs;

  while(len) {
    unsigned int clen;
    uint32_t cp = uni_int_utf<<@sz>>_decode(str, &clen);
    if(len < 0 && !cp)
      break;
    uint16_t both = uni_FCD_of(cp);
    curs = both >> 8;
    if(curs && curs < prev)
      return 0;
    prev = both & 0xff;
    len -= clen; /* underflow if clen incorrect, but I don't care */
    str += clen;
  }
  return 1;
}
@

To test collation, the CollationTest files are used.  As with the
normalization tests, the files are simply fed into standard input.
There are two separate files, each with a different configuration.

\lstset{language=make}
<<C Test Support Executables>>=
tstuca \
@

<<Additional Tests>>=
./tstuca <$(UCD_LOC)/CollationTest/CollationTest_NON_IGNORABLE.txt
./tstuca -s <$(UCD_LOC)/CollationTest/CollationTest_SHIFTED.txt
@

\lstset{language=C}
<<tstuca.c>>=
<<Common C Header>>
#include "uni_all.h"
#include "mfgets.h"
#include "mallocdef.h"

<<UCA test support>>

int main(int argc, const char **argv)
{
  uni_uca_opts_t opts = {};
  int do_shift = 0, pr_keys = 0;
  while(argc-- > 1) {
    if(**++argv == '-') {
      const char *s = *argv;
      while(*++s) {
	if(*s == 's')
	  do_shift = 1;
	else if(*s == 'v')
	  pr_keys = 1;
	<<Additional [[tstuca]] options>>
      }
    }
  }
  <<Read and process CollationTest.txt>>
  return 0;
}
@

<<Read and process CollationTest.txt>>=
unsigned int fcd_saved = 0, tot = 0;
if(do_shift) {
  opts.var_mode = UNI_UCA_VAR_MODE_SHIFTED;
  opts.max_level = 4;
} else {
  opts.var_mode = UNI_UCA_VAR_MODE_NON_IGNORABLE;
  opts.max_level = 3; /* undocumented requirement, apparently */
}
opts.do_literal = 1;
char *lbuf = NULL, *s;
unsigned int lbuflen, llen;
uint32_t *buf, *key, *fcd;
unsigned int buf_len, max_buf, key_len, fcd_len;
uint32_t *prev_key = NULL, *prev_buf = NULL;
unsigned int prev_buf_len = 0, prev_key_len = 0;
unsigned int i;
inisize(buf, (max_buf = 10));
#if 1 /* ensure key in comment matches generated key */
char *kbuf;
unsigned int kbuflen;
inisize(kbuf, (kbuflen = 800)); /* just make it big enough rather than resizing */
#endif
while(mfgets(&lbuf, &lbuflen, &llen, 0, stdin)) {
  if(!isxdigit(lbuf[0]))
    continue;
  buf[0] = strtol(lbuf, &s, 16);
  buf_len = 1;
  while(1) {
    while(isspace(*s))
      s++;
    if(!isxdigit(*s))
      break;
    check_size(buf, max_buf, buf_len + 1);
    buf[buf_len++] = strtol(s, &s, 16);
  }
  tot++;
  if(uni_is_FCD32(buf, buf_len)) {
    fcd_saved++;
    fcd = uni_str_uca_raw32(buf, buf_len, &opts, &fcd_len, NULL);
  } else
    fcd = NULL;
  /* needs to be NFD for literal append */
  buf_len = uni_NFD32(NULL, buf_len, &buf, 0, &max_buf);
  key = uni_str_uca_raw32(buf, buf_len, &opts, &key_len, NULL);
  if(fcd && (fcd_len != key_len || cmpbuf(fcd, key, fcd_len))) {
    fprintf(stderr, "key/fcd mismatch\n%s", lbuf);
    s = build_kbuf1234(kbuf, fcd, fcd_len, opts.max_level > 3);
    strcpy(s, "]\n");
    fputs("fcd: ", stderr);
    fputs(kbuf, stderr);
    s = build_kbuf1234(kbuf, key, key_len, opts.max_level > 3);
    strcpy(s, "]\n");
    fputs("nfd: ", stderr);
    fputs(kbuf, stderr);
    /* CLDR data does not conform! */
    /* main data no longer conforms; has one screwy entry */
    /* has algorithm changed? */
    /* 0FB3 0334 0F81 appears OK, but is wrong multiple ways */
    /*   - FCD and non-FCD key mismatch */
  } else
    free(fcd);
#if 1 /* build string to match text in comment */
  s = build_kbuf1234(kbuf, key, key_len, opts.max_level > 3);
  if(pr_keys)
    fputs(kbuf, stdout);
  /* literal */
  strcpy(s, "|]"); /* error in data file: assumes lit level is empty */
  if(pr_keys) {
    fputs("| ", stdout);
    for(i = 0; i < buf_len; i++)
      printf("%04X ", (int)buf[i]);
    puts("]");
  }
  /* comment in test file has UCA key at end of line, formatted as above */
  {
    s = strrchr(lbuf, '[');
    strrchr(s, ']')[1] = 0;
    if(strcmp(s, kbuf)) {
      fprintf(stderr, "comment mismatch\n%.*s\n%s\n%s\n", (int)(s - lbuf), lbuf,
              s, kbuf);
      for(i = 0; i < key_len; i += 1 + (opts.max_level > 3))
        fprintf(stderr, " %04X.%03X.%02X.%04X",
	        (key[i] & UNI_DUCET_LEV1_MASK) >> UNI_DUCET_LEV1_SHIFT,
		(key[i] & UNI_DUCET_LEV2_MASK) >> UNI_DUCET_LEV2_SHIFT,
		(key[i] & UNI_DUCET_LEV3_MASK) >> UNI_DUCET_LEV3_SHIFT,
		opts.max_level > 3 ? key[i+1] : 0);
      putc('\n', stderr);
      /* exit(1); */
    }
  }
#endif
  if(prev_key) {
    /* note: can't use memcmp except on big-endian machines */
    int c = uni_uca_raw_cmp(prev_key, prev_key_len, key, key_len, &opts);
    if(!c)
      c = uni_int_utf32_strcmp(prev_buf, prev_buf_len, buf, buf_len);
    if(c > 0) {
      fflush(stdout);
      fprintf(stderr, "keys are out of order at %s\n", lbuf);
      continue; /* keep using last good one */
    }
    if(uni_uca_strcmp32(prev_buf, prev_buf_len, buf, buf_len, &opts) > 0) {
      fflush(stdout);
      fprintf(stderr, "strcmp error at %s\n", lbuf);
      continue; /* keep using last good one */
    }
    free(prev_key);
    free(prev_buf);
  }
  prev_key = key;
  inisize(prev_buf, buf_len);
  prev_buf_len = buf_len;
  cpybuf(prev_buf, buf, buf_len);
}
if(prev_key) {
  free(prev_key);
  free(prev_buf);
}
printf("%u keys generated; could have skipped NFD on %u tests\n",
       tot, fcd_saved);
@

<<UCA test support>>=
static char *build_kbuf1234(char *s, const uint32_t *key, unsigned int key_len,
                            int has_l4)
{
  unsigned int i;

  *s++ = '[';
  /* lev 1 */
  for(i = 0; i < key_len; i++) {
    uint32_t l1 = (key[i] & UNI_DUCET_LEV1_MASK) >> UNI_DUCET_LEV1_SHIFT;
    if(l1)
      s += sprintf(s, "%04X ", (int)l1);
    if(has_l4)
      i++;
  }
  *s++ = '|';
  *s++ = ' ';
  /* lev 2 */
  for(i = 0; i < key_len; i++) {
    uint32_t l2 = (key[i] & UNI_DUCET_LEV2_MASK) >> UNI_DUCET_LEV2_SHIFT;
    if(l2)
      s += sprintf(s, "%04X ", (int)l2);
    if(has_l4)
      i++;
  }
  *s++ = '|';
  *s++ = ' ';
  /* lev 3 */
  for(i = 0; i < key_len; i++) {
    uint32_t l3 = (key[i] & UNI_DUCET_LEV3_MASK) >> UNI_DUCET_LEV3_SHIFT;
    if(l3)
      s += sprintf(s, "%04X ", (int)l3);
    if(has_l4)
      i++;
  }
  if(has_l4) {
    *s++ = '|';
    *s++ = ' ';
    /* lev 4 */
    for(i = 1; i < key_len; i += 2) {
      uint32_t l4 = key[i];
      if(l4)
        s += sprintf(s, "%04X ", (int)l4);
    }
  }
  return s;
}
@

Since the standard tests only test Shifted and Non-ignorable variable
weighting options, a sample program to exercise the other options is
provided.  It takes the numeric variable treatment type as its first
argument (zero if not present), and if a second argument is present,
the search keys are printed as well.  There is no standard test data;
it must be run and checked manually.  This is not the way to sort a
file.  However, it is sufficient for running a small bit of text
through its paces.

<<C Test Support Executables>>=
tstucavar \
@

<<tstucavar.c>>=
<<Common C Header>>
#include "uni_all.h"
#include "mfgets.h"
#include "mallocdef.h"

static uni_uca_opts_t opts;

struct ustr {
  uint8_t *str, len;
};

static int uca_cmp(const void *_a, const void *_b)
{
  const struct ustr *a = _a, *b = _b;
  return uni_uca_strcmp8(a->str, a->len, b->str, b->len, &opts);
}

int main(int argc, const char **argv)
{
  clearbuf(&opts, 1);
  opts.max_level = 5;
  if(argc > 1)
    opts.var_mode = atoi(argv[1]);
  int pr_keys = argc > 2;
  char *lbuf = NULL;
  unsigned int lbuflen, llen;
  struct ustr *strs;
  int nstrs, maxstrs;
  inisize(strs, (maxstrs = 10));
  nstrs = 0;
  while(mfgets(&lbuf, &lbuflen, &llen, 0, stdin)) {
    /* assume utf-8 input */
    if(!*lbuf || *lbuf == '\n')
      continue;
    unsigned int slen = strlen(lbuf);
    while(slen > 0 && isspace(lbuf[slen - 1]))
      slen--;
    if(!slen)
      continue;
    check_size(strs, maxstrs, nstrs + 1);
    strs[nstrs].str = malloc(slen);
    strs[nstrs].len = slen;
    memcpy(strs[nstrs].str, lbuf, slen);
    nstrs++;
  }
  int i;
  if(pr_keys)
    for(i = 0; i < nstrs; i++) {
      uint32_t *key;
      unsigned int key_len, j;
      fwrite(strs[i].str, strs[i].len, 1, stdout);
      key = uni_str_uca_raw8(strs[i].str, strs[i].len, &opts, &key_len, NULL);
      for(j = 0; j < key_len; j += 2)
        printf(" %04X", (key[j] & UNI_DUCET_LEV1_MASK) >> UNI_DUCET_LEV1_SHIFT);
      putchar('|');
      for(j = 0; j < key_len; j += 2)
        printf(" %04X", (key[j] & UNI_DUCET_LEV2_MASK) >> UNI_DUCET_LEV2_SHIFT);
      putchar('|');
      for(j = 0; j < key_len; j += 2)
        printf(" %04X", (key[j] & UNI_DUCET_LEV3_MASK) >> UNI_DUCET_LEV3_SHIFT);
      putchar('|');
      for(j = 0; j < key_len; j += 2)
        printf(" %04X", key[j + 1]);
      putchar('\n');
    }
  qsort(strs, nstrs, sizeof(*strs), uca_cmp);
  for(i = 0; i < nstrs; i++)
    printf("%.*s\n", (int)strs[i].len, strs[i].str);
  return 0;
}
@

There is a table titled ``Comparison of Variable Ordering'' in the
standard; it is a good start.  It looks very similar to this (headers
have been shortened for formatting; IgnoreSP was dropped in 6.3 and
Shifted (CLDR) is handled elsewhere):

\lstset{language=txt}
{\let\Tt\unimono
<<varordtab.txt>>=
non-Ign	Blanked	Shifted	IgnSP	Shft-Tr
de luge	death	death	death	death
de Luge	de luge	de luge	de luge	deluge
de-luge	de-luge	de-luge	de-luge	de luge
de-Luge	deluge	deluge	deluge	de-luge
deluge	deluge	deluge	deluge	deluge
deLuge	de Luge	de Luge	de Luge	deLuge
death	de-Luge	de-Luge	de-Luge	de Luge
deluge	deLuge	deLuge	deLuge	de-Luge
deLuge	deLuge	deLuge	deLuge	deLuge
demark	demark	demark	demark	demark
				
happy	happy	happy	happy	happy
sad	happy	happy	sad	happy
happy	sad	sad	happy	sad
sad	sad	sad	sad	sad
				
@

}

To make the above table appear correctly, this document needs to
switch to UTF-8 mode.  The PDF fonts look a little worse, but at least
such files can display.  In addition, the skull-and-crossbones and
heart characters are not in the default font (Latin Modern), so
something else needs to be used.  I use DejaVu Sans Mono; see the
this noweb file's header for details.

\lstset{language=make}
<<makefile.vars>>=
uni.pdf uni.html: NW_UTF8=1
@

The following script generates a table like the above, given one or
more files to sort.  When more than one file is sorted, a blank line
is printed between them.  [[<<varord1.txt>>]] and [[<<varord2.txt>>]]
correspond to the two lists being sorted in the above table.  In order
to extract the table properly, tabs need to be expanded for it.

<<Test Support Scripts>>=
tstucavartab \
@

<<Plain Build Files>>=
varord1.txt varord2.txt varordtab.txt \
@

<<makefile.vars>>=
varordtab.txt: NOTANGLE_OPTS=-t8
@

<<makefile.rules>>=
test: varord1.txt varord2.txt varordtab.txt
@

\lstset{language=sh}
<<tstucavartab>>=
#!/bin/sh

t="non-Ign Blanked Shifted IgnSP Shft-Tr"
i=0
p=

for tt in $t; do
  (
    echo $tt
    for x; do
      ./tstucavar $i < $x
      echo
    done
  ) > out$$$i
  p="$p out$$$i"
  i=$((i+1))
done
eval "paste $p"
eval "rm $p"
@

<<Additional Tests>>=
./tstucavartab varord[12].txt | diff - varordtab.txt
@

\lstset{language=txt}
<<varord1.txt>>=
de luge
de Luge
de-luge
de-Luge
deluge
deLuge
death
deluge
deLuge
demark
@

{\let\Tt\unimono
<<varord2.txt>>=
happy
sad
happy
sad
@

}

One aspect of the data that is lost with such a simple return value is
a means to answer the question, ``what is a collation element?''  My
original plan to address this was to return a character count every
time a new key starts for [[uni_uca_lookup]], but tracking this was
messy, and, in some cases, impossible.  Instead, I provide a more
primitive search function, which could, in fact, be used to implement
the code above in a different way without having to worry too much
about the data format.  Rather than using a state machine and
operating on one character at a time, this function operates on a
buffer, and returns the number of buffer characters in the returned
key.  The key is returned as a simple pointer to static data, or
[[NULL]] if canonical decomposition or a synthetic key is needed.  A
function to produce the full synthetic key is provided as well.

One type of long collation element handled by [[uni_uca_lookup]] which
[[uni_DUCET_lookup]] can't is reordering of grapheme cluster elements
(see \ref{doc:uca-ccc-reorder}).  I really ought to implement this.

<<Unicode property exports>>=
/** Return raw DUCET entry for a collation element.
  * Returns a pointer to the raw DUCENT entry for a collation element
  * in \p buf/\p buf_len.  The collation element starts at \p buf, and
  * spans \p *ce_len code points (if \p ce_len is non-NULL).  The
  * raw data is pointed to by \p key, and its length is \p key_len.
  * A non-zero return indicates that providing more code points in \p buf
  * may give a different result.  However, the UCA grapheme cluster
  * reordering done by \ref uni_uca_lookup is not done by this routine,
  * so results may not be entirely accurate.  It is looked up from the
  * table \p tab / \p strs, or \ref uni_DUCET_mtab / \ref uni_DUCET_strs if
  * NULL.  The format of the raw data is documented in the main
  * documentation. */
int uni_DUCET_lookup(const uint32_t *buf, unsigned int buf_len,
                     unsigned int *ce_len, const uint32_t **key,
                     unsigned int *key_len, const uint32_t *tab,
		     const uint32_t *strs);
/** Return synthetic UCA key data for code point \p c.
  * \p key is a 2-word array, filled in with the two values for levels 1-3
  * of a synthetic key.  Level 4 is either 0 or \p c depending on whom you
  * ask. */
void uni_uca_synth_key(uint32_t c, uint32_t *key);
@

<<DUCET lookup functions>>=
int uni_DUCET_lookup(const uint32_t *buf, unsigned int buf_len,
                     unsigned int *ce_len, const uint32_t **key,
                     unsigned int *key_len, const uint32_t *tab,
		     const uint32_t *strs)
{
  if(ce_len)
    *ce_len = 0;
  if(key)
    *key = NULL;
  if(key_len)
    *key_len = 0;
  if(!buf_len)
    return 1;
  if(!tab) {
    tab = uni_DUCET_mtab;
    strs = uni_DUCET_strs;
  }
  const uni_str_ptr_t *v;
  uint32_t c = buf[0];
  if(c > UNI_MAX_CP)
    c = 0xFFFD;
  uni_multi_tab_lookup(tab, c * 4, (const uint8_t **)&v, 0);
  if(!v || !v->len)
    return 0;
  const uint32_t *str = strs + v->off;
  int i;
  for(i = 0; i < v->len && (str[i] & 3) != 3; i++);
  if(i == v->len || buf_len == 1) {
    if(ce_len)
      *ce_len = 1;
    if(key)
      *key = str;
    if(key_len)
      *key_len = i;
    return i < v->len;
  }
  /* FIXME: support gc reordering */
  int l = i, h = v->len - 1, cmp = 1;
  while(l <= h) {
    int m;
    <<Find start of middle index extension>>
    cmp = 0;
    for(i = 0; i < buf_len - 1; i++) {
      uint32_t bc = buf[i + 1];
      if(bc > UNI_MAX_CP)
        bc = 0xFFFD;
      uint32_t mc = str[m + i];
      if((mc & 3) != 3) {
        if(ce_len)
          *ce_len = i;
        if(key)
	  *key = str + m + i;
	if(key_len) {
	  int j;
	  for(j = m + i; j < v->len; j++)
	    if((str[j] & 3) == 3)
	      break;
	  *key_len = j - (m + i);
	}
        cmp = 1;
        break;
      }
      if(bc > mc) {
        cmp = 1;
	break;
      } else if(bc < mc) {
        cmp = -1;
	break;
      }
    }
    if(!cmp && (str[m + i] & 3) == 3)
      cmp = -2; /* special flag: string too long */
    if(!cmp) {
      l = m;
      break;
    } else if(cmp > 0)
      h = m - 1;
    else {
      l = m + 1;
      <<Skip to next index extension>>
    }
  }
  if(!cmp) {
    if(ce_len)
      *ce_len = buf_len;
    l += buf_len;
    if(key)
      *key = str + l;
    for(i = l; i < v->len; i++)
      if((str[i] & 3) == 3)
        break;
    if(key_len)
      *key_len = i - l;
    if(i < v->len) {
      for(l = 0; l < buf_len; l++) {
        uint32_t bc = buf[l];
	if(bc > UNI_MAX_CP)
	  bc = 0xFFFD;
	if((str[i + l] & 3) != 3 || (str[i + l] >> 2) != bc)
	  return 0;
      }
      return 1;
    }
  }
  return cmp == -2;
}
@

<<DUCET lookup functions>>=
void uni_uca_synth_key(uint32_t c, uint32_t *key)
{
  if(c > UNI_MAX_CP)
    c = 0xFFFD;
  key[0] = (uca_synth1(c) << 16) + (0x0020 << 7) + (0x0002 << 2);
  key[1] = (uca_synth2(c) << 16);
}
@

While implementation of the UCA algorithm with the raw tables was not
difficult, there are a few desirable functions which cannot be
accomplished with this data format:%
\footnote{In fact, the first two are the very reason this library
exists.  All of the prior information can be obtained from other
libraries, and in fact the sort keys can be obtained with Single UNIX
Specification functions.  However, no library to my knowledge gives
collation classes or a list of collation elements.  The lack of
locale-specific character classes is less important, since they can at
least be simulated using regular expressions.}

\begin{itemize}
\item Given a collation element, list all collation elements which
have the same key (i.e., determine its collation equivalence class).
This is required for regular expressions.  Then again, one could just
store the DUCET value for the element, and look up the DUCET value for
every single character that needs to match.  The primary advantage of
a reverse table for this is in the case where the equivalence class 
contains very few elements; in that case, the literal elements can be
matched instead, much more efficiently.  One aspect that this library
already provides that others do not is the ability to detect
multi-character collation elements, given the first character of such
an element.  Without that, even the forward lookup approach would not
be possible.
\item Given two collation elements, list all collation elements which
lie between them.  This is required for regular expressions.  Then
again, one could just store the DUCET values for each end of the
range, and look up the DUCET value for every single character that
needs to match.  The only advantage of a reverse table for this is if
the range is small, or the range translates directly to a code point
range.  In either case, a literal match test is more efficient.
\item Reorder elements.  This is required for static CLDR support. Then
again, the reverse table generated for that purpose doesn't need to be
exported.
\item Reorder element blocks.  This is required for static and dynamic
CLDR support.  Then again, I really need to analyze the table better.
If possible, a simple translation table may suffice.
\end{itemize}

At the minimum, these functions all require a lookup table ordered by
sort key rather than collation element.  Similar to the UCA, the sort
compares each level separately, ignoring zeroes.  The first level to
mismatch determines the order.  The table modified for this purpose is
the one which was saved before combining elements whose collation
element has the same starting character.  All synthetic elements and
canonical decompositions have already been filtered out.

\lstset{language=C}
<<DUCET parser globals>>=
<<DUCET lookup format defs>>

static int cmp_ducet_lev123(const uint32_t *kx, uint32_t xlen,
                            const uint32_t *ky, uint32_t ylen,
			    const uint32_t mask)
{
  uint32_t i = 0, j = 0;
  while(i < xlen && j < ylen) {
    while(i < xlen && !(kx[i] & mask))
      i += 1 + ((kx[i] & 3) == 2);
    while(j < ylen && !(ky[j] & mask))
      j += 1 + ((ky[j] & 3) == 2);
    if(j == ylen)
      return i != xlen; /* 0 if both at end, 1 otherwise */
    if(i == xlen)
      return -1;
    if((kx[i] & mask) > (ky[j] & mask))
      return 1;
    if((kx[i] & mask) < (ky[j] & mask))
      return -1;
    i += 1 + ((kx[i] & 3) == 2);
    j += 1 + ((ky[j] & 3) == 2);
  }
  while(i < xlen && !(kx[i] & mask))
    i += 1 + ((kx[i] & 3) == 2);
  while(j < ylen && !(ky[j] & mask))
    j += 1 + ((ky[j] & 3) == 2);
  return j == ylen ? i != xlen : -1;
}
@

<<DUCET parser globals>>=
static int cmp_by_key(const void *a, const void *b)
{
  const raw_cp_str_t *x = a, *y = b;
  const uint32_t *sx = ducet_strs + x->off, *kx,
                 *sy = ducet_strs + y->off, *ky;
  uint32_t xlen = x->len, ylen = y->len;
  /* find end of index extension */
  for(kx = sx; (*kx & 3) == 3; kx++, xlen--);
  for(ky = sy; (*ky & 3) == 3; ky++, ylen--);
  /* always compare level 1 on all keys, then 2 on all keys, etc. */
  int c = cmp_ducet_lev123(kx, xlen, ky, ylen, UNI_DUCET_LEV1_MASK);
  if(c)
    return c;
  c = cmp_ducet_lev123(kx, xlen, ky, ylen, UNI_DUCET_LEV2_MASK);
  if(c)
    return c;
  c = cmp_ducet_lev123(kx, xlen, ky, ylen, UNI_DUCET_LEV3_MASK);
  if(c)
    return c;
  /* level 4 */
  while(xlen && ylen) {
    uint32_t x4, y4;
    switch(*kx & 3) {
      case 0:
        x4 = 0;
	break;
      case 1:
        x4 = x->cp;
	break;
      default: /* can never be 3 */
      /* case 2: */
        x4 = *++kx;
	xlen--;
	break;
    }
    switch(*ky & 3) {
      case 0:
        y4 = 0;
	break;
      case 1:
        y4 = x->cp;
	break;
      default: /* can never be 3 */
      /* case 2: */
        y4 = *++ky;
	ylen--;
	break;
    }
    if(x4 && y4) {
      if(x4 < y4)
        return -1;
      if(x4 > y4)
        return 1;
      x4 = y4 = 0; /* skip */
    }
    if(!x4) {
      kx++;
      xlen--;
    }
    if(!y4) {
      ky++;
      ylen--;
    }
  }
  while(xlen && !(*kx & 3)) {
    kx++;
    xlen--;
  }
  while(ylen && !(*ky & 3)) {
    ky++;
    ylen--;
  }
  if(xlen)
    return 1;
  if(ylen)
    return -1;
  return 0;
}
@

<<Post-process DUCET>>=
qsort(raw_ents, raw_ents_size, sizeof(*raw_ents), cmp_by_key);
@

After sorting, the index and value should be swapped.  Using the full
51-bit key (30 for levels 1-3, plus up to 21 for level 4) as an index
is impractical.  Instead, much like the forward table, there should be
one ``character'' for the index, followed by ``index extensions''
containing the rest of the key.  In this case, the first level is the
most important, so that is the primary index.  The format of the
string at that index is similar to the forward lookup table:  the
first two bits indicate if something is part of the index extension,
or part of the value for that index.  A 3 in the lower two bits
indicates part of the value this time.  Unlike the forward table, the
value must include the first character, and there is little value in
suppressing the first index ``character'' in the ``index extension,''
so the ``index extension'' is the full key.

<<Post-process DUCET>>=
uint32_t *rev_strs, max_rev_strs, rev_strs_len = 0;
inisize(rev_strs, max_rev_strs = 2 * raw_ents_size);
for(i = raw_ents_size - 1; /* i >= 0 */ ; i--) {
  const uint32_t *si = prop->strs + raw_ents[i].off, *kpi, *sj, *kpj;
  int len = raw_ents[i].len + 1; /* + 1 for cp */
  for(kpi = si; (*kpi & 3) == 3; kpi++);
  for(j = i; j > 0; j--) {
    sj = prop->strs + raw_ents[j - 1].off;
    for(kpj = sj; (*kpj & 3) == 3; kpj++);
    if((*kpj & UNI_DUCET_LEV1_MASK) != (*kpi & UNI_DUCET_LEV1_MASK))
      break;
    len += raw_ents[j - 1].len + 1; /* + 1 for cp */
  }
  /* alread sorted by full index, so just append them all */
  check_size(rev_strs, max_rev_strs, rev_strs_len + len);
  uint32_t *dp = rev_strs + rev_strs_len;
  for(low = j; j <= i; j++) {
    sj = prop->strs + raw_ents[j].off;
    /* place key (new index extension) first */
    for(kpj = sj; (*kpj & 3) == 3; kpj++);
    cpybuf(dp, kpj, raw_ents[j].len - (kpj - sj));
    dp += raw_ents[j].len - (kpj - sj);
    /* then append full old index (including cp) */
    *dp++ = (raw_ents[j].cp << 2) + 3;
    cpybuf(dp, sj, kpj - sj);
    dp += kpj - sj;
  }
  raw_ents[low].cp = *kpi >> UNI_DUCET_LEV1_SHIFT;
  raw_ents[low].off = rev_strs_len;
  raw_ents[low].len = len;
  rev_strs_len += len;
  if(low != i)
    movebuf(raw_ents + low + 1, raw_ents + i + 1, raw_ents_size - i - 1);
  raw_ents_size -= i - low;
  if(!(i = low))
    break;
}
@

This transformation alone is already enough to implement the regular
expression requirements, so a property is created using the results.
The string table is rather large (53,053 words in 6.2, with no
redundancy due to the initial cp differing for each entry), but still
barely small enough for 16-bit offsets.

<<Post-process DUCET>>=
char rname[strlen(prop->name) + 5];
memcpy(rname, "rev_", 4);
strcpy(rname + 4, prop->name);
prop_t *rprop = &parsed_props[add_prop(rname)];
rprop->str_arr = raw_ents;
rprop->len = raw_ents_size;
rprop->strs = rev_strs;
rprop->max_strs = max_rev_strs;
rprop->strs_len = rev_strs_len;
enable_str_mt(rprop);
rprop->strs_char_size = 32;
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[add_prop("rev_DUCET")], "Reverse DUCET lookup",
              gen_h, tstf);
@

To compute the equivalence class of a collation element, its DUCET
value is looked up, and all elements from the reverse table are
returned.  If the collation element can't be found in the DUCET lookup
table, it is always unique (the level 1 values of synthesized weights
ensure this, and canonical decompostion must be done before calling). 
The main difficulty in returning a result for this is that collation
elements vary in size.  The easiest way to deal with is to place them
in order of length, with a count preceding each group.  Since the
length of the total return value is known, there is no need for a
special terminator.  Other than the length order, returned values are
in an arbitrary order.

<<Unicode property exports>>=
<<[[uni_uca_char_class]][[32]] Prototype>>;
<<[[uni_uca_char_class]][[16]] Prototype>>;
<<[[uni_uca_char_class]][[8]] Prototype>>;
@

<<DUCET lookup functions>>=
<<[[uni_uca_char_class]][[32]]>>
<<[[uni_uca_char_class]][[16]]>>
<<[[uni_uca_char_class]][[8]]>>
@

<<[[uni_uca_char_class]](@sz) Prototype>>=
/** Find the set of strings with the same UCA key.
  * Looks up the UCA key data for \p ch/\p chlen.  If this data only requires
  * a single lookup (i.e., \p ch/\p chlen is a single collation element),
  * the key data is looked up in \p rev_DUCET / \p rev_strs (or
  * \ref uni_rev_DUCET_mtab / \ref uni_rev_DUCET_strs if NULL), and all
  * matching strings are returned.  See \ref uni_return<<@sz>>_buf<<@sz>>
  * for how the return value works.  The returned array is a sequence of
  * lengths, followed by strings of those lengths (in array elements, not
  * code points).  The first such sequence is for results of length one,
  * the second of length two, etcetera.  The \p opts parameter can be
  * used to influence the initial lookup.  If the forward table,
  * specified by \p opts, and the reverse table, specified by
  * \p rev_DUCET/\p rev_strs, do not correspond to the same data,
  * the results are undefined.  */
int uni_uca_char_class<<@sz>>(const uint<<@sz>>_t *ch, int chlen,
                         const uni_uca_opts_t *opts,
                         const uint32_t *rev_DUCET, const uint32_t *rev_strs,
                         <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<[[uni_uca_char_class]](@sz)>>=
<<[[uni_uca_char_class]][[<<@sz>>]] Prototype>>
{
  /* there are many that have key==0, but this is not the way to find them */
  if(chlen <= 0)
    return 0;
  uint8_t level = opts && opts->max_level ? opts->max_level : 3;
  /* mask out l123 if needed */
  uint32_t l123_mask = level == 1 ? UNI_DUCET_LEV1_MASK :
                                2 ? UNI_DUCET_LEV1_MASK | UNI_DUCET_LEV2_MASK :
				~3;
  /* technically, levels > 4 are not supported */
  /* could mean "use literal char as well", but then return is always self */
  /* as such, "do_literal" is ignored as well */
  uint32_t lev123, l4;
  uni_uca_lookup_state_t *lst = NULL;
  if(opts)
    uni_uca_lookup_opts(&lst, opts);
  uint32_t c;
  unsigned int clen;
  c = uni_int_utf<<@sz>>_decode(ch, &clen);
  ch += clen;
  chlen -= clen;
  int lret = uni_uca_lookup(c, &lev123, &l4, &lst);
  while(lret == UNI_UCA_LOOKUP_NONE) { /* at least one lookup must succeed */
    if(chlen > 0) {
      c = uni_int_utf<<@sz>>_decode(ch, &clen);
      chlen -= clen;
      ch += clen;
    } else
      c = UNI_UCA_LOOKUP_END;
    lret = uni_uca_lookup(c, &lev123, &l4, &lst);
  }
  if(chlen) {
    do {
      lret = uni_uca_lookup(UNI_UCA_LOOKUP_END, &lev123, &l4, &lst);
    } while(lret == UNI_UCA_LOOKUP_AGAIN);
    return 0; /* ch is definitely not a single collation element */
  }
  /* valid forward lookup's first element now in lev123/l4 */
  if(!rev_DUCET)
    rev_DUCET = uni_rev_DUCET_mtab;
  const uint32_t *strs;
  const uni_str_ptr_t *v;
  uint32_t l1 = lev123 >> 16;
  uni_multi_tab_lookup(rev_DUCET, l1 * 4, (const uint8_t **)&v, 0);
  /* fails if synthetic or not found */
  /* could mean any number of things, but definitely class is 0 or 1 chars */
  /* should never be "not found" unless the tables are mismatched */
  if(!v || !v->len) {
    <<Return character set with only [[ch]]/[[chlen]]>>
  }
  strs = (rev_strs ? rev_strs : uni_rev_DUCET_strs) + v->off;
  /* find 1st match within result (l), setting hh to more than last */
  int l = v->off, h = v->off + v->len - 1, hh = h, m, i;
  /* repeat until full DUCET entry matched */
  int koff = 0;
  while(1) {
    lev123 &= l123_mask;
    while(l <= h) {
      m = (l + h) / 2;
      while(m > 0 && (strs[m] & 3) == 3)
        m--;
      while(m > 0 && (strs[m - 1] & 3) != 3)
        m--;
      for(i = 0; i < koff; i++, m++) {
        if((strs[m] & 3) == 1)
	  m++;
	else if((strs[m] & 3) == 3)
	  break;
      }
      if(i == koff) { /* else too low; handle with others below */
        uint32_t ol123 = strs[m] & l123_mask,
	         ol4 = !(strs[m] & 3) ? 0 : strs[m + 1] >> 2;
        if(lev123 == ol123 && (level < 4 || l4 == ol4)) {
          h = m - 1;
	  continue;
	} else if(lev123 < ol123 || (level >= 4 && lev123 == ol123 && l4 < ol4)) {
          hh = h = m - 1;
	  continue;
	} /* else too low; handle below */
      }
      while(m < v->len && (strs[m] & 3) != 3)
        m++;
      while(m < v->len && (strs[m] & 3) == 3)
        m++;
      l = m;
    }
    if(l > hh) { /* no possibility for match */
      while(lret == UNI_UCA_LOOKUP_AGAIN) {
        lret = uni_uca_lookup(UNI_UCA_LOOKUP_END, &lev123, &l4, &lst);
      }
      return 0;
    }
    if(lret != UNI_UCA_LOOKUP_AGAIN)
      break;
    ++koff;
    lret = uni_uca_lookup(UNI_UCA_LOOKUP_END, &lev123, &l4, &lst);
    h = hh;
  }
  /* return any results that have exactly koff+1 elements matching one at l */
  /* first, count result length, setting hh to actual end of results */
  int maxlen = 0, len = 0;
  for(h = l; h < hh; ) {
    for(i = 0, m = l; i <= koff; i++) {
      if((strs[m] & l123_mask) != (strs[h] & l123_mask))
        break;
      if(level > 3) {
        if(((strs[m] & 3) == 0 ? 0 : strs[m + 1] & ~3) !=
	   ((strs[h] & 3) == 0 ? 0 : strs[h + 1] & ~3))
	  break;
      }
      if((strs[m] & 3) == 1)
        m++;
      if((strs[h] & 3) == 1)
        h++;
    }
    if(i <= koff || (strs[h] & 3) != 3)
      break;
    for(i = 0; h < hh && (strs[h] & 3) == 3; i++, h++)
      len += uni_utf<<@sz>>_enclen(strs[h] >> 2);
    if(i > maxlen)
      maxlen = i;
  }
  hh = h;
  uint<<@sz>>_t *ret0, *retp;
  int maxl;
  if(off >= 0) {
    check_size(*buf, *buf_len, off + len + maxlen);
    maxl = len + maxlen;
    retp = *buf + off;
  } else {
    maxl = buf && buf_len ? *buf_len : 0;
    retp = buf ? *buf : NULL;
  }
  for(i = 0; i < maxlen; i++) {
    if(!maxl)
      return len + maxlen;
    ret0 = retp;
    *retp++ = 0;
    if(!--maxl)
      return len + maxlen;
    for(m = l; m < hh; ) {
      while((strs[m] & 3) != 3)
        m++;
      for(h = 1; h + m < hh; h++)
        if((strs[h + m] & 3) != 3)
 	  break;
      if(h != i)
        continue;
      ++*ret0; /* FIXME: no overflow check (may be needed for <<@sz>> == 8) */
      for(h = 0; h < i; h++) {
        c = strs[m + h] >> 2;
	clen = uni_utf<<@sz>>_enclen(c);
        if(maxl < clen) {
	  maxl = 0;
	  break;
	}
	(void)uni_int_utf<<@sz>>_encode(retp, c);
	retp += clen;
	maxl -= clen;
      }
    }
  }
  return len + maxlen;
}
@

<<Return character set with only [[ch]]/[[chlen]]>>=
/* finish up forward lookup to clean up its state */
do {
  lret = uni_uca_lookup(UNI_UCA_LOOKUP_END, &lev123, &l4, &lst);
} while(lret == UNI_UCA_LOOKUP_AGAIN);
int i;
if(off >= 0) {
  uni_return<<@sz>>_buf<<@sz>>(ch, chlen, buf, off + chlen, buf_len);
  if(*buf) {
    for(i = 0; i < chlen - 1; i++)
      (*buf)[i] = 0;
    (*buf)[i] = 1;
  }
} else {
  int l = buf && buf_len ? *buf_len : 0;
  for(i = 0; i < chlen - 1 && i < l; i++)
    (*buf)[i] = 0;
  if(l >= chlen)
    (*buf)[i] = chlen;
  if(l > chlen) {
    uint<<@sz>>_t *b = *buf + chlen;
    unsigned int bl = l - chlen;
    uni_return<<@sz>>_buf<<@sz>>(ch, chlen, &b, off, &bl);
  }
}
return chlen * 2;
@

\subsection{Parsing the UCD -- Others}

[[BidiMirroring.txt]] contains the last officially supported string
property.  However, like the simple case conversion properties, it
always returns just a single code point, and is therefore encoded as a
numeric property.

\lstset{language=C}
<<Initialize UCD files>>=
decl_num(bmg);
@

<<Parse UCD files>>=
open_f("BidiMirroring.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_num(bmg, strtol(fields[1], NULL, 16), 1);
}
fclose(f);
@

[[BidiBrackets.txt]] contains several properties that were introduced
in Unicode 6.3, and therefore not covered above.  The first is a code
point, and therefore numeric, and the second is an enumeration, even
though technically it could be encoded as a boolean property, using
the existence of the first property to provide the third value.
Instead, it's treated as a plain enumeration.

<<Initialize UCD files>>=
decl_num(bpb);
decl_enum(bpt, "n");
@

<<Parse UCD files>>=
open_f("BidiBrackets.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_num(bpb, strtol(fields[1], NULL, 16), 1);
  add_enum(bpt, fields[2]);
}
fclose(f);
@

While scx is officially an ``Other'' property, it can be treated just
like a string.  The difference is that the string elements are script
enumeration values, rather than code points.  The values fit in a
byte, so it's tempting to encode it as 8 bits, but there are zeroes in
the values, which makes the string table dumper sometimes strip them.
The table is small enough that the savings wouldn't be great, anyway.

<<Initialize UCD files>>=
decl_str(scx);
@

<<Parse UCD files>>=
open_f("ScriptExtensions.txt");
prop_scx = add_prop("scx");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  uint32_t str[32], len = 0;
  <<Parse dotted cp>>
  s = fields[1];
  while(1) {
    char *n = strchr(s, ' ');
    if(n)
      *n = 0;
    str[len++] = enum_val(prop_sc, s);
    if(!n)
      break;
    s = n + 1;
  }
  if(prop_scx < 0)
    prop_scx = add_prop("scx");
  add_str_rng(&parsed_props[prop_scx], low, high, str, len);
}
fclose(f);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_scx], "Script_Extensions", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_scx);
@

\lstset{language=txt}
<<FIXME>>=
Change 8-bit array format to require byte length instead of word
length, so scx can be dumped 8-bit (as well as any others with 0 in data)
@

<<FIXME>>=
EmojiSources.txt
  field 1 = index (may be multi-char)
  field 2 = 16-bit DoCoMo Shift-JIS code
  field 3 = 16-bit KDDI Shift-JIS code
  field 4 = 16-bit SoftBank Shift-JIS code
  [may add more fields in the future]
 encoding: all 3 fields as 16-bit numbers (0 if absent), followed by index ext
   # of fields is given as #define
   can't really provide vendor name to field # mapping w/o parsing comments
     '# <n - 1>: <vendor> Shift-JIS code'
 property name = ???
StandardizedVariants.txt
  Field 2: English descriptive text; probably not really useful for computers
  Field 3 could be enumerated, but is useless w/o field 2
USourceData.txt: [note: used to use cr for line termination]
  Hard to say if it's useful outside of IRG.  Index is U-source ID,
  not Unicode code point (UTC-x or UCI-x).  See tr45.
@

Unofficially, [[Unihan_Variants]] provides some properties as well.
Their fields are formatted differently, with U+ prefixes on every code
point.  Of these properties, only cjkCompatibilityVariant has an
official property name entry.  Only Traditional, Simplified, and
Compatibility variants are needed for regular expressions.  The others
(Semantic, SpecializedSemantic, and Z) will not be read into a
property unless I find a use for them.  Part of the reason for this is
that these properties contain additional information, in the form of
providence annotations.  The annotations could simply be dropped and
those properties added without issue (after all, static linking
ensures their removal if unused), but for now, they are left out.

Version 7 has moved the cjkCompatibilityVariant property to the
[[Unihan_IRGSources]] file.  Again, I just read from both files, and
whichever has the property will fill in the values.

\lstset{language=C}
<<UCD parser local definitions>>=
#define add_hstr(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[10]; \
    uint32_t len; \
    for(s = v, len = 0; *s; len++) { \
      while(isspace(*s)) s++; \
      if(*s == 'U') \
        s += 2; \
      str[len] = strtol(s, &s, 16); \
    } \
    add_str_rng(&parsed_props[prop_##n], low, high, str, len); \
  } \
} while(0)
@

<<Initialize Unihan files>>=
decl_str(cjkTraditionalVariant);
decl_str(cjkSimplifiedVariant);
decl_str(cjkCompatibilityVariant);
@

<<Parse Unihan files>>=
open_f("Unihan_Variants.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse Unihan cp>>
  if(!strcmp(fields[1], "kTraditionalVariant"))
    add_hstr(cjkTraditionalVariant, fields[2]);
  else if(!strcmp(fields[1], "kSimplifiedVariant"))
    add_hstr(cjkSimplifiedVariant, fields[2]);
  else if(!strcmp(fields[1], "kCompatibilityVariant"))
    add_hstr(cjkCompatibilityVariant, fields[2]);
}
@

<<Further processing of [[IRGSources]]>>=
if(!strcmp(fields[1], "kCompatibilityVariant"))
  add_hstr(cjkCompatibilityVariant, fields[2]);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_cjkTraditionalVariant],
              "cjkTraditionalVariant", gen_h, tstf);
dump_str_tabs(&parsed_props[prop_cjkSimplifiedVariant],
              "cjkSimplifiedVariant", gen_h, tstf);
dump_str_tabs(&parsed_props[prop_cjkCompatibilityVariant],
              "cjkCompatibilityVariant", gen_h, tstf);
@

<<Parse Unihan files>>=
enable_str_mt_p(prop_cjkTraditionalVariant);
enable_str_mt_p(prop_cjkSimplifiedVariant);
enable_str_mt_p(prop_cjkCompatibilityVariant);
@

\lstset{language=txt}
<<FIXME>>=
Unihan_DictionaryLikeData.txt:  kCangjie, kCheungBaur, kHDZRadBreak
Unihan_Readings.txt: kCantonese, kDefinition, ...
@

Another pair of optional string values comes from the IDNA compatiblity
database%
\footnote{Actually, there is a third field (IDNA2008 status), but it
is informative, and will not be read in as a property unless I find a
use.}%
.  This is available from a separate location
(\url{http://www.unicode.org/Public/idna/}), but is only checked for
in the UCD directory and a few minor variants.  If not present, it is
ignored.  The IDNA\_Status property is technically an enumeration, but
since it has no value aliases, it is instead accumulated as a string
property.  It is then converted to an enumeration.

\lstset{language=C}
<<Initialize UCD files>>=
decl_str(IDNA_Status);
decl_str(IDNA_Mapping);
@

<<Parse UCD files>>=
if((f = fopen("IdnaMappingTable.txt", "r")) ||
   (f = fopen("idna/IdnaMappingTable.txt", "r")) ||
   (f = fopen("../idna/IdnaMappingTable.txt", "r"))) {
  while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    <<Parse dotted cp>>
    add_str8(IDNA_Status, fields[1]);
    if(num_fields > 2)
      add_str(IDNA_Mapping, fields[2]);
  }
  fclose(f);
  str_to_enum(&parsed_props[prop_IDNA_Status], "disallowed");
}
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_IDNA_Mapping], "IDNA_Mapping", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_IDNA_Mapping);
@

<<UCD parser local functions>>=
static void str_to_enum(prop_t *p, const char *def)
{
  uint32_t i, poff = 0, plen = 0, pnum = (uint32_t)(p - parsed_props);
  int32_t seq = -1;

  merge_strs(p);
  p->def = def ? ~0 : 0;
  inisize(val_aliases[pnum], max_val_aliases[pnum] = 5);
  inisize(p->rng_dat8, p->len);
  for(i = 0; i < p->len; i++) {
    if(p->str_arr[i].off == poff && p->str_arr[i].len == plen) {
      p->rng_dat8[i].low = p->rng_dat8[i].high = p->str_arr[i].cp;
      p->rng_dat8[i].dat = seq;
      continue;
    }
    seq++;
    poff = p->str_arr[i].off;
    plen = p->str_arr[i].len;
    check_size(val_aliases[pnum], max_val_aliases[pnum], num_val_aliases[pnum] + 1);
    {
      uni_alias_t *va = &val_aliases[pnum][num_val_aliases[pnum]];
      char *n;
      inisize(n, plen * 4 + 1);
      memcpy(n, (char *)(p->strs + poff), plen * 4);
      n[plen * 4] = 0;
      if(def && !strcmp(n, def))
        p->def = seq;
      va->short_name = va->long_name = n;
      va->alt_name = va->alt_name2 = NULL;
      num_val_aliases[pnum]++;
    }
    i--; /* re-run for current item to actually add it */
  }
  if(p->def == (uint8_t)~0) {
    p->def = seq;
    check_size(val_aliases[pnum], max_val_aliases[pnum], num_val_aliases[pnum] + 1);
    {
      uni_alias_t *va = &val_aliases[pnum][num_val_aliases[pnum]];
      va->short_name = va->long_name = strdup(def);
      va->alt_name = va->alt_name2 = NULL;
      num_val_aliases[pnum]++;
    }
  }
  inisize(enum_vals[pnum], enum_vals_len[pnum] = num_val_aliases[pnum]);
  for(i = 0; i < enum_vals_len[pnum]; i++) {
    enum_vals[pnum][i].name = strdup(val_aliases[pnum][i].short_name);
    enum_vals[pnum][i].val = i;
  }
  /* while strs was sorted, it was sorted by long word, not byte */
  /* this sort is really only necessary on little-endian systems */
  qsort(enum_vals[pnum], enum_vals_len[pnum], sizeof(*enum_vals[pnum]),
        uni_cmp_valueof);
  free(p->strs);
  p->strs = NULL;
  free(p->str_arr);
  p->str_arr = NULL;
}
@

Similarly, the security information database, available separately at
\url{http://www.unicode.org/Public/security/}, contains two optional
enumeration properties without value aliases.

<<Initialize UCD files>>=
decl_str(ID_Restrict_Status);
decl_str(ID_Restrict_Type);
@

<<Parse UCD files>>=
if((f = fopen("IdentifierStatus.txt", "r")) ||
   (f = fopen("security/IdentifierStatus.txt", "r")) ||
   (f = fopen("../security/IdentifierStatus.txt", "r"))) {
  while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    <<Parse dotted cp>>
    add_str8(ID_Restrict_Status, fields[1]);
  }
  fclose(f);
  str_to_enum(&parsed_props[prop_ID_Restrict_Status], "restricted");
}
if((f = fopen("IdentifierType.txt", "r")) ||
   (f = fopen("security/IdentifierType.txt", "r")) ||
   (f = fopen("../security/IdentifierType.txt", "r"))) {
  while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    <<Parse dotted cp>>
    add_str8(ID_Restrict_Type, fields[1]);
  }
  fclose(f);
  str_to_enum(&parsed_props[prop_ID_Restrict_Type], "not-chars");
}
@

\lstset{language=txt}
<<FIXME>>=
security/intentional.txt: could be numeric (char-to-char mapping) (propname?)
security/confusables.txt: propname? 4 props or field 3 as part of value?
security/confusablesWholeScript.txt: propname?
  could be plain 32-bit value: <from><to><type>, but there are dup indices
As of version 9.0:
status -> IdentiiferStatus.txt; now just allowed/restricted, so it's a boolean
type -> IdentifierType.txt; is now "set" of values like script-ex,
        although few unique sets, so could just convert space to underscore
	and make it one enum value
	or it could be set of named flags (16-bit)
@

<<FIXME>>=
Unihan_IRGSources.txt: kIRG_* (not sure how to encode)
@

Finally, it may be useful to know what version of the UCD was used to
compile this library.  This is provided in two forms: a preprocessor
symbol and a constant variable.  The latter is in case the library is
used as a shared object, and is later linked at run-time to a version
that is compiled with a different set of data.

\lstset{language=C}
<<Dump character information as C code>>=
fprintf(gen_h, "/** The version of the UCD data files this library was generated with */\n"
               "#define UNI_UCD_VER %d\n"
	       "/** The version of the CLDR data files this library was generated with */\n"
	       "#define UNI_CLDR_VER %d\n", UCD_VER, CLDR_VER);
@

<<uni_prop.c>>=
const unsigned int uni_ucd_version = UNI_UCD_VER;
const unsigned int uni_cldr_version = UNI_CLDR_VER;
@

<<Unicode property exports>>=
/** The version of the UCD data files this library was generated with */
extern const unsigned int uni_ucd_version,
/** The version of the CLDR data files this library was generated with */
                          uni_cldr_version;
@

<<FIXME>>=
Versions of all other files used.
  UAX/UTS versions (normally synced w/ UCD)
    UTS-18 not synced with UCD, but not implemented here (only mentioned)
    UTS-37 not synced with UCD, and unused here
    UTS-35 included with/synced to CLDR
  Unihan (same as UCD version)
  idna (same as UCD version)
  UCA (same as UCD version)
  security (Version: 3.0-draft) - not really versioned separately any more
  XML entity database - no version information available
@

\chapter{XML Data Files}

A number of data files outside of the UCD are encoded in XML.  These
include in particular the CLDR files.  One way to deal with this would
be to turn the files into files more like the rest of the UCD using
XSLT.  In fact, an earlier version of this library did just that for
the single XML file it supported.  The CLDR has many more files,
though, and not all are in the same format, so something else needs to
be done.  Since everything else is being parsed in parse-ucd,
parse-ucd will read and parse the XML files directly, using libxml2.%
\footnote{\url{http://xmlsoft.org/}}
For maximum simplicity, each file is simply read in using the
document-at-once
parser\footnote{\url{http://xmlsoft.org/html/libxml-parser.html}}.
While the incremental (xmlreader) mode would probably be more
efficient for memory usage, this program is only run once, during
build time, so the memory usage should not matter much.  At least the
XML structure can be freed when finished.  Scanning the tree is done
using manual traversal.  While the associated libxslt could be used to
run the old code directly, the C code is actually much simpler.

\lstset{language=C}
<<Additional parse-ucd includes>>=
#include <libxml/parser.h>
#include <libxml/xmlerror.h>
@

\lstset{language=make}
<<makefile.vars>>=
XML_CFLAGS := $(shell xml2-config --cflags)
XML_LDFLAGS := $(shell xml2-config --libs)
# adding xml flags only to PARSER_CFLAGS would be nice, but hard
# to integrate w/ static_proto gen
EXTRA_CFLAGS += $(XML_CFLAGS)
PARSER_LDFLAGS += $(XML_LDFLAGS)
@

\lstset{language=C}
<<UCD parser local definitions>>=
<<XML Support Definitions>>
@

<<XML Support Definitions>>=
#define xml_opts XML_PARSE_NOENT | /* expand known entities */ \
                 XML_PARSE_NONET | /* forbid network access */ \
		 XML_PARSE_NOCDATA | /* merge CDATA as text */ \
                 XML_PARSE_COMPACT | /* compact small text nodes */ \
		 XML_PARSE_HUGE /* no hard-coded limits */
/* avoid type cast in every strcmp */
#define xml_isname(n, s) !strcmp((const char *)n->name, s)
@

<<Parse character data files>>=
xmlInitParser();  /* xmlCleanupParser() when done */
xmlDocPtr doc; /* the document under consideration */
xmlNodePtr n, c; /* for traversing the nodes */
@

\section{XML Entity Names}

As mentioned in the introduction, the XML standard's entity names are
generally much shorter than Unicode names, so a facility to use them
instead is provided.  This requires parsing the XML entity database.

\lstset{language=make}
<<makefile.vars>>=
PARSER_CFLAGS += -DXMLUNI=\"$(XMLUNI)\"
@

\lstset{language=C}
<<Parse character data files>>=
doc = xmlReadFile(XMLUNI, NULL, xml_opts);
if(!doc) {
  perror(XMLUNI);
  exit(1);
}
for(n = doc->children; n; n = n->next)
  if(n->children && xml_isname(n, "unicode"))
    break;
if(!n) {
  perror(XMLUNI);
  exit(1);
}
@


The name database has subsets called groups.  Using every single name
is certain to frequently collide with Unicode names, so a particular
group should be chosen.  The 2007 group is used by my own
applications, and seems like a reasonable default.

\lstset{language=make}
<<makefile.config>>=
# XML entity name group
XML_ENTITY_NAME_GROUP = 2007

@

<<makefile.vars>>=
PARSER_CFLAGS += -DXML_ENTITY_NAME_GROUP=\"$(XML_ENTITY_NAME_GROUP)\"
@

\lstset{language=C}
<<XML Support Definitions>>=
/* avoid multiple type casts in every attr search */
#define xml_prop(n, p) (char *)xmlGetProp(n, (const xmlChar *)p)
#define xmlprop_free(p) do { \
  if(p) { \
    xmlFree((xmlChar *)p); \
    p = NULL; \
  } \
} while(0)
@

<<UCD parser local definitions>>=
const char **grp_set;
int ngrp_set;
@

<<Parse character data files>>=
for(c = n->children; c; c = c->next) {
  if(!c->children || !xml_isname(c, "entitygroups"))
    continue;
  for(c = c->children; c; c = c->next) {
    if(!c->children || !xml_isname(c, "group"))
      continue;
    char *gn = xml_prop(c, "name");
    int is_gr = gn && !strcmp(gn, XML_ENTITY_NAME_GROUP);
    xmlprop_free(gn);
    if(is_gr)
      break;
  }
  break;
}
if(!c) {
  perror("group " XML_ENTITY_NAME_GROUP " not in " XMLUNI);
  exit(1);
}
{
  xmlNodePtr cc;
  for(ngrp_set = 0, cc = c->children; cc; cc = cc->next) {
    if(cc->type != XML_ELEMENT_NODE || !xml_isname(cc, "set"))
      continue;
    ngrp_set++;
  }
  inisize(grp_set, ngrp_set);
  for(ngrp_set = 0, cc = c->children; cc; cc = cc->next) {
    if(cc->type != XML_ELEMENT_NODE || !xml_isname(cc, "set"))
      continue;
    grp_set[ngrp_set++] = xml_prop(cc, "name");
  }
  qsort(grp_set, ngrp_set, sizeof(*grp_set), sort_strcmp);
}
@

<<UCD parser local functions>>=
static int sort_strcmp(const void *a, const void *b)
{
  return strcmp(*(const char **)a, *(const char **)b);
}
@

Once the group's set names are known, the charlist is scanned for
character tags with an entity descriptor belonging to one of those
sets.  The id of the entity is the entity name, and the id of the
character is the Unicode code point.  Some names are for
multi-code-point sequences, similar to the Unicode named sequences.
For now, sequences are read into a separate property.

<<Parse character data files>>=
decl_str(na_xml);
decl_str(na_xml_seq);
for(n = n->children; n; n = n->next)
  if(n->children && xml_isname(n, "charlist"))
    break;
if(!n) {
  perror("charlist not in " XMLUNI);
  exit(1);
}
for(n = n->children; n; n = n->next) {
  if(!n->children || !xml_isname(n, "character"))
    continue;
  for(c = n->children; c; c = c->next) {
    const char *set = NULL;
    const char *cid = NULL, *eid = NULL;
    if(c->type != XML_ELEMENT_NODE || !xml_isname(c, "entity") ||
       !(set = xml_prop(c, "set")) ||
       !bsearch(&set, grp_set, ngrp_set, sizeof(*grp_set), sort_strcmp) ||
       !(eid = xml_prop(c, "id")) || !(cid = xml_prop(n, "id"))) {
      xmlprop_free(set);
      xmlprop_free(eid);
      xmlprop_free(cid);
      continue;
    }
    if(!strchr(cid, '-')) {
      low = high = strtol(cid + 1, NULL, 16);
      /* note: duplicates will occur: */
      /*   both fully duplicated entries and multiple names for one cp */
      add_str8(na_xml, eid);
    } else
      add_strseq(na_xml_seq, cid + 1, eid);
    xmlprop_free(set);
    xmlprop_free(eid);
    xmlprop_free(cid);
  }
}
xmlFreeDoc(doc);
@

Some code points have multiple entity names.  This problem can be
solved the same way as with Name\_Alias:  prepend a length to each
string.  As an optimization, since the word encoding is not done, and
there is no name longer than 127 characters, the length byte has its
high bit set.  The last string does not have a length byte; it just
occupies the remainder of the space.

<<UCD parser local functions>>=
static int cmp_cp_name(const void *a, const void *b)
{
  int c = *(int32_t *)a - *(int32_t *)b;
  return c ? c : cmp_strs(a, b);
}
@

<<Post-process property data>>=
prop_t *xna = &parsed_props[prop_na_xml];
sort_strs = xna->strs;
qsort(xna->str_arr, xna->len, sizeof(*xna->str_arr), cmp_cp_name);
for(i = xna->len - 1; i > 0; i--) {
  unsigned int nlen = xna->str_arr[i].len;
  for(low = i; low > 0; low--) {
    if(xna->str_arr[low - 1].cp != xna->str_arr[i].cp)
      break;
    if(!cmp_strs(&xna->str_arr[low - 1], &xna->str_arr[low]))
      continue;
    nlen += xna->str_arr[low - 1].len + 1;
  }
  if(low == i)
    continue;
  check_size(xna->strs, xna->max_strs, xna->strs_len + nlen);
  uint8_t *strs8 = (uint8_t *)(xna->strs + xna->strs_len);
  unsigned int strs8_len = 0;
  for(j = low; j <= i; j++) {
    while(j < i && !cmp_strs(&xna->str_arr[j], &xna->str_arr[j + 1]))
      j++;
    uint8_t *p8 = (uint8_t *)(xna->strs + xna->str_arr[j].off);
    unsigned int len8 = xna->str_arr[j].len * 4;
    while(!p8[len8 - 1])
      len8--;
    if(j != i)
      strs8[strs8_len++] = 0x80 | (len8 - 1);
    memcpy(strs8 + strs8_len, p8, len8);
    strs8_len += len8;
  }
  while(strs8_len % 4)
    strs8[strs8_len++] = 0;
  movebuf(xna->str_arr + low + 1, xna->str_arr + i + 1,
          xna->len - (i + 1));
  xna->str_arr[low].off = xna->strs_len;
  xna->str_arr[low].len = strs8_len / 4;
  xna->strs_len += strs8_len / 4;
  xna->len -= i - low;
  if(!(i = low))
    break;
}
@

<<Dump character information as C code>>=
dump_str_tabs(xna, "XML entity names for code points", gen_h, tstf);
@

<<Parse character data files>>=
enable_str_mt_p(prop_na_xml);
@

Some sequences have multiple entries with the same starting character.
There is no guarantee that there are no multiple names for the same
sequence, either.  The encoding used for na\_seq above is good enough to
capture this.

<<Post-process property data>>=
prop_t *xna_seq = &parsed_props[prop_na_xml_seq];
sort_strs = xna_seq->strs;
qsort(xna_seq->str_arr, xna_seq->len, sizeof(*xna_seq->str_arr), cmp_cp_name);
for(i = xna_seq->len - 1; i > 0; i--) {
  for(j = i; j > 0 && xna_seq->str_arr[j].cp == xna_seq->str_arr[j - 1].cp &&
             !cmp_strs(&xna_seq->str_arr[j], &xna_seq->str_arr[j - 1]); j--);
  if(j != i) {
    movebuf(xna_seq->str_arr + j + 1, xna_seq->str_arr + i + 1, xna_seq->len - (i + 1));
    xna_seq->len -= i - j;
    if(!(i -= j))
      break;
  }
  while(i > 0 && xna_seq->str_arr[i].cp == xna_seq->str_arr[i - 1].cp) {
    int jlen = xna_seq->str_arr[i - 1].len, ilen = xna_seq->str_arr[i].len;
    check_size(xna_seq->strs, xna_seq->max_strs, xna_seq->strs_len + ilen + jlen);
    uint16_t *newseq = (uint16_t *)(xna_seq->strs + xna_seq->strs_len);
    jlen *= 2;
    cpybuf(newseq, xna_seq->strs + xna_seq->str_arr[i - 1].off, jlen);
    if(!newseq[jlen - 1])
      jlen--;
    ilen *= 2;
    cpybuf(newseq + jlen, xna_seq->strs + xna_seq->str_arr[i].off, ilen);
    if(!newseq[jlen + ilen - 1])
      ilen--;
    if((ilen + jlen) % 2)
      newseq[ilen++ + jlen] = 0;
    for(j = i - 1; j > 0; j--)
      if(cmp_strs(&xna_seq->str_arr[j], &xna_seq->str_arr[j - 1]))
        break;
    xna_seq->str_arr[j].len = (ilen + jlen) / 2;
    xna_seq->str_arr[j].off = xna_seq->strs_len;
    xna_seq->strs_len += (ilen + jlen) / 2;
    movebuf(xna_seq->str_arr + j + 1, xna_seq->str_arr + i + 1, xna_seq->len - (i + 1));
    xna_seq->len--;
    i = j;
  }
  if(!i)
    break;
}
@

<<Dump character information as C code>>=
dump_str_tabs(xna_seq, "XML entity names for code point sequences", gen_h, tstf);
@

<<Parse character data files>>=
enable_str_mt_p(prop_na_xml_seq);
@

Finally, although the encoding is fairly simple, a function analogous
to the plain Unicode name lookup function is provided.

<<Library [[uni]] Members>>=
xml_cp_to_name.o
@

<<xml_cp_to_name.c>>=
<<Common C Header>>
#include "uni_prop.h"

@

<<Unicode property exports>>=
/** Look up the XML entity name of code point(s).
 * Looks up name of \p len code points starting at \p cp.  If non-NULL,
 * the number of code points consumed to produce the name is returned
 * in the absolute value of \p *seq_len.  If \p *seq_len is less than zero,
 * using a longer input may produce a different name.  If non-NULL,
 * \p *alias can be used to select an alternate name.  If zero is passed in,
 * the canonical name is returned, and \p *alias is updated to the number
 * of aliases.  If \p *alias is a valid alias index, that alias is returned,
 * and \p *alias is decremented.  If \p *alias is not a valid alias index
 * or zero, it is updated to -1 and no name is returned.  If \p alias is
 * NULL, only the canonical name is returned.  The returned string is
 * described in \ref uni_return8_buf8.   An empty return string indicates
 * a code point without a name or an invalid alias */
int uni_cp_to_xml_name(const uint32_t *cp, unsigned int len, int *seq_len,
                       int *alias, <<Buffer return parameters for UTF-[[8]]>>);
@

<<xml_cp_to_name.c>>=
int uni_cp_to_xml_name(const uint32_t *cp, unsigned int len, int *seq_len,
                       int *alias, <<Buffer return parameters for UTF-[[8]]>>)
{
  if(!len) {
    if(seq_len)
      *seq_len = 0;
    if(alias && *alias)
      *alias = -1;
    return 0;
  }
  if(alias && *alias < 0) {
    if(seq_len)
      *seq_len = 0;
    return 0;
  }
  <<Return XML name for [[*cp]]>>
}
@

Just like with Unicode names, the first thing to do is to see if there
is a matching sequence name.

<<Return XML name for [[*cp]]>>=
if(seq_len)
  *seq_len = 1;
const uni_str_ptr_t *lu = len > 1 || seq_len ? uni_na_xml_seq_of(*cp) : NULL;
if(lu && lu->len && len == 1) {
  if(seq_len)
    *seq_len = -1;
} else if(lu && lu->len) {
  const uint8_t *found_na = NULL;
  unsigned int found_nalen = 0, found_seqlen = 0, found_alias = 0, seqlen, nalen;
  const uint8_t *ep = uni_na_xml_seq_strs + lu->off, *p = ep + lu->len;
  const uint16_t *seqptr;
  const uint32_t *cp2;
  while(p > ep) {
    nalen = *--p;
    seqlen = (nalen >> 6) + 1;
    nalen = (nalen & 0x3f) + 1;
    if(!p[-1])
      p--;
    p -= nalen + seqlen * 2;
    seqptr = (const uint16_t *)p;
    int sp = 0;
    for(cp2 = cp + 1; cp2 - cp < len; cp2++) {
      unsigned int clen;
      if(*cp2 != uni_int_utf16_decode(seqptr + sp, &clen))
        break;
      sp += clen;
      if(sp == seqlen) {
        if(seqlen > found_seqlen) {
          found_na = p + seqlen * 2;
	  found_nalen = nalen;
	  found_seqlen = seqlen;
	  found_alias = 0;
	} else if(seqlen == found_seqlen) {
	  found_alias++;
	  if(alias && *alias == found_alias) {
	    found_na = p + seqlen * 2;
	    found_nalen = nalen;
	  }
	}
	break;
      }
    }
    /* if at end of string, mark as possibly longer */
    if(seq_len && sp < seqlen && cp2 - cp == len)
      *seq_len = -1;
  }
  if(found_na) {
    if(alias && *alias > found_alias) {
      *alias = -1;
      *seq_len = 0;
      return 0;
    } else if(alias && !*alias)
      *alias = found_alias;
    else if(alias)
      --*alias;
    if(seq_len)
      *seq_len *= found_seqlen + 1; /* 1 or -1 */
    return uni_return8_buf8(found_na, found_nalen, buf, off, buf_len);
  }
}
@

For the single code point case, there is only one table to consult.
It behaves much like the alias table, except the last entry for each
code point does not have an explicit length, and the explicit lengths
for the others all have their high bit set.

<<Return XML name for [[*cp]]>>=
lu = uni_na_xml_of(*cp);
if(!lu->len)
  return 0;
const uint8_t *str = uni_na_xml_strs + lu->off;
if(!alias)
  return uni_return8_buf8(*str & 0x80 ? str + 1 : str,
                          *str & 0x80 ? (*str & 0x7f) + 1 : lu->len,
			  buf, off, buf_len);
int num_alias = 0;
int alen = lu->len, retlen = 0;
while(1) {
  if(num_alias == *alias) {
    retlen = uni_return8_buf8(*str & 0x80 ? str + 1 : str,
                              *str & 0x80 ? (*str & 0x7f) + 1 : alen,
			      buf, off, buf_len);
    if(num_alias) {
      --*alias;
      return retlen;
    }
  }
  if(!(*str & 0x80))
    break;
  alen -= (*str & 0x7f) + 2;
  str += (*str & 0x7f) + 2;
  num_alias++;
}
if(*alias) /* couldn't find requested alias */
  *alias = -1;
else /* set # of aliases */
  *alias = num_alias;
return retlen;
@

To test this, a simple program just generates every single name.  No
verification is done; this is meant to be verified manually.

<<C Test Support Executables>>=
tstcp_xml_na \
@

<<tstcp_xml_na.c>>=
<<Common C Header>>

#include "uni_prop.h"

int main(void)
{
  uint32_t cp;
  uint8_t *buf = NULL;
  unsigned int buf_len = 0, clen;
  int has_more, aliases;
  for(cp = 0; cp <= UNI_MAX_CP + 1; cp++) {
    aliases = 0;
    clen = uni_cp_to_xml_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
    if(!clen)
      continue;
    printf("%04X %d %.*s\n", cp, has_more, clen, buf);
    while(aliases > 0) {
      clen = uni_cp_to_xml_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
      printf("%04X %d &%.*s\n", cp, has_more, clen, buf);
    }
  }
  /* all sequences */
  int i;
  uint32_t seq[10];
  for(i = 0; i < uni_na_xml_seq_arr_len; i++) {
    const uint8_t *ep = uni_na_xml_seq_strs + uni_na_xml_seq_arr[i].off,
                  *p = ep + uni_na_xml_seq_arr[i].len;
    unsigned int seqlen, nalen;
    while(p > ep) {
      uint32_t *sp = seq;
      *sp = uni_na_xml_seq_arr[i].cp;
      printf("%04X", (int)*sp++);
      nalen = *--p;
      if(!p[-1])
        p--;
      seqlen = (nalen >> 6) + 1;
      nalen = (nalen & 0x3f) + 1;
      p -= nalen + seqlen * 2;
      const uint16_t *seqp = (const uint16_t *)p;
      while(seqlen > 0) {
        *sp = uni_int_utf16_decode(seqp, &clen);
	printf(":%04X", (int)*sp++);
	seqp += clen;
	seqlen -= clen;
      }
      clen = uni_cp_to_xml_name(seq, (int)(sp - seq), &has_more, &aliases, &buf, 0, &buf_len);
      printf(" %d %.*s\n", has_more, clen, buf);
      while(aliases) {
        clen = uni_cp_to_xml_name(seq, (int)(sp - seq), &has_more, &aliases, &buf, 0, &buf_len);
        uint32_t *sp2 = seq;
	printf("%04X", (int)*sp2++);
	while(sp2 < sp)
	  printf(":%04X", (int)*sp2++);
        printf(" %d &%.*s\n", has_more, clen, buf);
	nalen = *--p;
	if(!p[-1])
	  p--;
	seqlen = (nalen >> 6) + 1;
	nalen = (nalen & 0x3f) + 1;
	p -= nalen + seqlen * 2;
      }
    }
  }
  return 0;
}
@

As with the Unicode names, the primary expected usage of these names
is for looking up the code point(s) from the name, rather than the
other way around.  Again, it is expected that a program using the name
tables will dump a custom version of both forward and reverse lookups
as application-specific C code.

Converting the forward tables into reverse tables is much simpler than
with the Unicode names: the names are case-sensitive and only support
exact matching.  There are no synthesized names to worry about.  They
are also short enough that no special word encoding was done.  That
means the only work that needs to be done is to split multi-name code
points into separate entries.  In fact, the string tables can be
retained as-is; only the pointer arrays need adjusting.

The actual offsets and lengths are not available until the string
table has been dumped.  Instead, the offsets are set to the index in
the forward array shifted left 8 bits ored with the offset into that
entry's string.  The lengths are also the pure byte count rather than
the longword count as it currently stands in the forward array.

<<Post-process property data>>=
int prop_rev_na_xml = add_prop("rev_na_xml");
prop_t *rev_xna = &parsed_props[prop_rev_na_xml];
rev_xna->strs_name = "na_xml";
qsort(xna->str_arr, xna->len, sizeof(*xna->str_arr), uni_cmp_cp);
rev_xna->max_len = xna->len + xna_seq->len * 2;
inisize(rev_xna->str_arr, rev_xna->max_len);
for(i = 0; i < xna->len; i++) {
  const uint8_t *src = (const uint8_t *)(xna->strs + xna->str_arr[i].off);
  unsigned int srclen = xna->str_arr[i].len * 4;
  while(!src[srclen - 1])
    srclen--;
  unsigned int elen;
  for(low = 0; src[low] & 0x80; low += elen) {
    elen = (src[low++] & 0x7f) + 1;
    <<Add [[src[i]]] at [[low[elen]]] to [[rev_xna]]>>
  }
  elen = srclen - low;
  <<Add [[src[i]]] at [[low[elen]]] to [[rev_xna]]>>
}
@

<<Add [[src[i]]] at [[low[elen]]] to [[rev_xna]]>>=
check_size(rev_xna->str_arr, rev_xna->max_len, rev_xna->len + 1);
rev_xna->str_arr[rev_xna->len].cp = xna->str_arr[i].cp;
rev_xna->str_arr[rev_xna->len].off = (i << 8) + low;
rev_xna->str_arr[rev_xna->len].len = elen;
rev_xna->str_arr[rev_xna->len].flags = 0;
rev_xna->len++;
@

The name sequences must be split up as well.  As with the Unicode
names, high code points are used to indicate them, along with an
auxiliary table to convert those code points to the real sequences.
Since the high code point distinguishes these from regular names, the
offsets and lengths can be into the sequence property's string table
without conflict.

<<Post-process property data>>=
qsort(xna_seq->str_arr, xna_seq->len, sizeof(*xna_seq->str_arr), uni_cmp_cp);
int prop_na_xml_seq_id = add_prop("na_xml_seq_id");
prop_t *xna_seq_id = &parsed_props[prop_na_xml_seq_id];
inisize(xna_seq_id->str_arr, xna_seq_id->max_len = xna_seq->len * 2);
inisize(xna_seq_id->strs, xna_seq_id->max_strs = xna_seq->max_strs);
for(i = 0; i < xna_seq->len; i++) {
  uint8_t *src = (uint8_t *)(xna_seq->strs + xna_seq->str_arr[i].off);
  unsigned int srclen = xna_seq->str_arr[i].len * 4;
  while(!src[srclen - 1])
    srclen--;
  while(srclen > 0) {
    low = srclen - 1;
    unsigned int elen = src[low];
    unsigned int seqlen = (elen >> 6) + 1;
    elen = (elen & 0x3f) + 1;
    if(!src[low - 1])
      low--;
    low -= elen;
    <<Add [[src[i]]] at [[low[elen]]] to [[rev_xna]]>>
    <<Add [[src[i]]] sequence from [[xna_seq]] at [[low[seqlen]]] to [[xna_seq_id]]>>
    rev_xna->str_arr[rev_xna->len - 1].cp = xna_seq_id->len + UNI_MAX_CP;
    srclen = low - seqlen * 2;
  }
}
@

<<Dump character information as C code>>=
dump_str_ptr_arr(xna_seq_id, "XML entity code point sequence mapping", gen_h);
@

The only things left to do for this table are to sort by name and
correct the offsets.  The former needs to be done before the name
tables are mangled by dumping, and the latter needs to be done
afterwards.

<<Post-process property data>>=
sort_na = xna;
sort_na_seq = xna_seq;
qsort(rev_xna->str_arr, rev_xna->len, sizeof(*rev_xna->str_arr), 
      cmp_rev_xna);
@

<<UCD parser local functions>>=
static const prop_t *sort_na, *sort_na_seq;
static int cmp_rev_xna(const void *_a, const void *_b)
{
  const raw_cp_str_t *a = _a, *b = _b;
  const prop_t *a_na = a->cp > UNI_MAX_CP ? sort_na_seq : sort_na,
               *b_na = b->cp > UNI_MAX_CP ? sort_na_seq : sort_na;
  const uint8_t *astr = (const uint8_t *)(a_na->strs +
                                          a_na->str_arr[a->off >> 8].off) +
		        (a->off & 0xff),
                *bstr = (const uint8_t *)(b_na->strs +
                                          b_na->str_arr[b->off >> 8].off) +
		        (b->off & 0xff);
  unsigned int len = a->len > b->len ? b->len : a->len;
  int c = memcmp(astr, bstr, len);
  if(c)
    return c;
  if(a->len > b->len)
    return 1;
  else if(b->len > a->len)
    return -1;
  return 0;
}
@

<<Dump character information as C code>>=
for(i = 0; i < rev_xna->len; i++) {
  const prop_t *srcna = rev_xna->str_arr[i].cp > UNI_MAX_CP ? xna_seq : xna;
  uint32_t off = rev_xna->str_arr[i].off;
  rev_xna->str_arr[i].off = srcna->str_arr[off >> 8].off + (off & 0xff);
}
@


Now, the table can be dumped.

<<Dump character information as C code>>=
dump_str_arr(rev_xna, "Reverse lookup table for XML entity names", gen_h, NULL);
@

As with the Unicode names, a binary search function is provided,
mainly for illustrative purposes.  Since it is not actually meant to
be used, it is placed in a separate object file.

<<Unicode property exports>>=
/** Look up Unicode code point(s) for an XML entity name.
  * Look up \p name/\p len using binary, case-sensitive searching.  If
  * \p len is negative, \p name is zero-terminated.  If the return value
  * is negative, \p name does not match any known XML entity name.  If
  * the return value is greather than \ref UNI_MAX_CP,
  * it is the sequence \ref uni_na_xml_seq_id_arr[\e N - \ref UNI_MAX_CP - 1],
  * where \e N is the return value from this function.  */
int32_t uni_xml_name_to_cp(const char *name, int len);
@

<<Library [[uni]] Members>>=
xnarev_bin.o
@

<<xnarev_bin.c>>=
<<Common C Header>>
#include "uni_prop.h"

int32_t uni_xml_name_to_cp(const char *name, int len)
{
  unsigned int nlen = len < 0 ? strlen(name) : len;
  <<Look up XML name and return if present>>
  return -1;
}
@

Like with ranges, the next step is a binary search.  Unlike ranges, an
exact match is the only thing that matters.

<<Look up XML name and return if present>>=
int l = 0, h = uni_rev_na_xml_arr_len - 1, m;
while(l <= h) {
  m = (l + h) / 2;
  const uni_str_arr_t *retp = &uni_rev_na_xml_arr[m];
  const uint8_t *rev_na = retp->cp > UNI_MAX_CP ?
                                 uni_na_xml_seq_strs : uni_na_xml_strs;
  rev_na += retp->off;
  unsigned int rlen = retp->len;
  if(rlen > nlen)
    rlen = nlen;
  int c = memcmp(name, rev_na, rlen);
  if(!c)
    c = nlen > rlen ? 1 : retp->len > rlen ? -1 : 0;
  if(!c)
    return retp->cp;
  if(c > 0)
    l = m + 1;
  else
    h = m - 1;
}
@

Testing all of this can be difficult.  Instead of a thorough test, the
following program takes the output of [[tstcp_xml_na]] above, and
looks up the name.  If the looked up code points do not match the
advertised ones, an error is returned.

\lstset{language=make}
<<Additional Tests>>=
./tstcp_xml_na | ./tstxml_na_cp >/dev/null
@

<<C Test Support Executables>>=
tstxml_na_cp \
@

\lstset{language=C}
<<tstxml_na_cp.c>>=
<<Common C Header>>

#include "uni_prop.h"

char buf[256];

int main(void)
{
  int lno;
  for(lno = 0; fgets(buf, 256, stdin); lno++) {
    <<Set [[e]] to start of name to check>>
    uint32_t ret = uni_xml_name_to_cp(e, -1);
    <<Print and check [[name_to_cp]] results using [[na_xml_seq_id]]>>
  }
  fprintf(stderr, "Successfully looked up %d entries\n", lno);
  return 0;
}
@

While the number of strings is much lower than with the Unicode names,
a hash implementation using [[cmph]] is provided as well.

\lstset{language=make}
<<Library [[uni_cmph]] Members>>=
cmph_rev_xna.o
cmph_na_xml_supt.o
cmph_na_xml_xlate.gen.o
@

<<Libraries to Install>>=
$(if $(CMPH),uni_cmph) \
@

<<makefile.rules>>=
ifneq ($(CMPH),)
cmph_in_na_xml.gen: cmph_in_na_xml_gen
	./cmph_in_na_xml_gen >$@

cmph_in_na_xml.gen.mph: cmph_in_na_xml.gen
	$(CMPH) -g $(CMPH_GEN) cmph_in_na_xml.gen

cmph_rev_xna.c: cmph_in_na_xml.gen.mph
	$(CMPH) -C uni_rev_na_xml_mph -o $@ cmph_in_na_xml.gen
cmph_na_xml_xlate.gen.c: cmph_in_na_xml.gen.mph
	$(call gen_xlate,na_xml) >$@
else
cmph_rev_xna.c: uni_prop.gen.h
	echo >$@
endif
@

<<Clean temporary files>>=
rm -f cmph_in_na_xml.gen{,.mph} cmph_na_xml_xlate.gen.c
@

\lstset{language=C}
<<Unicode property exports using [[cmph]]>>=
/** Pre-generated cmph perfect hash table for looking up XML entity names */
extern const cmph_t uni_rev_na_xml_mph;
/** Translate return from cmph XML entity name lookup return to a
  * \ref uni_rev_na_xml_arr array index.  If the return value is ~0, the
  * lookup failed.  */
extern const uint32_t uni_rev_na_xml_mph_xlate[];
/** The maximum valid value returned by a cmph XML entity name lookup. */
extern const uint32_t uni_rev_na_xml_max_mph_res;
@

\lstset{language=C}
<<cmph_na_xml_supt.c>>=
#ifdef USE_CMPH
<<Common C Header>>

#include "uni_prop.h"

<<[[cmph]] XML name lookup support>>
#endif
@

Once again, the reverse name list used for binary searching is also
used to generate names for [[cmph]].

<<C Build Executables>>=
cmph_in_na_xml_gen \
@

<<makefile.rules>>=
cmph_in_na_xml_gen: libuni.a cmph_in_na_xml_gen.o
	$(CC) -o $@ cmph_in_na_xml_gen.o -L. -luni
@

<<cmph_in_na_xml_gen.c>>=
<<Common C Header>>

#include "uni_prop.h"

int main(void)
{
  unsigned int i;
  
  for(i = 0; i < uni_rev_na_xml_arr_len; i++) {
    const uint8_t *p = uni_rev_na_xml_arr[i].cp > UNI_MAX_CP ?
                          uni_na_xml_seq_strs : uni_na_xml_strs;
    p += uni_rev_na_xml_arr[i].off;
    printf("%.*s\n", (unsigned int)uni_rev_na_xml_arr[i].len, (char *)p);
  }
  return 0;
}
@

The lookup function only does primary lookup, so it is completely
different from the binary search version.

<<Unicode property exports using [[cmph]]>>=
/** Look up Unicode code point(s) for an XML entity name.
  * Look up \p name/\p len using case-sensitive search with a cmph perfect
  * hash table.  If \p len is negative, \p name is zero-terminated.  If
  * the return value is negative, \p name does not match any known XML
  * entity name.  If the return value is greather than \ref UNI_MAX_CP,
  * it is the sequence \ref uni_na_xml_seq_id_arr[\e N - \ref UNI_MAX_CP - 1],
  * where \e N is the return value from this function.  */
int32_t uni_cmph_xml_name_to_cp(const char *name, int len);
@

<<[[cmph]] XML name lookup support>>=
int32_t uni_cmph_xml_name_to_cp(const char *name, int len)
{
  unsigned int nlen = len < 0 ? strlen(name) : len;
  <<Look up XML name using [[cmph]] and return if present>>
  return -1;
}
@

The primary name lookup requires a hash lookup, followed by an
explicit comparison.

<<Look up XML name using [[cmph]] and return if present>>=
uint32_t ret = cmph_search((cmph_t *)&uni_rev_na_xml_mph, name, nlen);
#if defined(CMPH_SPARSE) && defined(CMPH_XLATE)
if(ret < uni_rev_na_xml_max_mph_res)
  ret = uni_rev_na_xml_mph_xlate[ret];
#endif
if(ret < uni_rev_na_xml_arr_len) {
#if defined(CMPH_XLATE) && !defined(CMPH_SPARSE)
  ret = uni_rev_na_xml_mph_xlate[ret];
#endif
  const uni_str_arr_t *retp = &uni_rev_na_xml_arr[ret];
  if(nlen == retp->len) {
    const uint8_t *rev_na = retp->cp > UNI_MAX_CP ?
                                     uni_na_xml_seq_strs : uni_na_xml_strs;
    rev_na += retp->off;
    if(!memcmp(name, rev_na, nlen))
      return retp->cp;
  }
}
@

Testing this is mostly identical to testing the binary search
routines, but timings are also taken to verify that the hash is
actually faster.

\lstset{language=make}
<<Additional Tests>>=
$(if $(CMPH),./tstcp_xml_na | ./tstxml_na_cp_cmph >/dev/null)
@

<<C Test Support Executables>>=
$(if $(CMPH),tstxml_na_cp_cmph) \
@

<<makefile.rules>>=
tstxml_na_cp_cmph: LDFLAGS += $(if $(CMPH_LDFLAGS),$(CMPH_LDFLAGS),-lcmph)
@

\lstset{language=C}
<<tstxml_na_cp_cmph.c>>=
#ifdef USE_CMPH
<<Common C Header>>

#include "uni_prop.h"

<<POSIX timing support>>

char buf[256];

int main(void)
{
  double tb = 0, th = 0;
  unsigned long nn = 0;
  while(fgets(buf, 256, stdin)) {
    <<Set [[e]] to start of name to check>>
    uint32_t ret = uni_cmph_xml_name_to_cp(e, -1);
    <<Print and check [[name_to_cp]] results using [[na_xml_seq_id]]>>
    nn++;
    int j;
    tstart();
    for(j = 0; j < 25; j++)
      ret = uni_xml_name_to_cp(e, -1);
    tb += tend();
    tstart();
    for(j = 0; j < 25; j++)
      ret = uni_cmph_xml_name_to_cp(e, -1);
    th += tend();
  }
  /* report as ops per second rather than total seconds */
  tb = nn * 25000000.0 / tb;
  th = nn * 25000000.0 / th;
  fprintf(stderr, "b%.0f/s h%.0f/s %.2fx\n", tb, th, th/tb);
  return 0;
}
#endif
@

There may be better tuning options, but my experiments show that the
hash function is not, in fact, faster at all for any algorithms but
one: bdz\_ph.  The best performance I got, again with bdz\_ph with the
lowest possible [[-b]] setting, was a whole 6\% faster.  This just
goes to show that the computation of the hash function is far too
expensive.

\chapter{Code Index}
\nowebchunks

% Some might prefer having the Code Index in an appendix as well.
% For this document, I'm making the appendix the "user documentation"
\appendix

% Begin-doc Introduction
\chapter{Building}

Before starting, ensure that all prerequisites are present.  To build
this, build.nw and tjm-ext.nw are required.  If you obtained the PDF
or HTML version of this document, these are included.  Otherwise, they
may be obtained the same place you got this.  In addition, tjm-ext.nw
must be built (using the same build instructions as below, although
the command \texttt{make~libtjm-supt.a} is probably sufficient) in a
separate directory (i.e., without uni.nw in the same directory).  The
other prerequisites are the Unicode data files.  Make sure the file
locations are also set correctly in the config file.

\begin{itemize}
\item The Unicode Character Database; this library was tested using
version 14.0.0.\\
\url{http://www.unicode.org/Public/zipped/}\emph{version}/UCD.zip
\item The Unicode Han Database; this should match the UCD version.\\
\url{http://www.unicode.org/Public/zipped/}\emph{version}/Unihan.zip
\item The Default Unicode Collation Element Table; this should match
the UCD version.  It should be placed in the same directory as the UCD.\\
\url{http://www.unicode.org/Public/UCA/}\emph{version}/*
\item The Unicode Common Locale Data Repository; make sure that the
CLDR version is compatible with the UCD version.\\
\url{http://www.unicode.org/Public/cldr/latest/core.zip}
\item The XML entity database; the only version currently supported is
the one at the following URL:\\
\url{http://www.w3.org/2003/entities/2007xml/unicode.xml}
\item The Unicode IDNA compatibility database (optional); this should
match the UCD version.  If present, it should be in the same directory
as the UCD, or in a subdirectory or sibling directory named idna.\\
\url{http://www.unicode.org/Public/idna/}\emph{version}/*
\item The Unicode security information database (optional); for recent
versions, this should match the UCD version, and for older versions,
it should at least be compatible with the UCD version.  If present, it
should be in the same directory as the UCD, or in a subdirectory or
sibling directory named security.\\
\url{http://www.unicode.org/Public/security/latest/}uts39-data-\emph{version}.zip\\
or, if the zip file is not present, all of the text files in that
directory.
\end{itemize}

\input{build-doc.tex} %%% doc
The full, documented [[makefile.config]] is reproduced here for
convenience.  In particular, the non-generic configuration parameters
start at [[UCD_LOC]].

\input{makefile.config.tex} % make

% End-doc Introduction

% Begin-doc Users-Guide
\chapter{API Documentation}

The easiest way to include this library's definitions is using
[[#include "uni_all.h"]].  Linking requires [[-luni]].  When using the
[[cmph]]-related symbols, linking requires [[-luni_cmph]] and
[[-lcmph]]; compiling requires [[-DUSE_CMPH]] as well as any include
path and other options required by [[cmph]].

Since this library is primarily a C version of the Unicode data files,
the versions of those files used to generate the code may be queried
using preprocessor macros ([[UNI_UCD_VER]] and [[UNI_CLDR_VER]]) or
using variables ([[const unsigned int uni_ucd_version, uni_cldr_version]]).

\section{Unicode Encodings}

\input{unitypes.tex} %%% doc

To support using valid UTF-8 and UTF-16 for internal storage, macros
are provided to navigate the string.  For consistency, UTF-32
navigation functions are provided as well.  The [[nextc]] and
[[prevc]] macros take a pointer to the start of a character, and
return the offset (in words) to the next or previous character,
respectively. The [[startc]] macro takes a pointer to an arbitrary
word in the string, and returns the offset (in words) to the start of
the character this word belongs to.  Since they are macros, [[buf]]
may be evaluated more than once.

% uni_utf32_startc prototype
% uni_utf32_nextc prototype
% uni_utf32_prevc prototype
% uni_utf16_startc prototype
% uni_utf16_nextc prototype
% uni_utf16_prevc prototype
% uni_utf8_startc prototype
% uni_utf8_nextc prototype
% uni_utf8_prevc prototype

For indexed navigation, functions are provided to convert an offset
into a word array to/from an index into the string.  Again, the
behavior is undefined if the UTF-8 or UTF-16 encoding is not valid up
to (but not necessarily including) the desired offset/index.  These
functions are expensive in that they scan the string from the
beginning for every call; use the macros above for small index
adjustments.  The 32-bit versions are dummy functions that return
[[n]], since offset and index are always identical.  They are mainly
for writing generic code using macros.

% uni_utf32_offset_of prototype
% uni_utf32_index_of prototype
% uni_utf16_offset_of prototype
% uni_utf16_index_of prototype
% uni_utf8_offset_of prototype
% uni_utf8_index_of prototype

To determine the length (in code points) of a string of fixed word
length, the [[index_of]] function for the appropriate type can be used
with the word length as the offset.  To determine the length of a
zero-terminated string, the following functions are provided.

% uni_utf32_strlen prototype
% uni_utf16_strlen prototype
% uni_utf8_strlen prototype

For truly random access to Unicode code points, they should be
converted to UTF-32 first.  For internally generated, guaranteed valid
strings, the following functions can be used to do that.  They return
the code point starting at the valid UTF-8/UTF-16 encoding at [[s]]. 
The optional [[nread]] parameter returns the number of words actually
read, although the [[nextc]] macros above could be used instead.

% uni_valid_utf8_decode prototype
% uni_valid_utf16_decode prototype

In fact, they should always be stored in the architecture's native
endianness.  For this reason, a set of functions without endianness
control is provided.  The fact that they are for internal use only is
emphasized by the ``int'' in the name.  While the UTF-32 versions do
mostly nothing, and the UTF-8 versions just copy other functions, a
complete set is provided to make creation of parallel routines for
differing inputs using preprocessor macros easier.  Each encoder takes
a pointer to a sufficiently large buffer (1 word for UTF-32, 2 for
UTF-16, and 4 for UTF-8) and the code point to encode, and returns the
number of words actually written.  Each decoder takes a pointer to
valid data, and returns the code point stored at that pointer.
Optionally, int can also return the number of words to skip if
[[nread]] is non-[[NULL]].

% uni_int_utf8_decode prototype
% uni_int_utf16_decode prototype
% uni_int_utf32_decode prototype
% uni_int_utf8_encode prototype
% uni_int_utf16_encode prototype
% uni_int_utf32_encode prototype

If the required output buffer length is needed for a character, one of
the following macros may be used.  They may evaluate their arguments
more than once.  These compute the output length of the encoder
functions.  To compute the amount consumed by the decoder functions,
use the [[nextc]] macros above.

% uni_utf8_enclen prototype
% uni_utf16_enclen prototype
% uni_utf32_enclen prototype

Another way to compute the length of a UTF-16 string is to just check
for surrogates.  A macro is provided for that.  The macro may evaluate
its parameter more than once, but is as fast as a simple comparison.

% uni_is_surrogate prototype

To directly compare two strings in a valid UTF format, [[memcmp]] can
be used, given the computed length.  For convenience, the following
functions can be used instead.  Note that these do not use the Unicode
Collation Algorithm or perform any locale-specific transformations;
they simply compare code point values.  Each string length may be
positive to indicate the number of words in the string (not
necessarily the number of characters), and it may be negative to
indicate zero termination.

% uni_int_utf32_strcmp prototype
% uni_int_utf16_strcmp prototype
% uni_int_utf8_strcmp prototype

To transform UTF-32 data to and from UTF-32BE and UTF-32LE, the
following functions are used.  The endianness is specified by
[[bige]]; zero specifies little-endian and non-zero big-endian.  The
encoder takes a [[cp]] and stores its result in a buffer; the stored
length is returned (0 on error).  The decoder takes a character from a
buffer, and returns the decoded character.  The return value is a
signed integer; a negative result indicates an error (i.e., an invalid
Unicode code point).

% uni_utf32_encode prototype
% uni_utf32_decode prototype

Similarly, conversion of UTF-32 to and from UTF-16BE and UTF-16LE is
done using the following functions.  At least two words must be
available in the encoder's return buffer, unless [[uni_utf16_enclen]]
is used to compute the output buffer length.  If the input buffer to
the decoder may be invalid, ensure that at least two words are
available as well (padding with zero is safe).  The results and
parameters have the same meaning as for the utf32 functions above,
except that the decoder also takes the length of [[s]] (in words), and
returns the number of words it read from the buffer in [[*nread]]
(optionally, if [[nread]] is non-[[NULL]]).  If [[s]] is too short,
[[*nread]] is always zero.

% uni_utf16_encode prototype
% uni_utf16_decode prototype

Similarly, conversion of UTF-32 to and from UTF-8 is done using the
following functions.  At least 4 bytes must be available in the
encoder's return buffer, unless [[uni_utf8_enclen]] is used to compute
the output buffer length.  The results and parameters have the same
meaning as for the utf16 functions above.

% uni_utf8_encode prototype
% uni_utf8_decode prototype

The above functions generally convert to and from UTF-32, one
code point at a time.  To convert internal (i.e., valid) strings from
one encoding to another in bulk, the following functions are provided.
The number after [[return]] indicates the input ([[str]]/[[len]]) type.
% Begin-doc UTF buffer return
Their return type is indicated by the trailing number in the function
name: UTF-32 (32), UTF-16 (16), or UTF-8 (8).  The return buffer is
pointed to by [[buf]].
% Begin-doc buffer return
If [[off]] is less than zero, [[buf]] points to a fixed-length buffer
of size [[*buf_len]].  In that case, if [[*buf_len]] is zero, [[buf]]
may be NULL; in fact, [[buf_len]] may be [[NULL]] as well to indicate
a zero-length return buffer.  If [[off]] is zero or more, the results
are placed into a dynamically resizable buffer starting at [[off]].
[[*buf]] is newly allocated using [[malloc]] or expanded using
[[realloc]] as needed; the current allocated length is in
[[*buf_len]]. A memory allocation failure causes [[NULL]] to be
returned in [[*buf]] along with a non-zero length.  The return value
of the function is the number of words which would have been written
to the output buffer if it had enough space; on successful dynamic
resizing, or if given a large enough fixed-sized buffer, this is the
actual number of words written. Otherwise, for fixed-sized buffers, at
most [[*buf_len]] words are written, and the function must be called
again with a larger buffer for the full results.

% End-doc buffer return
% End-doc UTF buffer return
When the input and output types match, this basically degenerates to a
straight copy, except that the buffer is resized if possible and
needed, and the length is restricted if needed.

% uni_return8_buf8 prototype
% uni_return8_buf16 prototype
% uni_return8_buf32 prototype
% uni_return16_buf8 prototype
% uni_return16_buf16 prototype
% uni_return16_buf32 prototype
% uni_return32_buf8 prototype
% uni_return32_buf16 prototype
% uni_return32_buf32 prototype

To read and write these types directly from/to a file, the following
functions may be used.  The writers return the number of words in the
target format (e.g. 32-bit words for UTF-32) written to the file on
success, and zero or less on failure.  If the failure was due to a
short write, the return value is the negative of the number of words
actually written.  Use [[ferror]] to distinguish between invalid input
and read/write errors.  The readers return
% Begin-doc utf-reader-ret
$-1$ if no characters could be read, $-2$ on a short (less than a full
code point) read, $-3$ on invalid code points, and otherwise the read
character.
% End-doc utf-reader-ret

Note that due to limitations of the C [[FILE]] API, it may not be
possible to recover from errors reading UTF-16 input.  A read of an
initial surrogate character, followed by anything but a final
surrogate character will result in the file's read pointer advanced
past the second word, which can no longer be returned to the input
stream.  However, the [[nread]] parameter, if non-[[NULL]], can be
used to determine the number of actual bytes read from the input
stream.  If that is 4, and an error occurred, then recovery could be
done by seeking backwards in the input stream (assuming it is
seekable).

% uni_utf32_putc prototype
% uni_utf16_putc prototype
% uni_utf8_putc prototype
% uni_utf32_getc prototype
% uni_utf16_getc prototype
% uni_utf8_getc prototype

For convenience, UTF-32 strings may also be written to UTF-8 files.  A
UTF-32 buffer [[buf]] of length [[len]] is written to UTF-8 file
[[f]].  Like the single-character output functions above, the return
value is the number of words (bytes in this case) written on success,
or less than or equal to zero on error.  If less than zero, this is
the (short) number of bytes written.

% uni_utf8_fputs prototype

Reading files whose encoding is unknown (e.g. UTF-16 and UTF-32, whose
endianness is determined by the file contents) is supported by a
separate mechanism.  Instead of using the standard C [[FILE]] type, it
uses the opaque type [[uni_file_t]].  Functions to open, close, and
read from such files are provided.

The [[uni_fopen]] function supports multiple input file encodings, as
specified by the [[encoding]] parameter.  If the library was compiled
with [[iconv]] support, all available input encodings for [[iconv]]
are supported.  Otherwise, only the raw input formats, listed below,
are supported.  The default (if [[encoding]] is [[NULL]]) is to use
the current locale's encoding if [[iconv]] support is compiled in and
the current locale's encoding can be obtained, or \verb|UTF|
(generic Unicode) if not.

\paragraph*{Built-in Unicode encodings (case-sensitive):}
\begin{quote}
\begin{description}
\item[ANSI\_X3.4-1968] --- ASCII (i.e., raw bytes; no range checking)
\item[ISO-8859-1] --- ISO Latin-1 (i.e., raw bytes)
\item[UTF] --- Automatically detected UTF mode
\item[UTF-8] --- UTF-8
\item[UTF-16] --- UTF-16; if first character in file is U+FEFF in
inverse native byte order, use that order; otherwise, use native byte order
\item[UTF-16LE] --- Little-endian UTF-16
\item[UTF-16BE] --- Big-endian UTF-16
\item[UTF-32] --- UTF-32; if first character in file is U+FEFF in
inverse native byte order, use that order; otherwise, use native byte order
\item[UTF-32LE] --- Little-endian UTF-32
\item[UTF-32BE] --- Big-endian UTF-32
\end{description}
\end{quote}

Note that unlike [[uni_utf16_getc]], all of the internal encoding
types consume the longest erroneous sequence, and no more, when an
encoding error is encountered.

For generic Unicode input ([["UTF"]]), if the first character could be
Byte Order Mark (U+FEFF) in any supported format, it is considered to
be one, and the format of the file is the format of that Byte Order
Mark:

\begin{quote}
\begin{tabular}{ll}
First bytes of file&Format\\
\texttt{00 00 FE FF}&UTF-32BE\\
\texttt{FF FE 00 00}&UTF-32LE\\
\texttt{FE FF}&UTF-16BE\\
\texttt{FF FE}&UTF-16LE\\
Other&UTF-8\\
\end{tabular}
\end{quote}

Note that a UTF-16LE file whose first character after the Byte Order
Mark is zero is determined to be a UTF-32LE file, instead.  While a
potential conflict exists between all other formats and ``Other'',
there is no valid UTF-8 character which starts with FF or FE, so there
should be no conflict.  Also, the Byte Order Mark is part of the file
data, so it is returned as the first character.

The return value for [[uni_fopen]] is [[NULL]] on error, or a pointer
to the file structure.  This structure is freed by [[uni_fclose]].
The [[uni_fgetc]] return value is
\input{utf-reader-ret.tex} %%% doc

% uni_fopen prototype
% uni_fclose prototype
% uni_fgetc prototype

Since normalization is a common requirement for user input, a function
to read a single normalized character is provided as well.  The [[nt]]
parameter takes a normalization type, corresponding to the standard
normalization type names:  [[UNI_NORM_NFD]], [[UNI_NORM_NFKD]],
[[UNI_NORM_NFC]], [[UNI_NORM_NFKC]], and [[UNI_NORM_NFKC_CF]].  For
completeness, no normalization can be specified using
[[UNI_NORM_NONE]], although it is cheaper to just use [[uni_fgetc]].

The entire file must be read using this function in order for it to
work properly.  Changing the normalization type mid-stream is also
generally not supported, since any normalization in progress will not
be affected.

Errors in the input stream are ignored.  Encoding errors and invalid
Unicode characters (surrogates and code points larger than
[[0x10FFFF]]) are converted to U+FFFD, REPLACEMENT CHARACTER.  There
is no way to determine the difference between the presence of an
actual REPLACEMENT CHARACTER and invalid text.

% uni_fgetc_norm prototype

\section{Data Types for Property Storage}

\subsection{Bit Sets}

The following functions support storage of Unicode boolean properties
in simple bit sets.  [[a]] is an array of any unsigned integer data
type.  The [[UNI_BSET_ELT]] and [[UNI_BSET_BIT]] convert an index
[[i]] into an array index [[e]] and bit number within that array
element [[b]].  The [[UNI_BSET_ENTRY]] function performs the reverse
operation, giving an index [[i]] from an element index [[e]] and bit
index [[b]].  The [[UNI_BSET_MASK]] function converts the bit index
into a mask.  Several common uses for this mask are taken care of:
[[UNI_BSET_IS_SET]] tests if a given index is set, and
[[UNI_BSET_SET]] and [[UNI_BSET_CLEAR]] modify a specific bit.  The
final useful function is [[UNI_BSET_AELT]], which returns the element
at index [[UNI_BSET_ELT]].  This is an addressable lvalue; that is,
you can take its address or assign a value to it.

Normally, extra code would be added to shorten the array by indicating
a lower and upper bound.  For convenience, the common upper bound,
which is the largest valid Unicode code point, is provided as a
preprocessor symbol ([[UNI_MAX_CP]]).

% UNI_BSET_ELT prototype
% UNI_BSET_BIT prototype
% UNI_BSET_ENTRY prototype
% UNI_BSET_MASK prototype
% UNI_BSET_IS_SET prototype
% UNI_BSET_SET prototype
% UNI_BSET_CLEAR prototype
% UNI_BSET_AELT prototype

\subsection{Sorted Code Point Arrays}

An array of any data type whose first member is a 32-bit signed
integer can be searched and sorted using the standard C [[bsearch]]
and [[qsort]] functions with the following comparison routine:

% uni_cmp_cp prototype

Note that valid Unicode code points are always positive when stored in
a signed 32-bit integer.  A convenience wrapper for [[bsearch]] is
provided, but in general a hand-written binary search can perform
significantly faster.

% uni_is_cp prototype

For convenience, a structure is defined for code point arrays with
32-bit values.  As the [[cp]] member is first, [[uni_cmp_cp]] works
with arrays of this type as well, if [[cp]] is a valid Unicode code
point.  If not, care must be taken that the value never exceeds the
maximum [[int32_t]] value ([[0x7fffffff]]).

\input{[[uni_cp_val_t]].tex} % C

\subsection{Sorted Code Point Range Arrays}

Sorted code point range arrays are arrays of structures whose first
two members define an (inclusive) range; the ranges do not overlap, so
sorting can be done using only the low element.

\input{[[uni_chrrng_t]].tex} % C

An array of any data type whose first two members are 32-bit signed
integers indicating a contiguous range of values, as per the
[[uni_chrrng_t]] data type (if the [[low]] and [[high]] values do not
exceed the maximum [[int32_t]] value), may be searched and sorted
using the standard C [[bsearch]] and [[qsort]] functions with the
following comparison routine:

% uni_cmprng prototype

A convenience wrapper for binary searching is provided.  It uses a
hand-written binary search for significant performance improvement
over using [[bsearch]].  However, unlike the macro provided for single
code point arrays, this function can only be used for arrays of the raw
[[uni_chrrng_t]] type.  Since it uses the raw structure, values
exceeding the maximum [[int32_t]] value are treated as unsigned values
still, so care must be taken that sorting either uses a different
comparison function than above or that the values never exceed this
value.

% uni_is_cp_chrrng prototype

In addition to plain ranges, ranges may store data.  One variant,
[[uni_chrrng_dat8_t]], stores a byte of data along side a
24-bit high range value.  This keeps the structure the same size, but
makes it a bit less efficient for searching:

\input{[[uni_chrrng_dat8_t]].tex} % C

The comparison function for [[qsort]] and [[bsearch]] is different,
since it needs to mask out the value:

% uni_cmprng_dat8 prototype

Since the return value is a byte rather than a flag, the wrapper for
binary searching (again, using a hand-written implementation rather
than [[bsearch]]) returns the byte value, or a given default ([[def]])
if the code point is not in the array.

% uni_chrrng_dat8 prototype

16-bit and 32-bit variants are provided as well.  Each attempts to
keep the size of the structure down to 16 bytes.  The 16-bit version
does this the same way as the 8-bit version:  by storing the code
points as 24-bit numbers, and using the remaining 8 for storage.  The
32-bit version uses a low+length method rather than low-high method,
and disallows ranges longer than 256.  Again, special comparison
functions for [[qsort]] and [[bsearch]] are provided, along with
optimized binary searches.

\input{[[uni_chrrng_dat16_t]].tex} % C
\input{[[uni_chrrng_dat32_t]].tex} % C
% uni_cmprng_dat16 prototype
% uni_chrrng_dat16 prototype
% uni_cmprng_dat32 prototype
% uni_chrrng_dat32 prototype

A special variant of the 32-bit type above interprets the 32-bit value
as a rational number with a 24-bit signed numerator and an 8-bit
unsigned denominator.  The 32-bit number is directly cast to this data
type.  Technically, any pair of numbers could be stored in this
structure; the only thing the functions do to support this
interpretation is to set the denominator to one if the numerator is
zero, so that zero can be more conveniently stored as zero over zero.
Also, the default return value is always zero (over one).

\input{[[uni_frac_t]].tex} % C

While the [[uni_cmprng_dat32]] function could be used, the following
function returns the correctly interpreted numerator and denominator.

% uni_chrrng_val prototype

\subsection{Multilevel Tables}

Multi-level tables are bit arrays split into smaller chunks, with
pointers to those chunks in a table at the next level.  Each level
except the top is also split into smaller chunks with the next level
being pointers to that level.  Duplicates in lower levels are removed
by having only one copy with multiple pointers to that copy.  All-zero
data and all-one data are also stored with special pointer values,
making sparse and repetitive bit arrays fairly storage-efficient.

The tables are stored as an opaque sequence of 32-bit words which are
interpreted dynamically.  When reading such a data structure, only the
pointer to the first word is needed.  When creating such a structure,
both the sequence of words and the total length are returned.

Use the following function to convert a plain bit array to a
multi-level table.  It may take a long time to run, so generally it
should be used to pre-generate tables.  The bit array is described by
[[bits]], which is an array of [[len]] bytes.  The [[low]] and
[[high]] numbers are the valid bit numbers; it is up to the user to
ensure that [[high]] - [[low]] $<$ [[len]] times 8.  The data need
not be a bit array; arbitrary data structures are supported.  The
[[minwidth]] parameter is the number of bytes in the data structure;
this is ignored if less than 4.  It is possible to make values outside
of the [[low]] to [[high]] range be either all zeroes or all ones;
[[def]] should be one or zero to indicate which.  The return value,
[[*ml]], is always allocated using [[malloc]], and contains [[*ml_len]]
32-bit words.

% uni_bits_to_multi prototype

Once a table [[dat]] is generated, the first byte of data for a code
point [[val]] can be found using [[uni_multi_tab_lookup]].  For data
structures whose size is not one byte, the code point must be
multiplied by the number of bytes.  For bit arrays, this means
dividing by 8.  For bit arrays, the actual bit must then be obtained
using a bit mask.

The return value from the function is zero for all zeroes, [[~0]] for all
ones, and 1 for anything else.  [[*ret]] is [[NULL]] for all zeroes or
all ones, and a pointer to the first byte of the result otherwise.  If
the code point is outside the range defined by the table ([[low]] and
[[high]] during creation), and [[def]] is zero, the default provided
during table creation is returned (i.e., all zeroes or all ones).  If
[[def]] is non-zero, [[def]] is returned by the function, but [[*ret]]
is [[NULL]].  Note that if [[def]] is [[~0]], there is no way to tell
the difference between out-of-range and all ones.

% uni_multi_tab_lookup prototype

The above lookup function returns a pointer to raw data.  Since they
are most common, bit array tables can be queried using a simpler
function, which takes the unscaled code point and returns a boolean
flag.

% uni_is_x prototype

To get a list of all members of a multi-level table bit array, it can
be converted into a sorted range list.  The result ([[*ret]]) is always
allocated using [[malloc]].

% uni_multi_bit_to_range prototype

The inverse operation is also possible, but it consumes a large amount
of time and memory.  Internally, the range list is first converted
into a plain bit array, which is then converted using
[[uni_bits_to_multi]] above.  The return value is the multi-level table,
allocated using [[malloc]].  If [[ml_len]] is non-NULL, the storage
size of the table is returned as well.

% uni_rng_to_multi_bit prototype

The multi-level tables can store more information, as well.  The
lookup function merely returns a pointer to a byte.  This byte can be
a collection of bits, or it can be a byte value.  For the latter case,
a separate generic lookup function is provided.  This takes a
byte-sized default value, and returns this if the byte is out-of-range
or zero.  Otherwise it returns the stored value, minus one.  This way,
the default value is stored most efficiently (as a zero), and the rest
only require minor adjustment on return.

% uni_x8_of prototype

A function is provided to convert a [[uni_chrrng_dat8_t]] sorted range
list to an offsetted multi-level byte array.  However, no inverse
function is provided.  This operation is not really intended for end
users, but for the static table generator.  Note that while the
default value must be provided, it is not stored, but instead is used
to remove the default value from the table by converting it to zero.

% uni_rng_dat8_to_multi prototype

By multiplying the value size with the lookup index, larger values can
be stored as well.  Raw 16-bit or 32-bit values are directly
supported.  Both lookup and conversion from a [[uni_chrrng_dat16_t]]
type or a [[uni_chrrng_dat32_t]] type may be performed.  However, as
with the 8-bit values, the inverse conversion is not supported.

% uni_x16_of prototype
% uni_x32_of prototype
% uni_rng_dat16_to_multi prototype
% uni_rng_dat32_to_multi prototype

Another implementation is to store the numerator and denominator of a
rational number as a 32-bit value: 24 bits for a signed numerator, and
8 bits for an unsigned denominator.  A special lookup function is
provided to split out the numerator and denominator from the 32-bit
value; otherwise use the 32-bit functions above.

% uni_x_val prototype

For values where ranges are inappropriate, a function is provided to
convert a sorted code point array with 32-bit values (nominally
[[uni_cp_val_t]]) into a multi-level table.  No inverse function is
provided.  The [[uni_x32_of]] function above may be used for lookup.

% uni_cp_val_to_multi prototype

\subsection{Tries}

For simple (code point only) matching of multiple simultaneous
strings, a few support functions generate fast matchers.
\input{trie.tex} %%% doc

Several datatypes are also defined for more convenient declarations and
type casting (gcc at least considers double-constant pointers to
require implicit cast warnings when passing plain double pointers,
even though single constant pointers are implicitly compatible with
non-constant pointers).

\input{Trie Types.tex} % C

% uni_utf8_strings_to_trie prototype
% uni_utf16_strings_to_trie prototype
% uni_utf32_strings_to_trie prototype
% uni_chrrng_trie_next prototype
% uni_chrrng_trie_nstates prototype
% uni_free_chrrng_trie prototype
% uni_chrrng_trie_to_multi prototype
% uni_trie_next prototype
% uni_trie_nstates prototype
% uni_free_trie prototype

Here is a simple example of using the trie to scan UTF-8 data:

\input{trie-sample.tex} % C

\input{Aho-Corasick.tex} %%% doc

% uni_aho_corasick_prep prototype

Here is a simple example of using the Aho-Corasick modified trie to
scan UTF-8 data:

\input{Aho-Corasick-sample.tex} % C

\section{All Properties}

Information on all currently supported properties is provided by
several arrays.  They are intended to be used to scan for desired
properties, find property names and their aliases, and generate data
based on that.  However, there is no harm in using it if you intended
to support every single property, anyway.  Including a reference to
any of these symbols will likely defeat the static linking property of
this library, pulling every single property's data in.

\begin{itemize}
\item [[const uni_propdesc_t uni_propdesc[]]] is an array whose length
is given by the preprocessor symbol [[uni_propdesc_len]].  Each entry
describes a property, and points to its data.  See below for a
description of the elements' data structure.
\item [[uni_propdesc_valueof]] is an array whose length is given by
the preprocessor symbol [[uni_propdesc_valueof_len]].  It is a sorted
array of names for lookup of properties by name rather than sequence
number.  See Enumeration Properties (section \ref{sec:enum}) for
details on the structure and how to use it.
\item [[uni_propdesc_valueof_approx]] is an array whose length is
given by the preprocessor symbol [[uni_propdesc_valueof_len]].
Its usage is similar to the previous item; see Enumeration Properties
(section \ref{sec:enum}) for details.
\end{itemize}


The [[name]] field contains the canonical name and all known aliases
for the property name; see Enumeration Properties (section
\ref{sec:enum}) for details on the [[uni_alias_t]] structure and how
to use it..  The [[mtab]] and [[mtab_len]] fields describe the
multi-level table for the property.  The [[tab]] and [[tab_len]]
fields describe the sorted range table or sorted code point table for
the property.  See the sections below for details on what these tables
contain.  The [[type]] field determines which section describes the
tables:

\begin{itemize}
\item [[UNI_PROP_TYPE_BOOL]]: Boolean Properties
\item [[UNI_PROP_TYPE_ENUM]] and [[UNI_PROP_TYPE_ENUM2]]: Enumeration
Properties (8-bit and 16-bit, respectively); in addition, the name
translation tables are in [[nameof]], [[valueof]], [[valueof_approx]],
[[nameof_len]], and [[valueof_len]].  The default is in [[def]].
\item [[UNI_PROP_TYPE_NUM]]: Numeric Properties
\item [[UNI_PROP_TYPE_STR]]: String Properties; in addition, the
length of the string table is [[strs_len]], and the string table is in
either [[strs8]], [[strs16]] or [[strs32]], depending on the string
data type.
\item [[UNI_PROP_TYPE_DAT16]]: The FCD property (sec. \ref{FCD})
\end{itemize}

\input{[[uni_propdesc_t]].tex} % C

\section{Boolean Properties}

The supported boolean properties are listed in table
\ref{tab:boolprop}.  For each supported boolean property \emph{BP},
the following symbols are defined:

\begin{itemize}
\item [[const uni_chrrng_t uni_]]\emph{BP}[[_rng[]]] is a sorted range
table with entries for that property.
\item [[uni_]]\emph{BP}[[_rng_len]] is the number of entries in that
array (a preprocessor symbol).
\item [[const uint32_t uni_]]\emph{BP}[[_mtab[]]] is a multi-level
table for that property.
\item [[uni_]]\emph{BP}[[_mtab_len]] is the length of that table (a
preprocessor symbol), in case it is to be written out to a file.
\item [[int uni_is_]]\emph{BP}[[(uint32_t cp)]] is a boolean lookup
function for that property.  It is actually a preprocessor macro which
uses the multi-level table and [[uni_is_x]] internally.
\end{itemize}

For static linking, each property is in a separate object file, and
the sorted range table and multi-level table are stored separately, as
well.

\begin{table}
\begin{centering}
\begin{tabular}{lllllll}
AHex&ASSIGNED&Alpha&Bidi\_C&Bidi\_M&CE&CI\\
CWCF&CWCM&CWKCF&CWL&CWT&CWU&Cased\\
Comp\_Ex&DI&Dash&Dep&Dia&EBase&EComp\\
EMod&EPres&Emoji&Ext&ExtPict&Gr\_Base&Gr\_Ext\\
Hex&IDC&IDS&IDSB&IDST&Ideo&Join\_C\\
LOE&Lower&Math&NChar&NFD\_QC&NFKD\_QC&OAlpha\\
ODI&OGr\_Ext&OIDC&OIDS&OLower&OMath&OUpper\\
PCM&Pat\_Syn&Pat\_WS&QMark&RI&Radical&SD\\
STerm&Term&UIdeo&Upper&VS&WSpace&XIDC\\
XIDS\\
\end{tabular}
\end{centering}

~

{\small Note: NFD\_QC and NFKD\_QC are actually enumerated properties
with only two values: Y and N.}

\caption{\label{tab:boolprop}Boolean properties (Unicode canonical short name)}
\end{table}

\subsection{Bit Set Operations}

The following functions perform a generic bit operation (see table
\ref{tab:setop2} for a complete list of operations).

\input{op-table.tex} %%% doc

\caption{\label{tab:setop2}Set Operations}
\end{table}

Bit arrays consisting of a single unsigned integer can be operated on
using the following macro:

% UNI_BIT_SET_OP prototype

The [[op]] argument is intended to be one of the [[uni_setop_t]]
enumeration constants.

\input{[[uni_setop_t]].tex} % C

Arbitrary bit arrays of standard C unsigned integer types can be
operated on using the following functions.  The inputs are bit arrays
[[a]] and [[b]], which start at [[a_low]] and [[b_low]], respectively,
and contain [[a_len]] and [[b_len]] elements of the particular bit
size, respectively.  Although the endianness of each element of an
array is irrelevant, the array itself is assumed to be in
little-endian order if the arrays are not of identical size and
offset.  The result ([[res]]) is allocated from new memory using
[[malloc]] if the pointed-to pointer is NULL, or resized using
[[realloc]] if non-NULL and the reported [[res_max]] is too small. 
Its low bit is [[res_low]].  The number of relevant elements of the
bit size is returned in [[res_len]], and the actual number of elements
allocated is returned in [[res_max]].  All return pointers must be
non-[[NULL]].

% uni_setop_bit64 prototype
% uni_setop_bit32 prototype
% uni_setop_bit16 prototype
% uni_setop_bit8 prototype

Since sorted code point arrays are generally inefficient for storing
Unicode boolean properties (sets), no set operations are provided for
this data type.  However, the following sample code may be used if
needed:

\input{Perform set ops on cp array.tex} % C

For sorted code point range arrays, a single function is provided.  It
always allocates its results using [[malloc]].

% uni_chrrng_setop prototype

No set operations are provided for multi-level tables at this time.
Implementation would be insanely complex given the current
implementation, and should not be performed frequently, anyway.  The
only way to perform these operations right now is to convert the
multi-level tables to range arrays first, and then convert the result
back.  Alternately, the lookup function can simply be called for each
table, and the lookup results combined instead, using
[[UNI_BIT_SET_OP]].  For example:

\begin{quote}
\begin{verbatim}
res = UNI_BIT_SET_OP(uni_is_X(cp), op, uni_is_Y(cp)) & 1;
\end{verbatim}
\end{quote}

\section{\label{sec:enum}Enumerated Properties}

For lists of names, there are both the canonical names and aliases.
These are stored in an array of [[uni_alias_t]] structures.  Any
undefined names are [[NULL]].

\input{[[uni_alias_t]].tex} % C

For reverse lookups, the [[uni_valueof_t]] structure's [[val]] field
is the index of that name in the alias array.

\input{[[uni_valueof_t]].tex} % C

An array of [[uni_valueof_t]] structures can be searched or sorted
using the generic comparison function.

% uni_cmp_valueof prototype

If the name entry is stripped of separators and converted to
lower-case, then the Unicode approximate matching technique can be
applied.  This converts the input string to lower-case, strips out
separators (hyphens, underscores, and spaces), and, if the raw string
is not found, strips off a leading ``is''.

% uni_x_valueof_approx prototype

It performs its function by first stripping the name; manual stripping
can be done with the following function.  After stripping, the name
can be looked up directly using [[uni_cmp_valueof]] twice:  first
without further modification, and second after striping any leading
``is''.

% uni_enum_name_strip prototype

The list of supported enumeration properties is in table
\ref{tab:enumprop}.  For each enumerated property \emph{EP}, the
following symbols are defined:

\begin{itemize}
\item [[uni_]]\emph{EP}[[_t]] is a C enumeration type whose literals
correspond to the property's values.
\item For each canonical enumeration value name \emph{EN}, the symbol
[[UNI_]]\emph{EP}[[_]]\emph{EN} is defined, which is an enumeration
literal of type [[uni_]]\emph{EP}[[_t]].  Enumeration literals
corresponding to canonical aliases are defined as well, as long as they
do not contain a minus sign or a decimal point.
\item [[UNI_NUM_]]\emph{EP} is another enumeration literal of type
[[uni_]]\emph{EP}[[_t]], which corresponds to the number of unique
values.  Note that where canonical aliases are present, this number
is smaller than the actual number of enumeration symbols.  Also, where
numerical aliases exist, this may be larger than the number of symbols
present.
\item [[const uni_alias_t uni_]]\emph{EP}[[_nameof[]]] is an array of
names corresponding with the enumeration literals.  The index is
simply a valid enumeration literal value, or any positive integer less
than [[UNI_NUM_]]\emph{EP}.
\item [[const uni_valueof_t uni_]]\emph{EP}[[_valueof[]]] is a sorted
array of all unique names (including aliases) for this property; the
[[val]] field in each entry is the associated enumeration literal.
\item [[uni_valueof_t uni_]]\emph{EP}[[_valueof_approx[]]] is a sorted
array of all unique names (including aliases) for this property,
stripped of underscores and converted to lower-case for loose
matching.  Like the [[valueof]] array, the [[val]] field in each entry
is the associated enumeration literal.
\item [[uni_]]\emph{EP}[[_valueof_len]] is the length of the
[[valueof]] and [[valueof_approx]] arrays (a preprocessor symbol).
\item [[uni_]]\emph{EP}[[_t]] [[uni_]]\emph{EP}[[_lookup(const char *n)]]
is a lookup function which will strip underscores, hyphens, and spaces
from the lookup string, optionaly strip off any Is, and return the
enumeration constant matching that string.  This is actually a
preprocessor macro which uses the [[uni_x_valueof_approx]] function
with the [[valueof_approx]] array internally.
\item [[const uni_chrrng_dat8_t uni_]]\emph{EP}[[_rng[]]] (and
[[const uni_chrrng_dat16_t uni_]]\emph{EP}[[_rng[]]] for enumerations
with more than 255 values) is a sorted array of ranges.  The default
value is not provided here.
\item [[uni_]]\emph{EP}[[_rng_len]] is the length of the range
array (a preprocessor symbol).
\item [[uint32_t *uni_]]\emph{EP}[[_mtab]] is a multi-level table whose
byte-length (or 2-byte-length in the case of enumerations with more
than 255 values) values correspond to the enumeration literal values.
\item [[uni_]]\emph{EP}[[_mtab_len]] is the size of the multi-level
table (in words), in case the table is to transferred or written out.
\item [[int uni_]]\emph{EP}[[_of]] is a preprocessor macro which
returns the enumeration value for any code point.  The canonical
default is returned when the table has no entry.  Internally, the
multi-level table and [[uni_x8_of]] or [[uni_x16_of]] are used for
lookup.
\end{itemize}

For static linking, each property is in a separate object file.  In
addition, the range tables and multi-level tables are stored
separately.  The two plain name tables are stored together in order to
share constant name strings, but are stored separately from the range and
multi-level tables.  The approximate name table is stored separately,
as well.

\begin{table}
\begin{centering}
\begin{tabular}{lllll}
GCB&IDNA\_Status&ID\_Restrict\_Status&ID\_Restrict\_Type&InPC\\
InSc&NFC\_QC&NFKC\_QC&SB&WB\\
age&bc&blk&bpt&ccc\\
dt&ea&gc&hst&jg\\
jt&lb&nt&sc&vo\\
GCBp&WBp&lbp\\
\end{tabular}
\end{centering}

~

{\small Note: GCBp, WBp and lbp are derived from GCB, WB, and lb,
respectively.}\\
{\small Note: age and ccc are technically numeric properties.}\\
{\small Note: as of Unicode 9.0.0, the blk property is 16-bit.  As of
Unicode-14.0.0, all others are 8-bit.}

\caption{\label{tab:enumprop}Enumerated properties (Unicode canonical short name)}
\end{table}

The IDNA\_Status property will not be available if the IDNA data files
were not present when this library was compiled.  The
ID\_Restricted\_Status and ID\_Restricted\_Type fields will not be
available if the security data files were not available when this
library was compiled.  The ID\_Restrict\_Type enumeration literals
change minus signs to underscores; this is the only current
enumeration that supports enumeration names with minus signs.

The GCBp property is derived from GCB.  It is used as the default
property for [[uni_gcb_brk]].
\input{GCBp.tex} %%% doc

Similarly, the WBp property is derived from WB for use with
[[uni_word_brk]].
\input{WBp.tex} %%% doc

Similarly, the lbp property is derived from lb for use with
[[uni_line_brk]].
\input{lbp.tex} %%% doc

In addition, the gc property defines a symbol to support aliases which
cover more than one value.  For example, the enumeration literal
[[UNI_gc_Z]] is actually an alias for [[UNI_gc_Zl]], [[UNI_gc_Zp]],
and [[UNI_gc_Zs]].  The [[const uint64_t uni_gc_trans[]]] table takes
an enumeration literal as index, and returns a bit mask with all
appropriate base classes included with this literal set.  The base
classes themselves only have one bit set; to check if a class is a
base class, simply check if the bit is set for itself.

There are two additional support macros for the gc property.
[[uni_is_nl]] checks if a character is any valid newline-type character,
and [[uni_is_bs]] checks if a character is any valid backslash-type
character.  [[uni_is_bs]] is not necessary for text already processed
using NFKC\_CF.

% uni_is_nl prototype
% uni_is_bs prototype

\subsection{Boundary Detection}

Several of the enumerated properties are designed for boundary
detection.  These require an additional set of rules to be useful.
Those rules are provided in the string section, but require an
ICU-like regular expression engine to use.  Instead, the following
functions may be used.

A grapheme cluster generally corresponds to a single user-visible
character (glyph).  The following function can tell you if a grapheme
cluster boundary exists between any two characters; there is an
implicit boundary at the beginning and ending of a string.  For
improved performance, the return value can be passed in to [[prevret]]
when scanning a sequential string; that is, when [[next]] from the
previous call is passed in as [[prev]].  Prior versions of this
function required no state, but as of Unicode-9.0.0, state is required
to determine breaks between consecutive Regional Indicator Symbol
Letters (1F1E6..1F1FF) and between a Zero Width Joiner (200D) and an
Extended Pictographic code point ([[uni_is_ExtPict]]).  In the former
case, lack of state may lose a break point, and in the latter case,
lack of state may give a break point where none should be.

Unicode defines two types of grapheme cluster:  legacy and extended.
By default the extended form is detected.  Setting [[legacy]] to
non-zero selects the legacy form.

The return value is less than zero if there is no break between
[[prev]] and [[next]], and greater than zero otherwise.

% uni_gc_brk prototype

What constitutes a word in text is locale and context dependent.
Often detecting words requires a dictionary as well.  Unicode provides
two methods of detecting words, though:  a simple one based mostly on
the character being looked at, and a more complex one which detects
the boundaries between words.  As with grapheme clusters, there is an
implicit word boundary at the beginning and ending of a string.

The following function returns zero if a character is definitely not a
word character, more than zero if the character definitely is a word
character, and less than zero if it is a word character only if either
the previous character is also a word character or there is no
previous character.  Words are then defined as the longest contiguous
string of word characters.

No state is used or saved, so this function can be applied anywhere.
Manual backtracking is, however, required when a negative value is
returned.

% uni_is_simple_word prototype

Unicode also has an algorithm similar to the grapheme cluster
algorithm:  Given two characters, find if they belong to the same word
or not.  Non-word characters may be included in a word by this
definition, so additional filtering may be necessary.

Explicit backtracking is necessary.  It is not generally possible to
begin detecting a word in the middle of a string.  Instead, detection
must start either at the beginning of a string or at the last known
good word boundary.  If the last boundary was set by backtracking, it
is better to start one earlier.  The safest is to just always start
two words behind the current location.  At the start of a scan, pass
in zero for [[prevret]].  Otherwise, always pass in the previous
return value.

The return value is one of three different ranges:  less than $0$
indicates [[prev]] and [[next]] are in the same word; $0$ indicates
indicates [[prev]] and [[next]] are not in the same word, and grater
than $0$ indicates that the relationship is not yet known.

When the relationship is not yet known, the same value is returned
every time until the relationship is known.  For example, call the
first pair of characters [[prev1]] and [[next1]].  After the first
time some number $>0$ is returned, any subsequent number $>0$ indicates
that [[prev]] and [[next]] are definitely in the same word, but still
says nothing about [[prev1]] and [[next1]].  Once a value $<=0$ is
returned, that value indicates the relationship between [[next]] and
[[prev]], as well as the relationship between [[next1]] and [[prev1]].
If the end of string is reached before a value $<=0$ has been returned,
[[next1]] and [[prev1]] are not in the same word.

For example, given the string [[c1 c2 c3 c4 c5 c6]], and passing in
[[c1]] and [[c2]], and then [[c2]] and [[c3]], etc., to produce the
return values [[-5 8 8 8 0]] indicates that [[c1]] and [[c2]] are
in the same word ($-5$ indicates no break).  [[c3]] starts a new word
($8$ is deferred until $0$ is encontered, indicating a break),
containing [[c3]], [[c4]], and [[c5]] (subsequent $8$ values always
indicate no break).  [[c6]] once again starts a new word ($0$
indicates a break).  Note that there will never be more than one
backtrack position at once, and that the backtrack position is no
longer needed once it is resolved.  The next value $>0$ starts a new
backtracking session.

% uni_word_brk prototype

Currently, locale dependencies are hard-coded, and enabled by using
locale-dependent word break property tables ([[tab]]).

What constitutes a sentence in text is even more context and locale
dependent.  Unicode tries to provide a standard means to do so,
though.  Like the word boundary checker, the sentence boundary checker
determines if a sentence break exists between two characters, and
there is an implicit boundary at the beginning and ending of a string.

Explicit backtracking is necessary.  It is not generally possible to
begin detecting a sentence in the middle of a string.  Instead,
detection must start either at the beginning of a string or at the
last known good sentence boundary.  If the last boundary was set by
backtracking, it is better to start one earlier.  The safest is to
just always start two sentences behind the current location.  At the
start of a scan, pass in zero for [[prevret]].  Otherwise, always pass
in the previous return value.

As with words, values less than zero always indicate that [[prev]] and
[[next]] are in the same sentence, zero indicates that they are not,
and positive values start a backtracking group, which ends at the next
non-positive value or end of string.  However, backtracking is more
complicated, especially if CLDR suppressions are used.  Whether or not
the bacaktracking ends with a break does not determine whether or not
a break occurs aat the start, and it is possible for additional breaks
to occur while backtracking.  Indicators are supplied in the lower 8
bits, which should be interpreted as an [[int8_t]].  If the value
returned at the end of backtracking is a negative value $>-8$, no
breaks occur at all within the backtracking region.  All other return
values, as well as the end of string, indicate that the start of the
backtracking region breaks.  Additional break point information is
supplied by positive values $<8$ during backtracking:

\begin{itemize}
\item $1$ indicates a break
\item $2$ indicates a break if there is no $3$ before the next $2$ or
end of backtracking
\item $3$ indicates no break here or at the previous $2$
\end{itemize}

For example, given the string [[c1 c2 c3 c4 c5 c6 c7]], and passing in
[[c1]] and [[c2]], and then [[c2]] and [[c3]], etc., to produce the
return values [[-11 1 8 1 2 9]] indicates that [[c1]] and [[c2]] are
in the same sentence ($-11$ indicates no break).  [[c3]] starts a new
sentence ($1$ is deferred until end-of-string, indicating a break),
containing [[c3]] and [[c4]] ($8$ never breaks), and another new
senctence starting at [[c5]] ($1$ breaks the same as [[c2]]), and
another new sentence spanning [[c6]] through [[c7]] ($2$ breaks
because there was no $3$, and $9$ does not break).

Note that there will never be more than one start-of-backtrack
position at once, and that the backtrack position is no longer needed
once it is resolved.  Also, mid-backtrack breaks will never be present
if CLDR suppression matching is not enabled.

% uni_sentence_brk prototype

In addition to a property table override for locale support, this
function also supports a break suppression list.  This list must be a
trie with modified Aho-Corasick suffix links, as generated by the
following function:

% uni_utf8_suppressions prototype

Any text matching a suppression never breaks a sentence after the next
character.  No attempts are made to ensure matches start at a word
boundary, and no transformations are done to the list or to text being
scanned prior to matching, other than to skip formatting characters
(SB property [[FO]]) and compress multiple spaces (SB property [[SP]])
into one in the text being scanned.

\input{[[uni_sb_locale_t]].tex} % C

Unicode assumes in the above function that paragraphs are ended by
either explicit end-of-line characters or paragraph separators.  If
this is not what you want (e.g. you are using double-newlines to
terminate paragraphs), you will need to either adjust the SB property
table to convert characters with the [[CR]] and [[LF]] to e.g. [[SP]],
or alter the characters being passed into the function appropriately,
or both.

\subsection{Word Wrapping}

Given a paragraph of text, Unicode provides an algorithm to narrow
that text (if possible) by inserting soft line breaks within the
paragraph.  These soft line breaks may be placed at any potential
break point between two characters.  As mentioned above, Unicode
assumes that paragraphs are delimited by single hard end-of-line
characters or explicit paragraph separators, so if this algorithm is
being used to take text that uses double-newlines to separate
paragraphs, the explicit line separators within the paragraph must be
stripped, first.

Explicit backtracking is necessary.  It is not generally possible to
begin detecting a potential line break in the middle of a string.
Instead, detection must start either at the beginning of a string or
at the last known good break point.  If the last break point was set by
backtracking, it is better to start one earlier.  The safest is to
just always start two break points behind the current location.  At the
start of a scan, pass in zero for [[prevret]].  Otherwise, always pass
in the previous return value.

There are now 4 different results for each pair:  a mandatory (hard)
break, indicated by a zero, no break, indicated by a negative return
value, an optional (soft) break, indicated by a value greater than or
equal to $8$, and an unknown value, indicated by a return value of
$1$.  Any positive return values following the unknown value indicate
no break.  The next $0$ or value less than $-8$ indicates a mandatory
break or no break as usual, and also that the unknown (first) value is
break.  The next value less than $0$ but larger than $-8$ indicates
that both the current value and the unknown value are no break.

% uni_line_brk prototype

As with the above three functions, [[tab]] may be set for
locale-specific property table overrides.

\section{Numeric Properties}

Each numeric property \emph{NP} is supported by the following symbols:

\begin{itemize}
\item [[const uni_chrrng_dat32_t uni_]]\emph{NP}[[_rng[]]] is a sorted
range table whose value is a rational representation of the numeric
value.  You can cast a pointer to the [[dat]] field to a pointer to
[[uni_frac_t]] to retrieve the value.
\item [[int uni_]]\emph{NP}[[_rng_len]] is the length of the sorted
range table.
\item [[const uint32_t *uni_]]\emph{NP}[[_mtab[]]] is a multi-level
table with the same data.
\item [[uni_]]\emph{NP}[[_mtab_len]] is the length of that table (a
preprocessor symbol), in case it is to be written out to a file.
\item [[void uni_]]\emph{NP}[[_of(uint32_t cp, int32_t *num, uint8_t *denom)]]
is a simple wrapper lookup function that uses the multi-level table to
retrieve the numerator and denominator.
\end{itemize}

The age property is officially numeric, but not supported as such. 
Instead, it is only supported as an enumerated property.  The ccc
property is similarly only enumerated, but the value of the
enumeration literal is equal to the numeric value of the property as
well.

The slc, suc, stc, scf, bmg and bpb properties are technically string
properties, not numeric properties.  However, since they always map to
exactly one character (or nothing), they can be considered numeric
properties whose value is the translated code point (or zero if nothing).

In addition, the value for the cjkRSUnicode field is interpreted as a
series of dotted numbers, with optional exclamation points.  This is
all encoded into the numerator and denominator fields, and can be
extracted using decoder macros.  The [[l]] parameter is set to the
number before the decimal point, and the [[r]] is set to the number
after the decimal point.  If both are zero, the number is not present.
If [[r]] is negative, then an exclamation point precedes the decimal
point, and the right side is the absolute value of [[r]].  When there
is more than one value, the [[val2]] macro will return a non-zero value.

% uni_cjkRSUnicode_val1 prototype
% uni_cjkRSUnicode_val2 prototype

\begin{table}
\begin{centering}
\begin{tabular}{lllll}
nv&cjkRSUnicode&slc&suc&EqUIdeo\\
stc&scf&bmg&bpb\\
\end{tabular}
\end{centering}

~

{\small Note: slc, suc, stc, scf, bmg and bpb are technically string
properties.}

\caption{\label{tab:numprop}Numeric properties (Unicode canonical short name)}
\end{table}

\section{String Properties}

Properties with string values are stored in two parts: a constant
string table, and tables which have offsets and lengths of strings
within that table.  Multiple properties may use the same string table.
The offset and length tables are either sorted code point arrays of
type [[uni_str_arr_t]], or multi-level tables of 32-bit values, which
are interpreted as (and can be cast into) [[uni_str_ptr_t]].

\input{[[uni_str_arr_t]].tex} % C
\input{[[uni_str_ptr_t]].tex} % C

Note that the [[uni_str_arr_t]] data structure can also be cast to the
[[uni_cp_val_t]] type, whose [[val]] element can then be cast to
the [[uni_str_ptr_t]] type.

Searching the multi-level table for a string can be done generically
using the following function; it is used internally by the
per-property lookup functions.  The returned pointer is always valid;
a lookup failure is detected by checking the offset and length.  If
both are zero, then a lookup failure occurred.  Zero-length strings
have non-zero offsets to avoid looking like a failure.

% uni_x_str_of prototype

The supported string properties are listed in table
\ref{tab:stringprop}.  Unless otherwise stated in the subsequent
sections, for each string property \emph{SP}, the following symbols
are defined:

\begin{itemize}
\item [[const uint32_t uni_]]\emph{SP}[[_strs[]]] is a string table
containing all string values.
\item [[uni_]]\emph{SP}[[_strs_len]] is the length of the string table (a
preprocessor symbol), in case it needs to be written out to a file.
\item [[const uni_str_arr_t uni_]]\emph{SP}[[_arr[]]] is a sorted list
of code points with their associated string offsets and lengths.
\item [[uni_]]\emph{SP}[[_arr_len]] is the length of the above array
(a preprocessor symbol).
\item [[const uint32_t uni_]]\emph{SP}[[_mtab[]]] is a multi-level
table of 32-bit [[uni_str_ptr_t]] values.
\item [[uni_]]\emph{SP}[[_mtab_len]] is the length of that table (a
preprocessor symbol), in case it is to be written out to a file.
\item [[const uni_str_ptr_t *uni_]]\emph{SP}[[_of(uint32_t cp)]]
is a simple wrapper lookup function that uses the multi-level table to
retrieve string information as an offset and length in the string
table.  A zero-length string is distinguished from a missed lookup by
the offset field:  only missed lookups have both offset and length of
zero.
\end{itemize}

For static linking, the string table, sorted code point table, and
multi-level table are all in separate object files.

\begin{table}
\begin{centering}
\begin{tabular}{llll}
dm&\emph{canon\_decomp}&\emph{compat\_decomp}&\emph{canon\_comp}\\
lc&uc&tc&cf\\
\emph{NFD\_CF}&\emph{NFKD\_CF}&NFKC\_CF&cjkTraditionalVariant\\
cjkSimplifiedVariant&cjkCompatibilityVariant&IDNA\_Mapping&\emph{DUCET}\\
\emph{DUCET\_CLDR}&\emph{rev\_DUCET}&\emph{rev\_DUCET\_CLDR}&scx\\
JSN&na&Name\_Alias&\emph{na\_seq}\\
\emph{na\_xml}&\emph{na\_xml\_seq}&&\\
\end{tabular}
\end{centering}

~

\small{Property names in italics are non-standard properties.}

\small{na, Name\_Alias, and scx are technically ``other'' properties.}

\caption{\label{tab:stringprop}String Properties (Unicode canonical short name)}
\end{table}

\subsection{Names}

The name properties are technically ``other'' properties, but are
treated by the library as string properties with auxiliary tables and
support functions.

To look up the name associated with a sequence of code points, use the
following functions.  [[uni_cp_to_name]] is for looking up standard
Unicode names, and [[uni_cp_to_xml_name]] is for looking up XML entity
names.  Note that [[uni_cp_to_name]] also generates Unicode labels for
unnamed code points, with less-than at the start and greater-than at
the end (e.g. <control-0000>).  If a lookup is meant to not support
labels, it is up to the caller to filter out these results.  In some
cases, valid names do exist for code points with just labels, in the
form of aliases.

They take a code point sequence [[cp]] of length [[len]].  The name
matching the longest initial sequence is returned, along with the
length of that sequence if [[seq_len]] is not [[NULL]]. If, given more
input, a different name may result, [[*seq_len]] is the negative
length of the actual returned sequence.  If [[alias]] is [[NULL]] or
[[*alias]] is zero, the first known name for the code point is
returned.  If [[*alias]] is zero, the returned [[*alias]] will be
largest alias index.  Passing in a non-zero [[*alias]] will retrieve
that alias, and return the next lower index in [[*alias]].  An
out-of-range [[*alias]] returns an empty string, and sets [[*alias]]
to [[-1]].

The returned name is placed in [[*buf]].
\input{buffer return.tex} %%% doc

% uni_cp_to_name prototype
% uni_cp_to_xml_name prototype

Looking up a code point given its name is not meant to be done
directly using the library.  Instead, the tables are meant to be
dumped in a format most useful for the particular application.
However, a few lookup functions are provided below.  These return a
code point or sequence ID given a name and the name's length (negative
to use [[strlen]]).  Both functions require normalized 8-bit input.
Additionally, the [[uni_name_to_cp]] function requires case-folded
input.  The return value is negative on lookup failure.

Note that these functions also accept Unicode labels for unnamed code
points, with less-than at the start and greater-than at the end (e.g.
<control-0000> or <surrogate-D816>).  If a lookup is meant to not
support labels, it is up to the caller to filter out these inputs.

A sequence ID is a code point larger than [[UNI_MAX_CP]].  To convert
it to a string, use the [[na_seq_id]] or [[na_xml_seq_id]] tables,
depending on the function being used.  For each of these two ID arrays
\emph{I}, two arrays are defined:

\begin{itemize}
\item [[const uni_str_ptr_t *uni_]]\emph{I}[[_arr[]]] is the main
lookup table.  Its index is the code point minus $([[UNI_MAX_CP]] + 1)$.
Its length is the preprocessor symbol [[uni_]]\emph{I}[[_arr_len]] in
case the table is to be written to a file.  Its contents are offsets
and lengths for the assocaited string table.
\item [[const uint16_t *uni_]]\emph{I}[[_strs[]]] is the associated
string table, of length [[uni_]]\emph{I}[[_strs_len]].  The string
value is the full UTF-16-encoded sequence.
\end{itemize}

% uni_name_to_cp prototype
% uni_xml_name_to_cp prototype

Both of the above two functions use binary searching for the primary
name lookup.  If [[cmph]] support is enabled, two additional functions
are provided; they perform the same function, except that they use
[[cmph]] hash tables instead of binary searching.

The [[cmph]] functions provide and use hash tables.  These are 
[[const cmph_t uni_rev_na_mph]] and [[const cmph_t uni_rev_na_xml_mph]].
As mentioned earlier, these require linking with the [[cmph]] library,
and may need parameter adjustment to find the include files.  The are
also in a separate library, so using this requires linking with
[[-luni_cmph]].

In addition to the hash tables, some hash algortihms require
translation tables to turn the hash result into an array index in the
reverse name table.  The are
[[const uint32_t uni_rev_na_mph_xlate[]]] and
[[const uint32_t un_rev_na_xml_mph_xlate[]]].  The size of those
tables is normally the same as the associated reverse name lookup
table, but some algorithms produce sparse hash functions, so they
require the explicit translation table size:
[[const uint32_t uni_rev_na_max_mph_res]] and
[[const uint32_t uni_rev_na_xml_max_mph_res]], respectively.  For
sparse tables, unused entries are always [[~0]].  There is currently
no way to query the library to determine if these are, in fact,
necessary, but the current fastest lookup algorithm always needs both,
and using them does not affect the results (only the timing), so it is
probably not a bad idea to assume that both are necessary.

% uni_cmph_name_to_cp prototype
% uni_cmph_xml_name_to_cp prototype

The na property's string value is the name for that code point, with
words encoded to reduce space usage.  The strings are 32-bit words,
but may be viewed as a 32-bit-aligned 8-bit character string padded
with zeroes. That is, the string pointer must be cast to an 8-bit
character type after adding the offset, and the string length must be
multiplied by four and all trailing zeroes removed to find the actual
string length.

The Name\_Alias property's string value is a concatenation of all
aliases for that code point, each with words encoded for reduced space
usage, and each name preceded by its length minus one in a single
byte.  Named sequences, which are not really a property, are provided
by the pseudo-property na\_seq.  The index code point for this
property is the first character of a named sequence.  The value is a
concatenation of all named sequences starting with this name.  Each
name is followed by an optional zero to pad the total entry (with
length byte) to an even number, followed by a length byte.  The low 6
bits of the length byte are the length of the name (minus one), and
the upper 2 bits are the length of the remainder of the sequence that
starts with the index code point.  The sequence extension length is
the number of 16-bit words minus one; the sequence extension is UTF-16
encoded.  So, to find an entry, read the last byte as a length byte.
Skip the preceding byte if it is a zero.  The name is then all
characters preceding that, up to the given name length.  Preceding
that is the sequence extension, with the given number of words (but
possibly fewer characters due to the encoding).  Note that although
the list must be read in reverse, the contents are not reversed.  Like
the na and Name\_Alias properties, the names in na\_seq have words
encoded to reduce space usage.

The rev\_na property is the reverse (name-to-code-point) lookup
property.  It combines the contents of the above three tables,
converts all names to lower-case, and strips them of spaces and
hyphens.  The array is sorted by the stripped name, and contains
multiple entries with the same code point (but no duplicated string
values).  No multi-level table lookup support is provided.  It is
intended for binary searching or linear traversal to print the names
for a different lookup mechanism.  Like the na tables, it uses 32-bit
string tables and encodes words for more compact storage.

The word encodings replace words with two-character sequences with the
first character's high bit set.  These two form a 15-bit index into
the associated word array.  There are two word arrays:  na\_words for
the code-point-to-name arrays, and rev\_na\_words for the rev\_na
array.  For each word array \emph{W}, the following are defined:

\begin{itemize}
\item [[const uni_str_ptr_t * uni_]]\emph{W}[[_arr[]]] is the main array;
it provides offsets and lengths of strings in the strings array.  Its
length is [[uni_]]\emph{W}[[_arr_len]].
\item [[const uint8_t uni_]]\emph{W}[[_strs[]]] is the associated
string array, of length [[uni_]]\emph{W}[[_strs_len]].  The
two-character index should be replaced by this string.
\end{itemize}

Several functions are provided to support this encoding.  First, a set
of functions simply expands word pointers to their associated string.
The generic function takes a set of word arrays, and the specific
functions already use the correct arrays.

% uni_naX_expand_words prototype
% uni_na_expand_words prototype
% uni_rev_na_expand_words prototype

Direct comparison between strings which may or may not contain word
pointers may be done using the following functions.  Since this is
commonly for reverse lookup, only that specific case is implemented.
The optional [[*eqlen]] parameter is set to the last character that
matched if the strings mismatch.

% uni_cmp_word_str prototype
% uni_cmp_rev_na_str prototype

Likewise, computing the expanded length of a string can be done using
one of the following functions.

% uni_word_strlen prototype
% uni_rev_na_strlen prototype

The above tables are not sufficient to fully process Unicode names.
First, some names in the rev\_na table map to multiple code points.
These code points require a hyphen before a particular character in
the stripped name.  The returned code point can be checked for this
condition by the following preprocessor macro (which evaluates is
arguments more than once):

% uni_rev_na_needs_hyphen prototype

If the above macro returns true, the
[[const uni_chrrng_dat32_t uni_na_hyphen_rng[]]] table (of length
[[uni_na_hyphen_rng_len]], but not sorted) may be consulted to
determine what to return instead if a hyphen is present, and where
that hyphen should be.  The [[low]] field contains the return value
from the rev\_na table.  The [[dat32]] field's lower 8 bits are an
index into the stripped name.  If a hyphen immediately preceded the
character at that position before stripping, then the return value
should be transformed to the [[dat32]] field shifted right 8 bits.

In addition to the hyphen problem, the above tables are missing
synthetic names.  Some ranges of code points are assigned
algorithmically generated names.  To find the name of a code point in
one of these ranges, a few tables and functions are provided.  The
na\_rng and rev\_na\_rng tables (string properties without multi-level
tables) consist of pairs of entries; the first in each pair maps the
prefix of a synthetic name to the first code point in its range, and
the second maps to the last code point in its range.  Both use word
encoding for the names, using the appropriate word table for its
function.  The only other differences are the order (forward by code
point, and reverse by stripped name) and the fact that the reverse
table uses stripped names.  Looking up in the forward table cannot be
done using [[uni_cmp_cp]], so the following function is provided.  It
always returns the lower end of the range, or [[NULL]] if not found.

% uni_na_rng_of prototype

These tables are not easy to use, and say nothing about how to
synthesize the names.  To synthesize a name, use the
[[uni_cp_to_name]] function above, with a code point that lies within
a synthetic range.  To look up a synthesized name, the following
function is provided.  While the main name table is better served with
a different encoding, it is recommended that this function be used for
synthetic code points, either after a main name lookup failure or to
test before even doing the main name lookup.

% uni_name_rng_to_cp prototype

The above function also takes advantage of the rev\_JSN
pseudo-property, which is the JSN table (without multi-level table
support), sorted by type (hst) first, and then by name.  Additionally,
a byte is inserted in front of all non-empty strings to indicate the
hst (but not the hst enumeration value itself):  0 for L, 1 for V, and
2 for T.

\subsection{Normalization}

Normalization data includes the decomposition, composition, full case
folding, and character combining class information.  This is all
combined into convenient functions for normalization.  Each
Unicode-defined normalization method has its own functions:  canonical
decomposition (NFD), canonical composition (NFC), compatibility
decomposition (NFKD), compatibility composition (NFKC), and
compatibility decomposition with case folding (NFKC\_CF).

There are three functions for each normalization type.
\input{UTF buffer return.tex} %%% doc

Overlap of the input buffer ([[ibuf]]) and the output buffer
([[*buf]]) is not generally supported; if the input is overwritten
during conversion, the output may be affected.  However, if the input
and output buffer are identical, [[NULL]] may be specified for
[[ibuf]], which indicates that the output position (including offset,
if specified) is also the location of the input.  This is usually done
by allocating new memory and copying the input buffer first.

% uni_NFD32 prototype
% uni_NFD16 prototype
% uni_NFD8 prototype
% uni_NFC32 prototype
% uni_NFC16 prototype
% uni_NFC8 prototype
% uni_NFKD32 prototype
% uni_NFKD16 prototype
% uni_NFKD8 prototype
% uni_NFKC32 prototype
% uni_NFKC16 prototype
% uni_NFKC8 prototype
% uni_NFKC_CF32 prototype
% uni_NFKC_CF16 prototype
% uni_NFKC_CF8 prototype

Most normalization procedures above perform canonical ordering.  This
can also be done outside of those functions.  The following function
takes a buffer ([[buf]]) of a given length ([[ilen]]), and sorts the
characters in-place.  It will never need to expand the length of the
array.  However, it may need more characters to fully sort the input.
The [[last]] parameter indicates that the function must assume there
are no more characters available. Otherwise, it will return the full
length of the input if no more characters are needed, or less than the
full length if more are needed.  If more are needed, the return value
indicates the number of characters which have already been sorted;
these can safely be removed before the next pass.

% uni_Canon_Order32 prototype
% uni_Canon_Order16 prototype
% uni_Canon_Order8 prototype

\label{FCD}In some cases, a chain of transformations begins with NFD
normalization.  However, subsequent steps may succeed even without
fully normalized text.  In particular, if the next step is canonically
closed, it can perform the normalization as part of its
transformation.  For this case, the data only needs to be in what is
called Fast C or D form, or FCD.  A pseudo-property is provided to
perform this test.  The following function performs this test on a
string, returning true if no normalization needs to be performed:

% uni_is_FCD32 prototype
% uni_is_FCD16 prototype
% uni_is_FCD8 prototype

The following symbols are defined for accessing the raw FCD data:

\begin{itemize}
\item [[const uni_chrrng_dat16_t uni_FCD_rng[]]] is a sorted range
table whose 16-bit unsigned data is a ccc pair for the FCD test.
\item [[uni_FCD_rng_len]] is the length of that table (a preprocessor
symbol).
\item [[const uint32_t uni_FCD_mtab[]]] is a multi-level table with
the same data.
\item [[uni_FCD_mtab_len]] is the length of that table, in case it is
to be written out to a file.
\item [[uint16_t uni_FCD_of(uint32_t cp)]] is a function
which returns the 16-bit ccc pair for the given [[cp]].  Internally,
this is a preprocessor macro which uses the multi-level table and
[[uni_x_dat16]].
\end{itemize}

Each 16-bit word has the ccc of the first character of any potential
canonical decomposition as its high byte, and the ccc of the last
character of any potential canonical decomposition as its low byte.
See Unicode Technical Note \#5 for details on the algorithm.  Also see
that technical note for a brief description of what it means for a
transformation to be canonically closed.  The only property currently
claiming to meet this requirement is the DUCET.%
\footnote{However, the supplied CLDR tests have several cases where
passing FCD is not enough to produce a valid sort key.  This may be a
bug in either the UCA or the UCD.}
The UCA is not always guaranteed to meet this requirement; if the
literal text is to be appended to the key, then the literal text must
be in NFD form first.

\subsection{Decomposition}

The raw decomposition data is not meant to be used directly.  Instead,
the following wrapper functions are provided.  They perform just the
decomposition step for the decomposition normal forms; canonical
ordering needs to be performed manually afterwards.  Like the full
normalization functions, they do not support input and output overlap,
but do support [[NULL]] for [[ibuf]] to indicate that input and output
locations are identical.  The return value is the length after
transformation.

% uni_NFD_dec32 prototype
% uni_NFD_dec16 prototype
% uni_NFD_dec8 prototype
% uni_NFKD_dec32 prototype
% uni_NFKD_dec16 prototype
% uni_NFKD_dec8 prototype

The NFKC\_CF property does not describe the full NFKC\_CF
transformation.  Instead, it provides only the steps up to and
including decomposition.  The full transformation requires canonical
ordering and composition, which are performed by the [[uni_NFKC_CF]]
functions listed in the previous section.  For just the case folding
and decomposition steps, the following functions are similar to the
[[_dec]] functions above.

% uni_NFKC_Casefold32 prototype
% uni_NFKC_Casefold16 prototype
% uni_NFKC_Casefold8 prototype

The functions listed so far in this section use a low-level routine
which takes the raw decomposition data as additional parameters to
perform their function.

% uni_do_any_dec32 prototype
% uni_do_any_dec16 prototype
% uni_do_any_dec8 prototype

Raw decomposition data (except for NFKC\_CF, which is a normal string
property) is stored in [[uni_dm_strs]].  This includes the raw dm
property, as well as two pseudo properties.  The canon\_decomp
property contains only full (recursive) canonical decompositions, and
the compat\_decomp property contains only full (recursive)
compatibility decompositions when they differ from the canon\_decomp
result.  The raw dm property is not recursively/fully decomposed.  The
length field cannot be used raw; a flag is ored into the upper bit.
This flag eliminates the need for a dt property lookup to determine if
an entry is canonical or compatibility: compatibilty decompositions
have this flag set.

Lookup with the multi-level tables is simplified with some
preprocessor macros, which return the associated string's offset and
length.  For the compatibility lookups, an optional pointer to a flag
([[compat]]) can be used to check if the returned decomposition was
not canonical.  Unlike the other string properties, the
[[uni_]]\emph{SP}[[_of]] function is not supported for these
properties, in order to force usage of these functions.

% uni_find_canon_decomp prototype
% uni_find_compat_decomp prototype
% uni_find_dm prototype

These macros use the internal-use only [[uni_x_dec]] function to do
their work.  For Hangul syllable strings, the [[h]] flag determines
behavior.  If positive, it acts as the [[full]] flag below, and the
returned offset is always $-1$, but the length is correct.  If
negative, the returned offset is always 1, and the length is zero.

% uni_x_dec prototype

For the Hangul syllables, a separate function must be use to
decompose.  A Hangul syllable string in the form LVT or LV can be
singularly or completely decomposed (dependent on the [[full]] flag)
using the following function.  The function will return nothing (-1)
if the input is not a decomposable Hangul syllable string, so it is
safe to call whenver no decompositionmapping is found in the tables.
Otherwise, it fills in the return buffer ([[res]]) and returns the
number of filled-in values (always 2 or 3).  The return buffer
must contain space for at least 2 characters if the [[full]] flag is
not set, or 3 if it is set.

% uni_hangul_syllable_decomp prototype

To make this easier, a wrapper macro is provided.  This assumes that
at least [[len]] words are available in the passed-in buffer
([[buf]]), and copies out the result.  This is meant to be called
after [[uni_find_]]\emph{xxx}, using its results combined with the
[[uni_hangul_syllable_decomp]] results.

% uni_get_decomp prototype

\subsection{Composition}

The raw composition data is not meant to be used directly.  Instead,
the following wrapper functions are provided.  They perform just the
composition step for the composition normal forms; canonical
decomposition needs to be performed manually beforehand.  Like the
full normalization functions, they modify buffers in-place.  However,
composition never extends the length of the input, so no extra space
needs to be available in the buffer.  However, more characters may be
needed to determine if composition is possible.  if [[nok]] is NULL,
the input buffer is assumed to be complete.  Otherwise, if the
returned [[*nok]] is less than the returned (updated) buffer length,
then the first [[*nok]] characters have been composed successfully,
but the remainder needs additional input before completion.  In any
case, the return value is the updated length of the buffer after
composition.

% uni_NFC_comp32 prototype
% uni_NFC_comp16 prototype
% uni_NFC_comp8 prototype

The raw composition tables, derived from the dm and Comp\_Ex
properties, are designed for multi-step lookup as well. In addition to
having a gap for the Hangul syllable strings, composition takes two
code points for input rather than just one.  The latter problem is
solved by looking up a ``string'' using the first input, which is
actually a sub-table that can be searched using the second input.  The
format of the sub-table is like a [[uni_cp_val_t]] structure; the
value is the composed character.  These sub-tables are stored in the
string table [[uni_canon_comp_strs]] (i.e., \emph{ST} is canon\_comp).
Only one pseudo-property is defined for this: canon\_comp.

The following macro takes the first element of the composition pair
and returns the offset and length of the sub-table by using
[[uni_x_dec]] and the multi-level table.

% uni_find_canon_comp prototype

To look up values in the sub-table, a separate search function is
provided.  Again, a hand-coded binary search is used instead of
[[bsearch]] for speed.

% uni_lookup_compent prototype

To look up Hangul syllable compositions, another separate function is
provided.  It returns zero if there is no valid composition; otherwise
it returns the result of composition.

% uni_hangul_syllable_comp prototype

Combining the above two steps can be done using a wrapper macro.  Thus
the preferred method of looking up compositions is to call
[[uni_find_canon_comp]] for a code point, and then to use the results
for all candidates for the second input to composition with
[[uni_canon_comp]].

% uni_canon_comp prototype

\subsection{Collation Tables}

The Default Unicode Collation Element Table is available as the DUCET
pseudo-property.  The CLDR version of the DUCET is available as the
DUCET\_CLDR pseudo property.  Both of these properties also define the
variable uni\_\emph{property}\_var\_top, which is the default variable
top parameter for that table.  As with the normalization tables, raw
lookup is not encouraged.

To perform a straight string comparison without fully generating keys,
use the following functions.  The [[opts]] parameter may be NULL, or
it may be a set of collation options.  Each string's length may be
either a positive number, indicating absolute length in words, or a
negative number, indicating that the string is zero-terminated.

% uni_uca_strcmp32 prototype
% uni_uca_strcmp16 prototype
% uni_uca_strcmp8 prototype

Collation options are set by the following structure.  The defaults
are selected by zeroing the options structure out first.

\input{[[uni_uca_opts_t]].tex} % C

The [[tab]] and [[strs]] options must be set together, and specify
the static data for the DUCET.  By default, the standard Unicode DUCET
is used.  Normally, the [[var_top]] option would be set at the same
time; by default it is the standard Unicode DUCET's variable top.

The [[max_level]] option is the maximum significant key level; by
default, it is 3.  The second level, if enabled, may be returned in
reverse order using the [[reverse_lev2]] flag. If it is larger than 4,
[[do_literal]] is implied. The [[do_literal]] option, if non-zero,
appends the literal input string to the key.  The UCA requires that
the string be in NFD form for this to work.

Keys whose primary component is non-zero and equal to or less than
[[var_top]] are variable weight keys.  The behavior of variable
weights is controlled by the [[var_mode]] parameter, whose values
correspond to the standard methods.

\input{UCA variable modes.tex} % C

There is no function to directly generate something like the UCA sort
key.  Users wishing to store these keys are encouraged to encode them
in a more efficient manner for comparison.  However, the raw keys may
be looked up and compared as if they were UCA sort keys.  To retrieve
the sort key for a string, use the following functions.  The string
length is negative for zero-terminated strings, or positive for
strings with an absolute length.  The function returns a sort key
allocated with [[malloc]], whose length is returned in [[*rlen]].  If
[[llen]] is non-[[NULL]], it points to a 4-element array into which to
place the number of non-zero elements at each level of the key.
Remember that the string input should be at least FCD form.%
\footnote{At least one test case for the CLDR DUCET fails when in FCD
form, but not NFD form, so it's safest to just use NFD form.}

% uni_str_uca_raw32 prototype
% uni_str_uca_raw16 prototype
% uni_str_uca_raw8 prototype

To compare the keys returned by the above function, use the following
functions.  Pass in the same options as passed into the key creation
routine; passing different options for different keys or different
options for keys and comparison will result in undefined behavior.
This combination of functions respects all the same options
as the string comparison routines, except for the [[do_literal]]
option.  To obtain the effect of this option, use something equivalent
to [[uni_int_utf32_strcmp]] on the normalized strings if
[[uni_uca_raw_cmp]] returns zero.

% uni_uca_raw_cmp prototype

Alternately, the keys can be directly examined.  Each key element is
returned as one or two 32-bit integers.  The first contains a
combination of the first three levels using shifts and masks.  If
[[max_level]] was larger than three when the key was created, a second
word for each key element is the fourth level.

\input{DUCET lookup format defs.tex} % C

Raw DUCET lookups for characters without the variable weight and level
processing can be performed using the following function.  The
[[lev123]] parameter is formatted just like the first word of each
pair returned by [[uni_str_uca_raw]].  Since there are DUCET entries
for strings of more than one character, the lookup requires string
input. This is done one character at a time, using a state structure
to keep track of progress.  Pass in the next character in [[c]],
unless at end of input, in which case pass in [[UNI_UCA_LOOKUP_END]].
The return value is one of [[UNI_UCA_LOOKUP_AGAIN]],
[[UNI_UCA_LOOKUP_OK]], or [[UNI_UCA_LOOKUP_NONE]].  If the return code
was [[NONE]], [[lev123]] and [[lev4]] must be ignored.  Otherwise,
they contain the next DUCET sort key element.  If the passed-in
character was not [[END]], or the return value was [[AGAIN]], the
function must always be called again to finish the lookup.  The [[c]]
parameter should be the next character (or [[END]]) for the next call,
unless the return value was [[AGAIN]], in which case [[c]] is ignored.

The [[state]] parameter is a pointer to a pointer to an opaque state
structure.  It must initially point to [[NULL]].  After the last
lookup returns ([[c]] is [[UNI_UCA_LOOKUP_END]] and the return value
was not [[UNI_UCA_LOOKUP_AGAIN]]), the structure is automatically freed
and reset to [[NULL]].  It should never be freed manually.

Note that the [[SHIFT_TRIMMED]] variable weight processing method can
be implemented more cheaply in an outer function by using [[SHIFTED]]
instead, and trimming after the entire key has been built.

% uni_uca_lookup prototype

The initial state can also be used to set options.  Currently, the
[[reverse_lev2]] and [[do_literal]] options are not supported; use the
higher-level functions above, instead.  The options may only be set
once; any further attempts will be ignored.

% uni_uca_lookup_opts prototype

While [[uni_uca_lookup]] is meant for mostly raw lookup, it does
support options, and automatically adds DUCET entries which were
suppressed in the raw data.  For truly raw lookup, the following
function is provided.  It returns the best key out of the table
specified by [[tab]] and [[strs]] (defaults to the standard DUCET if
[[NULL]] is passed in) for the passed-in string ([[buf]],
[[buf_len]]).  It returns a flag which is true if passing in a longer
buffer might give different results.  Other values may be returned in
the remaining parameters, if non-NULL.  [[ce_len]] contains the number
of characters actually used to generate the key. [[key]] is
[[key_len]] long words of raw key information.  If the lookup fails,
[[key]] is set to [[NULL]], and [[key_len]] is set to zero.

The raw data pointed to by [[key]] is similar to, but not quite the
same as the raw data returned by the functions above.  The first word
contains the first three levels, with nothing masked out.  Its low 2
bits indicate the value of level 4: 0 means 0, 1 means that it is the
first code point of [[buf]], and 2 means that it is the following word
in [[key]], shifted left 2 bits.

A lookup failure indicates that the first character in [[buf]] is not
in the table.  This can mean one of three things: either it is illegal
(such as a surrogate), or it is not in NFD form (in which case the
character should be normalized and passed in again), or it has a
synthetically generated value.  The [[uni_uca_synth_key]] can be used
to generate the correct synthetic value in the latter case; it is
passed in a 2-word [[key]] buffer into which to place the synthetic
levels 1-3 for [[c]].  Level 4 is officially [[c]] for the first, and
0 for the second, but it normally doesn't matter, and in fact the
routines above and older versions of the DUCET use [[c]] always, and
recent versions of the DUCET use 0 for both.

Note that there are some strings which this routine may identify as a
collation element, but which are not.  For example, there are entries
which are merely the reordering of the keys for multiple collation
elements.  There are also some strings which could be considered
collation elements, but are not, because there are fully ignorable
characters in the string.  The former may be mitigated by examining
the keys, and only accepting keys of a known single-element format,
but no assistance is provided by this library for that.  The latter
can be mitigated by simply ignoring returned zeroes.

One type of long collation element handled by [[uni_uca_lookup]] which
[[uni_DUCET_lookup]] can't is reordering of grapheme cluster elements
(S2.1).  This may be fixed in a future release.

% uni_DUCET_lookup prototype
% uni_uca_synth_key prototype

Using the raw tables is possible as well.  They are string arrays
indexed on the first character of the collation element, and the data
format is as described above, but each key is preceded by zero or more
``index extensions'' --- additional code points to append to the index
for the actual index of this key, each shifted left two bits and with
both low bits set.  The actual array element string consists of every
key with the same starting character, ordered by the index extension
string.

It may also be useful to do reverse lookups: given the sort of data
returned by the above functions, find a set of collation elements
which match.  Normally, this would be done by giving a collation
element, and finding its collation class set.  This is performed by
the following functions.  They take a collation element ([[ch]],
[[chlen]]), perform a lookup using [[opts]], and then a reverse lookup
(using the [[rev_DUCET]] and [[rev_strs]] parameters, or, if NULL, the
standard UCA DUCET), and return a string containing the results.
\input{UTF buffer return.tex} %%% doc

If the result string is of length zero, the input string is not a
collation element. Otherwise, the result string has collation elements
in order of increasing length, starting with one.  Each length group
consists of a count word (possibly zero), followed by that many
strings of the group's length (in code points, not words); if this
count is more than the maximum which can be stored in a word (e.g. 255
for 8-bit words), the count will silently and incorrectly overflow
(this may be fixed in a future release).  Other than being sorted by
length, there is no order imposed on the results.  The [[max_level]]
field of [[opts]] can be used to determine how accurate comparisons
are.  The [[do_literal]] field is ignored, and all values of
[[max_level]] over four are identical to four.  As above, the default
level is three.

% uni_uca_char_class32 prototype
% uni_uca_char_class16 prototype
% uni_uca_char_class8 prototype

The reverse lookups are done using the psuedo-properties [[rev_DUCET]]
and [[rev_DUCET_CLDR]].  These are stored in a string array indexed on
the level 1 value (shifted right 16 bits from the combined format).
The value of each array element is a string of key groups, each
followed by the corresponding collation element.  The keys are encoded
as described for the raw data above, and the collation element is
simlply all of its characters, shifted right 2 bits and with both low
bits set.  The array element is sorted by key for easier lookup of
detailed entries.

\subsection{Case Conversion}

The slc, suc, and stc properties are provided as if they were numeric
properties; the numerator of the result is the single character to
which the input maps.  A lookup failure in the stc table has the
result of a suc lookup as its default.  A lookup failure in either the
slc or suc tables has the input code point itself as its default.
None of these defaults is actually implemented in the generic lookup
functions.

The lc, uc, and sc properties, however, provide not only a mapping,
but also a condition under which the mapping takes place.  The
properties are each stored in separate string tables, so their
\emph{ST} name is the same as their \emph{SP} name.  All currently
possible conditions are mapped to a flag:

\input{Special casing conditions.tex} % C

A flag in [[UNI_SC_FL_LOCALE]] indicates that the translation only
applies in specific locales.  The others ([[UNI_SC_FL_CONTEXT]])
depend on context: generally the entire containing combining character
sequence, and possibly the next character must be known before
deciding on a translation.

Since each input code point may map to multiple translations
(depending on the condition),  the encoding of the string is to
provide all possible translations, each preceded by its length.  The
condition flags for that interpretation are then or-ed into the length.

The actual lc, uc, and tc properties combine the slc, suc, and stc
lookups with the contents of the lc, uc, and tc string tables, and may
vary depending on context.  To do a lookup, the following functions
are provided.  A [[cond]] of [[~0]] indicates that the caller does not
know the current context conditions.  The return is $-1$ if the
condition is unknown, but may affect the result.  In that case, the
function should be called again, but with valid [[cond]] flags.
Otherwise, it is the length of the result, with the output placed in
the output buffer.

There are three functions per case conversion type.
\input{UTF buffer return.tex} %%% doc

Note that the passed-in condition flags should never set
[[UNI_SC_FL_NOT]]; just leave a flag unset if it is not in effect.
Also, if the title casing functions ([[tc]]) return the input code
point, the user must manually call the upper-case ([[uc]]) function
for the final result.

% uni_lc32 prototype
% uni_uc32 prototype
% uni_tc32 prototype
% uni_lc16 prototype
% uni_uc16 prototype
% uni_tc16 prototype
% uni_lc8 prototype
% uni_uc8 prototype
% uni_tc8 prototype

These functions are actually preprocessor macros which use a generic
function that takes the tables to query:

% uni_case_convert32 prototype
% uni_case_convert16 prototype
% uni_case_convert8 prototype

Case folding converts to a canonical desired case, usually lower-case.
Like the plain case conversion properties, there are two cases:
simple and full.  In addition, some of the full folding is
conditional.  In this case, though, the only condition is whether or
not the tr or az locale is in use.  Encoding is identical to the case
conversion properties.  The scf property is numeric, and the cf
property can be derived using the [[uni_case_convert]] functions, and
their associated wrapper macros:

% uni_cf32 prototype
% uni_cf16 prototype
% uni_cf8 prototype

The pseudo-property tcf, which includes the Turkic language exceptions
(i.e., the mappings when in the tr or az locale), can be obtained
using a wrapper macro as well:

% uni_tcf32 prototype
% uni_tcf16 prototype
% uni_tcf8 prototype

Note that the cf and tcf properties are actually the same, and are
stored and treated the same way as the other case properties.  The
wrapper functions above filter the low-level cf property based on the
desired locale information.

The case conversion functions above are for one character at a time.
To convert a full string, use one of the following functions.  Behavior
of these functions is undefined if [[s]] overlaps the output buffer
([[*buf]]).  If an in-place conversion is desired, pass in [[NULL]]
for [[s]]; in that case, the input starts at the same place as the
output.  All but the case folding functions take a [[cond]] parameter
to provide context.

Unlike the character conversion functions, the title case conversion
function uses the upper-case and lower-case conversions as necessary
according to the full title case conversion algorithm.  That is, words
only have their first character capitalized, and everything else is
converted to lower-case.  This requires word boundary detection, which
can be localized with a property table override ([[WB_tab]]).

Also unlike the character conversion functions, the special value
[[~0]] is not supported for [[cond]]; it is not possible to query
whether or not context would alter the result.  In fact, only
locale-related flags are respected, and all others are filtered out
and generated internally based on the string contents.

% uni_str_lc32 prototype
% uni_str_lc16 prototype
% uni_str_lc8 prototype
% uni_str_uc32 prototype
% uni_str_uc16 prototype
% uni_str_uc8 prototype
% uni_str_tc32 prototype
% uni_str_tc16 prototype
% uni_str_tc8 prototype
% uni_str_cf32 prototype
% uni_str_cf16 prototype
% uni_str_cf8 prototype
% uni_str_tcf32 prototype
% uni_str_tcf16 prototype
% uni_str_tcf8 prototype

All but the title-case functions above are actually preprocessor
macros which use generic functions to perform their task, given the
tables to query:

% uni_str_case_convert32 prototype
% uni_str_case_convert16 prototype
% uni_str_case_convert8 prototype

Since a common operation to perform after case folding is comparison,
a function is provided to do both steps at the same time.  This is
equivalent to [[strcasecmp]] if the inputs are sufficiently
normalized.  The return value is $1$, $0$, or $-1$ on successful
comparison, indicating [[a]] is greater than, equal to, or less than
[[b]], respectively.  If there is an encoding error in [[a]], $2$ is
returned.  If there is an encoding error in [[b]], $-2$ is returned.

% uni_cf_strcmp32 prototype
% uni_cf_strcmp16 prototype
% uni_cf_strcmp8 prototype
% uni_tcf_strcmp32 prototype
% uni_tcf_strcmp16 prototype
% uni_tcf_strcmp8 prototype

These are all preprocessor macros using a more generic routine which
takes the locale flags as a parameter.

% uni_either_cf_strcmp32 prototype
% uni_either_cf_strcmp16 prototype
% uni_either_cf_strcmp8 prototype

If the inputs are not sufficiently normalized, the following functions
perform the comparison instead.  The return values have the same
values and interpretations, except that in addition, $3$ indicates a
memory allocation error while processing [[a]], and $-3$ indicates a
memory allocation error while processing [[b]].  Memory is allocated
for long canonical character sequences.

% uni_NFD_cf_strcmp32 prototype
% uni_NFD_cf_strcmp16 prototype
% uni_NFD_cf_strcmp8 prototype
% uni_NFD_tcf_strcmp32 prototype
% uni_NFD_tcf_strcmp16 prototype
% uni_NFD_tcf_strcmp8 prototype
% uni_NFKD_cf_strcmp32 prototype
% uni_NFKD_cf_strcmp16 prototype
% uni_NFKD_cf_strcmp8 prototype
% uni_NFKD_tcf_strcmp32 prototype
% uni_NFKD_tcf_strcmp16 prototype
% uni_NFKD_tcf_strcmp8 prototype

Again, these are preprocessor macros using more generic functions
which take the locale flags and NFD\_CF/NFKD\_CF pseudo-property
tables as parameters.

% uni_norm_cf_strcmp32 prototype
% uni_norm_cf_strcmp16 prototype
% uni_norm_cf_strcmp8 prototype

Finally, using the NFKC\_Casefold property instead of simple case
folding can be done using the following functions.  The capitalization
of CF should be enough to indicate that their function does not
involve the cf property directly.  Like the other normalizing
comparison functions, these may return memory errors as well when long
canonical character sequences are present.

% uni_NFKC_CF_strcmp32 prototype
% uni_NFKC_CF_strcmp16 prototype
% uni_NFKC_CF_strcmp8 prototype

There are also test functions for determining if a character or string
would change if converted to a case.  Normally, it would be more
efficient to check the gc property for the character, but this does
not necessarily match Unicode's strict definition of casing.  The
following functions opreate with one input character, and its full
context.  They return $-1$ if the context is insufficient, $0$ if the
character would change if the conversion function would apply, and $1$
othewise.  The [[uni_is_cased]] function, on the other hand, returns
$1$ if any of the conversion functions might change the character, and
$0$ (or $-1$, if unknown) otherwise.

Unlike the conversion functions, the title casing ([[tc]]) function
properly falls back to the upper-case ([[uc]]) table if there is no
direct title-casing mapping.  However, the user must manually switch
between title-casing and lower-casing ([[lc]]) depending on the
location within a word.

These are actually preprocessor macros, and the title-casing ([[tc]])
and any-casing ([[cased]]) functions may evaluate their parameters
more than once.

% uni_is_lc prototype
% uni_is_uc prototype
% uni_is_tc prototype
% uni_is_cf prototype
% uni_is_tcf prototype
% uni_is_cased prototype

The preprocessor macros use the following generic function with the
appropriate tables.

% uni_case_check prototype

The string equivalents of the above functions check if any part of the
string would change if a conversion function were applied.  Anything
doing title case checks needs to know about word boundaries, so a
locale word boundary property override can be specified for them
([[WB_tab]]).

% uni_str_is_lc8 prototype
% uni_str_is_lc16 prototype
% uni_str_is_lc32 prototype
% uni_str_is_uc8 prototype
% uni_str_is_uc16 prototype
% uni_str_is_uc32 prototype
% uni_str_is_tc8 prototype
% uni_str_is_tc16 prototype
% uni_str_is_tc32 prototype
% uni_str_is_cf8 prototype
% uni_str_is_cf16 prototype
% uni_str_is_cf32 prototype
% uni_str_is_tcf8 prototype
% uni_str_is_tcf16 prototype
% uni_str_is_tcf32 prototype
% uni_str_is_cased8 prototype
% uni_str_is_cased16 prototype
% uni_str_is_cased32 prototype

Again, the lower-case, upper-case and case folding functions are
actually macros, which use the generic functions taking a table:

% uni_str_case_check8 prototype
% uni_str_case_check16 prototype
% uni_str_case_check32 prototype

The only other case conversion string property is NFKC\_CF, which is a
normal string property.  See the normalization and decompostion
sections for additional functions related to this property.

\subsection{Other String Properties}

The scx property is officially not a string property, but it is stored
internally as a string property whose strings are sequences of
enumeration literal values of the [[uni_sc_t]] type.  For example, the
code point U+0363 has the scx property ``Arab Syrc'', which is stored
as string of length two.  The first character of the string is
[[UNI_sc_Arab]], and the second is [[UNI_sc_Syrc]].  The default value
for a code point's scx property if the lookup fails is the code
point's sc property value.

% End-doc Users-Guide

\lstset{language=txt}
<<FIXME>>=
Document or remove:
  enums w/o names:
    DUCET_dt
  strings:
    DUCET_dm
@

% Begin-doc Users-Guide
\section{Named Unicode Algorithms}

Table \ref{tab:unialg} lists named Unicode algorithms, and the API
functions which implement them.  The Bidirectional Algorithm (UBA) and
the Standard Compression Scheme for Unicode (SCSU) are not
implemented, and there are no plans to ever implement them.

% End-doc Users-Guide

<<FIXME>>=
Should probably implement UBA anyway, to ensure that data files are
usable, and to exercise BidiTest.txt and BidiCharacterTest.txt (see tr9)
@

% Begin-doc Users-Guide
\begin{table}[htb]
\begin{centering}
\begin{tabular}{ll}
\bf{Name}&\bf{Implementation}\\
Canonical Ordering&[[uni_Canon_Order]]\{[[8]],[[16]],[[32]]\}\\
Canonical Composition&[[uni_NFC_comp]]\{[[8]],[[16]],[[32]]\}\\
Normalization&[[uni_]]\{[[NFD]],[[NFKD]],[[NFC]],[[NFKC]]\}\{[[8]],[[16]],[[32]]\}\\
Hangul Syllable Composition&[[uni_hangul_syllable_comp]]\\
Hangul Syllable Decomposition&[[uni_hangul_syllable_decomp]]\\
Hangul Syllable Name Generation&[[uni_cp_to_name]], [[uni_name_rng_to_cp]]\\
Default Case Conversion*&[[uni_str_]]\{[[lc]],[[uc]],[[tc]],[[cf]]\}\{[[8]],[[16]],[[32]]\},\\
&[[uni_NFKC_CF]]\{[[8]],[[16]],[[32]]\}\\
Default Case Detection*&[[uni_str_is_]]\{[[lc]],[[uc]],[[tc]],[[cf]],[[cased]]\}\{[[8]],[[16]],[[32]]\}\\
Default Caseless Matching&[[uni_]]\{,[[NFD_]],[[NFKD_]],[[NFKC_]]\}[[cf_strcmp]]\\
Line Breaking Algorithm*&[[uni_line_brk]]\\
Character Segmentation&[[uni_gc_brk]]\\
Word Segmentation*&[[uni_word_brk]]\\
Sentence Segmentation*&[[uni_sentence_brk]]\\
Hangul Syllable Boundary Determination&[[uni_gc_brk]]\\
Default Identifier Determination&[[uni_is_IDS]], [[uni_is_IDC]]\\
Alternative Identifier Determination&[[uni_is_XIDS]], [[uni_is_XIDC]]\\
Pattern Syntax Determination&[[uni_is_Pat_WS]], [[uni_is_Pat_Syn]]\\
Identifier Normalization&[[uni_NFKC_CF]]\{[[8]],[[16]],[[32]]\}, [[uni_is_DI]]\\
Identifier Case Folding&[[uni_NFKC_CF]]\{[[8]],[[16]],[[32]]\}\\
Unicode Collation Algorithm (UCA)*&[[uni_uca_strcmp]]\{[[8]],[[16]],[[32]]\},\\
&[[uni_str_uca_raw]]\{[[8]],[[16]],[[32]]\},\\
&[[uni_uca_raw_cmp]]\\
\end{tabular}
\end{centering}

~

\small{* With localization support}

\small{Note that [[_brk]]-style functions are not provided for
identifiers and pattern syntax; the properties described by the
[[is_]]-style functions are mutually exclusive and
context-independent.}

\small{Note that identifier normalization is application-dependent,
so no specific function is provided.  Anything but NFKC\_CF requires
removal of default ignorable (DI) code points in addition to
normalization.}

\caption{\label{tab:unialg}Named Unicode Algorithms}
\end{table}

% End-doc Users-Guide

% Begin-doc Introduction
\section{Doxygen support}

The header files contain comments which can be parsed by doxygen%
\footnote{\url{http://www.doxygen.org/}}%
, for those who prefer that style of documentation over the
hand-written reference manual.  I do not like the way it organizes
and presents things, but it does at least provide a cheap (but flawed) 
way to generate man pages.  Running doxygen is entirely manual, but I
have included the [[Doxyfile]] I used during testing for reference.
Just extract it ([[make misc]] will do), maybe also manually make the
headers, and run [[doxygen]].  Note that once again, the rapid growth
of Unicode has broken something:  [[doxygen]] can no longer create its
PDF documentation due to too many language codes (actually,
[[pdflatex]] runs out of memory, even if its memory limits are maxed
out).  I may one day decide to drop this and use some other in-line
documentation method instead (although this particular issue is
probably telling me that I shouldn't create enumerations from CLDR
data).

\lstset{language=make}
<<Doxyfile>>=
PROJECT_NAME           = "Unicode Support Routines"
JAVADOC_AUTOBRIEF      = YES
TAB_SIZE               = 8
OPTIMIZE_OUTPUT_FOR_C  = YES
HIDE_UNDOC_MEMBERS     = YES
SHOW_INCLUDE_FILES     = NO
MAX_INITIALIZER_LINES  = 0
INPUT                  = uni_all.h uni_io.h uni_locale.h uni_norm.h \
                         uni_prop.gen.h uni_prop.h
VERBATIM_HEADERS       = NO
SORT_BRIEF_DOCS        = YES
SORT_GROUP_NAMES       = YES
QUIET                  = YES
IGNORE_PREFIX          = uni_ UNI_
GENERATE_MAN           = YES
MAN_LINKS              = YES
INLINE_GROUPED_CLASSES = YES
PAPER_TYPE             = letter
LATEX_BATCHMODE        = YES
# EXTRACT_ALL            = YES
@

<<Plain Files>>=
Doxyfile \
@

% End-doc Introduction

\end{document}

\input{begin-hidden.tex} %%% doc
Completely untouched & undocumented files:
  IDNA:
    IdnaTestV2.txt (IDNA algorithm not implemented, so no testing)

  IVD: (doesn't seem all that useful except to the supported vendors)
    IVD_Collections.txt
    IVD_Sequences.txt
    IVD_Stats.txt

  extracted:
    Seems duplicate of UnicodeData.txt w/ no added value:
      DerivedGeneralCategory.txt (3)
      DerivedCombiningClass.txt (4)
      DerivedDecompositionType.txt (6)
      DerivedBinaryProperties.txt (10)
    DerivedJoiningGroup.txt seems duplicate of ArabicShaping.txt, field 4
    DerivedLineBreak.txt doesn't add value to LineBreak.txt
    DerivedName includes synthesized names, but I synthesize instead
\input{end-hidden.tex} %%% doc
