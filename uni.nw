% -*- mode: Noweb; noweb-code-mode: c-mode; -*-
% Build with noweb:
%  notangle -t8 build.nw > makefile
%  make
\documentclass[twoside,english]{article}
\usepackage[letterpaper,rmargin=1.5in,bmargin=1in]{geometry}
%%% latex preamble
\RCS $Id$
\RCS $Revision$
\RCS $Date$

%%% requires tjm-ext

\begin{document}

\title{Unicode Support Routines}
\author{Thomas J. Moore}
\date{Version 1.0\\Revision \RCSRevision\\\RCSDate}
\maketitle

\begin{rawhtml}
<!-->
\end{rawhtml}
\iffalse
<<Sources>>=
$Id$
@

<<Version Strings>>=
"$Id$\n"
@

<<Common NoWeb Warning>>=
# $Id$
@
\fi
\begin{rawhtml}
<-->
\end{rawhtml}

\begin{abstract}

This document describes and implements a library which provides
basic Unicode (\url{http://www.unicode.org}) support in a convenient
manner.  Libraries already exist that do this:  GLib, ICU, and others.
However, this library does things my way, which allows me to e.g.
build a regular expression library and a compiler using these routines.

\end{abstract}

\tableofcontents

\section{Introduction}

The Unicode standard is a freely available standard describing a large
character set, and the features of those characters.  It also
describes several common transformations and processing methods for
those characters.  The ISO 10646 standard is also freely available at
this time, but like most ISO standards, this was not always so.  The
ISO standard describes the same character set, but has a much smaller
scope overall.  The Unicode Consortium also provides an electronically
readable form of its attribute information: the Unicode Character
Database.%
\footnote{\url{http://www.unicode.org/Public/zipped}}
The purpose of this library is to provide convenient access to that
information in C programs.  The ISO 10646 standard also provides such
information, but again, it is of more limited scope.  To the most
part, the ISO 10646 standard is completely ignored from this point on.

The Unicode consortium also provides a set of rules for interpreting
characters in different locales.  ISO also has standard to this
effect.  Both sets of standards and their data files are freely
available.  However, once again, the ISO data is ignored in favor of
the Unicode data.  The Unicode locale information is available in
electronically processable form in the Unicode Common Locale Data
Repository (CLDR).%
\footnote{\url{http://cldr.unicode.org},
\url{http://unicode.org/Public/cldr/}}
This library provides the CLDR data to C programs as well.

<<uni_all.h>>=
<<Common C Warning>>
#ifndef UNI_ALL_H
#define UNI_ALL_H

<<Library [[uni]] headers>>

#endif /* UNI_ALL_H */
@

This document does not contain the UCD or CLDR itself; it is expected
that the user download them manually and place them where they can be
found.  Some Linux distributions may already provide at least the UCD
as a package, although the CLDR is probably less common.  This package
was tested with versions 6.0.0 and 6.2.0 of the UCD, but the file
formats are relatively stable and it should work with later versions
as well.  I recommend at least version 6.1.0, as that is the first
version where the control character names were finally made official.
An earlier version of this library actually had hard-coded names for
ASCII control characters optionally added to the name database.  This
package was tested against CLDR version 22.1; I recommend always
getting the latest version available.  In particular, care must be
taken that the CLDR and UCD versions are compatible.

\lstset{language=make}
<<makefile.config>>=
# The location of the Unicode Character Database (unzipped)
# http://www.unicode.org/Public/zipped/<version>/UCD.zip
UCD_LOC = /usr/share/unicode-data
# The location of the Unicode Han Database (unzipped)
# http://www.unicode.org/Public/zipped/<version>/Unihan.zip
UNIHAN_LOC = $(UCD_LOC)
# The location of the Unicode Common Locale Data Repository (unzipped)
# http://www.unicode.org/Public/cldr/latest/core.zip
CLDR_LOC = /usr/share/unicode-data/cldr
@

Speaking of names, the Unicode character names are rather verbose.%
\footnote{It is recommended in some places that common names be
permitted in a less verbose manner, but that is locale-dependent and
will not be supported by this library.}
The XML standard, on the other hand, has fairly short names.  This
library can use either, or even both (where they do not conflict).  To
do this, the XML entity database is needed.  This is defined by the
W3C XML Entity Definitions for Characters.%
\footnote{\url{http://www.w3.org/TR/xml-entity-names/}}
This was tested with version 01 April 2010.%
\footnote{\url{http://www.w3.org/TR/2010/REC-xml-entity-names-20100401/}}
The only data file needed from that standard is unicode.xml,%
\footnote{\url{http://www.w3.org/2003/entities/2007xml/unicode.xml}}
which is linked from the standard.

<<makefile.config>>=
# The XML entity database http://www.w3.org/2003/entities/2007xml/unicode.xml
XMLUNI = /usr/share/unicode-data/unicode.xml
@

\section{Properties}

This library's primary purpose is to preparse the Unicode information
and query it efficiently.  The Unicode Character Database defines
numerous properties for characters.  Since there are many properties,
and any particular application may only need a few of them, an attempt
is made to keep each property in a separate object file.  Static
linking will only pull in the required properties.  There is little
penalty for shared libraries which include everything.  There is some
code that is shared by all properties, though.  This is placed in a
common file.

\lstset{language=C}
<<Library [[uni]] headers>>=
#include "uni_common.h"
@

<<uni_common.h>>=
#ifndef UNI_COMMON_H
#define UNI_COMMON_H

<<Common uni definitions>>

#endif /* UNI_COMMON_H */
@

<<Library [[uni]] Members>>=
uni_common.o
@

<<uni_common.c>>=
<<Common C Header>>

<<Common uni functions>>
@

These properties come in a variety of formats, but for the purpose of
this library, they may be generally classified according to the type
of value:  Boolean, String, Enumerated, or Numeric.  Each class of
values has a particular set of desirable operations related to a
property of that class.

\subsection{Boolean Properties}

Boolean properties are either true or false.  They define a subset of
characters: those for which the value is true.  As with any set, the
desirable operations are:

% l2h substitution oplus &oplus;
% l2h substitution wedge &and;
% l2h substitution vee &or;
% l2h substitution neg &not;
% l2h macro bar 1 #1&##773;

\begin{itemize}
\item Finding out if a character \emph{is} or is not an element of the
set.
\item Querying the \emph{members} of a set.
\item Applying \emph{logical} operators on a set.  There are 16
possible logical operators on two sets.  Operation numbers are
assigned using the formula $o_0 \oplus A\wedge o_1 \oplus B\wedge o_2
\oplus A\wedge B\wedge o_3$, where $o_n$ is true if bit $n$ is
true in the operation number.

\begin{quote}
\begin{tabular}{lllllrl}
$A$&false&false&true&true&\multicolumn{2}{l}{}\\
$B$&false&true&false&true&\multicolumn{2}{l}{}\\
&&&&&\multicolumn{2}{l}{Operation}\\
&false&false&false&false& 0&(false)\\
&true&true&true&true&     1&(true)\\
&false&false&true&true&   2&($A$)\\
&true&true&false&false&   3&($\neg A$)\\
&false&true&false&true&   4&($B$)\\
&true&false&true&false&   5&($\neg B$)\\
&false&true&true&false&   6&($A \oplus B$)\\
&true&false&false&true&   7&($A \bar \oplus B$)\\
&false&false&false&true&  8&($A \wedge B$)\\
&true&true&true&false&    9&($A \bar \wedge B$)\\
&false&false&true&false& 10&($A - B$)\\
&true&true&false&true&   11&($\neg(A - B)$)\\
&false&true&false&false& 12&($B - A$)\\
&true&false&true&true&   13&($\neg(B - A)$)\\
&false&true&true&true&   14&($A \vee B$)\\
&true&false&false&false& 15&($A \bar \vee B$)\\
\end{tabular}
\end{quote}
\end{itemize}

All of the above require that every member be stored.  The \emph{is}
test can be stored in a single bit.  However, there are over a million
valid character values, so the total storage for a simple bit array
would still be over 128 kilobytes, even if the array is limited to the
valid code point range.  Some properties only apply to a limited range
of characters, so these might use a little less space.  Even so, with
numerous properties, this adds up quickly, and may even slow things
down by keeping everything out of the cache.  A plain bit array also
makes it difficult to perform the \emph{members} operation: every
single potential member needs to be tested.  The \emph{logical}
operators are easy to perform, but require manipulating up to 256
kilobytes of data.  Other storage strategies are needed.

<<Common uni definitions>>=
#define MAX_UNI_CP 0x10ffff
typedef enum {
  UNI_LOG_FALSE, UNI_LOG_TRUE, UNI_LOG_A, UNI_LOG_NOT_A,
  UNI_LOG_B, UNI_LOG_NOT_B, UNI_LOG_A_XOR_B, UNI_LOG_A_XNOR_B,
  UNI_LOG_A_AND_B, UNI_LOG_A_NAND_B, UNI_LOG_A_MINUS_B, UNI_LOG_NOT_A_MINUS_B,
  UNI_LOG_B_MINUS_A, UNI_LOG_NOT_B_MINUS_A, UNI_LOG_A_OR_B, UNI_LOG_A_NOR_B
} uni_log_op_t;
#define BIT_OP_BIT(op, n) (~((((op) >> n) & 1) - 1))
#define BIT_OP(A, B, op) \
  (((A) & BIT_OP_BIT(op, 1)) ^ ((B) & BIT_OP_BIT(op, 2)) ^ \
   ((A) & (B) & BIT_OP_BIT(op, 3)) ^ BIT_OP_BIT(op, 0))

/* for bit arrays */
#define BSET_ELT(a, b) ((a)[(b)/(sizeof(*(a))*8)])
#define BSET_BIT(a, b) ((b)%(sizeof(*(a))*8))
#define BSET_ENTRY(a, e, b) ((e)*(sizeof(*(a))*8) + b)
#define BSET_MASK(a, b) (1 << BSET_BIT(a, b))
#define BSET_IS_SET(a, b) (BSET_ELT(a, b) & BSET_MASK(a, b))
#define BSET_SET(a, b) BSET_ELT(a, b) |= BSET_MASK(a, b)
#define BSET_CLEAR(a, b) BSET_ELT(a, b) &= ~BSET_MASK(a, b)
/* Logical ops not directly supported */
/* List members enumeration not directly supported */
@

<<Common uni functions>>=
#if 0
  <<Perform logical ops on bit array>>
#endif
#if 0
  <<List bit array members>>
#endif
@

<<Perform logical ops on bit array>>=
uint32_t i; elt_t e;
/* assumes both bit arrays cover entire set of possible values */
/* if not:
     if not starting at 0:
      if(BIT_OP(0, 0, op)) fill_up_to_min_offset_with_1s();
     if not starting at same point:
      if(start_a != start_b)
        for(i = min(start_a, start_b); i < max(start_a, start_b); i++)
	  tab[i] = BIT_OP(start_a < start_b ? a[i] : 0,
	                  start_b < start_a ? b[i] : 0, op);
     if not ending at same point: same basic idea as mismatched starts
     if not ending at end: same basic idea as not starting at zero
*/
for(i = 0; i < tab_size; i++)
  tab[i] = BIT_OP(taba[i], tabb[i], op);
@

<<List bit array members>>=
int i; elt_t e;
for(i = 0; i < tab_size; i++) {
  e = tab[i];
  while(e) {
    /* strings.h usually selects optimal built-in implementation of ffs() */
    int b = ffs(e); /* or ffsl, or ffsll if glibc */
    add_ent(BSET_ENTRY(tab, i, b - 1));
    e &= ~(1 << (b - 1));
  }
}
@

One of the simplest strategies is to provide a sorted table of
members.  Membership is tested by binary search, so the speed of
lookup is proportional to $\log_2(N)$, where $N$ is the number of set
members.  Querying the \emph{members} of the set is trival: the table
is already the list.  Applying \emph{logical} operators only requires
looking at the members already present, with the exception of the
latter eight:  inversion may be more expensive than with bit arrays.
This does not save much space for properties which apply to
a large number of characters; in fact, it might even take more space.

<<Common uni functions>>=
int uni_cmp_cp(const void *a, const void *b)
{
  return *(int32_t *)a - *(int32_t *)b;
}
@

<<Common uni definitions>>=
#define uni_is_cp(c, tab, tab_len) \
  bsearch(&c, tab, tab_len, sizeof(tab[0]), uni_cmp_cp)

/* members are already in list */
/* logical operations not directly supported */
@

<<Common uni functions>>=
#if 0
  <<Perform logical ops on cp array>>
#endif
@

<<Perform logical ops on cp array>>=
uint32_t i = 0;
uint32_t tab[max], tab_len = 0;
int invert = BIT_OP(0, 0, op);
uint32_t aptr, bptr;
for(aptr = bptr = 0; aptr < a_len || bptr < b_len; ) {
  int res;
  uint32_t cp;
  if(bptr == b_len || (aptr < a_len && a[aptr] < b[bptr])) {
    res = BIT_OP(~0, 0, op);
    cp = a[aptr++];
  } else if(aptr == a_len || (bptr < b_len && a[aptr] > b[bptr])) {
    res = BIT_OP(0, ~0, op);
    cp = b[bptr++];
  } else {
    res = BIT_OP(~0, ~0, op);
    cp = a[aptr++];
    bptr++;
  }
  if(invert)
    while(i < cp)
      tab[tab_len++] = i++;
  if(res)
    tab[tab_len++] = cp;
}
if(invert)
  while(i < max)
    tab[tab_len++] = i++;
@

A possible improvement on that strategy is to store ranges instead of
individual code points.  This relies on the fact that most properties
apply to large consecutive ranges; if this is not true, then this is
actually worse than storing individual code points.  Whenever the
number of ranges is less than half of the number of individual code
points, this table will be smaller, requiring less time to search, and
less time to perform \emph{logical} operations.  In particular,
negation is usually much faster with a range table than with an
individual code point table.  Not only can absent members be skipped,
but present members within a range can be skipped as well.

<<Common uni definitions>>=
typedef struct {
    uint32_t low, high;
} uni_chrrng_t;
int uni_cmprng(const void *a, const void *b); /* for qsort/bsearch */
int is_cp_chrrng(uint32_t cp, const uni_chrrng_t *tab, uint32_t tab_len);
@

<<Known Data Types>>=
uni_chrrng_t,%
@

<<Common uni functions>>=
int uni_cmprng(const void *a, const void *b)
{
    const uni_chrrng_t *_a = a, *_b = b;

    if(_a->high < _b->low)
        return -1;
    else if(_b->high < _a->low)
        return 1;
    else
        return 0;
}

int is_cp_chrrng(uint32_t cp, const uni_chrrng_t *tab, uint32_t tab_len)
{
#if 0
  uni_chrrng_t cr = {cp, cp};
  return bsearch(&cr, tab, tab_len, sizeof(uni_chrrng_t), uni_cmprng) ? 1 : 0;
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > tab[j].high)
      l = j + 1;
    else
      return 1;
  }
  return 0;
#endif
}
@

<<Common uni definitions>>=
/* members are already in list, albeit in compact form */
void uni_chrrng_invert(const uni_chrrng_t *a, uint32_t a_len,
		      uni_chrrng_t **rtab, uint32_t *r_len);
void uni_chrrng_logic(const uni_chrrng_t *a, uint32_t a_len, uni_log_op_t op,
                      const uni_chrrng_t *b, uint32_t b_len,
		      uni_chrrng_t **rtab, uint32_t *r_len);
@

<<Common uni functions>>=
void uni_chrrng_invert(const uni_chrrng_t *a, uint32_t a_len,
		      uni_chrrng_t **rtab, uint32_t *r_len)
{
  uni_chrrng_t *tab;
  uint32_t tab_len = 0;
  inisize(tab, a_len + 1);
  uint32_t i;
  if(a_len && a[0].low) {
    tab[0].low = 0;
    tab[0].high = a[0].low - 1;
    tab_len++;
  }
  for(i = 0; i < a_len - 1; i++, tab_len++) {
    tab[tab_len].low = a[i].high + 1;
    tab[tab_len].high = a[i + 1].low - 1;
  }
  if(a_len && a[a_len - 1].high < MAX_UNI_CP) {
    tab[tab_len].low = a[a_len - 1].high + 1;
    tab[tab_len].high = MAX_UNI_CP;
    tab_len++;
  }
  *rtab = tab;
  *r_len = tab_len;
}

void uni_chrrng_logic(const uni_chrrng_t *a, uint32_t a_len, uni_log_op_t op,
                      const uni_chrrng_t *b, uint32_t b_len,
		      uni_chrrng_t **rtab, uint32_t *r_len)
{
  uni_chrrng_t *tab;
  uint32_t tab_len = 0, max_tab = 8;
  /* shortcut all ops that don't involve both sets */
  if(op == UNI_LOG_FALSE) {
    inisize(tab, 1); /* so free always works */
    *rtab = tab;
    *r_len = 0;
    return;
  } else if(op == UNI_LOG_TRUE) {
    inisize(tab, 1);
    tab[0].low = 0;
    tab[0].high = MAX_UNI_CP;
    *rtab = tab;
    *r_len = 1;
    return;
  } else if(op == UNI_LOG_A) {
    inisize(tab, a_len);
    memcpy(tab, a, a_len * sizeof(*a));
    *rtab = tab;
    *r_len = a_len;
    return;
  } else if(op == UNI_LOG_NOT_A) {
    uni_chrrng_invert(a, a_len, rtab, r_len);
    return;
  } else if(op == UNI_LOG_B) {
    inisize(tab, b_len);
    memcpy(tab, b, b_len * sizeof(*b));
    *rtab = tab;
    *r_len = b_len;
    return;
  } else if(op == UNI_LOG_NOT_B) {
    uni_chrrng_invert(b, b_len, rtab, r_len);
    return;
  }
  inisize(tab, max_tab);
  int invert = BIT_OP(0, 0, op);
  uint32_t aptr = 0, bptr = 0;
  uint32_t alow, ahigh, blow, bhigh;
  int32_t lasthigh = -1, curlow = -1, curhigh = -1;
#define set_low_high(x) do { \
  x##low = x##ptr < x##_len ? x[0].low : MAX_UNI_CP + 1; \
  x##high = x##ptr < x##_len ? x[0].high : MAX_UNI_CP + 1; \
} while(0)
  set_low_high(a);
  set_low_high(b);
  while(alow <= MAX_UNI_CP || blow <= MAX_UNI_CP) {
    uint32_t nextlow, nexthigh, res;
    if(alow < blow) {
      nextlow = alow;
      res = BIT_OP(~0, 0, op);
      if(ahigh < blow) {
        nexthigh = ahigh;
	aptr++;
	set_low_high(a);
      } else {
        nexthigh = blow - 1;
	alow = blow;
      }
    } else if(alow > blow) {
      nextlow = blow;
      res = BIT_OP(0, ~0, op);
      if(bhigh < alow) {
        nexthigh = bhigh;
	bptr++;
	set_low_high(b);
      } else {
        nexthigh = alow - 1;
	blow = alow;
      }
    } else {
      nextlow = alow;
      res = BIT_OP(~0, ~0, op);
      nexthigh = ahigh;
      if(ahigh <= bhigh) {
        aptr++;
	set_low_high(a);
      } else
        alow = bhigh + 1;
      if(bhigh <= nexthigh) {
        nexthigh = bhigh;
	bptr++;
	set_low_high(b);
      } else
        blow = nexthigh + 1;
    }
    if(invert && nextlow > lasthigh + 1) {
      if(curlow < 0)
        curlow = lasthigh + 1;
      curhigh = nextlow - 1;
    }
    if(res) {
      if(curlow < 0)
        curlow = nextlow;
      curhigh = nexthigh;
    } else if(curlow >= 0) {
      if(tab_len == max_tab)
        resize(tab, max_tab *= 2);
      tab[tab_len].low = curlow;
      tab[tab_len++].high = curhigh;
      curlow = -1;
    }
    lasthigh = nexthigh;
  }
  if(invert && lasthigh < MAX_UNI_CP) {
    if(curlow < 0)
      curlow = lasthigh + 1;
    curhigh = MAX_UNI_CP;
  }
  if(curlow >= 0) {	
    if(tab_len == max_tab)
      resize(tab, max_tab *= 2);
    tab[tab_len].low = curlow;
    tab[tab_len++].high = curhigh;
  }
  *rtab = tab;
  *r_len = tab_len;
}
@

Another possible simple improvement is to store fixed ranges, for
example 32 characters at at time, with a bit array for that range.
Again, two words are required per entry (start and bit mask), so this
only saves storage space and time if each block has on average two or
more bits set.  This loses the range advantage of being able to skip
large contiguous groups of present members, but also partly loses the
range disadvantage of requiring contiguous members for efficiency.  It
also partly gains the disadvantage of requiring scanning of bit arrays
to produce the \emph{members} list.  No sample implementation is
provided here.

A more complex possible improvement is to split the original bit array
into a multi-level table, with duplicate subtables shared.  Basically,
this is an optimized prefix tree (trie) on the code point bit string.
The multi-level table approach requires fewer lookups (just as many
table lookups as there are levels, vs. a binary search against a long
range list that might take, say, 10 or more table probes), but even
when using space-saving measures will likely still produce larger
tables than even unoptimized range tables (range tables could be
optimized for space by using only 24 bits per code point, for
example.)  The sharing requires additional management of a list of
unique lower-level tables at each level, along with their users.
Making a change of any kind is a non-trivial operation.  This
additional storage and computational expense makes these more suited
to static one-time generation.  On the other hand, their straight
lookup performance benefit over ranged tables is three-fold or more
for many Unicode tables, without a need to do any special
optimizations.

There are probably very efficient algorithms to generate multi-level
tables, but I am not aware of them.  The optimal number of levels and
level size is difficult to find, but a pretty good number can be found
by taking each level in turn, and finding the size which produces the
minimum total length taking into account all uniques and the length of
the array at the next level.  Of course the size of the next level is
determined by redundancy elimination as well, so this requires a
recursive search with backtracking.  In addition, having too many
levels eliminates some of the performance benefit, so backtracking is
not infinite.  The initial bit array may be a subset of the desired
values, in which case an initial range can (actually, must) be
specified as well.  When a value lies outside of that range, it can be
given a default; currently only all zeroes and all ones are supported.
The structure may also store multiple bit arrays at once, in which
case it may be necessary to keep more than one byte together
contiguously at the lowest level; this may be specified using
[[minwidth]].

<<Common uni definitions>>=
#define UNI_MAX_MULTILEV 3
<<[[multi_tab_t]] definition>>
void bits_to_multi(const uint8_t *bits, uint32_t len, uint32_t low,
                   uint32_t high, uint8_t def, uint32_t minwidth,
		   <<Multi-level table type>> **ml, uint32_t *ml_len);
/* return: 0 == all 0, ~0 == all 1, 1 == *ret is ptr to data */
/* *ret is also set to NULL for 0 and ~0 returns */
int multi_tab_lookup(const <<Multi-level table type>> *dat, uint32_t val,
                     const uint8_t **ret, uint8_t def);
@

<<Common uni functions>>=
void bits_to_multi(const uint8_t *bits, uint32_t len, uint32_t low,
                   uint32_t high, uint8_t def, uint32_t minwidth,
		   <<Multi-level table type>> **ml, uint32_t *ml_len)
{
  <<Split [[bits]] into multi-level table [[*ml]]>>
}
@

<<Common uni functions>>=
int multi_tab_lookup(const <<Multi-level table type>>*dat, uint32_t val,
                     const uint8_t **ret, uint8_t def)
{
  <<Find multi-level table entry [[val]]>>
}
@

Recursion is actually accomplished using a backtracking stack.

<<Split [[bits]] into multi-level table [[*ml]]>>=
uint32_t i;
<<Saved multi-level table information>>
int curlev = 0;
/* The block size of the block currently being worked on */
uint32_t curblk[UNI_MAX_MULTILEV];

curblk[0] = 0;
while(1) {
  <<Unsaved multi-level table information>>

  <<Compute next [[curblk[curlev]]]>>
  if(!curblk[curlev]) {
    if(!curlev--)
      break;
    <<Free [[curlev]]>>
    continue;
  }
  <<Compute level [[curlev]]>>
  <<Create next level bit array>>
  ++curlev;
  curblk[curlev] = 0;
  <<Save or free [[curlev]]>>
  if(curlev == UNI_MAX_MULTILEV - 1) {
    curlev--;
    <<Free [[curlev]]>>
  }
}
@

The minimum block size is twice the pointer size at the next level.
Any smaller, and the next level could just as well be a copy.
Actually, any smaller than the architecture's memory alignment may
cause performance issues.  For now, this is set to a minimum of 4. 
The pointer size at the next level is 1, 2, or 4 depending on the
number of blocks at this level.  The maximum block size is one half
the total size; if the total size is not binary, the data is padded
with zeroes.

<<Saved multi-level table information>>=
/* raw data array for current set of levels */
uint8_t *lev[UNI_MAX_MULTILEV] = { (uint8_t *)bits };
uint32_t curlen[UNI_MAX_MULTILEV] = { len };
@

<<Unsaved multi-level table information>>=
uint32_t blks;
@

<<Compute next [[curblk[curlev]]]>>=
if(!curblk[curlev]) {
  if(curlen[curlev] <= 256 * 2)
#if 0 /* too small, really */
    curblk[curlev] = 2;
#else
    curblk[curlev] = 4;
#endif
  else if(curlen[curlev] <= 256 * 256 * 4)
    curblk[curlev] = 4;
  else
    curblk[curlev] = 8;
  if(!curlev && curblk[curlev] < minwidth)
    curblk[curlev] = minwidth;
} else {
  curblk[curlev] *= 2;
  if(curblk[curlev] >= curlen[curlev])
    curblk[curlev] = 0;
}
if(curblk[curlev])
  blks = (curlen[curlev] + curblk[curlev] - 1) / curblk[curlev];
@

For each pass, a new pointer array is created.  While this will
eventually be reduced to the pointer size, 32-bit pointers are used
during the search.  In addition, a side array is created to store only
the unique block indices.  This alone prevents having to scan the
entire return array for block matches, speeding things up orders of
magnitude with sparse data.  Additionally, this array is kept sorted,
allowing binary searching to further reduce the number of comparisons
needed.  Using a hash table for this would require extra storage, and
may speed things up further.  In any case, each block is simply added
to the pointer array, checking first for duplicates.  As another
special compensation for sparse (or dense) arrays, all-zero entries
are always encoded as the pointer zero, and all-one entries are always
encoded as the maximum pointer value (all ones).  Not only does this
save a tiny bit of space, but scanning through dense or sparse
sections of the array can be much faster.  Care must be taken during
the comparisons to not access data past the end of the actual bit
array, instead treating them as zeroes.

<<Unsaved multi-level table information>>=
uint32_t j;
uint32_t *ptr, *ublocks, nublocks = 0;
uint8_t *data = lev[curlev];
uint32_t blklen, shortlen;
@

<<Compute level [[curlev]]>>=
blklen = shortlen = curblk[curlev];
ptr = malloc(blks * sizeof(*ptr));
ublocks = malloc(blks * sizeof(*ublocks));
for(i = 0; i < blks; i++) {
  int h, l;
  /* first, check for 0 or 1 */
  int is0 = 1, is1 = 1;
  if(i == blks - 1) {
    shortlen = curlen[curlev] % blklen;
    if(!shortlen)
      shortlen = blklen;
  }
  for(l = 0; l < shortlen && (is0 || is1); l++) {
    if(data[blklen * i + l])
      is0 = 0;
    if(data[blklen * i + l] != (uint8_t)~0)
      is1 = 0;
  }
  if(is0 && (!def || shortlen == blklen)) {
    ptr[i] = 0;
    continue;
  }
  if(is1 && (def || shortlen == blklen)) {
    ptr[i] = ~0;
    continue;
  }
  h = nublocks - 1;
  l = 0;
  while(l <= h) {
    j = (l + h) / 2;
    /* comparing in reverse order to make shortlen cmp easier */
    int c = memcmp(data + blklen * ublocks[j], data + blklen * i, shortlen);
    if(!c) {
      uint32_t k;
      if(shortlen == blklen)
        break;
      /* make c > 0 if there is any non-0 element in ublock */
      for(k = shortlen; k < blklen && !c; k++)
        c = data[blklen * ublocks[j] + k];
      if(!c)
        break;
    }
    if(c > 0)
      h = j - 1;
    else
      l = j + 1;
  }
  if(l > h) {
    if(++h == nublocks)
      ublocks[nublocks++] = i;
    else {
      memmove(ublocks + h + 1, ublocks + h,
              (nublocks - h) * sizeof(*ublocks));
      ++nublocks;
      ublocks[h] = i;
    }
    j = h;
  }
  ptr[i] = ublocks[j] + 1;
}
@

Now that the search is complete, the pointers can be compressed to the
minimum word size required.  The data will eventually be shifted down,
but it is not necessary (or safe) yet.  However, some preparation must
be done here, so that the pointers will eventually point to the
shifted blocks.  The actual pointers need to be adjusted to use the
offset of that pointer in the unique blocks array.  This could not
have been done above, because the array was having members inserted in
the middle, invalidating all of the indices.  To look up the index
more quickly, the block array is stored by block number first (it was
sorted by block contents, instead).

<<Saved multi-level table information>>=
/* unique block pointers */
uint32_t *curublk[UNI_MAX_MULTILEV] = {NULL};
uint32_t curnublk[UNI_MAX_MULTILEV];
@

<<Create next level bit array>>=
curublk[curlev] = ublocks;
curnublk[curlev] = nublocks;
qsort(ublocks, nublocks, sizeof(uint32_t), uni_cmp_cp);
/* for comparison with bsearch, adjust to actual stored value */
for(i = 0; i < nublocks; i++)
  ++ublocks[i];
for(i = 0; i < blks; i++) {
  if(!ptr[i] || ptr[i] == (uint32_t)~0)
    continue;
  uint32_t *p = bsearch(ptr + i, ublocks, nublocks, sizeof(uint32_t), 
                        uni_cmp_cp);
  ptr[i] = p - ublocks + 1;
}
lev[curlev + 1] = (uint8_t *)ptr;
/* note: 0 and ~0 are reserved values, so space for 2 must be reserved */
if(nublocks <= 254) {
  uint8_t *p = (uint8_t *)ptr;
  for(i = 0; i < blks; i++)
    p[i] = ptr[i];
  curlen[curlev + 1] = blks;
} else if(nublocks <= 65534) {
  uint16_t *p = (uint16_t *)ptr;
  for(i = 0; i < blks; i++)
    p[i] = ptr[i];
  curlen[curlev + 1] = blks * 2;
} else
  curlen[curlev + 1] = blks * 4;
@

To retain the minimum-sized level, the current set of minima is
stored.  If the current level is either smaller or the same size, but
with fewer levels, it is saved.  Otherwise, it and all of its children
are freed.

<<Saved multi-level table information>>=
uint32_t cursize[UNI_MAX_MULTILEV];
uint8_t *minlev[UNI_MAX_MULTILEV] = { (uint8_t *)bits };
uint32_t *minublk[UNI_MAX_MULTILEV] = { NULL };
uint32_t minlen[UNI_MAX_MULTILEV] = { len }, minnublk[UNI_MAX_MULTILEV];
uint32_t minblk[UNI_MAX_MULTILEV] = { 0 }, minsize = len;
@

<<Create next level bit array>>=
cursize[curlev] = nublocks * curblk[curlev];
cursize[curlev + 1] = curlen[curlev + 1];
@

<<Save or free [[curlev]]>>=
uint32_t save_size = 0;
for(i = 0; curblk[i]; i++)
  save_size += cursize[i];
save_size += cursize[i];
int save_lev = save_size < minsize;
if(save_size == minsize) {
  for(j = 0; minblk[j]; j++);
  save_lev = i < j;
}
if(save_lev) {
  for(i = 0; i < UNI_MAX_MULTILEV; i++) {
    if(minlev[i] != lev[i] && minlev[i])
      free(minlev[i]);
    if(minublk[i] != curublk[i] && minublk[i])
      free(minublk[i]);
    minblk[i] = curblk[i];
    minlev[i] = lev[i];
    minlen[i] = curlen[i];
    minublk[i] = curublk[i];
    minnublk[i] = curnublk[i];
  }
  minsize = save_size;
}
@

<<Free [[curlev]]>>=
if(curublk[curlev] != minublk[curlev])
  free(curublk[curlev]);
curublk[curlev] = NULL;
if(lev[curlev + 1] != minlev[curlev + 1])
  free(lev[curlev + 1]);
lev[curlev + 1] = NULL;
@

Now, the data can be combined into a single buffer, which is parsed
for every lookup.  This buffer is an opaque buffer of 32-bit integers,
with each data chunk aligned to a 32-bit boundary.  That way, 16-bit
and 32-bit values can be read directly from the buffer without
alignment issues.  The buffer is read from top to bottom, traversing
pointer tables until the data is reached.


<<Multi-level table type>>=
uint32_t
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
#define lev_psz(l) \
  (!(l) ? 1 : minnublk[l - 1] <= 254 ? 1 : minnublk[l - 1] <= 65534 ? 2 : 4)
for(curlev = 0; curlev < UNI_MAX_MULTILEV - 1 && minlev[curlev + 1]; curlev++);
*ml_len = 0<<Multi-level table buffer length>>;
inisize(*ml, *ml_len);
uint32_t *mp = *ml;
@

To determine the size of the buffer, we need to know what the buffer
will contain.  I will approach this by developing the lookup function
at the same time.  This function returns either a pointer into the
lowest-level data arrays, or an integer indicating that this is a zero
or one entry.

<<Find multi-level table entry [[val]]>>=
*ret = NULL;
@

First, we need to filter out the main range.  The default, 0 or 1, is
encoded as the first bit of the low end, after shifting that up.

<<Find multi-level table entry [[val]]>>=
if(val < (*dat >> 1) || val > dat[1])
  return def ? def : *dat & 1 ? ~0 : 0;
val -= *dat >> 1;
dat += 2;
@

<<Multi-level table buffer length>>=
+2
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
*mp++ = (low << 1) + (def & 1);
*mp++ = high;
@

After looking up the value at a particular level, the level needs to
be skipped.  The size of the table can be stored as a compressed
pointer beyond the end of the next level's data.  This is not possible
for the first level, though, so the length needs to be stored
separately.  There is no point in storing a compression length as
well, so this is stored as a 32-bit integer.

<<Find multi-level table entry [[val]]>>=
uint32_t skip = *dat++;
@

<<Multi-level table buffer length>>=
+1
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
*mp++ = minlen[curlev] / lev_psz(curlev);
@

Following this is a list of levels.  The first word of the last level
(i.e., the data level) is zero to terminate the list.

<<Find multi-level table entry [[val]]>>=
<<Prepare to scan multi-level table pointers>>
while(*dat) {
  <<Scan multi-level table pointers>>
}
@

<<Multi-level table buffer length>>=
+1;
for(i = 1; i <= curlev; i++)
  *ml_len += 0<<Multi-level table buffer pointer [[i]] length>>;
*ml_len += 0<<Multi-level table buffer data length>>
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
while(curlev > 0) {
  <<Dump multi-level table pointers>>
  --curlev;
}
*mp++ = 0;
@

Next, we need information regarding this level of the array.  First,
each array corresponds to a particular set of bits; all bits below
that are ignored.  The bit number below which the reset may be ignored
is stored next.  For the lowest level of the array, this is always
zero, so this corresponds with the loop terminator.  Rather than store
the number of bits in an entire 32-bit word, it is stored in a byte.
The other three bytes are available for other information that is only
needed for a pointer level.

<<Scan multi-level table pointers>>=
uint32_t desc = *dat++;
uint8_t shift = desc;
uint32_t idx = val >> shift;

val -= idx << shift;
@

<<Multi-level table buffer pointer [[i]] length>>=
+1
@

<<Dump multi-level table pointers>>=
uint32_t desc;
<<Compute pointer level descriptor>>
*mp++ = desc;
@

<<Compute pointer level descriptor>>=
for(i = 0, desc = 0; i < curlev; i++)
  desc += lg2(minblk[i] / lev_psz(i));
@

Now that the index into the subarray is known, to actually look
something up in the table requires the size of the table entries.
This is stored next.

<<Scan multi-level table pointers>>=
desc >>= 8;
uint8_t psz = desc;
@

<<Compute pointer level descriptor>>=
uint8_t psz = lev_psz(curlev);
desc |= psz << 8;
@

It also needs the offset of the subtable.  Since that is what we are
loooking up, it can be obtained from the previous pass.  For the first
pass, the offset is always zero.

<<Prepare to scan multi-level table pointers>>=
uint32_t toff = 0;
@

Now the lookup can be performed.  First, the offset beyond the table
is read, and then the pointer itself.  Then, the data pointer is
advanced past the end of the entire level.

<<Scan multi-level table pointers>>=
switch(psz) {
  case 1: {
    const uint8_t *p = (const uint8_t *)dat;
    toff = p[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint8_t)~0)
      return ~0;
    dat += skip / 4 + 1; /* aka (skip + 1 + 3) / 4 */
    skip = *p;
    break;
  }
  case 2: {
    const uint16_t *p = (const uint16_t *)dat;
    toff = p[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint16_t)~0)
      return ~0;
    dat += skip / 2 + 1; /* aka (skip + 1 + 1) / 2 */
    skip = *p;
    break;
  }
  case 4:
    toff = dat[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint32_t)~0)
      return ~0;
    toff = *dat;
    dat += skip + 1;
    break;
}
@

<<Multi-level table buffer pointer [[i]] length>>=
+((i != curlev ? minnublk[i] * minblk[i] : minlen[i]) + lev_psz(i) + 3) / 4
@

<<Dump multi-level table pointers>>=
uint8_t *mpp = (uint8_t *)mp;
switch(psz) {
 case 1:
   *mpp++ = minnublk[curlev - 1];
   break;
 case 2:
   *(uint16_t *)mpp = minnublk[curlev - 1];
   mpp += 2;
   break;
 case 4:
   *(uint32_t *)mpp = minnublk[curlev - 1];
   mpp += 4;
   break;
}
<<Copy table data out>>
free(minlev[curlev]);
@

<<Copy table data out>>=
if(minblk[curlev]) {
  /* copy all but last blindly */
  for(i = 0; i < minnublk[curlev] - 1; i++) {
    memcpy(mpp, minlev[curlev] + minblk[curlev] * (minublk[curlev][i] - 1),
           minblk[curlev]);
    mpp += minblk[curlev];
  }
  /* the last may be short */
  uint32_t shortlen;
  if(minublk[curlev][i] == (minlen[curlev] + minblk[curlev] - 1) / minblk[curlev]) {
    shortlen = minlen[curlev] % minblk[curlev];
    if(!shortlen)
      shortlen = minblk[curlev];
  } else
    shortlen = minblk[curlev];
  memcpy(mpp, minlev[curlev] + minblk[curlev] * (minublk[curlev][i] - 1),
         shortlen);
  /* silence valgrind */
  if(shortlen < minblk[curlev])
    memset(mpp + shortlen, 0, minblk[curlev] - shortlen);
  mpp += minblk[curlev];
  free(minublk[curlev]);
} else {
  memcpy(mpp, minlev[curlev], minlen[curlev]);
  mpp += minlen[curlev];
}
/* align mpp */
if((mpp - (uint8_t *)mp) & 3)
  mpp += 4 - ((mpp - (uint8_t *)mp) & 3);
mp = (uint32_t *)mpp;
@

The table offset just acquired is a block number (actually, plus one
to allow for zero).  To convert it to a word offset, it needs to be
multiplied by the next block size, in pointer size units.  This is
stored next in [[desc]].  Since there are two bytes left, it is stored
as a 16-bit integer.  If another byte is ever needed, it can be
obtained by changing this to a shift value (it is always a power of
2).

<<Scan multi-level table pointers>>=
desc >>= 8;
toff = (toff - 1) * desc;
skip *= desc;
@

<<Compute pointer level descriptor>>=
desc |= (minblk[curlev - 1] / lev_psz(curlev - 1)) << 16;
@

The final lookup simply finds the byte offset of the remaining value
in the selected subtable.  Note that only bytes may be addressed;
storing more than one byte requires shifting the value left first
before finding the first byte of the value.  Similarly, storing only
one bit requires that the value be shifted right first for addressing,
and then using the shifted out bits to create a bit mask.

<<Find multi-level table entry [[val]]>>=
*ret = (const uint8_t *)dat + 4 + toff + val;
return 1;
@

<<Multi-level table buffer data length>>=
+((curlev ? minnublk[0] * minblk[0] : len) + 3) / 4
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
uint8_t *mpp = (uint8_t *)mp;
<<Copy table data out>>
if(mp - *ml != *ml_len) {
  fprintf(stderr, "len mismatch: %d %d\n", (int)(mp - *ml), (int)*ml_len);
  exit(1);
}
@

Generally, even with the complexity I added by compressing pointers,
the table lookups are faster than equivalent range table lookups.  As
long as duplicates are removed, the space taken up is comparable to
ordinary range tables.  With the suppression of zero and one blocks,
it is faster to find the \emph{members} list than with plain bit
tables, but is still not very fast.  On the other hand, making
modifications, or creating new tables from scratch, is very expensive.
Even if we stick with a table's current structure, there needs to be
side storage that deals with the shared arrays.  If a change is made
to an array at any level, then all pointers at the next level need to
be checked: if they point to the same place, the block needs to be
duplicated first, and after updating the block (duplicate or not), it
needs to be compared against all existing blocks at the same level for
re-merging.  The last step is not strictly necessary for run-time
operations, but there is no way with the current structure to detect
shared blocks.  An additional bit for each pointer could be used to
indicate sharing, which would never be turned off even if the last
owner disappears.  There are many possibilities, and I do not wish to
dwell on it any more.  For now, the multi-level tables are expected to
be read-only and only provide the \emph{in} function.

% FIXME: table_to_range
% FIXME: range_to_table (already implemented, but not as a function)
% FIXME: invert(table) (easy: doesn't need duplicate scanning)
% FIXME: logical(table) (hard: needs duplicate scanning, but maybe not)

A parser program is used to generate these tables as static,
compilable C code from the UCD tables.  One of the advantages of the
range table method is that creating range tables is trival, and could
be done using a simple shell script.  However, during the stages of
this project where that was in use, the generation time began to
dominate compilation time, so this was converted to C.  Since it is in
C anyway, it may as well generate multi-level tables directly, as
well.  In fact, the logic outside of the scripts was becoming
cumbersome as well, so all of that is now incorporated into the C
program.

This program has a special build rule so it can be used to generate
other C code without depending on that C code itself (as with
[[cproto.h]]).  The code itself can't use the standard header macro,
either, since it needs to limit its include files.  One thing it does
need, though, is the ability to read lines of arbitrary length.  This
is provided in my support library, which is referenced rather than
built here, again to avoid building [[cproto.h]].

\lstset{language=make}
<<C Build Executables>>=
parse-ucd \
@

<<makefile.config>>=
# location of support library
SUPT_LIB=../build/libsupt.a
@

<<makefile.rules>>=
parse-ucd: parse-ucd.c mfgets.h $(SUPT_LIB)
	$(CC) $(CFLAGS) $(SUPT_INCL) $(CFLAGS_EXTRA) -o $@ parse-ucd.c \
	  $(SUPT_LIB)
@

\lstset{language=C}
<<parse-ucd.c>>=
<<Common C Warning>>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <ctype.h>
#include <unistd.h>
#include "mallocdef.h"
#include "btricks.h"
#include "mfgets.h"
#include "uni_common.h"

<<UCD parser local definitions>>

<<UCD parser local functions>>

int main(int argc, const char **argv)
{
  <<UCD parser variables>>

  <<Process UCD parser arguments>>
  <<Parse character data files>>
  <<Post-process property data>>
  <<Dump character information as C code>>
  return 0;
}
@

All files will be parsed by the same program.  The only necessary
parameters are the locations of the UCD files.  Since the main UCD and
the Unihan components are distributed separately, they may come from
separate locations.

<<UCD parser variables>>=
const char *ucd_loc, *unihan_loc;
@

<<Process UCD parser arguments>>=
if(argc < 3)
  exit(1); /* internal use only, no help */
ucd_loc = *++argv;
unihan_loc = *++argv;
@

Rather than construct file names using the provided paths, they are
assumed to be absolute, and the program just changes to the location
directory before reading files, and all files are read relative to the
current directory.

<<UCD parser variables>>=
FILE *f;
char *lbuf = NULL;
unsigned int lbuflen, llen;
@

<<Parse character data files>>=
#define open_f(fn) do { \
  if(!(f = fopen(fn, "r"))) { \
    perror(fn); \
    exit(1); \
  } \
} while(0)
<<Parse character data in current directory>>
<<Initialize UCD files>>
<<Parse UCD files>>
<<Initialize Unihan files>>
<<Parse Unihan files>>
@

<<Initialize UCD files>>=
chdir(ucd_loc);
@

<<Initialize Unihan files>>=
chdir(unihan_loc);
@

UCD text files consist of semicolon-separated fields, with the code
point(s) in the first field.  Comments are introduced with the number
sign.  Blank lines and purely comment lines are permitted.  Whitespace
is stripped from the beginning and ending of each field.  One
particular file stores relevant information in a line comment, so all
comments on the same line as non-whitespace data are retained.  Rather
than process each field as it is encountered, like I would normally
do, each line is split into an array of fields first.  This makes the
parsing routines more readable and obvious as well.

<<Process a line of [[UnicodeData.txt]]>>=
split_line(lbuf);
if(num_fields < 15) { /* should never happen! */
  perror("UnicodeData.txt");
  exit(1);
}
@

<<UCD parser local definitions>>=
static char **fields;
static int num_fields, max_fields = 0;
@

<<UCD parser local functions>>=
static void split_line(char *buf)
{
  if(!max_fields)
    inisize(fields, (max_fields = 16));
  num_fields = 0;
  while(isspace(*buf)) buf++;
  if(!*buf || *buf == '#')
    return;
  while(1) {
    while(isspace(*buf))
      buf++;
    char *f = buf, *nf, fc;
    for(nf = buf; *nf && *nf != ';' && *nf != '#'; nf++);
    fc = *nf;
    buf = nf + 1;
    while(nf > f && isspace(nf[-1])) --nf;
    *nf = 0;
    if(num_fields == max_fields)
      resize(fields, (max_fields *= 2));
    fields[num_fields++] = f;
    if(!fc)
      return;
  }
}
@

Since named properties are being parsed, and named variables will be
created, it is useful to have a database of all of the property names
and their aliases before starting.  For this reason, the first file to
process is the file which contains those names:  [[PropertyAliases.txt]].
This file is read in and stored in a local table, sorted by name.  The
sorting is done after the fact to reduce complexity.

<<Common uni definitions>>=
typedef struct {
  const char *short_name, *long_name, *alt_name, *alt_name2;
} uni_alias_t;
@

<<UCD parser local definitions>>=
static uni_alias_t *prop_aliases;
static int num_prop_aliases = 0, max_prop_aliases = 0;
@

<<Initialize UCD files>>=
open_f("PropertyAliases.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  split_line(lbuf);
  if(num_fields < 2)
    continue;
  if(!max_prop_aliases)
    inisize(prop_aliases, (max_prop_aliases = 128));
  if(max_prop_aliases == num_prop_aliases)
    resize(prop_aliases, (max_prop_aliases *= 2));
  prop_aliases[num_prop_aliases].short_name = strdup(fields[0]);
  prop_aliases[num_prop_aliases].long_name = strdup(fields[1]);
  /* there are no comments on alias lines, so field count is correct */
  prop_aliases[num_prop_aliases].alt_name =
                 num_fields > 2 ? strdup(fields[2]) : NULL;
  prop_aliases[num_prop_aliases].alt_name2 =
                 num_fields > 3 ? strdup(fields[3]) : NULL;
  if(num_fields > 4) {
    perror("PropertyAliases.txt");
    exit(1);
  }
  num_prop_aliases++;
}
fclose(f);
@

There are actually two fields to sort by: long name and short name.
The main table will be sorted by long name, and a side table with
pointers will be used to sort the table by short name as well.  That
way, lookups can use binary searching on either field.

<<UCD parser local functions>>=
static int cmp_longname(const void *a, const void *b)
{
  return strcmp(((uni_alias_t *)a)->long_name, ((uni_alias_t *)b)->long_name);
}
@

<<Initialize UCD files>>=
qsort(prop_aliases, num_prop_aliases, sizeof(uni_alias_t), cmp_longname);
@

<<UCD parser local definitions>>=
uni_alias_t **prop_aliases_short;
@

<<UCD parser local functions>>=
static int cmp_shortname(const void *a, const void *b)
{
  return strcmp((*(uni_alias_t **)a)->short_name,
                (*(uni_alias_t **)b)->short_name);
}
@

<<Initialize UCD files>>=
inisize(prop_aliases_short, num_prop_aliases);
for(i = 0; i < num_prop_aliases; i++)
  prop_aliases_short[i] = &prop_aliases[i];
qsort(prop_aliases_short, num_prop_aliases, sizeof(*prop_aliases_short),
      cmp_shortname);
@


Properties are added to a master property list, containing the name
and the parsed contents.  This is simply tacked on next to the
like-named property in the property name table, if applicable.  Some
properties are artificial, though, so they are appended to the end.  A
separate count is provided for this purpose.

<<UCD parser local definitions>>=
typedef struct {
  const char *name;
  <<Property parsed contents>>
} prop_t;
static prop_t *parsed_props;
static uint32_t nparsed, maxparsed = 0;
@

<<Known Data Types>>=
prop_t,%
@

<<UCD parser local functions>>=
static int add_prop(const char *name)
{
  uni_alias_t *pn, n;
  uint32_t i;
  if(!maxparsed) {
    inisize(parsed_props, (maxparsed = nparsed = num_prop_aliases));
    clearbuf(parsed_props, nparsed);
  }
  n.long_name = name;
  pn = bsearch(&n, prop_aliases, num_prop_aliases, sizeof(*prop_aliases),
               cmp_longname);
  if(!pn) {
    uni_alias_t **ppn;
    pn = &n;
    n.short_name = name;
    ppn = bsearch(&pn, prop_aliases_short, num_prop_aliases,
                  sizeof(*prop_aliases_short), cmp_shortname);
    if(ppn)
      pn = *ppn;
    else
      pn = NULL;
  }
  if(pn)
    i = (int)(pn - prop_aliases);
  else {
    if(maxparsed == nparsed)
      resize(parsed_props, (maxparsed *= 2));
    i = nparsed;
    clearbuf(&parsed_props[nparsed], 1);
    nparsed++;
  }
  parsed_props[i].name = strdup(name);
  return i;
}
@

During the read loop, only range tables are constructed.  That way,
multiple, large bit arrays required for multi-level array construction
are only allocated one at a time, using prepared information from the
range table.  A function is provided to do just that.

<<Common uni definitions>>=
uint32_t *rng_to_multi(const uni_chrrng_t *tab, uint32_t tab_len,
                         uint32_t *ml_len);
@

<<Common uni functions>>=
uint32_t *rng_to_multi(const uni_chrrng_t *tab, uint32_t tab_len,
                       uint32_t *ml_len)
{
  uint32_t low, high, len, i;
  uint32_t *ml;
  uint8_t *bits, def;

  /* degenerate cases:  all 0, all 1 */
  if(!tab_len) {
    bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  if(tab_len == 1 && !tab[0].low && tab[0].high == MAX_UNI_CP) {
    bits_to_multi(NULL, 0, 1, 0, 1, 0, &ml, ml_len);
    return ml;
  }
  /* not all 0 or all 1 */
  low = tab[0].low;
  high = tab[tab_len - 1].high;
  /* if starts & ends with 1, make it an inverse array */
  def = !low && high == MAX_UNI_CP;
  if(def) {
    low = tab[0].high + 1;
    high = tab[tab_len - 1].low - 1;
  }
  /* align with byte boundary */
  low &= ~7;
  len = ((high - low + 1 + 7) / 8);
  inisize(bits, len);
  clearbuf(bits, len);
  /* FIXME: use quick mask calculation instead of loops */
  if(def)
    /* fill in ones if low not aligned with byte boundary */
    for(i = 0; i < (tab[0].high + 1) % 8; i++)
      BSET_SET(bits, i);
  for(i = def; i < tab_len - def; i++) {
    uint32_t j;
    for(j = tab[i].low; j <= tab[i].high && (j & 7); j++)
      BSET_SET(bits, j - low);
    if(j < ((tab[i].high + 1) &~7)) {
      memset(&BSET_ELT(bits, j - low), 0xff, (tab[i].high + 1) / 8 - j / 8);
      j += ((tab[i].high + 1) / 8 - j / 8) * 8;
    }
    for(; j <= tab[i].high; j++)
      BSET_SET(bits, j - low);
  }
  if(def && tab[tab_len - 1].low % 8)
    /* fill in ones if high + 1 not aligned with byte boundary */
    for(i = tab[tab_len - 1].low % 8; i < 8; i++)
      BSET_SET(bits, tab[tab_len - 1].low - i);
  bits_to_multi(bits, len, low / 8, high / 8, def, 0, &ml, ml_len);
  free(bits);
  return ml;
}
@

The first file to parse for actual data is the main database file,
[[UnicodeData.txt]].

<<Parse UCD files>>=
open_f("UnicodeData.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Process a line of [[UnicodeData.txt]]>>
}
fclose(f);
@

The first field is a code point.  While it appears that each line only
deals with one code point, there are actually ranges as well.  These
are specified by two consecutive entries with names ending in \texttt{,
First>} and \textt{, Last>}, respectively.

<<UCD parser variables>>=
uint32_t low, high;
char *s;
@

<<Process a line of [[UnicodeData.txt]]>>=
low = high = strtol(fields[0], NULL, 16);
if(fields[1][0] == '<' && (s = strchr(fields[1], ',')) &&
   !strcasecmp(s, ", First>")) {
  if(!mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    perror("UnicodeData.txt");
    exit(1);
  }
  split_line(lbuf);
  if(num_fields < 15 ||
     fields[1][0] != '<' || !(s = strchr(fields[1], ',')) ||
     strcasecmp(s, ", Last>")) { /* should also never happen! */
    perror("UnicodeData.txt");
    exit(1);
  }
  high = strtol(fields[0], NULL, 16);
}
@

The parsed contents for boolean properties are the range table and the
associated multi-level table.  In order to build the range table in
place, the storage size of the range table is kept as well.

<<Property parsed contents>>=
uni_chrrng_t *rng;
uint32_t rng_len, max_rng_len;
uint32_t *mt, mt_len;
@

<<UCD parser variables>>=
uint32_t i, j;
@

<<UCD parser local functions>>=
static void add_bool_rng(prop_t *p, uint32_t low, uint32_t high)
{
  if(!p->max_rng_len)
    inisize(p->rng, (p->max_rng_len = 8));
  if(p->rng_len && p->rng[p->rng_len - 1].high == low - 1)
    p->rng[p->rng_len - 1].high = high;
  else {
    if(p->rng_len == p->max_rng_len)
      resize(p->rng, (p->max_rng_len *= 2));
    p->rng[p->rng_len].low = low;
    p->rng[p->rng_len].high = high;
    ++p->rng_len;
  }
}
@

<<UCD parser local definitions>>=
#define decl_bool(n) int prop_##n = -1
#define add_bool(n) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  add_bool_rng(&parsed_props[prop_##n], low, high); \
} while(0)
@

The only directly derived boolean properties in [[UnicodeData.txt]] are
ASSIGNED, which is true if the character is present, and
Bidi\_Mirrored, which is true if field 10 is Y.

<<UCD parser variables>>=
decl_bool(ASSIGNED);
decl_bool(Bidi_Mirrored);
@

<<Process a line of [[UnicodeData.txt]]>>=
add_bool(ASSIGNED);
if(fields[9][0] == 'Y')
  add_bool(Bidi_Mirrored);
@

The [[PropList.txt]] file has more boolean properties, though.  In
fact, all entries in this file specify boolean properties.

<<Parse UCD files>>=
open_f("PropList.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Process a line of [[PropList.txt]]>>
}
fclose(f);
@

Like most of the rest of the UCD files, this file specifies ranges
using double-dot-separated code points.  Also, there are numerous
blank lines and comments, which need to be skipped.

<<Process a line of [[PropList.txt]]>>=
<<Parse dotted cp>>
@

<<Parse dotted cp>>=
split_line(lbuf);
if(num_fields < 2)
  continue;
low = high = strtol(lbuf, &s, 16);
if(s && *s == '.' && s[1] == '.')
  high = strtol(s + 2, NULL, 16);
@

Each boolean property is true if field two is the name of that
property.  Since the [[addprop]] routine finds the correct entry based
on the name (assuming the name exists), it can be called every time,
and the correct entry will be updated.  However, the Hyphen property
needs to be excluded, because it is deprecated.  Also, I have no
applications which require the Other\_* poperties, so these are
ignored as well. They are mostly used to derive other properties, but
it's easier to just process [[DerivedCoreProperties.txt]] than to
generate them manually.

<<Process a line of [[PropList.txt]]>>=
if(!strcmp(fields[1], "Hyphen"))
  continue;
if(!strncmp(fields[1], "Other_", 6))
  continue;
<<Just add field two as binary property>>
@

<<Just add field two as binary property>>=
add_bool_rng(&parsed_props[add_prop(fields[1])], low, high);
@

Speaking of [[DerivedCoreProperties.txt]], this file has entirely
boolean properties as well.  The only deprecated property is
Grapheme\_Link.

<<Parse UCD files>>=
open_f("DerivedCoreProperties.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  if(!strcmp(fields[1], "Grapheme_Link"))
    continue;
  <<Just add field two as binary property>>
}
fclose(f);
@

The [[CompositionExclusions.txt]] file simply lists all code points
having the property Composition\_Exclusion.  Since I process comments
as fields, and all lines have trailing comments, the number of fields
is sufficient to use the same parsing technique as for the other
files.

<<UCD parser variables>>=
decl_bool(Composition_Exclusion);
@

<<Parse UCD files>>=
open_f("CompositionExclusions.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_bool(Composition_Exclusion);
}
fclose(f);
@

Unfortunately, this file does not have all entries in order. Instead,
it lists multiple groups, and each group is in order.  This requires
post-processing.  In fact, since this might happen with other files as
well, this processing may as well be done for all tables.

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng)
    fixup_rng(&parsed_props[i]);
@

<<UCD parser local functions>>=
static void fixup_rng(prop_t *p)
{
  uint32_t i;
  qsort(p->rng, p->rng_len, sizeof(uni_chrrng_t), uni_cmprng);
  for(i = 0; i < p->rng_len - 1; i++)
    if(p->rng[i + 1].low == p->rng[i].high - 1) {
      p->rng[i].high = p->rng[i + 1].high;
      if(i + 2 < p->rng_len) {
        memmove(p->rng + i + 1, p->rng + i + 2,
	        (p->rng_len - (i + 2)) * sizeof(uni_chrrng_t));
        p->rng_len--;
      }
    }
}
@

The only remaining required boolean UCD properties come from
[[DerivedNormalizationProps.txt]].  Unlike the other three, there are
some non-boolean properties in this file as well.  Luckily, there is a
comment after every line, so some boolean values can be detected by
simply counting the fields.  The Expands\_On\_* properties are
deprecated.  Technically, the quick check fields could be booleans as
well, but some values are Maybe.  In any case, the Unicode standard
defines them as enumerated properties.

<<Parse UCD files>>=
open_f("DerivedNormalizationProps.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  <<Process a line of [[DerivedNormalizationProps.txt]]>>
}
fclose(f);
@

<<Process a line of [[DerivedNormalizationProps.txt]]>>=
if(num_fields == 3) {
  if(!strncmp(fields[1], "Expands_On_", 11))
    continue;
  <<Just add field two as binary property>>
  continue;
}
@

There are no boolean fields exported from Unihan.  The only field
which might be considered boolean (kIICore) is not required by
anything I use.

% FIXME: add fields from other sources, if necessary.
%  XML: none
%  CLDR: probably none, and in any case, should probably be loaded at
%  run-time

Now, to finish up post-processing, the multi-level table is generated.

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng)
    parsed_props[i].mt = rng_to_multi(parsed_props[i].rng, parsed_props[i].rng_len,
                                      &parsed_props[i].mt_len);
@

The boolean properties can now be printed, using the canonical short
name.  Converting these to lower-case would be more consistent, but
once the name is known, casing shouldn't really matter.  A header
needs to be generated as well, to contain the external variable
declarations.  This is written to standard error.  It can still be
distinguished from error messages by the fact that no error code is
returned by the program.

In addition to the property tables, a simple query function is
printed.  This calls a generic search function using the multi-level
table.

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    printf("const uni_chrrng_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].rng_len; j++)
      printf("\t{ %d, %d }%s\n", parsed_props[i].rng[j].low,
                                 parsed_props[i].rng[j].high,
				 j < parsed_props[i].rng_len - 1 ? "," : "");
    puts("};");
    fprintf(stderr, "#define uni_%s_rng_len %d\n", name,
                                                   parsed_props[i].rng_len);
    printf("const uint32_t uni_%s_mtab[] = {\n\t", name);
    for(j = 0; j < parsed_props[i].mt_len; j++)
      printf("0x%X%s", parsed_props[i].mt[j], j == parsed_props[i].mt_len - 1 ?
                                                     "\n" : !((j + 1) % 8) ?
						           ",\n\t" : ", ");
    puts("};");
    fprintf(stderr, "extern const uni_chrrng_t uni_%s_rng[];\n"
		    "extern const uint32_t uni_%s_mtab[];\n"
		    "#define is_uni_%s(x) is_uni_x(x, uni_%s_mtab)\n",
		    name, name, name, name);
  }
}
@

<<Common uni definitions>>=
int is_uni_x(uint32_t cp, const uint32_t *tab);
@

<<Common uni functions>>=
int is_uni_x(uint32_t cp, const uint32_t *tab)
{
  const uint8_t *mr;
  uint8_t mv = multi_tab_lookup(tab, cp / 8, &mr, 0);
  if(mr)
    mv = *mr;
  return mv & BSET_MASK(mr, cp) ? 1 : 0;
}
@

\subsection{Enumerated Properties}

Enumerated properties have a limited number of values, which have
names of their own.  These names may have aliases, as well.  Of course
all properties have a limited number of values, so it is a matter of
interpretation whether it qualifies as an enumeration.  The general
rule followed here is that this only includes properties with an
explicit list of aliases.  That includes one psuedo-property: the list
of properties itself.  An enumerated property can be split into sets,
each of which consists of the characters having one specific
enumerated value.  This means that each enumerated property also
defines a number of boolean properties, which are covered above.
Additional useful operations include:

\begin{itemize}
\item Find out the value \emph{of} the property for a character.  This
should be a numeric value indicating the enumeration literal index.
\item Given a value, find out its \emph{name} and primary \emph{alias}.
\item Given a name or alias, find out its \emph{value}.  This should
support Unicode sloppy matching:  case-insensitive, with dashes and
underscores removed (except as noted).
\item Obtain a \emph{list} of all possible values and aliases, for more
general searching.
\end{itemize}

Again, for the \emph{of} operation, the multi-level table seems
appropriate.  Rather than storing the actual string value, the
enumeration is simply converted into an integer from zero to the
number of elements, and that integer is stored.  For all enumerations
but the character name, this just one byte.  For the character name, a
different approach is required.  Since every code point has a
different name, there will never be any sharing of lower-level tables.
The only possible sharing would be in the algorithmically generated
code points.  Since this is a significant special case, it gets its
own section, later on.

To obtain a subset for boolean operations, the range list is provided
as well.  It is augmented by a data field.  No enumeration actually
requires 32 bits, so adding another separate field is wasteful.
Instead, an 8-bit bit field is added.  This does slow things down
slightly, but for performance, the multi-level table is better, anyway.

<<Common uni definitions>>=
typedef struct {
    uint32_t low, high:24, dat:8;
} uni_chrrng_dat_t;
int uni_cmprng_dat(const void *a, const void *b); /* for bsearch/qsort */
uint32_t find_cp_chrrng(uint32_t cp, const uni_chrrng_dat_t *tab,
                        uint32_t tab_len, uint32_t def);
@

<<Known Data Types>>=
uni_chrrng_dat_t,%
@

<<Common uni functions>>=
int uni_cmprng_dat(const void *a, const void *b)
{
    const uni_chrrng_dat_t *_a = a, *_b = b;

    if(_a->high < _b->low)
        return -1;
    else if(_b->high < _a->low)
        return 1;
    else
        return 0;
}

uint32_t find_cp_chrrng(uint32_t cp, const uni_chrrng_dat_t *tab,
                        uint32_t tab_len, uint32_t def)
{
#if 0
  uni_chrrng_dat_t cr = {cp, cp}, *tab_el;
  tab_el = bsearch(&cr, tab, tab_len, sizeof(uni_chrrng_dat_t), uni_cmprng_dat);
  return tab_el ? tab_el->dat : def;
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > tab[j].high)
      l = j + 1;
    else
      return tab[j].dat;
  }
  return def;
#endif
}
@

For the \emph{name} and \emph{alias} operations, a simple lookup table
can be provided, indexed on the enumeration value (again, except for
the name property).  The table fulfulls the \emph{list} requirement as
well.

For the \emph{value} operation, either a sorted table of names or some
other structure can be provided.  The advantage of the sorted table of
names is that it takes no effort to create, search, and provides the
\emph{list} operation (specifially for searching) as well.  A
statically generated hash function might be faster, though.  Since
applications that need to look up a lot of \emph{value}s are probably
rare, only the simple sorted table is provided, and the library user
is expected to convert that to a hash table, prefix tree, or some
other structure as needed.

The first thing to read in is the list of aliases.  These are keyed on
property short name.  Since there is already a list of property names,
the array of aliases may as well correspond.

<<UCD parser local definitions>>=
uni_alias_t **val_aliases;
int *num_val_aliases, *max_val_aliases;
@

<<Initialize UCD files>>=
inisize(val_aliases, num_prop_aliases);
clearbuf(val_aliases, num_prop_aliases);
inisize(num_val_aliases, num_prop_aliases);
clearbuf(num_val_aliases, num_prop_aliases);
inisize(max_val_aliases, num_prop_aliases);
clearbuf(max_val_aliases, num_prop_aliases);
@

Then, the alias file can be read and stored in the array.  While it
would be more efficient to only look up the destination when things
change, it's easier to just go ahead and put it in place.

<<Initialize UCD files>>=
open_f("PropertyValueAliases.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  split_line(lbuf);
  if(num_fields < 3)
    continue;
  uni_alias_t me, *mep = &me;
  me.short_name = fields[0];
  uni_alias_t **ret = bsearch(&mep, prop_aliases_short, num_prop_aliases,
                              sizeof(*prop_aliases_short), cmp_shortname);
  if(!ret) {
    perror("PropertyValueAliases.txt");
    exit(1);
  }
  int idx = (int)(*ret - prop_aliases);
  if(!max_val_aliases[idx])
    inisize(val_aliases[idx], (max_val_aliases[idx] = 16));
  if(max_val_aliases[idx] == num_val_aliases[idx])
    resize(val_aliases[idx], (max_val_aliases[idx] *= 2));
  val_aliases[idx][num_val_aliases[idx]].short_name = strdup(fields[1]);
  val_aliases[idx][num_val_aliases[idx]].long_name = strdup(fields[2]);
  /* there are no comments on alias lines, so field count is correct */
  val_aliases[idx][num_val_aliases[idx]].alt_name =
                 num_fields > 3 ? strdup(fields[3]) : NULL;
  val_aliases[idx][num_val_aliases[idx]].alt_name2 =
                 num_fields > 4 ? strdup(fields[4]) : NULL;
  if(num_fields > 5) {
    perror("PropertyValueAliases.txt");
    exit(1);
  }
  num_val_aliases[idx]++;
}
fclose(f);
@

Since we're about to scan files with the enumerations in text form,
the next logical step is to build a search mechanism.  In this case,
just a simple sorted array, using the standard loose matching.  This
is necessary because some fields do not match any of the literal
values we have gathered so far.  This table will be dumped as well,
for the \emph{value} function.

<<Common uni definitions>>=
typedef struct {
  const char *name;
  int val;
} uni_valueof_t;
@

<<UCD parser local definitions>>=
uni_valueof_t **enum_vals;
uint32_t *enum_vals_len;

<<UCD parser local functions>>=
static int cmp_valueof(const void *a, const void *b)
{
  return strcmp(((uni_valueof_t *)a)->name, ((uni_valueof_t *)b)->name);
}
@

<<Initialize UCD files>>=
inisize(enum_vals, num_prop_aliases);
clearbuf(enum_vals, num_prop_aliases);
inisize(enum_vals_len, num_prop_aliases);
clearbuf(enum_vals_len, num_prop_aliases);
for(i = 0; i < num_prop_aliases; i++) {
  if(!val_aliases[i])
    continue;
  /* ignore boolean properties */
  if(num_val_aliases[i] == 2 && !strcmp(val_aliases[i][0].short_name, "N") &&
     !strcmp(val_aliases[i][1].short_name, "Y"))
    continue;
  uni_valueof_t *valueof;
  int num_valueof = 0;
  inisize(valueof, num_val_aliases[i] * 4);
  for(j = 0; j < num_val_aliases[i]; j++) {
    const uni_alias_t *va = &val_aliases[i][j];
    valueof[num_valueof].name = strdup(va->short_name);
    valueof[num_valueof++].val = j;
    valueof[num_valueof].name = strdup(va->long_name);
    valueof[num_valueof++].val = j;
    if(va->alt_name && !strchr(va->alt_name, '|')) {
      valueof[num_valueof].name = strdup(va->alt_name);
      valueof[num_valueof++].val = j;
    }
    if(va->alt_name2 && !strchr(va->alt_name2, '|')) {
      valueof[num_valueof].name = strdup(va->alt_name2);
      valueof[num_valueof++].val = j;
    }
  }
  for(j = 0; j < num_valueof; j++) {
    char *n = (char *)valueof[j].name, *d = n;
    for(; *n; n++) {
      if(isupper(*n))
        *d++ = tolower(*n);
      else if(*n != '_' && *n != '-' && !isspace(*n))
        *d++ = *n;
    }
    *d = 0;
  }
  qsort(valueof, num_valueof, sizeof(*valueof), cmp_valueof);
  /* remove duplicates now that they are always contiguous */
  for(j = 1; j < num_valueof; j++)
    if(!strcmp(valueof[j].name, valueof[j - 1].name)) {
      free((char *)valueof[j].name);
      if(valueof[j].val != valueof[j - 1].val) {
        perror(valueof[j - 1].name);
	exit(1);
      }
      memmove(valueof + j, valueof + j + 1, (num_valueof - (j + 1)) *
                                               sizeof(*valueof));
      --j;
      --num_valueof;
    }
  enum_vals[i] = valueof;
  enum_vals_len[i] = num_valueof;
}
@

For each value alias, an enumeration needs to be printed to the header
file.  Both the short and long name should be supported, but there is
little value in supporting the secondary aliases.  This excludes
binary values, which have pointless yes/no aliases, and, if possible,
deprecated values.  Special care must be taken with the version
aliases, as the numeric values have decimal points.  The CCC aliases
are numeric as well, but in this case, the short name should be
ignored and used as the value directly instead.

In order to support the \emph{name} and \emph{value} functions, tables
need to be dumped as well.  First, a table similar to the locally
stored table is printed; this does the \emph{name} operation.  Next,
value table built just previously is dumped for the \emph{value}
operation.  Unlike the enumerations, no values may be skipped when
generating the string tables.

<<Dump character information as C code>>=
for(i = 0; i < num_prop_aliases; i++) {
  const char *pn = prop_aliases[i].short_name;
  if(!enum_vals[i])
    continue;
  /* ignore boolean properties */
  if(parsed_props[i].rng)
    continue;
  <<Generate C code for enumeration constants>>
}
@

<<Generate C code for enumeration constants>>=
/* generating nameof array on stdout (except extern decl on stderr) */
fprintf(stderr, "extern const uni_alias_t uni_%s_nameof[];\n", pn);
printf("const uni_alias_t uni_%s_nameof[] = {\n", pn);
/* generating enum on stderr */
fputs("typedef enum {\n", stderr);
for(j = 0; j < num_val_aliases[i]; j++) {
  const uni_alias_t *va = &val_aliases[i][j];
  /* print nameof table entry */
  printf("\t{ \"%s\"", va->short_name);
  if(strcmp(va->long_name, va->short_name))
    printf(", \"%s\"", va->long_name);
  if(va->alt_name && !strchr(va->alt_name, '|') &&
     strcmp(va->alt_name, va->long_name))
    printf(", \"%s\"", va->alt_name);
  if(va->alt_name2 && !strchr(va->alt_name2, '|') &&
     strcmp(va->alt_name, va->alt_name2))
    printf(", \"%s\"", va->alt_name2);
  fputs(" }", stdout);
  if(j < num_val_aliases[i] - 1)
    putchar(',');
  putchar('\n');
  /* filter out bad enum values */
  /* skip version short name */
  if(strchr(va->short_name, '.')) {
    fprintf(stderr, "\tU_%s_%s,\n", pn, va->long_name);
    continue;
  }
  /* use numeric values directly instead of making into enum */
  if(!isdigit(va->short_name[0]))
    fprintf(stderr, "\tU_%s_%s,", pn, va->short_name);
  if(strcmp(va->long_name, va->short_name)) {
    fprintf(stderr, "%cU_%s_%s = ", isdigit(va->short_name[0]) ? '\t' : ' ',
                    pn, va->long_name);
    if(!isdigit(va->short_name[0]))
      fprintf(stderr, "U_%s_%s,", pn, va->short_name);
    else
      fprintf(stderr, "%s,", va->short_name);
  }
  /* filter out alt names with dashes */
  if(va->alt_name && !strchr(va->alt_name, '|') &&
     !strchr(va->alt_name, '-') &&
     strcmp(va->alt_name, va->long_name))
    fprintf(stderr, " U_%s_%s = U_%s_%s,", pn, va->alt_name, pn, va->long_name);
  if(va->alt_name2 && !strchr(va->alt_name2, '|') &&
     !strchr(va->alt_name2, '-') &&
     strcmp(va->alt_name, va->long_name) && strcmp(va->alt_name, va->alt_name2))
    fprintf(stderr, " U_%s_%s = U_%s_%s,", pn, va->alt_name2, pn, va->long_name);
  fputc('\n', stderr);
}
puts("};");
fprintf(stderr, "\tU_NUM_%s\n} uni_%s_t;\n", pn, pn);
/* now, print valueof table */
fprintf(stderr, "extern const uni_valueof_t uni_%s_valueof[];\n", pn);
printf("const uni_valueof_t uni_%s_valueof[] = {\n", pn);
for(j = 0; j < enum_vals_len[i]; j++) {
  const char *name = val_aliases[i][enum_vals[i][j].val].short_name;
  if(isdigit(*name))
    name = val_aliases[i][enum_vals[i][j].val].long_name;
  printf("\t{ \"%s\", U_%s_%s }%s\n", enum_vals[i][j].name, pn, name,
	 j < enum_vals_len[i] - 1 ? "," : "");
}
puts("};");
fprintf(stderr, "#define uni_%s_valueof_len %d\n", pn, enum_vals_len[i]);
@

There is still one nagging issue:  the gc aliases include some that
are actually combinations of the others.  Since each value can only
hold one value, this is not going to work.  To support this, an extra
table is generated which translates a value into a 64-bit mask that
includes all base values.  For the base values themselves, only their
own bit will be set.  The pseudo values are recognized by a comment in
their last field, which is stored in one of the aliases.  This is the
reason the vertical bar was filtered out above:  all of the aliases
have vertical bars in their last field.

<<Generate C code for enumeration constants>>=
if(!strcmp(pn, "gc")) {
  fputs("extern const uint64_t uni_gc_trans[];\n", stderr);
  puts("const uint64_t uni_gc_trans[] = {");
  for(j = 0; j < num_val_aliases[i]; j++) {
    const char *desc;
    printf("\t/* %2s */ ", val_aliases[i][j].short_name);
    desc = val_aliases[i][j].alt_name;
    if(desc && !strchr(desc, '|')) {
      desc = val_aliases[i][j].alt_name2;
      if(desc && !strchr(desc, '|'))
        desc = NULL;
    }
    if(!desc)
      printf("1ULL << U_gc_%s", val_aliases[i][j].short_name);
    else {
      const char *desce;
      while(1) {
        desce = strchr(desc, ' ');
	if(!desce)
	  desce = desc + strlen(desc);
	printf("(1ULL << U_gc_%.*s)%s", (int)(desce - desc), desc, *desce ? " | " : "");
	if(!*desce)
	  break;
	desc = desce + 3; /* past ' | ' */
      }
    }
    if(j < num_val_aliases[i] - 1)
      putchar(',');
    putchar('\n');
  }
  puts("};");
}
@

Now that the aliases have been taken care of, it's time to read in the
enumeration properties themselves.  As with the boolean types, a range
table will be built first, and then converted to a multi-level table
when finished.

<<Property parsed contents>>=
uni_chrrng_dat_t *rng_dat;
uint8_t def;
@

<<UCD parser local functions>>=
static void add_enum_rng(prop_t *p, uint32_t low, uint32_t high, uint8_t val)
{
  if(p->def == val)
    return;
  if(!p->max_rng_len)
    inisize(p->rng_dat, (p->max_rng_len = 8));
  if(p->rng_len && p->rng_dat[p->rng_len - 1].high == low - 1 &&
     p->rng_dat[p->rng_len - 1].dat == val)
    p->rng_dat[p->rng_len - 1].high = high;
  else {
    if(p->rng_len == p->max_rng_len)
      resize(p->rng_dat, (p->max_rng_len *= 2));
    p->rng_dat[p->rng_len].low = low;
    p->rng_dat[p->rng_len].high = high;
    p->rng_dat[p->rng_len].dat = val;
    ++p->rng_len;
  }
}
@

<<UCD parser local functions>>=
static uint8_t enum_val(int pno, const char *v)
{
  char *s = strdup(v), *t, *d;
  uni_valueof_t me, *vp;
  me.name = s;

  d = t = s;
  for(; *t; t++) {
    if(isupper(*t))
      *d++ = tolower(*t);
    else if(*t != '_' && *t != '-' && !isspace(*t))
      *d++ = *t;
  }
  *d = 0;
  vp = bsearch(&me, enum_vals[pno], enum_vals_len[pno], sizeof(me),
               cmp_valueof);
  /* permit excess prefix of "is" */
  if(!vp && s[0] == 'i' && s[1] == 's' && s[2]) {
    me.name = s + 2;
    vp = bsearch(&me, enum_vals[pno], enum_vals_len[pno], sizeof(me),
                 cmp_valueof);
  }
  free(s);
  if(!vp) {
    perror(v);
    exit(1);
  }
  return vp->val;
}
@

<<UCD parser local definitions>>=
#define decl_enum(n, d) \
  int prop_##n = -1; \
  const char *def_##n = d
#define add_enum(n, v) do { \
  if(prop_##n < 0) { \
    prop_##n = add_prop(#n); \
    parsed_props[prop_##n].def = def_##n ? enum_val(prop_##n, def_##n) : 0; \
  } \
  if(*v) { \
    add_enum_rng(&parsed_props[prop_##n], low, high, enum_val(prop_##n, v)); \
  } \
} while(0)
#define add_num(n, val) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(isdigit(*val)) \
    add_enum_rng(&parsed_props[prop_##n], low, high, strtol(val, NULL, 0)); \
  else \
    add_enum(n, val); \
} while(0)
@

First, [[UnicodeData.txt]] has a few fields.  Field 3 is gc.  Field 4
is ccc.  Field 5 provides bc. Field 6 provides dt, albeit indirectly.
Fields 7 through 9 provide nt, even more indirectly.

<<Initialize UCD files>>=
decl_enum(gc, "Cn");
decl_enum(ccc, 0);
decl_enum(bc, "L"); /* default val depends on block */
                    /* probably ought to use extracted/DerivedBidiClass.txt */
decl_enum(dt, "None");
decl_enum(nt, "None");
@

<<Process a line of [[UnicodeData.txt]]>>=
add_enum(gc, fields[2]);
add_num(ccc, fields[3]);
add_enum(bc, fields[4]);
if(fields[5][0] == '<') {
  char *eval = fields[5] + 1;
  while(*eval && *eval != '>')
    eval++;
  if(!*eval) {
    perror("dt");
    exit(1);
  }
  *eval = 0;
  add_enum(dt, fields[5] + 1);
  *eval = '>';
} else if(fields[5][0])
  add_enum(dt, "can");
if(fields[6][0])
  add_enum(nt, "decimal");
else if(fields[7][0])
  add_enum(nt, "digit");
else if(fields[8][0])
  add_enum(nt, "numeric");
@

The other file that has mixed field types is
[[DerivedNormalizationProps.txt]].  The Boolean parser simply looked
at the number of fields, and added if the number of fields was too low
to be boolean.  This parser can't really do the same, without first
loading the expected default values.  There are only four properties,
so it's safer to just to them by hand.  In fact, two of them can only
have two values: Yes and No.  This makes them boolean in my view, so
that they will become.  There is one other caveat:  the default value
for those two is Yes, and the listed value is always No, so they need
to be inverted when finished.

<<Initialize UCD files>>=
decl_enum(NFC_QC, "Y");
decl_bool(NFD_QC);
decl_enum(NFKC_QC, "Y");
decl_bool(NFKD_QC);
@

<<Process a line of [[DerivedNormalizationProps.txt]]>>=
if(num_fields == 4) {
  if(!strcmp(fields[1], "NFC_QC"))
    add_enum(NFC_QC, fields[2]);
  else if(!strcmp(fields[1], "NFD_QC"))
    add_bool(NFD_QC);
  else if(!strcmp(fields[1], "NFKC_QC"))
    add_enum(NFKC_QC, fields[2]);
  else if(!strcmp(fields[1], "NFKD_QC"))
    add_bool(NFKD_QC);
}
@

<<Parse UCD files>>=
uni_chrrng_t *new;
uint32_t new_len;
uni_chrrng_invert(parsed_props[prop_NFD_QC].rng,
                  parsed_props[prop_NFD_QC].rng_len, &new, &new_len);
free(parsed_props[prop_NFD_QC].rng);
parsed_props[prop_NFD_QC].rng = new;
parsed_props[prop_NFD_QC].rng_len = new_len;
uni_chrrng_invert(parsed_props[prop_NFKD_QC].rng,
                  parsed_props[prop_NFKD_QC].rng_len, &new, &new_len);
free(parsed_props[prop_NFKD_QC].rng);
parsed_props[prop_NFKD_QC].rng = new;
parsed_props[prop_NFKD_QC].rng_len = new_len;
@

Then there are a number of files that describe just one enumerated
property.  Many of these follow here.

<<Initialize UCD files>>=
decl_enum(sc, "Zzzz");
decl_enum(blk, "No_Block");
decl_enum(hst, "NA");
decl_enum(lb, "XX");
decl_enum(GCB, "XX");
decl_enum(SB, "XX");
decl_enum(WB, "XX");
decl_enum(ea, "N");
@

<<Parse UCD files>>=
open_f("Scripts.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(sc, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("Blocks.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(blk, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("HangulSyllableType.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(hst, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("LineBreak.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(lb, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("auxiliary/GraphemeBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
   add_enum(GCB, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("auxiliary/SentenceBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(SB, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("auxiliary/WordBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(WB, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("EastAsianWidth.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(ea, fields[1]);
}
fclose(f);
@

And there is one file that has two properties per line.

<<Initialize UCD files>>=
decl_enum(jt, "U"); /* note: Mn, Me, Cf are T */
decl_enum(jg, "No_Joining_Group");
@

<<Parse UCD files>>=
open_f("ArabicShaping.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(jt, fields[2]);
  add_enum(jg, fields[3]);
}
fclose(f);
@

Once finished, the tables can be dumped.  Just as with boolean
properties, only range tables have been built, so it's time to
generate the multi-level table, assuming that the range data is correct.

<<Common uni definitions>>=
uint32_t *rng_dat_to_multi(const uni_chrrng_dat_t *tab, uint32_t tab_len,
                           uint32_t *ml_len, uint8_t def);
@

<<Common uni functions>>=
uint32_t *rng_dat_to_multi(const uni_chrrng_dat_t *tab, uint32_t tab_len,
                           uint32_t *ml_len, uint8_t def)
{
  uint32_t low, high, len, i;
  uint32_t *ml;
  uint8_t *bits;

  /* degenerate case:  always out-of-range */
  if(!tab_len) {
    bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  low = tab[0].low;
  high = tab[tab_len - 1].high;
  len = high - low + 1;
  inisize(bits, len);
  /* FIXME: only set def on unspecified ranges; may be faster */
  memset(bits, def, len);
  for(i = 0; i < tab_len; i++)
    memset(bits + tab[i].low - low, tab[i].dat, tab[i].high - tab[i].low + 1);
  bits_to_multi(bits, len, low, high, 0, 0, &ml, ml_len);
  free(bits);
  return ml;
}
@

<<UCD parser local functions>>=
static void fixup_rng_dat(prop_t *p)
{
  uint32_t i;
  qsort(p->rng_dat, p->rng_len, sizeof(uni_chrrng_dat_t), uni_cmprng_dat);
  for(i = 0; i < p->rng_len - 1; i++)
    if(p->rng_dat[i + 1].low == p->rng_dat[i].high - 1 &&
       p->rng_dat[i + 1].dat == p->rng_dat[i].dat) {
      p->rng_dat[i].high = p->rng_dat[i + 1].high;
      if(i + 2 < p->rng_len) {
        memmove(p->rng_dat + i + 1, p->rng_dat + i + 2,
	        (p->rng_len - (i + 2)) * sizeof(uni_chrrng_dat_t));
        p->rng_len--;
      }
    }
}
@

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng_dat) {
    fixup_rng_dat(&parsed_props[i]);
    parsed_props[i].mt = rng_dat_to_multi(parsed_props[i].rng_dat, parsed_props[i].rng_len,
                                          &parsed_props[i].mt_len, parsed_props[i].def);
}
@

In addition to the property tables, a simple query function is
printed.  This calls a generic search function using the multi-level
table.

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng_dat) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    printf("const uni_chrrng_dat_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].rng_len; j++)
      printf("\t{ %d, %d, %d }%s\n", parsed_props[i].rng_dat[j].low,
                                     parsed_props[i].rng_dat[j].high,
				     (int)parsed_props[i].rng_dat[j].dat,
				     j < parsed_props[i].rng_len - 1 ? "," : "");
    puts("};");
    fprintf(stderr, "#define uni_%s_rng_len %d\n", name,
                                                   parsed_props[i].rng_len);
    printf("const uint32_t uni_%s_mtab[] = {\n\t", name);
    for(j = 0; j < parsed_props[i].mt_len; j++)
      printf("0x%X%s", parsed_props[i].mt[j], j == parsed_props[i].mt_len - 1 ?
                                                     "\n" : !((j + 1) % 8) ?
						           ",\n\t" : ", ");
    puts("};");
    fprintf(stderr, "extern const uni_chrrng_dat_t uni_%s_rng[];\n"
		    "extern const uint32_t uni_%s_mtab[];\n"
		    "#define uni_%s_of(x) uni_x_of(x, uni_%s_mtab, %d)\n",
		    name, name, name, name, parsed_props[i].def);
  }
}
@

<<Common uni definitions>>=
int uni_x_of(uint32_t cp, const uint32_t *tab, uint8_t def);
@

<<Common uni functions>>=
int uni_x_of(uint32_t cp, const uint32_t *tab, uint8_t def)
{
  const uint8_t *mr;
  uint8_t mv = multi_tab_lookup(tab, cp, &mr, 0);
  if(mr)
    mv = *mr;
  else if(cp < (tab[0] << 1) || cp > tab[1])
    mv = def;
  return mv;
}
@

\subsection{Numeric Properties}

Numeric properties are a subset of enumerated properties.  The
property's primary values are integers or two dot-separated integers
(i.e., floating point values).  They may have non-numeric values and
aliases as well.  The main difference is that the sloppy matching
should also support matching the plain integer against any like-valued
integer (e.g., 01 matches 1) and the dotted value against any
like-valued floating point number (e.g., 01.10 matches 1.1).  In
addition, the age property is meant to match equal to or less than the
search value.  For example, 3.0 matches 1.1 as well.

\subsection{String Properties}

String properties are those which have a value.  Technically, boolean
properties have the string values representing true and false, but
they are never considered this way.  Useful operations include:

\begin{itemize}
\item Find out the string-valued value \emph{of} the property for a
character.  For variable-length strings, this should support two-part
retrieval: first obtain the length, and then obtain the string.  If
there is a maximum length, it should be listed so that the sizing step
can be avoided.
\end{itemize}

The storage methods used for boolean properties can be used for
strings as well, with some adjustments.  One way to store the string
would be as a pointer to a zero-terminated string.  However, this
requires computing string length at every access.  A better way would
be to store a pointer and a length.  For 64-bit systems, this would
require at least 8 bytes for the pointer, and probably another 8 bytes
to align the structure, so 4 bytes for the code point and 4 bytes for
the length.  However, a more efficient storage method would be to use
a 32-bit pointer into a single string containing all possible values,
reducing storage by 4 bytes.  Additional savings can be had if the
string size can be kept below 64K, in which case 16-bit integers can
represent the offset and length.  The length could be stored in the
string as well, eliminating the need for a separate length, and
generally reducing the space required for that to a byte.  Since fewer
than 24 bits are used for the code point, the length could be stored
in the code point's lower byte as well; masking would be required to
test equality, though.

Since lookup (\emph{of}) is the only supported operation, the
multi-level table is probably the appropriate data structure for all
string-valued properties.  The routine to split tables can be given a
minimum width for the lowest level, which can be set to the width of
the string descriptor (32 bits for a 16-bit offset and length, for
example).

From UnicodeData.txt, we have case mappings and decomposition
mappings.  For normalization, the composition mapping needs to be
inverted as well, and the composition tables need to be pre-executed
as much as possible.

<<Common uni definitions>>=
typedef struct {
  int32_t cp; /* can use sign bit for extra storage */
              /* could also add a byte by making this :24 or smaller */
  uint32_t data; /* have whole word available here */
} uni_cp_dat_t;
@

<<Known Data Types>>=
uni_cp_dat_t,%
@

<<Common uni functions>>=
const uni_cp_dat_t *find_cp_dat(uint32_t cp, const uni_cp_dat_t *tab,
                                uint32_t tab_len)
{
#if 0
  uni_cp_dat_t d = {cp};
  return (uni_cp_dat_t *)bsearch(&d, tab, tab_len, sizeof(uni_cp_dat_t),
                                 uni_cmp_cp);
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].cp)
      h = j - 1;
    else if(cp > tab[j].cp)
      l = j + 1;
    else
      return &tab[j];
  }
  return 0;
#endif
}
@

Here is a test program which compares the performance of several tables.

<<C Test Executables>>=
tsttab \
@

<<makefile.rules>>=
tsttab.o: uni_chartype.gen.h
@

<<tsttab.c>>=
<<Common C Header>>

#include "uni_chartype.h"
// static_proto

#include <sys/resource.h>
struct rusage ru;

static void tstart(void)
{
  getrusage(RUSAGE_SELF, &ru);
}

static unsigned long tend(void)
{
  struct rusage ru2;
  getrusage(RUSAGE_SELF, &ru2);
  return (ru2.ru_utime.tv_sec - ru.ru_utime.tv_sec) * 1000000 +
            (ru2.ru_utime.tv_usec - ru.ru_utime.tv_usec);
}

static void doit_dat(const char *name, const uni_chrrng_dat_t *rng, uint32_t nent,
                     const uint32_t *mtab, uint8_t def)
{
    uint32_t i;

    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      uint8_t b = find_cp_chrrng(i, rng, nent, def);
      const uint8_t *mr;
      uint8_t mv = multi_tab_lookup(mtab, i, &mr, def);
      if(mr)
        mv = *mr;
      if(b != mv) {
        fprintf(stderr, "mismatch %s@%d %d %d %p\n", name, i, (int)b, (int)mv, mr);
	/* exit(1); */
	return;
      }
    }
    /* check performance */
    int j;
    unsigned long tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        find_cp_chrrng(i, rng, nent, def);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++) {
        const uint8_t *mr;
        uint8_t mv = multi_tab_lookup(mtab, i, &mr, def);
        if(mr)
          mv = *mr;
      }
    tt = tend();
    printf("%s r%ld t%ld\n", name, tr, tt);
}

static void doit_bool(const char *name, const uni_chrrng_t *rng, uint32_t nent,
                      const uint32_t *mtab)
{
    uint32_t i;
    
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      int res = is_cp_chrrng(i, rng, nent);
      uint8_t mv = is_uni_x(i, mtab);
      if(!mv != !res) {
        fprintf(stderr, "mismatch %s@%d %d %d\n", name, i, res, mv);
	/*  exit(1); */
	return;
      }
    }
    /* check performance */
    volatile int res;
    int j;
    unsigned long tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        res = is_cp_chrrng(i, rng, nent);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        res = is_uni_x(i, mtab);
    tt = tend();
    printf("%s r%ld t%ld\n", name, tr, tt);
}

int main(void)
{
#define dat(x, d) doit_dat(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab, d)
#define bool(x) doit_bool(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab)
  <<Convert ranges to tables>>
  return 0;
}
@

<<Convert ranges to tables>>=
#if 0
bool(AHex);
bool(Alpha);
bool(Bidi_C);
bool(Bidi_M);
bool(CI);
bool(Cased);
bool(CWCF);
bool(CWCM);
bool(CWL);
bool(CWKCF);
bool(CWT);
bool(CWU);
bool(CE);
bool(Dash);
bool(DI);
bool(Dep);
bool(Dia);
bool(Ext);
bool(Comp_Ex);
bool(Gr_Base);
bool(Gr_Ext);
bool(Hex);
bool(IDSB);
bool(IDST);
bool(IDC);
bool(IDS);
bool(Ideo);
bool(Join_C);
bool(LOE);
bool(Lower);
bool(Math);
bool(NChar);
bool(Pat_Syn);
bool(Pat_WS);
bool(QMark);
bool(Radical);
bool(STerm);
bool(SD);
bool(Term);
bool(UIdeo);
bool(Upper);
bool(VS);
bool(WSpace);
bool(XIDC);
bool(XIDS);
bool(ASSIGNED);
#endif
#if 0
dat(bc, U_bc_L);
dat(ccc, 0);
dat(dt, U_dt_None);
dat(gc, U_gc_Cn);
dat(nt, U_nt_None);
dat(NFC_QC, U_NFC_QC_Y);
dat(NFKC_QC, U_NFKC_QC_Y);
#endif
bool(NFD_QC);
bool(NFKD_QC);
#if 0
dat(sc, U_sc_Zzzz);
dat(blk, U_blk_No_Block);
dat(hst, U_hst_NA);
dat(lb, U_lb_XX);
dat(GCB, U_GCB_XX);
dat(SB, U_SB_XX);
dat(WB, U_WB_XX);
dat(ea, U_ea_N);
dat(jg, U_jg_No_Joining_Group);
dat(jt, U_jt_U);
#endif
@

Another possibility would be to use more complex data structures.  I
doubt I could come up with a more space-efficient storage method than
the sorted range table, but some are much better at operations like
lookup, scanning, or modification.  An appropriately designed hash
table could be optimized off-line to provide even faster lookups, at
the cost of fragility due to the hash algorithm probably depending on
a particular Unicode release.  Prefix trees (tries, if you prefer)
have good theoretical performance, and in fact the multi-level table
is just a prefix tree of sorts.  It and many other faster tree
approaches achieve their performance by assuming a constant time
parent-to-child traversal with a large set of children, which once
again equates to large storage penalties.  I have reduced the penalty
somewhat in my own implementation by using smaller pointers, at the
expense of slowing the lookup down a bit.

\section{Unicode Normalization}

After reading raw Unicode, it needs to be normalized.

<<Library [[uni]] Members>>=
uni_norm.o
@

<<Library [[uni]] headers>>=
#include "uni_norm.h"
@

<<uni_norm.h>>=
<<Common C Warning>>
#ifndef UNI_NORM_H
#define UNI_NORM_H

#include "uni_common.h"
<<Unicode normalization support exports>>
#endif /* UNI_NORM_H */
@

<<uni_norm.c>>=
<<Common C Header>>
#include "uni_norm.h"

<<Unicode normalization support local definitions>>

<<Unicode normalization support functions>>
@

The UCD contains files which can be processed to obtain the tables
needed for normalization.  In particular, UnicodeData.txt, field 6
contains the decomposition information, and field 4 contains the
canonical ordering information.  Case folding information is
scattered, but combined into the DerivedNormalizationProps.txt file.
The latter file also contains information regarding valid
compositions.

\lstset{language=make}
<<makefile.vars>>=
UNIDATA = $(UCD_LOC)/UnicodeData.txt
UNINORM = $(UCD_LOC)/DerivedNormalizationProps.txt
@

The canonical ordering information is a table of all reorderable code
points, along with their canonical combining class (the number
indicating its sort order) ([[unirng_ccc]]).  My original implementation
used a [[uni_cp_dat_t]]-like structure, but upon further examination,
it appears that there is enough redundancy to justify a range table or
multi-level table.

The decomposition information is a table of decomposable source code
points and their destination strings, sorted by source code point for
binary searching.  The canonical composition information consists of
all pairs of composable code points along with their composed form,
sorted by source code points for binary searching.

The decompose and compose tables are long, and the decompose
destination strings can vary from zero to over thirty characters.  In
order to keep the tables from being both long and wide, an auxiliary
table ([[deccp]]) is used to store the actual decomposition strings.
The decomposition table then just stores the offset into the string
table of the first character, and the length of the string.  Using
short integers for the length and offset further reduces the width of
the composition table by 4 bytes.

As another space-saving measure, the canonical and compatibility
decomposition tables are shared ([[decomp]]).  This is because
compatibility decomposition is a superset of canonical decomposition.
A negative length indicates compatibility decomposition.  However, the
canonical composition table only operates on pairs, and only one flag
can be contained in a sign bit anyway, so the canonical composition
table ([[cancomp]]) is separate.  Case folding is too different as
well, so it is also separate ([[nfkc_cf]]).

\lstset{language=C}
<<Unicode normalization support exports>>=
/* Raw info */

/* for canonical/compatibility decomposition: */
/*  off is offset in deccp */
/*  |len| is length in deccp; len is negative if compatibility */
/* stored in cp_dat_t uint32 */
typedef struct {
  int cp;
  uint16_t off;
  int16_t len;
} uni_decent_t;
/* The decomposition strings */
extern const int deccp[];
@

<<Known Data Types>>=
uni_decent_t,%
@

As a convention, the generated file is the same as the C file which
uses it, but with a [[.gen]] extension.  The generated code creates
static variables where possible, to keep the information contained to
the C file that uses it.

<<Unicode normalization support local definitions>>=
#include "uninorm.gen"
@

\lstset{language=make}
<<makefile.rules>>=
uni_norm.o: uninorm.gen
@

<<Plain Built Files>>=
*.gen \
@

Functions are then provided to look up this information.  To look up
the canonical or compatibility decomposition, the [[decomp]] table is
binary searched, and the sign of the length is taken into account.
Rather than hide the [[deccp]] table and make the function call more
complicated, the [[decent]] is returned raw.  The same is true of any
other function that returns variable-length information from [[deccp]].

\lstset{language=C}
<<Unicode normalization support exports>>=
const uni_decent_t *find_decomp(int cp, int canon);
@

<<Unicode normalization support functions>>=
const uni_decent_t *find_decomp(int cp, int canon)
{
    uni_decent_t *dret;
    uint32_t tab_len = sizeof(decomp)/sizeof(*decomp);
    dret = (uni_decent_t *)find_cp_dat(cp, (uni_cp_dat_t *)decomp, tab_len);
    if(!dret)
        return NULL;
    if(canon && dret > decomp && dret[-1].cp == cp)
        return dret - 1;
    if(!canon && dret < decomp + tab_len - 1 &&
       dret[1].cp == cp)
        return dret + 1;
    if(canon && dret->len < 0)
        return NULL;
    return dret;
}
@

The case folding table is independent, so it's just a simple binary
search.

<<Unicode normalization support exports>>=
/* same as above, but only returns nfkc_cf, and len is always positive */
const uni_decent_t *find_nfkc_cf(int cp);
@

<<Unicode normalization support functions>>=
const uni_decent_t *find_nfkc_cf(int cp)
{
  return (uni_decent_t *)find_cp_dat(cp, (uni_cp_dat_t *)nfkc_cf,
                                     sizeof(nfkc_cf)/sizeof(*nfkc_cf));
}
@

The canonical combining class table only holds one value, so the
structure is private and the function returns the actuall ccc.  The
ccc is zero for anything not in the table.

<<Unicode normalization support exports>>=
/* the ccc for canonical ordering and composition */
int find_ccc(int cp);
@

<<Unicode normalization support functions>>=
int find_ccc(int cp)
{
  return find_cp_chrrng(cp, unirng_ccc, sizeof(unirng_ccc)/sizeof(*unirng_ccc), 0);
}
@

Canonical composition is a lookup on a pair.  For convenience, you can
look up just one character to see if a pair might be coming.  Since
the table is independent, the function is pretty simple.

<<Unicode normalization support exports>>=
/* given a pair, give canonical composition */
int find_cancomp(int cp, int cp2);
/* see if cp might be first member of a composition pair */
int is_cancomp1(int cp);
@

<<Unicode normalization support local definitions>>=
static int cmp_comp(const void *a, const void *b)
{
    int *_a = (int *)a;
    uni_decent_t *_b = (uni_decent_t *)b;
    int acp1, acp2, bcp1, bcp2;

    acp1 = _a[0];
    acp2 = _a[1];
    bcp1 = deccp[_b->off];
    bcp2 = deccp[_b->off + 1];
    if(acp1 == bcp1)
        return acp2 - bcp2;
    else
        return acp1 - bcp1;
}
@

<<Unicode normalization support functions>>=
int find_cancomp(int cp, int cp2)
{
    int de[2] = {cp, cp2};
    uni_decent_t *res;
    /* FIXME: change to manual bsearch */
    res = bsearch(&de, cancomp, sizeof(cancomp)/sizeof(*cancomp),
                  sizeof(*cancomp), cmp_comp);
    if(res)
        return res->cp;
    else
        return -1;
}
@

<<Unicode normalization support local definitions>>=
static int cmp_comp1(const void *a, const void *b)
{
    uni_decent_t *_a = (uni_decent_t *)a, *_b = (uni_decent_t *)b;
    int acp1, bcp1;

    acp1 = _a->cp;
    bcp1 = deccp[_b->off];
    return acp1 - bcp1;
}
@

<<Unicode normalization support functions>>=
int is_cancomp1(int cp)
{
    uni_decent_t de = {cp};
    /* FIXME: change to manual bsearch */
    return bsearch(&de, cancomp, sizeof(cancomp)/sizeof(*cancomp),
                   sizeof(*cancomp), cmp_comp1) ? 1 : 0;
}
@

The raw functions are not meant to be used directly.  Instead, here
are some more high-level functions.  The first step in any
normalization is to decompose.  The only difference in the three
functions is what table they use, and whether or not they support
negative lengths.

<<Unicode normalization support exports>>=
/* wrappers */

/* decomp functions can operate on any number of chars, including 1 */
/* decomp functions return final length */
int NFD_dec(int *buf, int blen);
int NFKD_dec(int *buf, int blen);
/* NFKC_Casefold assumes NFD has already been run */
int NFKC_Casefold(int *buf, int blen);
@

<<Unicode normalization support functions>>=
#define any_dec(n, fn, len) \
  int n(int *buf, int blen) \
  { \
    int i; \
    for(i = 0; i < blen; i++) { \
      const uni_decent_t *de = fn; \
      if(de) { \
        int l = len; \
        memmove(buf + i + l, buf + i + 1, (blen - i - 1)*sizeof(int)); \
        blen += l - 1; \
        memcpy(buf + i, deccp + de->off, l * sizeof(int)); \
        /* result is fully expanded, so skip it */ \
        i += l - 1; \
      } \
    } \
    return blen; \
  }
any_dec(NFKD_dec, find_decomp(buf[i], 0), de->len < 0 ? -de->len : de->len)
any_dec(NFD_dec, find_decomp(buf[i], 1), de->len)
any_dec(NFKC_Casefold, find_nfkc_cf(buf[i]), de->len)
@

Canonical ordering is the second step in any normalization.  This
requires that any consecutive characters with non-zero canonical
combining class be ordered by their canonial combining class.  The
procedure is described as a bubble sort, so the sort must be stable.
If the last character passed in has a non-zero ccc, there may be more,
so a continuation is requested.

<<Unicode normalization support exports>>=
/* canon order function may need more chars than passed in */
/* set last to 1 if no continuations possible */
/* returns less than blen if more chars needed */
/* can skip # of chars equal to return value */
int Canon_Order(int *buf, int blen, int last);
@

<<Unicode normalization support local definitions>>=
/* for stable sort */
struct ccs {
    int cp, ccc;
    int opos;
};

static int cmpcc(const void *a, const void *b)
{
    const struct ccs *p1 = (const struct ccs *)a;
    const struct ccs *p2 = (const struct ccs *)b;
    if(p1->ccc != p2->ccc)
        return p1->ccc - p2->ccc;
    return p1->opos - p2->opos;
}
@

<<Unicode normalization support functions>>=
/* pass in last=1 if string is whole */
/* otherwise, if return is < blen, needs more chars */
/* but can safely skip return-val chars */
int Canon_Order(int *buf, int blen, int last)
{
    int i;
    for(i = 0; i < blen; i++) {
        int cc = find_ccc(buf[i]);
        if(cc) {
            int j;
            for(j = i + 1; j < blen; j++) {
                int cc2 = find_ccc(buf[j]);
                if(!cc2)
                    break;
            }
            if(j == blen && !last)
                return i;
            struct ccs ccbuf[j-i];
            ccbuf[0].ccc = cc;
            ccbuf[0].cp = buf[i];
            ccbuf[0].opos = i;
            int k;
            for(k = i + 1; k < j; k++) {
                ccbuf[k-i].ccc = find_ccc(buf[k]);
                ccbuf[k-i].cp = buf[k];
                ccbuf[k-i].opos = k;
            }
            qsort(ccbuf, j - i, sizeof(*ccbuf), cmpcc);
            for(k = i; k < j; k++)
                buf[k] = ccbuf[k-i].cp;
        }
    }
    return blen;
}
@

The last step in any composition normalization is canonical
composition.  Two characers may be combined if the first character has
a canonical combining class of zero and all intervening characters
have a non-zero canonical combining class less than the canonical
combining class of the second character.

<<Unicode normalization support exports>>=
/* canon comp function may need more chars than passed in */
/* set nok to non-NULL if continuation possible */
/* returns nok < blen if more chars needed */
/* can skip nok chars */
/* return value is updated length */
int NFC_comp(int *buf, int blen, int *nok);
@

<<Unicode normalization support functions>>=
int NFC_comp(int *buf, int blen, int *nok)
{
  int i, last0 = -1;
  for(i = 0; i < blen; i++) {
    int ccc = find_ccc(buf[i]);
    /* first char must be ccc=0 */
    if(ccc)
      continue;
    /* could also skip if !is_cancomp1(buf[i]) */
    last0 = i;
    int lastccc = 0;
    int j;
    for(j = i + 1; j < blen; j++) {
      ccc = find_ccc(buf[j]);
      /* second char must have no intervening equal ccc */
      /* this assumes canonical ordering has been done */
      if(ccc && lastccc && ccc == lastccc)
        continue;
      /* or greater ccc; this only happens if back down to 0 */
      if(!ccc && lastccc) {
        i = j - 1;
        break;
      }
      lastccc = ccc;
      int ccp = find_cancomp(buf[i], buf[j]);
      if(ccp >= 0) {
        /* composition replaces char 1 and deletes char 2 */
        buf[i] = ccp;
        memmove(buf + j, buf + j + 1, (blen - j - 1)*sizeof(int));
        blen--;
	/* start over at first potential 2nd char */
        j = i;
        lastccc = 0;
        continue;
      }
      /* if no match and hit a ccc=0, try next 1st char */
      if(!ccc) {
        i = j - 1;
        break;
      }
    }
  }
  if(nok)
    /* need more if we got a potential start char */
    /* but don't need to look at anything before that char */
    *nok = last0 >= 0 && is_cancomp1(buf[last0]) ? last0 : blen;
  return blen;
}
@

To demonstrate proper normalization, here are some macros which do the
full procedure.

<<Unicode normalization support exports>>=
#define NFKD(buf, blen) Canon_Order(buf, NFKD_dec(buf, blen), 1)
#define NFD(buf, blen) Canon_Order(buf, NFD_dec(buf, blen), 1)
#define NFKC(buf, blen) NFC_comp(buf, NFKD(buf, blen), NULL)
#define NFC(buf, blen) NFC_comp(buf, NFD(buf, blen), NULL)
@

To actually generate the tables, a C program ([[mkuninorm]]) is used.  A
simple table generator could be done using shell and sed, but there
are some caveats that make the C program much better suited to the
task.  Rather than compile in the file locations, they are supplied on
the command line.

\lstset{language=make}
<<C Build Executables>>=
mkuninorm \
@

<<makefile.rules>>=
uninorm.gen: mkuninorm $(UNIDATA) $(UNINORM)
	./$< $(UNIDATA) $(UNINORM) >$@

mkuninorm: mkuninorm.o
	$(CC) $(CFLAGS) $(LDFLAGS) -o $@ $^
@

\lstset{language=C}
<<mkuninorm.c>>=
<<Common C Header>>

<<[[mkuninorm]] globals>>

int main(int argc, const char **argv)
{
  if(argc != 3) {
    fprintf(stderr, "Usage: %s path/to/UnicodeData.txt"
                       " path/to/DerivedNormalizationProps.txt\n",
            argv[0]);
    exit(1);
  }
    const char *unif = argv[1];
    const char *normf = argv[2];
  <<Make [[uninorm.gen]]>>
  return 0;
}
@

In order to save a little space on the [[deccp]] table, all entries
are read in, sorted, and combined if possible.  Thus the information
is stored in an expandable master decomposition table ([[dec]]) while
being read in.  A [[type]] field indicates which table it eventually
goes to.

<<[[mkuninorm]] globals>>=
typedef struct {
    int cp;
    uint32_t off;
    unsigned char len;
    unsigned char type;
    /* 0 = composition */
    /* 1 = canonical decomposition */
    /* 2 = compatibility decomposition */
    /* 3 = NFKC_CF */
} uni_decent_t;
uni_decent_t *dec;
int ndec = 0, maxdec;
@

<<Known Data Types>>=
uni_decent_t,%
@

<<Make [[uninorm.gen]]>>=
maxdec = 128;
inisize(dec, maxdec);
@

[[mkuninorm]] also has its own private [[deccp]] equivalent expandable
table ([[decs]]).

<<[[mkuninorm]] globals>>=
int *decs;
int ndecs = 0, maxdecs;
@

<<Make [[uninorm.gen]]>>=
maxdecs = 128;
inisize(decs, maxdecs);
@

For a first pass, the information provided by UnicodeData.txt is read.
This includes the ccc (field 4) and the non-casefolding decomposition
(field 6).  In order to create the composition table at the same time,
the DerivedNormalizationInfo.txt is read at the same time, only to
extract the Full Composition Exclusion information.

<<[[mkuninorm]] globals>>=
/* longest line is actually 218 chars */
char lbuf[512];
@

<<Make [[uninorm.gen]]>>=
FILE *f, *f2;
if(!(f = fopen(unif, "r"))) {
  perror(unif);
  exit(1);
}
if(!(f2 = fopen(normf, "r"))) {
  perror(normf);
  exit(1);
}
<<Stuff to do before reading [[unif]]>>
while(fgets(lbuf, sizeof(lbuf), f)) {
  <<Process a line of [[unif]]>>
}
fclose(f);
fclose(f2);
@

There is one list that is not in any file: Hangul syllables.  These
are only described in the standard. A Hangul syllable consists of
three parts, called L, V, and T.  There are code points for each L, V,
and T, as well as every combination of LV and LVT.  While LVT can
technically be decomposed directly to L, V, and T, it is instead
decomposed to LV and T, so that canonical composition (which only
works on pairs) can be performed. Thus, all entries for Hangul
syllables are type 0 (canonical composition).  No check is made in
Full Composition Exception, since as of version 6, there are no
matches.  If a future version has matches, this will need to change.

<<Add Hangul syllables>>=
/* hangul syllables */
int L, V, T;
int cp = 0xac00;
for(L = 0; L < 19; L++)
  for(V = 0; V < 21; V++)
    for(T = 0; T < 28; T++, cp++) {
      if(ndec == maxdec) {
        maxdec *= 2;
        resize(dec, maxdec);
      }
      dec[ndec].cp = cp;
      dec[ndec].off = ndecs;
      dec[ndec].len = 2;
      /* assuming none of these will get added to F_C_E */
      dec[ndec].type = 0;
      if(ndecs + 2 > maxdecs) {
        maxdecs *= 2;
        resize(decs, maxdecs);
      }
      if(!T) {
        decs[ndecs++] = 0x1100 + L;
        decs[ndecs++] = 0x1161 + V;
      } else {
        decs[ndecs++] = 0xac00 + 28 * (L * 21 + V);
        decs[ndecs++] = 0x11a7 + T;
      }
      ndec++;
    }
@

All relevant information is stored, so there is no need to avoid
intermingling data.  First, the decomposition mapping field is
extracted and stored. Compatibility decompositions are denoted by a
reason in angle brackets; all others are canonical decompositions.  In
order to retain numerical order, the Hangul syllables are added at the
appropriate point.

<<Process a line of [[unif]]>>=
if(isspace(*lbuf) || *lbuf == '#') /* technically impossible */
  continue;
/* field 1: code point */
char *s = strchr(lbuf, ';');
if(!s)
  exit(1);
*s++ = 0;
/* field 2 */
s = strchr(s, ';');
if(!s)
  exit(1);
/* field 3 */
s = strchr(s + 1, ';');
if(!s)
  exit(1);
/* field 4 */
s = strchr(s + 1, ';');
if(!s)
  exit(1);
/* field 5 */
s = strchr(s + 1, ';');
if(!s)
   exit(1);
/* field 6: decomp */
char *deccp = s + 1;
s = strchr(deccp, ';');
if(!s)
  exit(1);
*s = 0;
if(!strcmp(lbuf, "AC00")) {
  <<Add Hangul syllables>>
}
/* store non-empty deccp for later processing */
while(isspace(*deccp))
  deccp++;
if(!*deccp)
  continue;
int iscomp = *deccp == '<'; /* compatibility */
if(iscomp) {
  /* skip compatibility reason */
  while(*++deccp && *deccp != '>');
  if(!*deccp)
    exit(1);
  while(isspace(*++deccp));
  if(!*deccp)
    exit(1);
}
/* count # of code points */
int len;
for(len = 0, s = deccp; *s; len++) {
  while(*s && !isspace(*s))
    s++;
  while(isspace(*s)) s++;
}
/* store a new decomp entry */
if(ndec == maxdec) {
  maxdec *= 2;
  resize(dec, maxdec);
}
dec[ndec].cp = strtol(lbuf, NULL, 16);
dec[ndec].off = ndecs;
dec[ndec].len = len;
dec[ndec].type = iscomp ? 2 : 1;
/* store decomp string */
if(ndecs + len > maxdecs) {
  maxdecs *= 2;
  resize(decs, maxdecs);
}
for(len = 0, s = deccp; *s && len < dec[ndec].len; len++) {
  decs[ndecs + len] = strtol(s, &s, 16);
  while(isspace(*s)) s++;
}
if(dec[ndec].len != len)
  exit(1);
ndecs += len;
@

Before leaving this new decomposition entry, it needs to be checked to
see if it is a canonical composition entry as well.  This requires
checking the Full Composition Exclusion flag.  This flag is listed
in code point order, as a set of ranges.  The most relevant range is
kept; once the range becomes irrelevant, a new range is read from the
file.

<<[[mkuninorm]] globals>>=
char lbuf2[512];
@

<<Stuff to do before reading [[unif]]>>=
int exl = -1, exh = -1;
@

<<Process a line of [[unif]]>>=
/* composition is always in pairs, so no point if len != 2 */
if(dec[ndec].type == 1 && len == 2) {
  /* load a new range if currently above range */
  while(dec[ndec].cp > exh) {
    exl = exh = 0x110000;
    while(fgets(lbuf2, sizeof(lbuf2), f2)) {
      if((s = strchr(lbuf2, '#')))
        *s = 0;
      if(!strstr(lbuf2, "; Full_Composition_Exclusion "))
       continue;
      exl = strtol(lbuf2, &s, 16);
      if(*s == '.' && s[1] == '.')
        exh = strtol(s + 2, NULL, 16);
      else
        exh = exl;
      break;
    }
  }
  if(dec[ndec].cp < exl)
    dec[ndec].type = 0;
}
@

<<Process a line of [[unif]]>>=
++ndec;
@

The case folding decomposition strings are fully expanded; that is,
any code point in the target string which might also be decomposed has
already been decomposed.  The entries pulled from UnicodeData.txt,
however, are not fully expanded.  Neither are the Hangul syllable
entries.  Having them fully expanded reduces processing at run-time,
so they will be expanded here.

Each entry is extracted, converted to a fully expanded form, and added
back.  The old entries are kept, in order, for further expansion, so
the old entry count is retained.

<<[[mkuninorm]] globals>>=
int decbuf[256];
@

<<Make [[uninorm.gen]]>>=
/* Fully exapnd entries added so far */
int ondec = ndec;
int i;
for(i = 0; i < ondec; i++) {
  int declen = dec[i].len;
  memcpy(decbuf, decs + dec[i].off, declen * sizeof(int));
  <<Fully expand [[dec[i]]]/[[decbuf]]>>
}
@

Type 0 expansions and type 1 expansions can be applied to type 0 and 1
expansions to form type 1 expansions.  Type 2 expansions can be
applied to type 0, 1, and 2 expansions to form type 2 expansions.
When a new expansion type is created, a new entry with duplicate code
point but new type is created.  The target type is stored in [[ntype]].

<<Fully expand [[dec[i]]]/[[decbuf]]>>=
int ntype = dec[i].type;
@

First, all type 0 and 1 expansions are applied.  The number of
canonical expansions applied is saved to indicate if anything was
done.  Since the initial segment of entries is in code point order,
expansions are searched on that list using a binary search.  Any
expansion changes the type to 1.

<<[[mkuninorm]] globals>>=
static int cmpdeccp(const void *a, const void *b)
{
    const uni_decent_t *d1 = a, *d2 = b;

    return d1->cp - d2->cp;
}
@

<<Fully expand [[dec[i]]]/[[decbuf]]>>=
int didex = 0;
/* apply canonical decomp only */
if(ntype < 2) {
  int j;

  for(j = 0; j < declen; j++) {
    int cp = decbuf[j];
    uni_decent_t de = {cp}, *mdec;
    /* FIXME: change to manual bsearch */
    if((mdec = bsearch(&de, dec, ondec, sizeof(*dec), cmpdeccp)) &&
       mdec->type < 2) {
      ++didex;
      ntype = 1;
      if(mdec->len != 1) {
        memmove(decbuf + j + mdec->len, decbuf + j + 1,
                (declen - (j + 1)) * sizeof(int));
        declen += mdec->len - 1;
      }
      if(mdec->len)
        memcpy(decbuf + j, decs + mdec->off, mdec->len * sizeof(int));
      /* redo newly added character(s) */
      j--;
    }
  }
}
int didex_canon = didex;
@

Next, any type of expansion is allowed.  This may perform additional
canonical expansion, but only if a compatibility expansion was done
first.  Thus, all exansions done in this phase are considered
compatibility expansions, and change the type to 2.  If a previous
expansion was already done, the result must be saved before changing
the type to 2.

<<Fully expand [[dec[i]]]/[[decbuf]]>>=
int j;

for(j = 0; j < declen; j++) {
  int cp = decbuf[j];
  uni_decent_t de = {cp}, *mdec;
    /* FIXME: change to manual bsearch */
  if((mdec = bsearch(&de, dec, ondec, sizeof(*dec), cmpdeccp))) {
    ++didex;
    if(didex_canon) {
      <<Add/replace fully expanded entry>>
       didex_canon = 0;
     }
     ntype = 2;
     if(mdec->len != 1) {
       memmove(decbuf + j + mdec->len, decbuf + j + 1,
               (declen - (j + 1)) * sizeof(int));
       declen += mdec->len - 1;
     }
     if(mdec->len)
       memcpy(decbuf + j, decs + mdec->off, mdec->len * sizeof(int));
      /* redo newly added character(s) */
     j--;
   }
}
@

Finally, the fully expanded entry replaces the old one (if types did
not change) or gets added (if types changed).

<<Fully expand [[dec[i]]]/[[decbuf]]>>=
if(didex) {
  <<Add/replace fully expanded entry>>
}
@

<<Add/replace fully expanded entry>>=
if(ntype == dec[i].type) {
  /* if type didn't change, just replace */
  dec[i].off = ndecs;
  dec[i].len = declen;
} else {
  /* otherwise, add a new, duplicate entry */
  if(ndec == maxdec) {
    maxdec *= 2;
    resize(dec, maxdec);
  }
  dec[ndec].off = ndecs;
  dec[ndec].len = declen;
  dec[ndec].cp = dec[i].cp;
  dec[ndec].type = ntype;
  ndec++;
}
if(ndecs + declen > maxdecs) {
  maxdecs *= 2;
  resize(decs, maxdecs);
}
memcpy(decs + ndecs, decbuf, declen * sizeof(int));
ndecs += declen;
@

Now all canonical and compatibility decomposition information has been
read in.  The only thing that remains is the case folding information.
Like the Full Composition Exclusion flag, the code points for case
folding are ranges.  The values are fully decomposed strings.

<<Make [[uninorm.gen]]>>=
/* NFKC_CF */
if(!(f = fopen(normf, "r"))) {
  perror(normf);
  exit(1);
}
while(fgets(lbuf, sizeof(lbuf), f)) {
  char *s;
  if(isspace(*lbuf) || *lbuf == '#' || !strstr(lbuf, "; NFKC_CF;"))
    continue;
  if((s = strchr(lbuf, '#')))
    *s = 0;
  /* field 1: code point(s) */
  if(!(s = strchr(lbuf, ';')))
    exit(1);
  *s++ = 0;
  /* field 2: flag name */
  if(!(s = strchr(s, ';')))
    exit(1);
  /* field 3: decomposition string */
  while(isspace(*++s));
  char *deccp = s;
  /* count # of code points */
  int len;
  for(len = 0, s = deccp; *s; len++) {
    while(*s && !isspace(*s))
      s++;
    while(isspace(*s))
      s++;
  }
  /* parse field 1 */
  int rlow = strtol(lbuf, &s, 16), rhigh = rlow;
  if(*s == '.' && s[1] == '.') {
    rhigh = strtol(s + 2, &s, 16);
    if(rhigh < rlow)
      exit(1);
  }
  /* add entry for each code point in field 1 */
  /* all sharing the same target string */
  for(;rlow <= rhigh; rlow++, ndec++) {
    if(ndec == maxdec) {
      maxdec *= 2;
      resize(dec, maxdec);
    }
    dec[ndec].cp = rlow;
    dec[ndec].off = ndecs;
    dec[ndec].len = len;
    dec[ndec].type = 3;
  }
  /* add target string */
  if(ndecs + len > maxdecs) {
    maxdecs *= 2;
    resize(decs, maxdecs);
  }
  for(len = 0, s = deccp; *s && len < dec[ndec-1].len; len++) {
    decs[ndecs + len] = strtol(s, &s, 16);
    while(isspace(*s)) s++;
  }
  if(dec[ndec-1].len != len)
    exit(1);
  ndecs += len;
}
fclose(f);
@

Now that everything has been read in, it is time to compress the
decomposition strings.  There are two easy compressions:  all equal
strings are shared, and a string may have the previous string as its
prefix.  It is also possible to have a string share a suffix of the
previous string as a prefix, but that is more difficult and probably
not worth the effort.  The easy compressions are made easy by first
sorting by decomposition string.

<<[[mkuninorm]] globals>>=
static int cmpdecs(const void *a, const void *b)
{
    const uni_decent_t *d1 = a, *d2 = b;
    int l = d1->len < d2->len ? d1->len : d2->len;
    int i, c;
    for(i = 0; i < l; i++) {
        c = decs[d1->off + i] - decs[d2->off + i];
        if(c)
            return c;
    }
    return (int)d1->len - (int)d2->len;
}
@

<<Make [[uninorm.gen]]>>=
/* Reorder/merge decs */
qsort(dec, ndec, sizeof(*dec), cmpdecs);
@

Then, a new string buffer is created, where the compressed strings
will be added.  It is initially as large as the old one, so there is
no need to check the size.

<<Make [[uninorm.gen]]>>=
int *decs2, ndecs2 = 0;
inisize(decs2, ndecs);
@

Next, just add strings if they are not identical to the previous
entry, or just add a suffix if they have a prefix equal to the
previous entry.  Since there are gaps, the savings due to compression
cannot be calculated simply, so instead every time a string is not
added, it is counted towards the saved space.

<<Make [[uninorm.gen]]>>=
int saved = 0;
for(i = 0; i < ndec; ) {
  int l = dec[i].len;
   if(i > 0) {
    /* share prefix if identical to previous entry */
    int l2 = dec[i-1].len;
    if(l2 < l && !memcmp(decs2 + dec[i-1].off, decs + dec[i].off,
                         l2 * sizeof(*decs))) {
      ndecs2 -= l2;
      saved += l2;
    }
  }
  memcpy(decs2 + ndecs2, decs+dec[i].off, l * sizeof(*decs));
  /* share decomp string if identical to previous entry */
  int j;
  for(j = i + 1; j < ndec; j++) {
    if(cmpdecs(&dec[i], &dec[j]))
      break;
    else
      saved += l;
  }
  for(; i < j; i++)
    dec[i].off = ndecs2;
  ndecs2 += l;
}
@

Now that the table has been compressed, the old table can be removed.
The string table statistics are printed for my amusement.

<<Make [[uninorm.gen]]>>=
free(decs);
fprintf(stderr, "Total string table length %d (%d saved)\n", ndecs2, saved);
@

In this program, string offsets were stored as integers.  However, in
the main program, string offsets are short integers.  Thus, if the
final, compressed table is large enough for offset overflow, this is a
good time to say it.  The size was already printed, so die without
comment.

<<Make [[uninorm.gen]]>>=
if(ndecs2 > 0x10000)
  exit(1);
@

Now that all the tables have been generated, it is time to print them.
Order doesn't really matter, so here's the string table.

<<Make [[uninorm.gen]]>>=
/* dump string table */
fputs("const int deccp[] = {", stdout);
for(i = 0; i < ndecs2 - 1; i++) {
  if(!(i%8))
    fputs("\n\t", stdout);
  else
    putchar(' ');
  printf("0x%04X,", decs2[i]);
}
if(!(i%8))
  fputs("\n\t", stdout);
else
  putchar(' ');
printf("0x%04X\n"
       "};\n", decs2[i]);
@

Since we already sorted by decomposition string, the canonical
composition table should be next.  This consists of all type 0
entries.  In order to assist debugging, the pair is displayed in a
comment after each entry.

<<Stuff to do before reading [[unif]]>>=
const char *nxt = "";
@

<<Make [[uninorm.gen]]>>=
/* Print canonical composition table ordered by decomp */
puts("static const uni_decent_t cancomp[] = {");
nxt = "";
for(i = 0; i < ndec; i++)
  if(!dec[i].type) {
    printf("%s\t{0x%04X, %u, %u} /* %04X %04X */", nxt, dec[i].cp,
           (int)dec[i].off, (int)dec[i].len, decs2[dec[i].off],
           decs2[dec[i].off + 1]);
    nxt = ",\n";
  }
puts("\n};");
@

The decomposition tables need to be sorted by code point.  In
addition, the combined canonical/compatibility table should be sorted
by type for duplicate code points.

<<[[mkuninorm]] globals>>=
static int cmpdeccp_t(const void *a, const void *b)
{
  const uni_decent_t *d1 = a, *d2 = b;

  if(d1->cp == d2->cp)
    return d1->type - d2->type;
  else
    return d1->cp - d2->cp;
}
@

<<Make [[uninorm.gen]]>>=
/* next tables need ordering by cp again */
qsort(dec, ndec, sizeof(*dec), cmpdeccp_t);
@

The combined table for canonical and compatibility decomposition
consists of all type 1 and 2 entries, and all type 0 entries for which
there is not also a type 1 entry.  The sign of the length needs to be
negative for type 2 entries.

<<Make [[uninorm.gen]]>>=
/* combined table for canonical/compatible decomp */
puts("static const uni_decent_t decomp[] = {");
nxt = "";
for(i = 0; i < ndec; i++)
  if(dec[i].type < 3) {
    /* skip canonial comp if canonical decomp exists for it */
    if(!dec[i].type && i < ndec - 1 && dec[i + 1].cp == dec[i].cp &&
       dec[i + 1].type == 1)
      i++;
    printf("%s\t{0x%04X, %u, %s%u}", nxt, dec[i].cp,
           (int)dec[i].off, dec[i].type > 1 ? "-" : "", (int)dec[i].len);
    nxt = ",\n";
  }
puts("\n};");
@

Finally, the case folding table is just all type 3 entries.

<<Make [[uninorm.gen]]>>=
/* NFKC_CF table */
puts("static const uni_decent_t nfkc_cf[] = {");
nxt = "";
for(i = 0; i < ndec; i++)
  if(dec[i].type == 3) {
    printf("%s\t{0x%04X, %u, %u}", nxt, dec[i].cp,
           (int)dec[i].off, (int)dec[i].len);
    nxt = ",\n";
  }
puts("\n};");
@

\section{Character Classification}

Once the characters arrive, they need to be classified for lexical
analysis and other purposes.

<<Library [[uni]] Members>>=
uni_chartype.o
@

<<Library [[uni]] headers>>=
#include "uni_chartype.h"
@

<<uni_chartype.h>>=
<<Common C Warning>>
#ifndef UNI_CHARTYPE_H
#define UNI_CHARTYPE_H

#include "uni_common.h"
<<Character type exports>>
#endif /* UNI_CHARTYPE_H */
@

<<uni_chartype.c>>=
<<Common C Header>>
#include "uni_chartype.h"

<<Character type local definitions>>

<<Character type functions>>
@

Unicode is a 32-bit encoding, leaving 4 billion possible values.
Filtering for valid code points still leaves over 1 million possible
values.  Rather than having huge lookup tables for character types as
would be done with only 256 code points, each flag is represented as
an array of ranges, in numerical order.  Binary searches can be used
to determine if the flag is set.

<<Character type functions>>=
#define is_x(x) \
    int is_uni_##x(int cp) \
    { \
        uni_chrrng_t cr = {cp, cp}; \
    /* FIXME: change to manual bsearch */ \
        return bsearch(&cr, unirng_##x, sizeof(unirng_##x)/sizeof(uni_chrrng_t), \
                       sizeof(uni_chrrng_t), uni_cmprng) ? 1 : 0; \
    }
@

For classification, the array of ranges is augmented by a value for
each range.  The value can be a plain integer or an enumeration.

<<Return table lookup for range table>>=
/* prefix with return type */
#define range_ret(x, i) \
     uni_##x##_of(int cp) \
    { \
        uni_chrrng_dat_t cr = {cp, cp}, *res; \
    /* FIXME: change to manual bsearch */ \
        res = bsearch(&cr, unirng_##x, sizeof(unirng_##x)/sizeof(uni_chrrng_dat_t), \
                      sizeof(uni_chrrng_dat_t), uni_cmprng_dat); \
        if(!res) \
            return i; \
        return res->dat; \
    }
@

<<Character type local definitions>>=
<<Return table lookup for range table>>
/* enum return type */
#define x_of(x, i) uni_##x##_t range_ret(x, i)
@

<<Character type local definitions>>=
#include "uni_chartype.gen"
@

<<Character type exports>>=
<<[[uni_chartype.gen]] dependencies>>
#include "uni_chartype.gen.h"
@

\lstset{language=make}
<<makefile.vars>>=
PARSE_UCD_DEPS = uni_common.c uni_common.h mfgets.c mfgets.h
@

<<makefile.rules>>=
uni_chartype.gen: parse-ucd.c $(PARSE_UCD_DEPS)
	$(CC) $(CFLAGS) $(EXTRA_CFLAGS) -o parse-ucd $(filter %.c, $^)
	./parse-ucd $(UCD_LOC) $(UNIHAN_LOC) >uni_chartype.gen 2>uni_chartype.gen.h

# dummy rule to generate this file
uni_chartype.gen.h: uni_chartype.gen
	touch $@
@

<<C Headers>>=
uni_chartype.gen.h \
@

First, although technically all code points from 0000 through 10FFFF
are valid, some do not have an entry in the UCD.  These should be
considered invalid as well.  To generate the table, UnicodeData.txt is
parsed.  The parsing is too confusing when done directly in the make
rule, so a script is used instead.

\lstset{language=C}
<<[[uni_chartype.gen]] dependencies>>=
typedef struct {
  const char *name;
  uint32_t mask;
} uni_gc_desc_t;
@

<<Known Data Types>>=
uni_gc_desc_t,%
@

\lstset{language=make}
<<Generate [[uni_chartype.gen]]>>=
# standard character classes
echo "const uni_gc_desc_t uni_gc_names[] = {" >>$@
grep '^gc ;' $(UNIVALALIAS) | while read -r d d s d l t1 l2 t2 l3; do \
  case "$$t1" in \
    ";") ;; \
    "#") l3="$$l2 $$t2 $$l3"; l2=; t2="#" ;; \
    *) l2=; t2=; ;; \
  esac; \
  if [ "#" = "$$t2" ]; then \
    fl=; \
    while :; do \
      fl="$$fl (1 << U_gc_$${l3%%|*})"; \
      case "$$l3" in \
        *"|"*) fl="$$fl |"; l3="$${l3#*| }" ;; \
	*) break ;; \
      esac; \
    done; \
  else \
    fl="1 << U_gc_$$s"; \
  fi; \
  echo "  { \"$$s\", $$fl },"; \
  echo "  { \"$$l\", $$fl },"; \
  test -n "$$l2" && echo "  { \"$$l2\", $$fl },"; \
done | LANG=C sort >>$@
echo "};" >>$@
echo "const uint32_t uni_gc_names_len = sizeof(uni_gc_names)/sizeof(uni_gc_names[0]);" >>$@
@

After creating all of the range structures, we can convert them to
multi-level tables as well.

<<C Build Executables>>=
rngtotab \
@

<<Character types generation deps>>=
rngtotab.c uni_chartype.h \
@

<<Post-process [[uni_chartype.gen]]>>=
mkdir cp.$$$$; \
trap "rm -rf cp.$$$$" 0; \
touch cp.$$$$/{cproto.h,uni_chartype.gen.h,rngtotab.c.static_proto}; \
$(CC) $(CFLAGS) $(EXTRA_CFLAGS) -Icp.$$$$ -o rngtotab rngtotab.c uni_common.c;
./rngtotab >>$@
@

For regex:
\begin{verbatim}
Don't forgot sloppiness rules:
numeric (UAX44-LM1):
  compare as floating-point number w/ limited precision
  e.g. 01.00 == 1
name (UAX44-LM2):
  ignore case, whitespace, underscore, and medial hyphens except 1180
name (UTS18-2.5.1):
  ignore case, whitespace, underscore, and all hyphens except 1180,
     0F60, 0FB0
  allow "namespace": prefix to apply if name doesn't match otherwise
    e.g. "LATIN LETTER".  Not sure how useful, but could make rules to
      insert CAPITAL and LOWER into appropriate spot.
enum/propname (UAX44-LM3):
  - case-insensitive
  - remove *PREFIX* of is (not entire value if value is "is")
  - remove all spaces, underscores, dashes

bool:
  punct == gc==P & gc!=S & !alpha
  uni_punct == gc==P
  digit == 0-9
  uni_digit == gc==Nd
  xdigit == 0-9a-fA-F
  uni_xdigit == gc=Nd + Hex_Digit
  alnum == alpha + digit
  uni_alnum == alpha + uni_digit
  blank == gc=Zs + TAB
  graph == anything but White_Space, gc=Cc, gc=Cs, gc=Cn
  print == graph + blank - gc=Cc
  word == alnum + _ (by default)
  uni_word == uni_alnum + gc=M + gc=Pc plus Join_Control
l2:
other:
  Name (na) == UnicodeData.txt, field 2
  Name_Alias == NameAliases.txt
  Script_Extensions (scx) == ScriptExtensions.txt (multi)
numeric:
  Age (age) == DerivedAge.txt
  Numeric_Value (nv) == UnicodeData.txt, field 8
string:
  Lowercase_Mapping (lc) == SpecialCasing.txt (def = Simple_Lowercase_Mapping?)
  Titlecase_Mapping (tc) == SpecialCasing.txt (def = Simple_Titlecase_Mapping?)
  Uppercase_Mapping (uc) == SpecialCasing.txt (def = Simple_Uppercase_Mapping?)
  Case_Folding (cf) == CaseFolding.txt (C + F)
  Simple_Lowercase_Mapping (slc) == UnicodeData.txt, field 13
  Simple_Titlecase_Mapping (stc) == UnicodeData.txt, field 14/12
  Simple_Uppercase_Mapping (suc) == UnicodeData.txt, field 12
  Simple_Case_Folding (scf) == CaseFolding.txt (C + S)
  Decomposition_Mapping (dm) == UnicodeData.txt, field 5 (w/o <>)
  NFKC_Casefold (NFKC_CF) == DerivedNormalizationProps.txt
  Bidi_Mirroring_Glyph (bmg) == BidiMirroring.txt
----
boolean
  isxxx; xxx == Cased, Lowercase, Uppercase, Titlecase -> xxx above
string
  toxxx; xxx == Lowercase, Uppercase, Titlecase -> xxx_Mapping above
  cjkTraditionalVariant == unihan/Unihan_Variants, kTraditionalVariant
  cjkSimplifiedVariant == unihan/Unihan_Variants, kSimplifiedVariant
  exemplar == DUCET://ldml/characters/exemplarCharacters (UnicodeSet)
  IDNA_Mapping == idna/IdnaMappingTable.txt, field 3
  Collation_Primary == DUCET: collation order (from various)
numeric
  CJK_Radical == unihan/Unihan_RadicalStrokeCounts.txt, kRSUnicode
enumerated
  IDNA_Status == idna/IdnaMappingTable.txt, field 2
  ID_Restrict_Status == security/xidmodifications.txt, field 2
  ID_Restrict_Type == security/xidmodifications.txt, field 3
\end{verbatim}

Some other standard categories which might be useful are ignored and
deprecated code points.  These are extracted from PropList.txt and
DerivedCoreProperties.txt, which is in a different format, requiring a
different parser script.

The Script (sc) property comes from Scripts.txt in a similar way.  It
is a one-to-one mapping, so the same technique can be used as with the
gc property.  On the other hand, it is convenient to store the
Script\_Extensions (scx) property at the same place, since sc is the
default value for scx.

\lstset{language=C}
<<Character type exports>>=
/* need to manually convert U_SC_Hrkt to U_SC_Hira+U_SC_Kana */
/* uni_sc_t uni_sc_of(int cp); */
/* if -1, use uni_sc_of() instead */
/* otherwise, use uni_scx_exp[] */
int uni_scx_of(int cp);
@

<<Character type functions>>=
#if 0
x_of(sc, U_SC_Zzzz)
int range_ret(scx, -1)
#endif
@

<<makefile.vars>>=
UNISCR = $(UCD_LOC)/Scripts.txt
UNISCRX = $(UCD_LOC)/ScriptExtensions.txt
@

<<Character types generation deps>>=
$(UNISCR) $(UNIVALALIAS) \
@

<<makefile.rules>>=
scriptnames.gen: $(UNIVALALIAS)
	sed -n 's/^sc *; \([^ ]*\) *; \([^ ]*\)[ ;]*\([^ ]*\)/\1 \2 \3/p' \
	  < $(UNIVALALIAS) >$@
@

<<Character types generation deps>>=
scriptnames.gen \
@

<<Generate [[uni_chartype.gen.h]]>>=
echo "typedef enum {" >>$@.h
while read s l s2; do \
  printf %s "  U_SC_$$s,"; \
  test "$$s" = "$$l" || printf %s " U_SC_$$l = U_SC_$$s,"; \
  test -n "$$s2" && printf %s " U_SC_$$s2 = U_SC_$$s,"; \
  echo; \
done < scriptnames.gen | sed '$$s/,$$//' >>$@.h
echo "} uni_sc_t;" >>$@.h
@

<<Known Data Types>>=
uni_sc_t,%
@

<<[[uni_chartype.gen]] dependencies>>=
typedef struct {
  const char *name;
  /* uni_sc_t */int script;
} uni_sc_desc_t;
@

<<Known Data Types>>=
uni_sc_desc_t,%
@

<<Generate [[uni_chartype.gen]]>>=
# scripts
echo "const uni_chrrng_dat_t unirng_sc[] = {" >>$@
# Zzzz not explicitly listed; covers anything not in any other
# Hrkt not explicitly listed; covers Kana+Hira
./ucd-ranges -p U_SC_ 2 < $(UNISCR) | LANG=C sort | sed '$$s/,$$//' >>$@
echo "};" >>$@
echo "const uint32_t unirng_sc_len = sizeof(unirng_sc)/sizeof(unirng_sc[0]);" >>$@
# note: assumes at most 6 scripts; may change in future
# compiler should give an array overflow warning if so, though
echo "const uni_sc_t uni_scx_exp[][6] = {" >>$@
grep '^[0-9A-F]' $(UNISCRX) | cut -d\; -f2 | cut -d\# -f1 | uniq | \
    sed 's/ *$$//;s/ /, U_SC_/g;s/, //' | while read s; do \
  echo "  {$$s},"; \
done | sed '$$s/,$$//' >>$@
echo "};" >>$@
echo "const uni_chrrng_dat_t unirng_scx[] = {" >>$@
grep '^[0-9A-F]' $(UNISCRX) | cut -d\; -f2 | cut -d\# -f1 | uniq | ( \
      i=0; \
      while read s; do \
        ./ucd-ranges 2 "$$s" < $(UNISCRX) | sed "s/}/, $$i}/"; \
	i=$$((i+1)); \
      done) | sort -t\{ -k 2 | sed '$$s/,$$//' >>$@
echo "};" >>$@
echo "const uint32_t unirng_scx_len = sizeof(unirng_scx)/sizeof(unirng_scx[0]);" >>$@
# Zzzz not explicitly listed; covers anything not in any other
# Hrkt not explicitly listed; covers Kana+Hira
egrep -v Zzzz\|Hrkt scriptnames.gen | ( \
  tt=; \
  while read s l s2; do \
    for n in $$l $$s $$s2; do \
      tt="$$tt { \"$$n\", U_SC_$$s },;"; \
    done; \
  done; \
  echo "const uni_sc_desc_t uni_sc_names[] = {" >>$@; \
  echo "$${tt%,;}" | tr \; \\n | LANG=C sort -u >>$@; \
  echo "};" >>$@; \
  echo "const uint32_t uni_sc_names_len = sizeof(uni_sc_names)/sizeof(uni_sc_names[0]);" >>$@ \
)
@

A few specific character classes that are useful for the compiler do
not warrant a range table.  Instead, they are implemented as macros
that compare directly to the limited set they contain.  These are:
characters which act as line terminators, and characters which may
initiate backslash-escapes.

\lstset{language=C}
<<Character type exports>>=
#define is_nl(x) ((x) == '\n' || (x) == '\r' || (x) == '\v' || (x) == '\f' || \
                  (x) == 0x0085 || (x) == 0x2028 || (x) == 0x2029)
/* just use == '\\' for nfkc_cf */
#define is_bs(x) (c == '\\' || c == 0xFE68 || c == 0xFF3C)
@

A mainline is provided for testing some of the mess above.  It also
makes it easy to figure out what the gaps are, and what to do about
them.

\lstset{language=make}
<<C Test Support Executables>>=
chartypes \
@

<<makefile.rules>>=
CHARTYPES_OBJS = uni_norm.o uni_io.o uni_common.o
chartypes: uni_chartype.c uni_chartype.o $(CHARTYPES_OBJS)
	$(CC) $(CFLAGS) -DRNG_MAIN $(LDFLAGS) -o $@ $< $(CHARTYPES_OBJS)
@

\lstset{language=C}
<<Character type functions>>=
#ifdef RNG_MAIN
#include "uni_norm.h"

int main(int argc, const char **argv)
{
  <<Test character type functions>>
  return 0;
}
#endif
@

First is to loop over all possible code points, and print the flags and
classes.

<<Test character type functions>>=
int i;
for(i = 0; i < 0x110000; i++) {
  printf("%06X ", i);
  <<Print flags and classes of cp [[i]]>>
  putchar('\n');
}
@

<<Print flags and classes of cp [[i]]>>=
/* standard character class */
fputs(uni_gc_nameof[uni_gc_of(i)].short_name, stdout);
putchar(' ');
@

<<Print flags and classes of cp [[i]]>>=
/* other flags */
if(find_ccc(i))
  putchar('-'); /* can be moved by canon_order */
if(!is_uni_ASSIGNED(i))
  putchar('?');
if(is_uni_Dep(i))
  putchar('D');
if(is_uni_DI(i))
  putchar('!');
@

\section{Character Names}

Another way to get input is to use backslash-escapes.  These require
knowledge of all Unicode character names.

First, let's collect the names and their values in comma-separated
value files.  We can generate all single-character Unicode names which
are not algorithmically generated directly from UnicodeData.txt.

\lstset{language=make}
<<Plain Built Files>>=
charnames.uni \
@

<<makefile.rules>>=
charnames.uni: $(UNIDATA)
	cut -d\; -f1-2 $(UNIDATA) | \
	  egrep -v '<|CJK COMPATIBILITY IDEOGRAPH' | \
	    while IFS=\; read cp n; do \
              echo "$$n,$$((0x$$cp))"; \
            done | sort >$@
@

Aliases come from NameAliases.txt, from the exact same feild numbers.

<<makefile.vars>>=
UNIALTNAM = $(UCD_LOC)/NameAliases.txt
@

<<Plain Built Files>>=
charnames.unia \
@

<<makefile.rules>>=
charnames.unia: $(UNIALTNAM)
	grep '^[0-9A-F]' $(UNIALTNAM) | cut -d\; -f1-2 | \
	  while IFS=\; read cp n; do \
            echo "$$n,$$((0x$$cp))"; \
          done | sort >$@
@

Similarly, all multi-character Unicode names can be generated directly
from NamedSequences.txt.

<<makefile.vars>>=
UNISEQ = $(UCD_LOC)/NamedSequences.txt
@

<<Plain Built Files>>=
charnames.uniseq \
@

<<makefile.rules>>=
charnames.uniseq: $(UNISEQ)
	grep '^[A-Z]' $< | while IFS=\; read n cp; do \
           echo -n "$$n"; \
           for y in $$cp; do \
              echo -n ,$$((0x$$y)); \
           done; \
           echo; \
         done > $@
@

Extracting the XML names could be done with sed as well, but since it
is XML, and the file format is more likely to retain the same XML
structure than to retain the same exact text format, XSLT is used
instead.  The name database has subsets called groups.  Using every
single name is certain to frequently collide with Unicode names, so a
particular group should be chosen.  The 2007 group is used by my own
applications.

<<makefile.config>>=
# XML entity name group
XML_ENTITY_NAME_GROUP = 2007
@

<<Plain Built Files>>=
charnames.xml \
@

<<makefile.rules>>=
charnames.xml: uxml.xslt $(XMLUNI)
	xsltproc --stringparam GROUP '$(XML_ENTITY_NAME_GROUP)' $^ | \
	  sed 's/,U/,0x/;s/;//;s/-/,0x/g' | sort -u >$@
@

<<Plain Build Files>>=
uxml.xslt
@

\lstset{language=xml}

<<uxml.xslt>>=
<?xml version="1.0"?>
<xsl:stylesheet version="1.0" xmlns:xsl="http://www.w3.org/1999/XSL/Transform">
<<Output XML entity names in more managable form>>
</xsl:stylesheet>
@

First, the output format needs to set to plain text.  In order to
avoid formatting of the XML to creep into the text file, all elements
strip plain spaces.

<<Output XML entity names in more managable form>>=
<xsl:output method="text"/>
<xsl:strip-space elements="*"/>
@

Next, we extract the group.  Each group defines which sets it contains.

<<Output XML entity names in more managable form>>=
<xsl:param name="GROUP"/>
<xsl:template match="/unicode/entitygroups">
  <xsl:for-each select="group[@name=$GROUP]/set">
    <<Process XML entity set>>
  </xsl:for-each>
</xsl:template>
@

To process each set, a separate template is used.  This template takes
the set name as a parameter.

<<Process XML entity set>>=
<xsl:apply-templates select="/unicode/charlist">
  <xsl:with-param name="set" select="@name"/>
</xsl:apply-templates>
@

The per-set template searches the charlist for entities which claim
membership in the set.  For each such entity, it prints the id
attribute, whose value is the XML entity name, and the parent's id,
whose value is the Unicode code point.  A semicolon is gratuitously
appended to force a newline (otherwise the extreme whitespace
stripping will remove it).  The Unicode code point is in the form
\verb|U######| (the code point in hexadecimal) or
\verb|U######-######| (two hexadecimal code points).  It may be
possible to convert the dash to a comma and eliminate the semicolon
and U using XSLT, but I don't know how.  Instead, that transformation
is done by sed above.

<<Output XML entity names in more managable form>>=
<xsl:template match="/unicode/charlist">
  <xsl:param name="set"/>
  <xsl:for-each select="character/entity[@set=$set]"
><xsl:value-of select="@id"/>,<xsl:value-of select="parent::*/@id"/>;
</xsl:for-each>
</xsl:template>
@

So, what do we do with all of these names?  The obvious thing is to
create a hash table to look up the code point using the name, and
another hash table to look up the name using the code point.  Since
the cost of comparing integers is small, a binary search might suffice
for the code point-to-name lookup.  For the hash table, we could
either use some random hash function and hash table implementation, or
we could generate a perfect hash function using [[gperf]].%
\footnote{After I wrote this, a program called [[cmph]]
(\url{http://sourceforge.net/projects/cmph}) caught my attention. This
generates minimal perfect hash functions in linear time (sub second
time for the entire Unicode name set!).  Its main problems are that it
generates data files rather than code (requiring changes to the
library to support code generation instead; see
\url{http://sourceforge.net/tracker/?func=detail\&aid=3590339\&group_id=126608\&atid=706189}),
and that it does not perform the final comparison step, essentially
just returning the integer index of a possible hashed string (easily
correctable).}
My initial implementation used [[gperf]], but generating a perfect
hash for the full Unicode set took 2 hours on my machine, and an
additional 1 hour to compile.  Instead, I decided to go with the first
thing I ever implemented in a compiler: a single hash table used for
all strings in the system, preloaded with known symbols.

Of course a hash function may not even be ideal.  For example, Unicode
regular expressions may allow searching for a name using a pattern.
For that, the ideal structure is probably a suffix table.  Of
course such structures can be much larger than the equivalent hash
table.

\section{Persistent Strings}

My original compiler-wide string hash table had a variable-sized hash
table, and an array of all strings in the table.  The array index was
the \emph{string number}, which was the only thing remembered about
the string from that point on, unless the string needed to be printed.
Strings never needed to be compared lexically.  Only a single function
is needed to look up the string number; it will automatically add the
string if not present and [[autoadd]] is non-zero.  For printing, a
second function returns a pointer to the start of the string.  As a
convention, a zero-length string is always string number zero.  Zero
is also returned for a non-zero-length string if it is not present and
was not added.  For unique strings, it may also be desirable to add
data to store additional information.  In the Unicode name
application, for example, the Unicode code point could be stored.  Of
course it could also be stored in a separate table, but that would add
four bytes of overhead minimum per code point (i.e., the string
number) as well as longer lookup times (i.e., having to do a binary
search on the supplemental table).  To support storing information
directly in the string table, [[autoadd]] can be a negative number,
indicating the number of bytes to add before the string to store data.

<<Library [[uni]] Members>>=
uni_strs.o
@

\lstset{language=C}
<<Common C Includes>>=
#include "uni_strs.h"
@

<<Library [[uni]] headers>>=
#include "uni_strs.h"
@

<<uni_strs.h>>=
<<Common C Warning>>
#ifndef UNI_STRS_H
#define UNI_STRS_H

#include "uni_common.h"
/* if 0-len string, use 0; don't call this function! */
/* if len == 0, function does strlen() for you */
/* 0 returned if len == 0 or !autoadd and not already there */
/* autoadd < 0 to make -autoadd bytes room before entry */
/* make sure the entry was actually added, though! */
uint32_t strno(const char *str, int len, int autoadd);

/* str is not 0-terminated!  pass non-NULL len if len desired */
const char *str_for(uint32_t no, int *len);

<<Master string table exports>>

#endif /* UNI_STRS_H */
@

<<uni_strs.c>>=
<<Common C Header>>

<<Master string table definitions>>

uint32_t strno(const char *str, int len, int autoadd)
{
  if(!len)
    len = strlen(str);
  if(!len)
    return 0;
  <<Convert [[str]]/[[len]] to string number>>
}

const char *str_for(uint32_t no, int *len)
{
  if(!no) {
    if(len)
      *len = 0;
    <<Return 0-len string>>
  }
  <<Convert [[no]] to string/[[len]]>>
}
@

For this hash table, though, I'll use a fixed-sized table at an
arbitrary size.  A 128K-entry table occupies one megabyte in memory
for a 64-bit system.  Nothing will be stored in the data cache for
very long for such a table.  Of course the comparison only needs to be
made on receiving strings, so it probably does not matter much.

As with my original hash tables, I am using a binary modulus.  I have
found very little difference in hash table performance with actual
program identifiers, and on some processors, there is a significant
performance benefit to using binary moduli.  The hash function simply
xors the length and all characters in the string after rotating the
previous hash value 3 bits left (there is no C function for this, so
equivalent code is used in the hope that the compiler will be smart
enough to figure it out; the compiler I'm using at least seems to
understand).  An xor is used instead of addition to avoid having to
deal with overflow.  The final hash value wraps any bits above 17 down
as well.  This is not done in the loop because there is no instruction
to rotate 17 bits on most processors, leaving the loop fast and the
slower 17-bit operation for the finishing touch.

<<Master string table exports>>=
#define HTAB_SIZE (128*1024)

#ifdef __GNUC__
__attribute__((unused))
#endif
static uint32_t hashfun(const char *nam, int len)
{
  uint32_t hash = len;

  while(--len >= 0)
    /* @<< 3 + @>> 29 should translate to a single rotate instruction */
    hash = ((hash @<< 3) + (hash @>> 29)) ^ *(unsigned char *)nam++;

  /* 17 * 2 == 34, so only need to do this once */
  /* / HTAB_SIZE should translate to >> log2(HTAB_SIZE) */
  /* % HTAB_SIZE should translate to & ~(HTAB_SIZE - 1) */
  return (hash / HTAB_SIZE ^ hash) % HTAB_SIZE;
}
@

The master string hash table requires static storage for strings
(i.e., storage which will not move during the string's lifetime), the
hash table itself, and the array of strings.  The array can store
offsets into the string storage rather than pure pointers, allowing
the strings to move if desired, and removing the need for 64-bit
pointers on 64-bit systems.  In addition, the hash table entries
themselves can be an index into the string array, giving them the same
benefits, as well as removing the need for a 64-bit hash bucket pointer.

<<Master string table exports>>=
extern uint32_t strhash[HTAB_SIZE];

typedef struct {
  uint32_t off; /* offset into string data */
  uint32_t nextent;
} hashent_t;
@

<<Known Data Types>>=
hashent_t,%
@

There are actually two sources of hash table entries:  built-in and
dynamically added.  The built-in strings should be just one large
array of hash table entries along with one large string buffer.  The
most efficient representation would be as arrays (i.e. C [[[]]]), but
in order to allow sharing the same routines with the programs which
build the arrays, I also allow representation as pointers, which
requires an additional read before use rather than a static address.
Rather than introduce this inefficiency into the compiler, I'll use a
[[#define]] to select, and build an extra binary for the alternative.
Similarly, the size of the string can be gotten with [[sizeof]] or
with an external integer, with the former being more efficient.
Taking the size directly only works if the string is included directly
in [[uni_strs.c]], though.

\lstset{language=make}
<<makefile.rules>>=
uni_strs-dyn.c: uni_strs.c cproto.h
	ln -sf $< $@
uni_strs-dyn.o: EXTRA_CFLAGS+=-DDYN_BUILTIN_STR
uni_strs.o: strs.gen
@

<<Clean temporary files>>=
rm -f uni_strs-dyn.[oc]
@

\lstset{language=C}
<<Master string table exports>>=
#ifdef DYN_BUILTIN_STR
extern const char *builtin_str;
extern int builtin_str_size;

extern const hashent_t *builtin_hashe;
extern int builtin_hashe_size;
#else
/* builtin_str and builtin_hashe are local to uni_strs.c */
#define builtin_str_size (sizeof(builtin_str) - 1) /* remove trailing C 0 */
#define builtin_hashe_size (sizeof(builtin_hashe)/sizeof(hashent_t))
#endif
@

<<Master string table definitions>>=
#ifndef DYN_BUILTIN_STR
#include "strs.gen"
#endif
@

The strings themselves are actually arbitrary binary data, so they
cannot be terminated by a sentinel as in normal C strings.  Instead,
the length needs to be stored along with the string.  My old string
routines stored the length before the string, so that the pointer
always points to the first character rather than the length.  This
seems harmless, so I'll do it here as well.  The actual format of the
length is a variable-length integer, with the high bit set on all but
the last byte.  This is obviously just to save space, since most
strings will only require one byte of length.  Since the length is
stored before the string, any additional data stored with the string
must be stored ahead of the length.  To assist in both parsing the
string length and finding a pointer to the space before it,
[[num_before]] can be used.

<<Master string table exports>>=
/*
 * strtab format:
 *  [<parm>] <len> * <utf8-str>
 *  len is big-endian variable-len, with high bit set for all bytes but first
 *  parm is dependent on string #, and interpreted based on that
 *  * indicates position of string offset
 */
/* 1st strtab (built-in) is contiguous string of length BUILTIN_STR_LEN */

/* len of str_for(n) is num_before(str_for(n)). */
/* length of len is returned in len */
#ifdef __GNUC__
__attribute__((unused))
#endif
static uint32_t num_before(const char *s, int *len)
{
  const unsigned char *p = (const unsigned char *)s;
  uint32_t ret = *--p & 0x7f;
  uint32_t shift = 0;
  while(*p-- > 0x7f)
    ret += (*p & 0x7f) << (shift += 7);
  if(len)
    *len = (int)((const unsigned char *)s - p) - 1;
  return ret;
}
@

<<Return 0-len string>>=
return "" + 1; /* in case num_before called on it */
@

The use of an array index or string offset to identify strings and
hash table entries eliminates the need to keep them at fixed
addresses.  This allows the use of simple dynamically resized arrays
for both.  However, as they grow, the expense of potentially copying
every single byte added so far to a newly allocated entry, as well as
the memory fragmentation caused by reallocation, may be too great.
Instead, they are stored in arrays of buffers.  The only resizable
arrays are the buffer pointers, which will grow very slowly.

<<Master string table definitions>>=
#define BUFARRAY_MIN 16 /* # of initial elements in indirect arrays */

#define STRBUF_LEN 65536 /* bytes */
static struct strbuf {
    uint16_t len;
    char buf[STRBUF_LEN];
} **ostrings = NULL;
static int nbufs = 0;

#define HBUF_LEN 1024 /* * sizeof(hashent_t) -> * 8 */
static struct hashebuf {
    uint16_t len;
    hashent_t ent[HBUF_LEN];
} **hashe = NULL;
static int nhashe = 0;
@

The separate storage for built-in and dynamically added strings
requires extra calculation.  Given an index, simply use a builtin
entry if the index is low enough, or subtract the number of builtin
entries and then convert to an indirect index plus an offset into the
buffer.   The other way around simply requires adding the built-in
size to any index returned.  Only the former conversion is presented
here, for hash table entries.  The same method must be applied to
builtin string offsets as well, though.

<<Master string table definitions>>=
static const hashent_t *he_for(uint32_t hn)
{
    if(hn < builtin_hashe_size)
        return &builtin_hashe[hn];
    hn -= builtin_hashe_size;
    return hashe[hn / HBUF_LEN]->ent + hn % HBUF_LEN;
}
@

First, we'll take care of the easy case.  Converting a string number
to a string requires looking up the hash entry, and then looking up
the string specified by its offset.  This routine is not for general
use, so there is no need to check that the string number is within
range first.

<<Convert [[no]] to string/[[len]]>>=
const hashent_t *he = he_for(no - 1);
no = he->off;
const char *ret;
if(no < builtin_str_size)
  ret = builtin_str + no;
else {
  no -= builtin_str_size;
  ret = ostrings[no / STRBUF_LEN]->buf + no % STRBUF_LEN;
}
if(len)
  *len = num_before(ret, NULL);
return ret;
@

The easy case for returning the string number is to find it in the
hash table and just return the hash entry's array index.  This is what
is actually stored in the hash table and the hash bucket next
pointers.

One potential optimization would be to move the found entry to the top
of the hash bucket, on the assumption that it will be accessed again
soon.  This may be revisited in the future, but for now, a simpler
loop structure is used with no modification of built-in hash entries.

<<Convert [[str]]/[[len]] to string number>>=
uint32_t h = hashfun(str, len);
int hen;
/* traverse hash bucket */
for(hen = strhash[h]; hen; ) {
  const hashent_t *he = he_for(hen - 1);
  int helen;
  const char *s = str_for(hen, &helen);
  if(len == helen && !memcmp(str, s, len))
    return hen;
  hen = he->nextent;
}
@

If the string was not found, it is automatically added.

<<Convert [[str]]/[[len]] to string number>>=
/* couldn't find it, so add it */
if(!autoadd)
  return 0;
@

If no strings have been previously added, the expandable arrays are
initialized.  This could have been done in some master initialization
routine instead, but it is not too expensive to do it here.

<<Convert [[str]]/[[len]] to string number>>=
if(!ostrings) {
  inisize(ostrings, BUFARRAY_MIN);
  clearbuf(ostrings, BUFARRAY_MIN);
  nbufs = BUFARRAY_MIN;
  inisize(hashe, BUFARRAY_MIN);
  clearbuf(hashe, BUFARRAY_MIN);
  nhashe = BUFARRAY_MIN;
}
@

Then, we need to compute the amount of memory actually needed for the
string.  In addition to the length of the string, the length of the
length and any requested additional storage must be allocated.

<<Convert [[str]]/[[len]] to string number>>=
uint32_t fulllen = 0, tl;
for(tl = len; tl > 0; tl >>= 7)
   fulllen++;
fulllen += len;
if(autoadd < 0)
  fulllen += -autoadd;
@

To actually store the string, we find the first buffer with enough
room.  If no such buffer exists, we add one.  Just in case the string
is too large to fit in any string buffer, the newly added buffer may
be enlarged to fit the new string.

<<Convert [[str]]/[[len]] to string number>>=
int i;
for(i = 0; i < nbufs && ostrings[i]; i++)
  if(STRBUF_LEN - ostrings[i]->len >= fulllen)
    break;
if(i == nbufs) {
  resize(ostrings, nbufs * 2);
  clearbuf(ostrings + nbufs, nbufs);
  nbufs *= 2;
}
if(!ostrings[i]) {
  if(fulllen > STRBUF_LEN) {
    ostrings[i] = malloc(sizeof(**ostrings) + fulllen - STRBUF_LEN);
    if(!ostrings[i]) {
      perror("string buffers");
      exit(1);
    }
  } else
    inisize(ostrings[i], 1);
  ostrings[i]->len = 0;
}
@

Then, the string is appended to the string buffer, leaving room before
it as needed.  The length is written in as well.  The newly added
string is added to the length, being careful not to overflow the
unsigned short, while allowing the length to equal or exceed 64k.
Since no added string could be just one byte (the only one-byte string
is the empty string), we indicate the buffer is full by stopping at
64k - 1.

<<Convert [[str]]/[[len]] to string number>>=
struct strbuf *sb = ostrings[i];
int off = builtin_str_size + i * STRBUF_LEN + sb->len + (fulllen - len);
unsigned char *p = (unsigned char *)sb->buf + sb->len + (fulllen - len);
memcpy(p, str, len);
while(len > 0) {
  *--p = len & 0x7f;
  len >>= 7;
  if(len)
    *p |= 0x80;
}
if(fulllen + ostrings[i]->len < STRBUF_LEN)
  ostrings[i]->len += fulllen;
else
  ostrings[i]->len = STRBUF_LEN - 1;
@

Next, we add the hash table entry for the string.  Room needs to be
made in the hash entry array for one more entry.

<<Convert [[str]]/[[len]] to string number>>=
for(i = 0; i < nhashe && hashe[i]; i++);
if(i > 0 && hashe[i-1]->len < HBUF_LEN)
  i--;
if(i == nhashe) {
  resize(hashe, nhashe * 2);
  clearbuf(hashe + nhashe, nhashe);
  nhashe *= 2;
}
if(!hashe[i]) {
  inisize(hashe[i], 1);
  hashe[i]->len = 0;
}
hen = builtin_hashe_size + i * HBUF_LEN + hashe[i]->len;
hashent_t *he = hashe[i]->ent + hashe[i]->len;
++hashe[i]->len;
@

Finally, we initialize the entry, link it into the hash bucket, and
update the hash table to point to it.  The index (plus one) is
returned as the string number.

<<Convert [[str]]/[[len]] to string number>>=
he->off = off;
he->nextent = strhash[h];
strhash[h] = hen + 1;
return hen + 1;
@

\section{Character Names, Revisited}

Now that we have the ability to use a table of built-in strings, the
various name tables can be added.  A general string table generator is
needed, since the Unicode names are not the only built-in strings.

\lstset{language=make}
<<makefile.rules>>=
strs.gen: mkstrs <<Built-in string table components>>

	<<Generate built-in string table using [[mkstrs]]>>
	<<Post-process built-in string table>>
@

<<C Build Executables>>=
mkstrs \
@

\lstset{language=C}
<<mkstrs.c>>=
<<Common C Header>>

<<[[mkstrs]] definitions>>

int main(int argc, const char **argv)
{
  <<Generate built-in string table>>
}
@

One of the things useful for the character tables is to store the
integer code point along with the name.  This requires, of course,
that all names be unique, since only one number can be stored.  This
is already required by the Unicode standard, so that is not a problem.
In addition, as mentioned above, it might save space to store Unicode
names as individual words, rather than full strings.

\lstset{language=make}
<<makefile.rules>>=
mkstrs.o: EXTRA_CFLAGS+=-DDYN_BUILTIN_STR
mkstrs: mkstrs.o uni_strs-dyn.o
	$(CC) $(LDFLAGS) -o $@ $^
@

\lstset{language=C}
<<[[mkstrs]] definitions>>=
const char *builtin_str;
int builtin_str_size;
const hashent_t *builtin_hashe;
int builtin_hashe_size = 0;

uint32_t strhash[HTAB_SIZE];
@

There are three outputs of the program.  First is the built-in strings
it reads from the input file(s), in the order given, as C string
constants, one per line (automatically concatenated into a single
string constant by the C compiler).  Next, rather than printing the
hash structure, each entry is printed as a pair of hash value and
string offset.  This allows the outputs to be produced on the fly, as
entries are read in.  A separate program can collect the pairs of
numbers into a hash table.  Finally, in order to use the string
constants in the program without having to look them up first,
[[#define]] statements can be generated for the strings.  The first
string's index and the last string's index can be generated, as well
as an individual index for each string, named after a safe version of
the string itself.  The output files, as well as the names of the
[[#define]]s are all set by command line options. The files can be
either created or appended to, in case of adding to an existing set of
built-in strings.

<<Generate built-in string table>>=
<<[[mkstrs]] option variables>>
int exit_loop = 0;
while(--argc > 0 && !exit_loop && *(*++argv) == '-' && (*argv)[1])
  switch((*argv)[1]) {
    <<Process [[mkstrs]] option>>
    case '-':
      exit_loop = 1;
      break;
  }
@

<<[[mkstrs]] option variables>>=
const char *print_def = NULL, *enum_prefix = NULL,
           *out_fname = "-", *out_fmode = "w",
           *def_fname = "-", *def_fmode = "w",
           *hash_fname = "-", *hash_fmode = "w";
@

<<Process [[mkstrs]] option>>=
case 'o':
  out_fname = *argv + 2;
  if(*out_fname == '+') {
    out_fmode = "a";
    out_fname++;
  }
  break;
case 'd':
  def_fname = *argv + 2;
  if(*def_fname == '+') {
    def_fmode = "a";
    def_fname++;
  }
  break;
case 'h':
  hash_fname = *argv + 2;
  if(*hash_fname == '+') {
    hash_fmode = "a";
    hash_fname++;
  }
  break;
case 'D':
  print_def = *argv + 2;
  break;
case 'a':
  enum_prefix = *argv + 2;
  break;
@

In addition to appending to existing files, multiple input sources are
supported by supplying an initial string and hash table entry number
offset.

<<[[mkstrs]] option variables>>=
uint32_t pos = 1, spos = 0;
@

<<Process [[mkstrs]] option>>=
case 'l':
  pos = strtol(*argv + 2, NULL, 0) + 1;
  break;
case 'L':
  spos = strtol(*argv + 2, NULL, 0);
  break;
@

The remainder of the command-line arguments specify input files.  If
none are specified, standard input is used.

<<Generate built-in string table>>=
int do_stdin = !argc;
@

When merging string tables, the old string table and hash table need
to be read first.  The option to merge tables is selected by using the
append mode for the output files, and specifying a hash entry offset
for validation.  The number of entries in both the string and hash
table files should be equal to the supplied number.  The string offset
need not be supplied, as it is harder to get at (the [[-D]] option
gives the hash entry offset).

<<Generate built-in string table>>=
/* if appending to strings & hash, read old strings */
/* when done, open strings output */
int read_strings = pos > 1 &&
    out_fmode[0] == 'a' && hash_fmode[0] == 'a' &&
    strcmp(out_fname, "-") && strcmp(hash_fname, "-") &&
    strcmp(out_fname, hash_fname) && strcmp(def_fname, hash_fname) &&
    strcmp(out_fname, def_fname);
FILE *in_f;
int lbuf_len = 1024;
char *lbuf;
inisize(lbuf, lbuf_len);
if(read_strings) {
  if(!(in_f = fopen(out_fname, "r"))) {
    perror(out_fname);
    exit(1);
  }
  /* builtin_str is const, so read into regular char * first */
  char *prevstr = NULL;
  /* also, track allocated length rather than actual length */
  int prevstr_size = 0;
  inisize(prevstr, (prevstr_size = 1024));
  /* and track current insertion location instead */
  char *d = prevstr;
  while(1) {
    <<Read a full line into [[lbuf]]>>
    <<Parse and append old builtin string>>
  }
  fclose(in_f);
  builtin_str = prevstr;
  spos = builtin_str_size = d - prevstr;
}
@

<<Read a full line into [[lbuf]]>>=
int off = 0;
while(1) {
  if(!fgets(lbuf + off, lbuf_len - off, in_f) || !*lbuf)
    break;
  off += strlen(lbuf + off);
  if(lbuf[off - 1] == '\n')
    break;
  lbuf_len *= 2;
  resize(lbuf, lbuf_len);
}
if(!off)
  break;
@

The format of each line is a string constant.  The constant may be
followed by a comment, such as the string number and offset.  Rather
than parse an arbitrary C string constant, only the output format is
supported.  The output format is plain ASCII characters unescaped,
backslash and quote characters backslash-escaped, and anything else
escaped as a 3-digit octal escape.

<<[[mkstrs]] definitions>>=
typedef unsigned char uchar;
static void put_str(const char *buf, int len, FILE *f)
{
  for(; len--; buf++) {
    if(*buf == '\\')
      fputs("\\\\", f);
    else if(*buf == '"')
      fputs("\\\"", f);
    else if(*buf < ' ' || *buf > '~')
      fprintf(f, "\\%03o", (uchar)*buf);
    else
      fputc(*buf, f);
  }
}
@

<<Known Data Types>>=
uchar,%
@

<<Parse and append old builtin string>>=
if(*lbuf != '"') /* lines that aren't strings are ignored */
  continue;
char *s = lbuf + 1;
while(1) {
  for( ; *s && *s != '"' && (d - prevstr) < prevstr_size; s++, d++) {
    if(*s == '\\') {
      if(s[1] == '\\' || s[1] == '"')
        *d = *++s;
      else if(s[1] && s[2] && s[3]) {
        char c = s[4];
        s[4] = 0;
        *d = strtol(s + 1, NULL, 8);
        s[4] = c;
        s += 3;
      } else {
        fprintf(stderr, "Invalid previous string file\n");
        exit(1);
      }
    } else
      *d = *s;
  }
  /* here, either at end of string or end of buffer space */
  if(*s == '"')
    break;
  if(d == prevstr + prevstr_size) {
    int l = d - prevstr;
    resize(prevstr, (prevstr_size *= 2));
    d = prevstr + l;
    continue;
  }
  /* or end of input... */
  fprintf(stderr, "Invalid previous string file\n");
  exit(1);
}
@

The old hash entries need to be read in as well, in order to determine
string locations at the very least.  The format of the file is a pair
of numbers per line, separated by a space.  The first is the hash
value of the string, and the second is its offset.

<<Generate built-in string table>>=
if(read_strings) {
  if(!(in_f = fopen(hash_fname, "r"))) {
    perror(hash_fname);
    exit(1);
  }
  hashent_t *hashe = NULL; /* writable version of builtin_hashe */
  int hashe_size = 1024; /* its size */
  inisize(hashe, hashe_size);
  unsigned long h, str;
  uint32_t hen = 0;
  while(fscanf(in_f, "%lu %lu\n", &h, &str) == 2) {
    if(hen == hashe_size) {
      hashe_size *= 2;
      resize(hashe, hashe_size);
    }
    hashent_t *he = &hashe[hen];
    he->off = str;
    h %= HTAB_SIZE;
    he->nextent = strhash[h];
    strhash[h] = ++hen;
  }
  fclose(in_f);
  if(pos != hen + 1) {
    fprintf(stderr, "Invalid hash file; expected %u entries but got %u\n",
            pos - 1, hen);
    exit(1);
  }
  builtin_hashe_size = hen;
  builtin_hashe = hashe;
}
@

Now that the old data has been read in, the output files can be opened
for writing.  Technically, they could have used "r+" to avoid a second
open, but this way is easier to write.  In case any file names are
repeated, the file is only opened once.  The only reason this would
probably happen is if debugging, though.

<<Generate built-in string table>>=
/* open strings output */
FILE *out_f;
if(out_fname && *out_fname && strcmp(out_fname, "-")) {
  out_f = fopen(out_fname, out_fmode);
  if(!out_f) {
    perror(out_fname);
    exit(1);
  }
} else
  out_f = stdout;
/* open definitions output */
FILE *def_f;
if(def_fname && *def_fname && strcmp(def_fname, "-")) {
  if(!strcmp(def_fname, out_fname))
    def_f = out_f;
  else {
    def_f = fopen(def_fname, def_fmode);
    if(!def_f) {
      perror(def_fname);
      exit(1);
    }
  }
} else
  def_f = stdout;
/* open hash output */
FILE *hash_f;
if(hash_fname && *hash_fname && strcmp(hash_fname, "-")) {
  if(!strcmp(hash_fname, out_fname))
    hash_f = out_f;
  else if(!strcmp(hash_fname, def_fname))
    hash_f = def_f;
  else {
    hash_f = fopen(hash_fname, hash_fmode);
    if(!hash_f) {
      perror(hash_fname);
      exit(1);
    }
  }
} else
  hash_f = stdout;
@

The format of the input file is one string per line.  In order to
specify a value to insert before the string, an integer (in C integer
format) may be specified as well.  It is separated from the string by
a custom character.

<<[[mkstrs]] option variables>>=
char sep = 0;
@

<<Process [[mkstrs]] option>>=
case 'v':
  sep = isdigit((*argv)[2]) ? atoi(*argv + 2) : (*argv)[2];
  break;
@

One difficulty is that the built-in strings are massive: nearly 700K
for the Unicode names alone.  How could this be reduced, though?  One
way would be to 5-bit-encode the Unicode names, saving 30\% without
much effort.  However, space on modern machines is not at such a
premium any more, so this probably just wastes time.  Similarly, the
Unicode names can be split out as words, saving nearly 40\% in string
storage space, at the expense of adding 10000 more entries into the
hash table (for a total savings of 25\%, actually).  Again, this is
probably not worth it.  However, I may change my mind some time, so
this program supports splitting Unicode names into words.

<<[[mkstrs]] option variables>>=
int do_words = 0;
@

<<Process [[mkstrs]] option>>=
case 'w':
  /* NOTE: this option is for Unicode character name separation only! */
  /*  - words are separated by spaces */
  /*  - it assumes strings are ASCII (no high bits set!) */
  /*  - the words it adds are not usable for anything else */
  /*    unless already added (so load order is important) */
  do_words = 1;
  break;
@

Before spitting out the first string, we can add a definition to the
file indicating the start of the range.  However, if splitting words,
there may be hash entries for the words themselves first, so only
print it when not doing words.  In addition, it might help debugging
to print a comment before the first string in the output file as well.

<<Generate built-in string table>>=
if(print_def && !do_words) {
  fprintf(def_f, "#define %s_FIRST %u\n", print_def, pos);
  fprintf(out_f, "/* %s_FIRST */\n", print_def);
}
@

Then, we need to loop over all strings read from the inputs.

<<Generate built-in string table>>=
/* now, loop through new strings */
<<Prepare to loop over new builtin strings>>
do { /* for every input file */
  /* open next input file (stdin if none) */
  if(do_stdin || !strcmp(*argv, "-"))
    in_f = stdin;
  else {
    in_f = fopen(*argv, "r");
    if(!in_f) {
      perror(*argv);
      exit(1);
    }
  }
  if(!do_stdin)
    argv++;
  while(1) {
    <<Read a full line into [[lbuf]]>>
    <<Process a new built-in string>>
  }
} while(!do_stdin && --argc);
@

The format is not actually as described before:  in order to allow any
strings to be added, backslash escapes are supported as well.  These
are a simple escaped backslash, an escaped value separator, or a
two-digit hexadecimal character code.  So, the first thing to do is to
parse the string, replacing backslash-escapes and locating the value,
if present.

<<Process a new built-in string>>=
char *s, *d;
uint32_t val = 0;
for(s = d = lbuf; *s; s++, d++) {
  if(*s == '\\') {
    if(*++s == '\\' || *s == sep)
      *d = *s;
    else {
      if(!isxdigit(s[1]) || !isxdigit(s[2])) {
        fprintf(stderr, "Bad line: %s\n", lbuf);
        exit(1);
      }
      char t = s[3];
      s[3] = 0;
      *d = strtol(s, &s, 16);
      *s = t;
    }
  } else if(*s == sep) {
    val = strtol(s + 1, NULL, 0);
    break;
  } else if(*s == '\r' || *s == '\n')
    break;
}
uint32_t len = (uint32_t)(d - lbuf);
/* now, lbuf/len is string, val is value */
@

Now, if not in word mode, we simply spit out the string and move on.
The value and length are of course output first, and the string position
based on how many characters were output for the prefix. Rather than
check for ASCII characters, the length and code number are output as
octal escapes only.

<<[[mkstrs]] definitions>>=
static int put_ln(uint32_t n, FILE *f)
{
  uint32_t vl = 0, v[5], ret;
  while(n > 0x7f) {
    v[vl++] = n & 0x7f;
    n >>= 7;
  }
  ret = vl + 1;
  fprintf(f, "\\%03o", n);
  while(vl > 0)
    fprintf(f, "\\%03o", v[--vl] + 128);
  return ret;
}
@

<<Process a new built-in string>>=
if(!do_words) {
  <<Spit out built-in string>>
  /* spos is updated by prefix above */
  spos += len;
  pos++;
  continue;
}
@

<<Spit out built-in string>>=
fputc('"', out_f);
if(sep)
  spos += put_ln(val, out_f);
spos += put_ln(len, out_f);
put_str(lbuf, len, out_f);
if(sep)
  fprintf(out_f, "\" /* %u/%u%c%u */\n", spos, pos, sep, val);
else
  fprintf(out_f, "\" /* %u/%u */\n", spos, pos);
@

In addition to the string file, the definitions file may need
updating.  The format of the definition name is the prefix, followed
by an underscore, followed by the characters of the string.  If a
character is a space, it is output as double-underscores.  Otherwise,
if it is not a valid C identifier character, it is printed as an
underscore, followed by a lower-case x, follwed by the 2-digit hex
code.  This does not guarantee uniqueness, but it does guarantee that
the identifier is valid C, and in combination with the prefix, can
give a good assurance of uniqueness.

<<[[mkstrs]] definitions>>=
static void pr_enc(const char *s, int len, FILE *f)
{
  while(--len >= 0) {
    if(*s == ' ')
      fputs("__", f);
    else if(*s < ' ' || *s > '~' || (!isalnum(*s) && *s != '_'))
      fprintf(f, "_x%02x", *s);
    else
      fputc(*s, f);
    ++s;
  }
}
@

<<Spit out built-in string>>=
if(enum_prefix) {
  fprintf(def_f, "#define %s_", enum_prefix);
  pr_enc(lbuf, (int)(d - lbuf), def_f);
  fprintf(def_f, " %u\n", pos);
}
@

Finally, the hash table entry needs to be printed.

<<Spit out built-in string>>=
fprintf(hash_f, "%u %u\n", hashfun(lbuf, len), spos);
@

For word mode, processing strings is a little more complicated.
Instead of spitting out words as they arrive, they are added to the
hash table.  The original string is then converted to an encoded form.
The encoding retains spaces, but converts other contiguous strings
into their encoded string number.  Since encoding a single-word
character this way could not possible make it shorter, these are left
unencoded.

<<Process a new built-in string>>=
/* from this point on, it's word mode */
char *we, *ws = lbuf;
for(ws = lbuf; ws < lbuf + len; ws = we + 1) {
  for(we = ws; we < lbuf + len; we++)
    if(*we == ' ')
      break;
  if(we == ws)
    continue;
  int wlen = (int)(we - ws);
  /* don't bother if entire string is one word */
  if(wlen == len)
    break;
  <<Add word to string table and encode>>
}
@

In fact, the encoding of the words requires at least two bytes per
word, so words less than three characters are pointless to encode.

<<Add word to string table and encode>>=
if(wlen <= 2)
  continue; /* too short to bother */
@

Since there is no guarantee that substrings of Unicode names are not
also Unicode names, space needs to be allocated in front of the newly
added word to allow for a value.  Rather than mess with efficient
coding, the value is placed as a 32-bit value, along with a flag
indicating presence.  All newly added words get this.  When writing
the strings out, the words without values will be printed before the
words with values.  This will cause the string numbers to change.  In
order to support this, the updated string number is stored before the
string as a 32-bit number as well.  This means that 9 bytes need to be
allocated before any newly added word.  The first byte is the value
flag; the next four are the value, and the rest are for the updated
string number.  For newly added words, there is never a value
(although it may be added later).

<<Prepare to loop over new builtin strings>>=
int cur_hen = pos, old_hen = pos;
@

<<Add word to string table and encode>>=
/* auto-add and leave 9 bytes of room before word */
int str = strno(ws, wlen, -9);
/* if auto-added, set "value" flag to 0 */
if(str == cur_hen) {
  s = (char *)str_for(str, NULL);
  int ll; /* skip strlen */
  num_before(s, &ll);
  s -= ll + 1;
  *s = 0;
  cur_hen++;
}
@

Since UTF-8 does not cover all 32-bit values, and in order to avoid
conflict with unencoded words and spaces, the encoding of the
string number is a little-endian representation, with 7 bits per byte
and the high bit set on each byte.  This should not increase
the length, but just in case, we'll compute the replacement into a
buffer, and shift the remainder of the string to make room.

<<Add word to string table and encode>>=
/* encode strno(word) and replace word w/ encoding */
/* FIXME: expand lbuf if needed (not needed for Unicode 6.0) */
char ebuf[5];
int eblen;
uint32_t strout = str;
for(eblen = 0; eblen < 5 && strout; eblen++, strout >>=7)
  ebuf[eblen] = 0x80 | (strout & 0x7f);
memmove(ws + eblen, we, len - (int)(we - lbuf));
memcpy(ws, ebuf, eblen);
len -= wlen - eblen;
we -= wlen - eblen;
@

Finally, the encoded form (or unencoded if there was just one word) is
added to the string table.  Again, 9 bytes need to be added if the
string is newly added.  If it isn't, it is assumed to match a word
that was previously added, and so it also has 9 bytes of space.  If
that assumption does not hold, there is a big problem, so print the
unencoded bad string and exit.  To print the unencoded string, the
encoded string numbers are extracted and expanded.

<<Process a new built-in string>>=
int str = strno(lbuf, len, -9);
if(str < old_hen) {
  pr_lw(lbuf, len, stderr);
  fprintf(stderr, ": short name conflict (already there)\n");
  exit(1);
}
@

<<[[mkstrs]] definitions>>=
static void pr_lw_mayenc(const char *buf, int len, FILE *f, int enc)
{
  const char *s;
  
  while(1) {
    for(s = buf; len && !(*s & 0x80); s++, len--);
    if(s != buf) {
      if(enc)
        pr_enc(buf, (int)(s - buf), f);
      else
        fwrite(buf, (int)(s - buf), 1, f);
    }
    if(!len)
      return;

    uint32_t str;

    str = *s & 0x7f;
    int shift = 7;
    while(--len && (*++s & 0x80)) {
      str += (uint32_t)(*s & 0x7f) << shift;
      shift += 7;
    }
    buf = s;
    int l;
    s = str_for(str, &l);
    if(enc)
      pr_enc(s, l, f);
    else
      fwrite(s, l, 1, f);
  }
}
#define pr_lw(a,b,c) pr_lw_mayenc(a,b,c,0)
#define pr_lw_enc(a,b,c) pr_lw_mayenc(a,b,c,1)
@

We can now modify the 9 prefix bytes regardless of whether or not this
was added, but we still need to keep track of how many have been added
in order to detect word additions.  The value flag is set depending on
whether or not there is a value.  Since this is for Unicode character
names, which always have values, this flag also indicates whether it
is a full name or just a word.

<<Process a new built-in string>>=
int ll;
s = (char *)str_for(str, NULL);
num_before(s, &ll);
s -= ll + 1;
if(str == cur_hen) {
  *s = 0;  
  cur_hen++;
}
if(*s) { /* there should not already be a value there */
  pr_lw(lbuf, len, stderr);
  fprintf(stderr, ": short name conflict (already processed)\n");
  exit(1);
}
*s = 1;
if(sep) {
  *--s = val & 0xff;
  val >>= 8;
  *--s = val & 0xff;
  val >>= 8;
  *--s = val & 0xff;
  val >>= 8;
  *--s = val & 0xff;
}
@

After adding all of the strings to string table, they need to be
printed.

<<Generate built-in string table>>=
if(do_words) {
  <<Print word mode builtin strings>>
}
@

The standalone words need to be printed separately from the real
words, so that the range of string numbers printed for the real
numbers is accurate.  While it may seem that it does not matter which
order these are printed in, the words must be printed first.  As they
are added, their string number is updated, and these updated numbers
must be known before an encoded string using that word can be printed.
They are stored in the four bytes beyond the value.

<<Print word mode builtin strings>>=
uint32_t i;
for(i = old_hen; i < cur_hen; i++) {
  const char *str = str_for(i, NULL);
  int ll, len = num_before(str, &ll);
  if(str[-ll - 1]) /* has value */
    continue;
  fputc('"', out_f);
  spos += put_ln(len, out_f);
  put_str(str, len, out_f);
  fprintf(out_f, "\" /* %u/%u>%u */\n", spos, i, pos);
  fprintf(hash_f, "%u %u\n", hashfun(str, len), spos);
  spos += len;
  uint32_t npos = pos++;
  char *s = (char *)str - ll - 5;
  *--s = npos & 0xff;
  npos >>= 8;
  *--s = npos & 0xff;
  npos >>= 8;
  *--s = npos & 0xff;
  npos >>= 8;
  *--s = npos & 0xff;
}
@

Now that the words are all printed, the first element of the range is
known.

<<Print word mode builtin strings>>=
if(print_def) {
  fprintf(def_f, "#define %s_FIRST %u\n", print_def, pos);
  fprintf(out_f, "/* %s_FIRST */\n", print_def);
}
@

All encoded strings are printed next.  However, they must be updated
to new word locations first.  Since some may not really be encoded,
and therefore may be used by other words, the updated string number is
stored in them as well.

<<Print word mode builtin strings>>=
for(i = old_hen; i < cur_hen; i++) {
  const char *str = str_for(i, NULL);
  int ll, len = num_before(str, &ll);
  if(!str[-ll - 1]) /* has no value */
    continue;
  fputc('"', out_f);
  uint32_t val = 0;
  if(sep) {
    val =
      (uint32_t)(uchar)str[-ll - 2] +
      ((uint32_t)(uchar)str[-ll - 3] << 8) +
      ((uint32_t)(uchar)str[-ll - 4] << 16) +
      ((uint32_t)(uchar)str[-ll - 5] << 24);
    spos += put_ln(val, out_f);
  }
  <<Reencode builtin string [[str]] into [[lbuf]]/[[len]]>>
  spos += put_ln(len, out_f);
  put_str(lbuf, len, out_f);
  if(sep)
    fprintf(out_f, "\" /* %u/%u%c%u ", spos, pos, sep, val);
  else
    fprintf(out_f, "\" /* %u/%u ", spos, pos);
  /* print unencoded string */
  pr_lw(str, num_before(str, NULL), out_f);
  fputs(" */\n", out_f);
  fprintf(hash_f, "%u %u\n", hashfun(str, len), spos);
  if(enum_prefix) {
    fprintf(def_f, "#define %s_", enum_prefix);
    pr_lw_enc(str, num_before(str, NULL), def_f);
    fprintf(def_f, " %u\n", pos);
  }
  spos += len;
  uint32_t npos = pos++;
  d = (char *)str - ll - 5;
  *--d = npos & 0xff;
  npos >>= 8;
  *--d = npos & 0xff;
  npos >>= 8;
  *--d = npos & 0xff;
  npos >>= 8;
  *--d = npos & 0xff;
  npos++;
}
@

To reencode the string, all substrings with the high bit set must be
extracted, converted to numbers, looked up, and replaced with the
number that was stored 6-9 bytes before the length.

<<Reencode builtin string [[str]] into [[lbuf]]/[[len]]>>=
const char *s, *e = str + len;
char *d = lbuf;
for(s = str; s < e; s++) {
  if(!(*s & 0x80))
    *d++ = *s;
  else {
    uint32_t os = *s & 0x7f;
    int shift = 7;
    while(++s < e && (*s & 0x80)) {
      os += (uint32_t)(*s & 0x7f) << shift;
      shift += 7;
    }
    s--;
    if(os >= old_hen) {
      int oll;
      const char *ostr = str_for(os, NULL);
      num_before(ostr, &oll);
      os = (uint32_t)(uchar)ostr[-oll - 6] +
           ((uint32_t)(uchar)ostr[-oll - 7] << 8) +
           ((uint32_t)(uchar)ostr[-oll - 8] << 16) +
           ((uint32_t)(uchar)ostr[-oll - 9] << 24);
    }
    while(os > 0) {
      *d++ = 0x80 | (os & 0x7f);
      os >>= 7;
    }
  }
}
len = d - lbuf;
@


Finally, the last string has been printed, so spit out its number if
required.

<<Generate built-in string table>>=
if(print_def) {
  fprintf(def_f, "#define %s_LAST %u\n", print_def, pos - 1);
  fprintf(out_f, "/* %s_LAST */\n", print_def);
}
return 0;
@

Now that the string buffer is generated, we need to generate a hash
table.  This generates both the initial hash entry array and the hash
table itself.

\lstset{language=make}
<<C Build Executables>>=
mkstrs-hash \
@

<<Built-in string table components>>=
mkstrs-hash \
@

<<makefile.rules>>=
mkstrs-hash: mkstrs-hash.o
	$(CC) $(LDFLAGS) -o $@ $^
@

\lstset{language=C}
<<mkstrs-hash.c>>=
<<Common C Header>>
@

In order to generate the hash table, it needs to be built in memory.
Because the buckets are built by modifying only the newly added entry,
there is no need to keep any entries once they are output.  The only
additional storage kept is a count of how many entries were added to
each slot, so that an estimate of table badness can be printed on
completion.

<<mkstrs-hash.c>>=
uint32_t strhash[HTAB_SIZE], hdepth[HTAB_SIZE];
@

The mainline prints a header for the hash entry array for the static
version (requiring sed to alter it for non-static versions).  It then
reads every line from standard input, assuming it is the hash output
from the previous stage, and prints the hash table entry.  When done
with that, it dumps the hash table itself, as well as any statistics
it accumulated.

<<mkstrs-hash.c>>=
int main(void)
{
  <<Print hash entries>>
  <<Print hash table>>
  <<Print hash statistics>>
  return 0;
}
@

<<Print hash entries>>=
puts("const hashent_t builtin_hashe[] = {");
int he;
unsigned long h, str;
for(he = 0; scanf("%lu %lu", &h, &str) == 2; he++) {
  h %= HTAB_SIZE;
  if(he)
    fputs(",\n", stdout);
  printf("\t{%lu, %u}", str, strhash[h]);
  strhash[h] = he + 1;
  ++hdepth[h];
}
puts("\n};");
@

<<Print hash table>>=
fputs("uint32_t strhash[HTAB_SIZE] = {", stdout);
int i;
for(i = 0; i < HTAB_SIZE - 1; i++) {
  if(!(i & 15))
    fputs("\n\t", stdout);
  else
    putchar(' ');
  printf("%d,", strhash[i]);
}
printf(" %d\n"
       "};\n", strhash[i]);
@

<<Print hash statistics>>=
int maxd = 1, nhid = 0, nfill = 0;
for(i = 0; i < HTAB_SIZE; i++) {
  if(maxd < hdepth[i])
    maxd = hdepth[i];
  if(hdepth[i]) {
    nhid += hdepth[i] - 1;
    ++nfill;
  }
}
printf("/* max depth %d num hidden %d of %d; full %d/%d (%.2f%%) */\n",
       maxd, nhid, he, nfill, HTAB_SIZE, (double)nfill/(double)HTAB_SIZE*100);
@

Now we can start generating string tables.

Multi-character entries will be extracted from another table.  In
order to support this, multi-character entries have their code point
list replaced with a sequence number (the table entry index plus the
max Unicode code point), and the code points are printed as well, in a
format that is easily distinguishable from normal entries (i.e., with
a dash in the name).

<<Prepare for XML filter>>=
int nmulti = 0;
@

<<Print XML character list entry>>=
l = strchr(++s, ',');
if(!l)
  printf("%s,%s", buf, s);
else {
  printf("%s,0x%04X\n", buf, nmulti+0x110000);
  printf("%04X-0x%04X,0x%04X\n", nmulti,
         (int)strtol(s, NULL, 0), 
         (int)strtol(l + 1, NULL, 0));
  ++nmulti;
}
@

The main string table consists of all Unicode character names.
Any multi-character sequences are converted to an offset from
the maximum code point so that the sequences can be stored in a
separate table.

\lstset{language=make}
<<Built-in string table components>>=
charnames.uni charnames.unia charnames.uniseq \
@

<<makefile.config>>=
# Set to -w to use word mode (needs more work)
#UNI_NAME_WORD=-w
@

<<Generate built-in string table using [[mkstrs]]>>=
( \
  cat -n charnames.uniseq | while read -r n s; do \
    echo "$${s%%,*},$$((n+0x10ffff))"; \
  done; \
  cat charnames.uni{,a}; \
) | tr 'A-Z\055' 'a-z ' | \
  ./mkstrs $(UNI_NAME_WORD) -v, -DUNI_CHARS -hstrs-hash.gen -ostrs-strs.gen \
           -dbuiltin-strings.h
@

<<Clean temporary files>>=
rm -f builtin-strings.h
@

<<Post-process built-in string table>>=
./mkstrs-hash <strs-hash.gen >$@
echo "const char builtin_str[] =" >>$@
echo '#include "strs-strs.gen"' >>$@
echo ";" >>$@
@

<<makefile.rules>>=
builtin-strings.h: strs.gen
	touch builtin-strings.h
strs-strs.gen: strs.gen
	touch strs-strs.gen
@

The automatically generated names were not included in the above
lists.  In order to make it easier to look them up in the name-to-code
direction, their prefixes are added to the name table.  In addition, a
range table is added to [[chartypes]] to support the reverse lookup.

First, we'll generate a listing of the ranges, similar to the other
listings.

<<Build Script Executables>>=
mkunirng \
@

<<Plain Built Files>>=
charnames.unirng \
@

<<makefile.rules>>=
charnames.unirng: $(UNIDATA) mkunirng
	./mkunirng $(UNIDATA) >$@
@

\lstset{language=sh}
<<mkunirng>>=
#!/bin/sh
<<Common NoWeb Warning>>

@

There are three groups of automatically generated character names:
the Hangul syllables, the other ranges in UnicodeData.txt, and CJK
COMPATIBILITY IDEOGRAPH groups.  The first has a slightly more
involved generation algorithm; the other two simply append their code
point in hex after a dash.  What sets the CJK COMPATIBILITY IDEOGRAPH
code points apart is that they are listed individually in
UnicodeData.txt rather than as ranges.

The routine grabs both types at once, and outputs a start token (the
prefix followed by a caret) at the start of any range, and an end
token (the prefix followed by a dollar sign) at the end.

<<mkunirng>>=
cut -d\; -f1-2 $1 | egrep '<[^c]|CJK COMPATIBILITY IDEOGRAPH' | \
  dd conv=ucase 2>/dev/null | (
    last=; lastb=
    while IFS=\;, read a b c; do
      an=$((0x$a))
      b="${b%-*}"
      if [ -n "$last" ]; then
        case "$c" in
          *">") echo "$lastb \$,$last"; last= ;;
          *) last=$((last+1))
             if [ $last -ne $an -o "$lastb" != "$b" ]; then
               echo "$lastb \$,$((last-1))"
               last=$an
               lastb="$b"
               echo "$b ^,$an"
             fi ;;
        esac
      fi
      case "$c" in
        " FIRST>") echo "${b#<} ^,0x$a" ;;
        *">") echo "${b#<} \$,0x$a" ;;
        *) if [ -z "$last" ]; then
             last=$an
             lastb="$b"
             echo "$b ^,$an"
           fi ;;
      esac;
    done;
  )
@

For the name lookup, the start name of the range is added.  Rather
than try to look up both ends from the symbol table, further
verification is done using ranges.  In fact, there is no point in
storing the code point at all.

<<Built-in string table components>>=
charnames.unirng \
@

<<Generate built-in string table using [[mkstrs]]>>=
tr 'A-Z-' 'a-z ' < charnames.unirng | fgrep '^' | cut -d, -f1 | sort -u | \
  ./mkstrs $(UNI_NAME_WORD) -DUNI_RANGES -aUNI_RANGE \
           -h+strs-hash.gen -o+strs-strs.gen -d+builtin-strings.h \
           -l`fgrep UNI_CHARS_LAST builtin-strings.h | cut -d\  -f3`
@

The range table is generated using yet a different script, setting the
auxiliary value to the name from the symbol table.  To verify that a
symbol lookup was correct, just ensure that [[uni_range_of]]'s return
value matches the symbol lookup after extracting the code point.

<<Built-in string table components>>=
charnames.unirng \
@

\lstset{language=make}
<<Generate character name data>>=
# Unicode range reverse lookup
echo '#include "builtin-strings.h"' >>$@
echo "const uni_chrrng_dat_t unirng_range[] = {" >>$@
tr 'A-Z' 'a-z' < charnames.unirng | sed 's/[ -]/__/g;s/\^/_x5e/' | \
 while IFS=, read n cp; do \
   printf '\t{%s, ' $$cp; \
   IFS=, read ign cp; \
   echo "$$cp, UNI_RANGE_$${n}},"; \
done | sed '$$s/,$$//' >>$@
echo "};" >>$@
echo "const uint32_t unirng_range_len = sizeof(unirng_range)/sizeof(unirng_range[0]);" >>$@
@

\lstset{language=C}
<<Character name lookup exports>>=
/* returns string # of low-end of range */
int uni_range_of(int cp);
@

<<Character name lookup functions>>=
<<Return table lookup for range table>>
int range_ret(range, 0)
@

Now enough pieces are together to create some routines for character
name lookup.

<<Library [[uni]] headers>>=
#include "uni_charname.h"
@

<<uni_charname.h>>=
<<Common C Warning>>
#ifndef CHARNAMES_H
#define CHARNAMES_H

#include "uni_common.h"
<<Character name lookup exports>>

#endif
@

<<Library [[uni]] Members>>=
uni_charname.o
@

<<uni_charname.c>>=
<<Common C Header>>
#include "uni_charname.h"

<<Character name lookup local definitions>>

<<Character name lookup functions>>
@

To find the code point given a name, we need to pass in a string and
its length, and expect to get a code point in return.  For the
multi-character sequences, we instead receive an array.  The length of
the array is returned instead of a code point; the existence of the
array distinguishes between the two.  If nothing is found, a negative
value is returned (since zero is a valid code point, and negative
numbers aren't).

<<Character name lookup exports>>=
/* returns < 0 if no match */
/* returns array in arr and length of arr if > 1 char */
/* otherwise returns cp and sets arr to NULL */
int cp_of_uni(const char *name, int len, const int **arr);
@

<<Character name lookup functions>>=
int cp_of_uni(const char *name, int len, const int **arr)
{
  <<Look up code point of Unicode character [[name]]/[[len]]>>
}
@

Both lookup functions use the number stored before the length in the
string table.

<<Character name lookup local definitions>>=
static int cp_for(uint32_t str)
{
    int llen;
    const char *cns = str_for(str, NULL);
    num_before(cns, &llen);
    return num_before(cns - llen, NULL);
}
@

<<makefile.rules>>=
uni_charname.o: builtin-strings.h
@

<<Character name lookup local definitions>>=
#include "builtin-strings.h"
@

<<Character name lookup local definitions>>=
<<Character name generated structures>>
#include "charnames.gen"
@

\lstset{language=make}
<<makefile.rules>>=
charnames.gen: <<Character name data generation dependencies>>

	echo >$@
	<<Generate character name data>>
uni_charname.o: charnames.gen
@

Looking up Unicode names is much more difficult.  On a lookup failure,
we need to downcase the entire string and remove dashes and
underscores.  This requires allocating a buffer.  For now, this is
done on the stack.  To avoid stack overflow, no character names larger
than 128 characters are accepted (the actual maximum is around 83).

If the transformed name still can't be looked up, it needs to be
looked up as a range.

<<Look up code point of Unicode character [[name]]/[[len]]>>=
if(len > 128)
  return -1;
int cn = strno(name, len, 0);
if(cn < UNI_CHARS_FIRST || cn > UNI_CHARS_LAST) {
  char buf[len + 1];
  int i;
  for(i = 0; i < len; i++)
    buf[i] = name[i] == '_' || name[i] == '-' ? ' ' :
             isupper(name[i]) ? tolower(name[i]) : name[i];
  cn = strno(buf, len, 0);
  if(cn < UNI_CHARS_FIRST || cn > UNI_CHARS_LAST) {
    <<Look up code point of Unicode range element [[buf]]>>
  }
}
int cp = cp_for(cn);
@

To look up a range entry, the last word needs to be replaced by a
caret.  If that lookup fails, it isn't a range entry, so return $-1$.

<<Look up code point of Unicode range element [[buf]]>>=
char *s;
for(s = buf + len - 1; --s > buf; )
  if(*s == ' ')
    break;
if(s <= buf)
  return -1;
char c = *++s;
*s = '^';
cn = strno(buf, (int)(s - buf + 1), 0);
if(cn < UNI_RANGES_FIRST || cn > UNI_RANGES_LAST)
  return -1;
*s = c;
@

Then, that last word needs to be validated.  For all but the Hangul
syllables, the last word is a hexadecimal code point of at least four
characters with no superfluous leading zeroes.  There are no
multi-character sequences matching these names.  The table of ranges
returns the low end of the range, which is what we looked up, so if
they match, we're good to go.

<<Character name lookup local definitions>>=
#include "uni_chartype.h"
@

<<Look up code point of Unicode range element [[buf]]>>=
buf[len] = 0;
if(cn != UNI_RANGE_hangul__syllable___x5e) {
  if(len - (s - buf) < 4)
    return -1;
  if(len - (s - buf) > 4 && *s == '0')
    return -1;
  char *es;
  int cp = strtol(s, &es, 16);
  if(*es || uni_range_of(cp) != cn)
    return -1;
  *arr = NULL;
  return cp;
}
@

Hangul is more difficult, requiring character-by-character parsing.
It might be possible to come up with a regular expression to do the
match, but that would not help with returning the correct code point.
Instead, lookup tables are used for all three parts.  The tables are
filled from the abbreviations listed in Jamo.txt.

\lstset{language=make}
<<makefile.vars>>=
UNIJAMO = $(UCD_LOC)/Jamo.txt
@

<<Character name data generation dependencies>>=
$(UNIJAMO) \
@

<<Generate character name data>>=
for y in CHO JUNG JONG; do \
  echo "static const struct abent $${y}_dec[] = {"; \
  fgrep $${y}SEONG $(UNIJAMO) | cut -d\# -f1 | fgrep \; | \
    dd conv=lcase 2>/dev/null | cut -d\; -f2 | cat -n | while read a b; do \
      echo " {\"$$b\", $$((a-1))},"; \
    done | sort | sed '$$s/,$$//'; \
  echo "};"; \
done >>$@
@

\lstset{language=C}
<<Character name generated structures>>=
struct abent {
    const char *name;
    uint32_t cp;
};
@

<<Character name lookup local definitions>>=
#define NUM_CHO (sizeof(CHO_dec)/sizeof(CHO_dec[0]))
#define NUM_JUNG (sizeof(JUNG_dec)/sizeof(JUNG_dec[0]))
#define NUM_JONG (sizeof(JONG_dec)/sizeof(JONG_dec[0]))
@

The L portion is always either one character or one doubled character
or blank.  The blank can be detected by the fact that V shares no
characters with L.  In order to search the table quickly, the first
character, and, if doubled, the second cahracter are joined into a
string and searched using plain binary search.

<<Look up code point of Unicode range element [[buf]]>>=
/* L is 0 char or 1 char or 1 doubled char */
if(*s == s[1]) {
  c = s[2];
  s[2] = 0;
} else {
  c = s[1];
  s[1] = 0;
}
/* look up in table */
/* could use bsearch() but seems silly */
int l, h, m, cmp = 0;
l = 0; h = NUM_CHO - 1;

while(l <= h) {
  m = (l + h) / 2;
  cmp = strcmp(s, CHO_dec[m].name);
  if(cmp < 0)
    h = m - 1;
  else if(cmp > 0)
    l = m + 1;
  else
    break;
}
/* if not found, assume L is blank */
if(cmp)
  m = 0;
int L = CHO_dec[m].cp;
s += strlen(s);
*s = c;
@

Binary search fails too easily due to possible T presence, and the V
table is short with short strings, so the search for V is linear.

<<Look up code point of Unicode range element [[buf]]>>=
/* find first match */
int ll = 0;
for(m = 0; m < NUM_JUNG; m++) {
  ll = strlen(JUNG_dec[m].name);
  if(!strncmp(s, JUNG_dec[m].name, ll))
    break;
}
/* there is no blank V, so not found == error */
if(m == NUM_JUNG)
  return -1;
/* find longest match */
int m2;
for(m2 = m + 1; m2 < NUM_JUNG; m2++) {
  int ll2 = strlen(JUNG_dec[m2].name);
  if(ll2 <= ll || memcmp(JUNG_dec[m2].name, JUNG_dec[m].name, ll))
    break;
  if(!strncmp(s, JUNG_dec[m2].name, ll2)) {
    ll = ll2;
    m = m2;
  }
}
int V = JUNG_dec[m].cp;
s += strlen(JUNG_dec[m].name);
@

Finally, the remaining characters are a full string again, so binary
search is the easiest.

<<Look up code point of Unicode range element [[buf]]>>=
/* last part can be binary searched again, if present */
int T = 0;
if(*s) {
  l = 0; h = NUM_JONG - 1;
  while(l <= h) {
    m = (l + h) / 2;
    cmp = strcmp(s, JONG_dec[m].name);
    if(cmp < 0)
      h = m - 1;
    else if(cmp > 0)
      l = m + 1;
    else
      break;
  }
  /* unrecognized suffix is error */
  if(cmp)
    return -1;
  T = JONG_dec[m].cp + 1;
}
*arr = NULL;
return 0xAC00 + T + (NUM_JONG + 1) * (V + NUM_JUNG * L);
@

None of the ranged characters are multi-character, but any of the
others might be.  The sequences are loaded into an array.  The
sequence length can vary from 2--4, so table entry is zero-terminated.

<<Look up code point of Unicode character [[name]]/[[len]]>>=
if(cp < 0x110000) {
  *arr = NULL;
  return cp;
}
const int *ccp = &charseq[cp-0x110000][0];
int i;
for(i = 0; i < 4; i++)
  if(!ccp[i])
    break;
*arr = ccp;
return i;
@

\lstset{language=make}
<<Generate character name data>>=
# unicode character sequences
echo "const int charseq[][4] = {" >>$@
cut -d, -f2- < charnames.uniseq | sed 's/.*/{&},/;$$s/,$$//' >>$@
echo "};" >>$@
@

To do reverse lookups, the static string should be returned.  However,
for Unicode, we have to generate a name in case the name is
algorithmically generated.

\lstset{language=C}
<<Character name lookup exports>>=
/* always returns target length, even if len is too small */
int uni_gen_name_for(int cp, char *buf, int len);
@

<<Character name lookup functions>>=
int uni_gen_name_for(int cp, char *buf, int len)
{
  <<Return Unicode name of [[cp]]>>
}
@

<<Character name generated structures>>=
struct chrnm {
    uint32_t name, cp;
};
@

<<Character name lookup local definitions>>=
static int nmcmp(const void *a, const void *b)
{
    return ((struct chrnm *)a)->cp - ((struct chrnm *)b)->cp;
}
@

\lstset{language=make}
<<Character name data generation dependencies>>=
strs.gen \
@

Unicode is more difficult, of course, and not just because the name
needs to be copied into an output buffer.  The first part is simple,
and the same as for XML, though:  look up the code point in the name
table.  The table is only filled with entries which are not
automatically generated, so this might fail even if there is a good
name.  Like with XML, the table is generated from the string data.

\lstset{language=C}
<<Return Unicode name of [[cp]]>>=
uint32_t str;
struct chrnm cps = { 0, cp }, *res;

/* FIXME: change to manual bsearch */
if((res = bsearch(&cps, uni_nmord, sizeof(uni_nmord)/sizeof(*uni_nmord), sizeof(*uni_nmord), nmcmp)))
  str = res->name;
else
  str = 0;
@

\lstset{language=make}
<<Generate character name data>>=
# cp -> string reverse lookup
echo 'static const struct chrnm uni_nmord[] = {' >>$@
sed -n -e '/UNI_CHARS_FIRST/,/UNI_CHARS_LAST/{s%.*/\* [0-9]*/\([0-9]*,[0-9]*\) .*\*/$$%  {\1},%;T;p;}' \
  strs-strs.gen | sort -t, -k 2n | sed '$$s/,$$//' >>$@
echo '};' >>$@
@

For strings that are found, we are not done, though.  Unless of course
no characters are actually to be copied out, since the length is
indeed accurate.

\lstset{language=C}
<<Return Unicode name of [[cp]]>>=
if(str) {
  int nlen;
  const char *s;

  s = str_for(str, &nlen);
  if(!len)
    return nlen;
  <<Post-process regular Unicode character name>>
  return nlen;
}
@

The first thing to do is zero-terminate the return string, as required
by the API.  At this point there is at least one character available
in the output buffer, so setting it blindly is OK.

<<Post-process regular Unicode character name>>=
if(len > nlen + 1)
  len = nlen + 1;
buf[--len] = 0;
@

Next, we copy out the characters, converting them to upper-case.  They
are stored lower-case in the symbol table because NFKC\_CF converts
strings to lower-case for comparison.

<<Post-process regular Unicode character name>>=
int i;
for(i = 0; i < len; i++, s++)
   buf[i] = islower(*s) ? toupper(*s) : *s;
@

The other transformation which was done for easy comparison is to
strip the dashes out.  To fix this, a table of dash locations is used.
This table stores the offsets into the name of up to three dashes; no
name was found with more than three.  The offset actually starts at one
so that the array of three can be zero-terminated.  This actually
requires a fourth byte as well.

% FIXME: use signed offset instead; sign == negative for dash, pos for
% space; also, remove cp and store offset into this table with name
% string, and expand d to 12 chars, and end loop at 12

<<Character name generated structures>>=
struct dashloc {
    int cp;
    unsigned char d[4];
};
@

<<Post-process regular Unicode character name>>=
const struct dashloc dl = {cp}, *dash;

/* FIXME: change to manual bsearch */
dash = bsearch(&dl, dashloc, sizeof(dashloc)/sizeof(*dashloc), sizeof(*dashloc),
               cmpdl);
if(dash) {
  const unsigned char *d = dash->d;
  while(*d && *d <= len)
    buf[*d++ - 1] = '-';
}
@

<<Character name lookup local definitions>>=
static int cmpdl(const void *a, const void *b)
{
    return ((struct dashloc *)a)->cp - ((struct dashloc *)b)->cp;
}
@

To generate the array, some shell code is used to find dashes and
count characters.

\lstset{language=make}
<<Generate character name data>>=
# dash locations (stripped out for stored name)
echo "const struct dashloc dashloc[] = {" >>$@
cut -d\; -f1,2 $(UNIDATA) | fgrep -v 'CJK COMPATIBILITY IDEOGRAPH' | \
  fgrep -- - | while IFS=\; read cp x; do \
    d="$${x%%-*}"; \
    n=$$(($${#d}+1)); \
    ns=$$n; \
    while :; do \
      x="$${x#*-}"; \
      case "$$x" in \
        *-*) d="$${x%%-*}"; \
	     n=$$((n+$${#d}+1)); \
             ns=$$ns,$$n ;; \
          *) break ;; \
      esac; \
    done; \
    echo "    {0x$$cp,{$$ns}},"; \
  done | sed '$$s/,$$//' >>$@
echo "};" >>$@
@

Once again, the ranges add more effort.  At least if it is not one of
the range code points, we can return immediately.  Otherwise, we may
as well copy out as much of the name as we already have.  There are no
dashes in any of these names, except just before the hexadecimal code
point on the ones which have this.

\lstset{language=C}
<<Return Unicode name of [[cp]]>>=
int low = uni_range_of(cp);
if(!low)
  return 0;
int nlen;
const char *s;
s = str_for(low, &nlen);
int fulllen = nlen - 1; /* remove the ^ */
if(len > 1) {
  /* back up to last space */
  if(--nlen > len - 1)
    nlen = len - 1;
  else
    --nlen;
  len -= nlen;
  while(--nlen >= 0) {
    if(islower(*s))
      *buf++ = toupper(*s++);
    else
      *buf++ = *s++;
  }
}
@

Speaking of which, those now require appending the hexadecimal code
point, and a dash.  Precomputing the length is pretty easy, since
there are either 4, 5, or 6 digits depending on the code point range.
That means we can finally return a value if no output buffer was
given.  Otherwise, we write the suffix, followed by a 0, and return
the computed length.

<<Return Unicode name of [[cp]]>>=
if(low != UNI_RANGE_hangul__syllable___x5e) {
  fulllen += 4;
  if(cp > 0xffff)
    fulllen++;
  if(cp > 0xfffff)
    fulllen++;
  if(!len)
    return fulllen;
  if(len > 1) {
    *buf++ = '-';
    --len;
  }
  if(--len)
    snprintf(buf, len + 1, "%04X", cp);
  else
    *buf = 0;
  return fulllen;
}
@

For Hangul, the computation of the letters to append is pretty simple,
but the table already generated does not have the letters in the right
order, so they are added again, but this time in all caps and in the
right order, and without any additional data.

<<Return Unicode name of [[cp]]>>=
int t = (cp - 0xac00) % 28;
int v = (cp - 0xac00) / 28;
int l = v / 21;
v %= 21;
const char *ls = CHO_abbrev[l];
const char *vs = JUNG_abbrev[v];
const char *ts = t ? JONG_abbrev[t - 1] : "";
@

\lstset{language=make}
<<Generate character name data>>=
# hangul syllable lookup table
for y in CHO JUNG JONG; do \
  echo "static const char *$${y}_abbrev[] = {"; \
  fgrep $${y}SEONG $(UNIJAMO) | cut -d\# -f1 | fgrep \; | \
    while read a b; do \
      echo " \"$$b\","; \
    done | sed '$$s/,$$//'; \
  echo "};"; \
done >>$@
@

Having the letters makes computation of the full length trivial.

\lstset{language=C}
<<Return Unicode name of [[cp]]>>=
int ll = strlen(ls);
int vl = strlen(vs);
int tl = strlen(ts);
fulllen += ll + vl + tl;
if(!len)
  return fulllen;
@

Now to tack on what fits:  a space, followed by the L abbreviation,
followed by the V abbreviation, followed by the T abbreviation.  Then
we can return the computed length and it's over.

<<Return Unicode name of [[cp]]>>=
if(len > 1) {
  *buf++ = ' ';
  --len;
}
/* append L */
if(ll > len)
  ll = len;
len -= ll;
while(--ll >= 0)
  *buf++ = *ls++;
/* append V */
if(vl > len)
  vl = len;
len -= vl;
while(--vl >= 0)
  *buf++ = *vs++;
/* append T */
if(tl > len)
  tl = len;
while(--tl >= 0)
  *buf++ = *ts++;
/* terminate and return */
*buf = 0;
return fulllen;
@

\section{File Input}

Basic I/O support is provided in the form of UTF decoders and
encoders.  More advanced I/O is provided in the form of arbitrary
encoding input.

<<Library [[uni]] Members>>=
uni_io.o
@

\lstset{language=C}
<<uni_io.c>>=
<<Common C Header>>
#include "uni_io.h"

<<Unicode I/O local definitions>>

<<Unicode I/O functions>>
@

<<Library [[uni]] headers>>=
#include "uni_io.h"
@

<<uni_io.h>>=
<<Common C Warning>>
#ifndef UNI_IO_H
#define UNI_IO_H

#include "uni_common.h"
<<Unicode I/O Exports>>
#endif
@

\subsection{Basic Unicode I/O}

Unicode defines several file formats:  UTF-8, UTF-16, and UTF-32.  No
special conversion needs to be made from UTF-32 to integer code
points, other than to filter out invalid values and possibly byte-swap
the code point.

<<Common C Includes>>=
#include <stdint.h>
/* FIXME: this is glibc-specific */
/* BSD apparently uses <sys/endian.h> */
/* OpenBSD additionally uses different fn names */
/* others may not even have any equivalent functions */
#include <endian.h>
@

\lstset{language=make}
<<makefile.vars>>=
# for endian
EXTRA_CFLAGS += -D_BSD_SOURCE
@

\lstset{language=C}
<<Unicode I/O Exports>>=
int utf32_encode(uint32_t *buf, int cp, int bige);
@

<<Unicode I/O functions>>=
int utf32_encode(uint32_t *buf, int cp, int bige)
{
  if(cp > 0x10ffff || (cp >= 0xd800 && cp < 0xe000))
    return 0;
  *buf = bige ? htobe32(cp) : htole32(cp);
  return 1;
}
@

<<Unicode I/O Exports>>=
int utf32_decode(const uint32_t *s, int bige);
@

<<Unicode I/O functions>>=
int utf32_decode(const uint32_t *s, int bige)
{
  uint32_t c = bige ? be32toh(*s) : le32toh(*s);
  if(c > 0x10ffff || (c >= 0xd800 && c < 0xe000))
    return -1;
  return c;
}
@

<<Unicode I/O Exports>>=
int utf32_putc(int c, FILE *f, int bige);
@

<<Unicode I/O functions>>=
int utf32_putc(int c, FILE *f, int bige)
{
  uint32_t w;

  if(!utf32_encode(&w, c, bige))
    return 0;
  return fwrite(&w, 4, 1, f);
}
@

<<Unicode I/O Exports>>=
int utf32_getc(FILE *f, int bige); /* always reads 4 bytes */
@

<<Unicode I/O functions>>=
int utf32_getc(FILE *f, int bige)
{
  uint32_t w;
  int ret;

  if((ret = fread(&w, 1, 4, f)) != 4)
    return ret ? -2 : -1;
  ret = utf32_decode(&w, bige);
  return ret < 0 ? -3 : ret;
}
@

For UTF-16, a slightly more complex scheme is used, involving the
surrogate code points (D800 through DFFF) in pairs.  The first half of
the surrogate code points (D800 through DBFF) is always used for the
first member of the pair, and the second half is used for the other.
The first member gives the first 5 bits, and the second member gives
the remainder. Since this is for representing code points 10000 and
up, an extra bit can be gained giving the full range through 10FFFF.

<<Unicode I/O Exports>>=
/* make sure at least 2 words avail in buf */
/* returns # of words written */
int utf16_encode(uint16_t *buf, int cp, int bige);
@

<<Unicode I/O functions>>=
int utf16_encode(uint16_t *buf, int cp, int bige)
{
  if((cp >= 0xD800 && cp < 0xE000) || cp > 0x10FFFF)
    return 0;
  if(cp < 0x10000) {
    *buf = bige ? htobe16(cp) : htole16(cp);
    return 1;
  }
  int u = (cp & 0x1f0000) >> 16;
  cp &= 0xffff;
  uint16_t w;
  w = 0xd800 + ((u - 1) @<< 10) + (cp @>> 10);
  *buf++ = bige ? htobe16(w) : htole16(w);
  w = 0xdc00 + (cp > 0x3fff);
  *buf = bige ? htobe16(w) : htole16(w);
  return 2;
}
@

<<Unicode I/O Exports>>=
/* this function returns -1 on errors */
/* if the buffer may be invalid, ensure at least 2 words available */
int utf16_decode(const uint16_t *s, int *nread, int bige);
@

<<Unicode I/O functions>>=
int utf16_decode(const uint16_t *s, int *nread, int bige)
{
  uint16_t w = bige ? be16toh(*s) : le16toh(*s);
  if(nread)
    *nread = 1;
  if(w < 0xd800 || w >= 0xe000)
    return w;
  if(w >= 0xdc00)
    return -1;
  uint16_t w2 = bige ? be16toh(s[1]) : le16toh(s[1]);
  if(w2 >= 0xe000 || w2 < 0xdc00)
    return -1;
  if(nread)
    *nread = 2;
  int c = w2 & 0x3ff;
  c += (w & 0x3f) << 10;
  w = (w >> 10) & 0xf;
  c += (w + 1) << 16;
  return c;
}
@

<<Unicode I/O Exports>>=
int utf16_putc(int c, FILE *f, int bige);
@

<<Unicode I/O functions>>=
int utf16_putc(int c, FILE *f, int bige)
{
  unsigned char buf[4];
  uint16_t enc[2], encl;
  
  encl = utf16_encode(enc, c, bige);
  if(!encl)
    return 0;
  if(bige) {
    buf[0] = enc[0] >> 8;
    buf[1] = enc[0];
    if(encl > 1) {
      buf[2] = enc[1] >> 8;
      buf[3] = enc[1];
    }
  } else {
    buf[1] = enc[0] >> 8;
    buf[0] = enc[0];
    if(encl > 1) {
      buf[3] = enc[1] >> 8;
      buf[2] = enc[1];
    }
  }
  return fwrite(buf, 2, encl, f);
}
@

The input function needs to read ahead to get the second member of a
pair.  If that character is not what was expected, the correct
behavior is to undo the readahead and return the current code point as
an error.  However, it is not possible in C standard I/O to push more
than one character back into the stream.  For now, this is going to
have to be erroneous behavior.

<<Unicode I/O Exports>>=
int utf16_getc(FILE *f, int bige, int *nread);  /* nread == # of bytes read */
@

<<Unicode I/O functions>>=
int utf16_getc(FILE *f, int bige, int *nread)
{
  int c = fgetc(f), c2;
  if(c == EOF) {
    if(nread)
      *nread = 0;
    return -1;
  }
  c2 = fgetc(f);
  if(c == EOF) {
    if(nread)
      *nread = 1;
    return -2;
  }
  if(bige)
    c = c2 + (c << 8);
  else
    c += c2 << 8;
  if(c < 0xd800 || c >= 0xE000) {
    if(nread)
      *nread = 1;
    return c;
  }
  if(c >= 0xdc00) {
    if(nread)
      *nread = 2;
    return -3;
  }
  int res = (c & 0x3f) << 10;
  int w = (c >> 10) & 0xf;
  res += (w + 1) << 16;
  if((c = fgetc(f)) == EOF) {
    if(nread)
      *nread = 2;
    return -2;
  }
  if((c2 = fgetc(f)) == EOF) {
    if(nread)
      *nread = 3;
    return -2;
  }
  if(bige)
    c = c2 + (c << 8);
  else
    c += c2 << 8;
  if(c < 0xdc00 || c >= 0xe000) {
#if 0 /* FIXME: this does not work; ungetc() can't be called twice */
    ungetc(c2, f);
    ungetc(bige ? c >> 8 : c & 0xff, f);
    if(nread)
      *nread = 2;
#else
    if(nread)
      *nread = 4;
#endif
    return -3;
  }
  if(nread)
    *nread = 4;
  res += c & 0x3ff;
  return res + (c & 0x3ff);
}
@

For UTF-8, an even more complex encoding scheme is used.  Again, a
standalone memory codec is provided.  All code points over 007F are
encoded using a multi-byte sequence.  All characters in a multi-byte
sequence have their high bit set; the first non-zero bit determines
the byte's role.  All but the first byte have only one high bit set,
and encode 6 bits.  The first byte determines how many trailing bytes
there are, and also encode 3--5 bits.  The number of high bits set in
the first byte is the total number of bytes in the sequence.

<<Unicode I/O Exports>>=
/* make sure at least 4 bytes avail in buf */
/* returns # of bytes written */
int utf8_encode(char *buf, int cp);
@

<<Unicode I/O functions>>=
int utf8_encode(char *buf, int cp)
{
    if((cp >= 0xD800 && cp < 0xE000) || cp > 0x10FFFF)
        return 0;
    if(cp < 128) {
        *buf = cp;
        return 1;
    } else if(cp < 0x800) {
        *buf = 0xc0 + (cp >> 6);
        buf[1] = 0x80 + (cp & 0x3f);
        return 2;
    } else if(cp < 0x10000) {
        *buf = 0xe0 + (cp >> 12);
        buf[1] = 0x80 + ((cp >> 6) & 0x3f);
        buf[2] = 0x80 + (cp & 0x3f);
        return 3;
    } else {
        *buf = 0xf0 + (cp >> 18);
        buf[1] = 0x80 + ((cp >> 12) & 0x3f);
        buf[2] = 0x80 + ((cp >> 6) & 0x3f);
        buf[3] = 0x80 + (cp & 0x3f);
        return 4;
    }
}
@

<<Unicode I/O Exports>>=
/* this function returns -1 on errors */
/* if the buffer may be invalid, ensure at least 4 chars available */
int utf8_decode(const char *buf, int *nread);
@

<<Unicode I/O functions>>=
int utf8_decode(const char *buf, int *nread)
{
    if(*buf < 0x7f) {
        *nread = 1;
        return (unsigned char)*buf;
    }
    int c = (unsigned char)*buf++, ec;
    if((c & 0xf8) == 0xf0) {
        c &= 0x07;
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 1;
            return -1;
        }
        c = (c << 6) + (ec & 0x3f);
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 2;
            return -1;
        }
        c = (c << 6) + (ec & 0x3f);
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 3;
            return -1;
        }
        *nread = 4;
        c = (c << 6) + (ec & 0x3f);
        if(c < 0x10000 || c > 0x10FFFF)
            return -1;
        return c;
    }
    if((c & 0xf0) == 0xe0) {
        c &= 0x0f;
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 1;
            return -1;
        }
        c = (c << 6) + (ec & 0x3f);
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 2;
            return -1;
        }
        *nread = 3;
        c = (c << 6) + (ec & 0x3f);
        if(c < 0x800 || (c >= 0xD800 && c < 0xE000))
            return -1;
        return c;
    }
    if((c & 0xe0) == 0xc0) {
        c &= 0x1f;
        ec = (unsigned char)*buf++;
        if((ec & 0xc0) != 0x80) {
            *nread = 1;
            return -1;
        }
        *nread = 2;
        c = (c << 6) + (ec & 0x3f);
        if(c < 0x80)
            return -1;
        return c;
    }
    *nread = 1;
    return -1;
}
@

<<Unicode I/O Exports>>=
int utf8_putc(int c, FILE *f);
@

<<Unicode I/O functions>>=
int utf8_putc(int c, FILE *f)
{
    char obuf[4];
    int nout = utf8_encode(obuf, c);
    return fwrite(obuf, 1, nout, f);
}
@

UTF-8 output is common enough to warrant a string version of the
output function as well.

<<Unicode I/O Exports>>=
int utf8_fputs(int *buf, int len, FILE *f);
@

<<Unicode I/O functions>>=
int utf8_fputs(int *buf, int len, FILE *f)
{
  int ret = 0;

  while(len-- > 0)
    ret += utf8_putc(*buf++, f);
  return ret;
}
@

Like UTF-16 input, UTF-8 input requires some readahead.  For anything
more than one character, C once again disallows returning the
characters to the stream.  So, once again, bad input causes erroneous
behavior.

<<Unicode I/O Exports>>=
int utf8_getc(FILE *f, int *nread);  /* nread == # of bytes read */
@

<<Unicode I/O functions>>=
int utf8_getc(FILE *f, int *nread)
{
  int c;
  int chrread = 1;

  if((c = getc(f)) == EOF) {
    c = -1;
    chrread = 0;
  } else if(c >= 0x80) {
    int ec;
    chrread = 1;
    if((c & 0xf8) == 0xf0) {
      <<Read 4-char utf-8>>
      if(c < 0x10000 || c > 0x10ffff)
        c = -3;
    } else if((c & 0xf0) == 0xe0) {
      <<Read 3-char utf-8>>
      if(c < 0x800 || (c >= 0xD800 && c < 0xE000))
        c = -3;
    } else if((c & 0xe0) == 0xc0) {
      <<Read 2-char utf-8>>
      if(c < 0x80)
        c = -3;
    } else
      c = -3;
  }
  if(nread)
    *nread = chrread;
  return c;
}
@

<<Read 4-char utf-8>>=
c &= 0x07;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
  if((ec = getc(f)) == EOF)
    c = -2;
  else if((ec & 0xc0) != 0x80) {
    ungetc(ec, f);
    c = -3;
  } else {
    chrread = 3;
    c = (c << 6) + (ec & 0x3f);
    if((ec = getc(f)) == EOF)
      c = -2;
    else if((ec & 0xc0) != 0x80) {
      ungetc(ec, f);
      c = -3;
    } else {
      chrread = 4;
      c = (c << 6) + (ec & 0x3f);
    }
  }
}
@

<<Read 3-char utf-8>>=
c &= 0x0f;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
  if((ec = getc(f)) == EOF)
    c = -2;
  else if((ec & 0xc0) != 0x80) {
    ungetc(ec, f);
    c = -3;
  } else {
    chrread = 3;
    c = (c << 6) + (ec & 0x3f);
  }
}
@

<<Read 2-char utf-8>>=
c &= 0x1f;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
}
@

\subsection{General File Input}

A more general UTF reader would have to scan for a byte order mark and
remember what it said.  This requires retaining state.

<<Unicode I/O Exports>>=
typedef struct {
  FILE *f;
  char *name;
  <<Unicode file buffer state>>
} unifile_t;
@

<<Known Data Types>>=
unifile_t,%
@

Rather than automatically attaching state to a file on first access
and never really knowing when to free it, explicit routines are
provided to create and remove the state.   

<<Unicode I/O Exports>>=
/* encoding = NULL for automatic detection */
/* automatically detects utf16/utf32 if encoding == UTF-8 */
unifile_t *uni_fopen(const char *name, const char *encoding);
void uni_fclose(unifile_t *uf);
@

<<Unicode I/O functions>>=
unifile_t *uni_fopen(const char *name, const char *encoding)
{
  unifile_t *uf;

  inisize(uf, 1);
  clearbuf(uf, 1);
  uf->name = strdup(name);
  if(!uf->name) {
    free(uf);
    return NULL;
  }
  uf->f = fopen(name, "rb");
  if(!uf->f) {
    free(uf->name);
    free(uf);
    return NULL;
  }
  <<Initialize unicode file buffer state>>
  return uf;
}
@

<<Unicode I/O functions>>=
void uni_fclose(unifile_t *uf)
{
  free(uf->name);
  fclose(uf->f);
  <<Free unicode file buffer state>>
  free(uf);
}
@

One limitation of standard I/O is that only one character may be
pushed back into the stream.  To correct this, a 4-byte buffer is
kept, along with a count.  In order to avoid reading from the file
after the buffered characters have been removed, an end-of-file flag
is kept as well.  In order to allow error messages to display the
correct file offset, that is also kept.

<<Unicode file buffer state>>=
size_t fpos;
char readahead[4];
unsigned char raptr;
unsigned char eof;
@

The first thing to do is to decide the file's encoding.  ASCII and
ISO-Latin-1 can be read raw, since they map directly to Unicode.
UTF-8 is taken to mean automatic Unicode detection, with UTF-8 as a
default.  For all other input types, the [[iconv]] function should be
used.%
\footnote{The C99 library provides something similar in the
[[mbstowcs]] function, but the POSIX [[iconv]] function is commonly
available (perhaps even as a third party library), and could be
provided as wrapper around [[mbstowcs]] if all else fails.  Unlike
[[mbstowcs]], the type of the output can always be set to Unicode
encodings, and the input type can be controlled more easily as well.}
This function should be in the standard C library, but may be provided
externally as well.  If so, [[CFLAGS]] and [[LDFLAGS]] may need to be
modified appropriately.  In order to accomodate the case where a
minimal C library has no working [[iconv]], or to otherwise avoid code
bloat, this can be disabled with a configuration variable.

\lstset{language=make}
<<makefile.config>>=
# Set to non-empty to enable iconv input support
#USE_ICONV=y
@

<<makefile.vars>>=
ifneq ($USE_ICONV,)
EXTRA_CFLAGS += -DUNI_USE_ICONV
endif
@

<<Unicode I/O local definitions>>=
#ifdef UNI_USE_ICONV
#include <langinfo.h>
#include <locale.h>
#include <iconv.h>
#endif
@


<<Unicode file buffer state>>=
unsigned char enctype;
/*
 * 0 = ASCII/Latin-1
 * 1 = iconv (if UNI_USE_ICONV)
 * 8/16/32 = utf-8/16/32
 */
#ifdef UNI_USE_ICONV
void *enchelp;
#endif
@

<<Unicode I/O local definitions>>=
#ifdef UNI_USE_ICONV
#define ICBUF_SIZE 1024
typedef struct {
  iconv_t ic;
  char ibuf[ICBUF_SIZE];
  int ilen;
  uint32_t obuf[ICBUF_SIZE];
  int optr, olen;
} iconv_supt_t;
#endif
@

<<Known Data Types>>=
iconv_supt_t,%
@

<<Initialize unicode file buffer state>>=
#ifdef UNI_USE_ICONV
if(!encoding) {
  setlocale(LC_CTYPE, "");
  encoding = nl_langinfo(CODESET);
}
#endif
if(!encoding)
  encoding = "UTF-8";
if(encoding && (!strcmp(encoding, "ISO-8859-1") ||
                !strcmp(encoding, "ANSI_X3.4-1968"))) /* ASCII */
  return uf; /* enctype == 0 */
#ifdef UNI_USE_ICONV
else if(encoding && strcmp(encoding, "UTF-8")) {
  iconv_supt_t *ics;
  inisize(ics, 1);
#if __BYTE_ORDER == __LITTLE_ENDIAN
#define bo "LE"
#else
#define bo "BE"
#endif
  ics->ic = iconv_open("UTF-32" bo, encoding);
  if(ics->ic == (iconv_t)-1) {
    perror(encoding);
    exit(1);
  }
  uf->enchelp = ics;
  uf->enctype = 1;
  return uf;
}
#endif
/* otherwise it's Unicode */
@

<<Free unicode file buffer state>>=
#ifdef UNI_USE_ICONV
if(uf->enctype == 1 && uf->enchelp) {
  iconv_supt_t *ics = uf->enchelp;
  iconv_close(ics->ic);
  free(ics);
}
#endif
@

The first thing to do on opening a new Unicode file is to check for
the byte order mark (U+FEFF).  Flags are kept to indicate what was
found.  Note that the byte order mark is still returned as the first
character, so all read characters must be backed out.

<<Unicode file buffer state>>=
unsigned char bige;
@

<<Initialize unicode file buffer state>>=
uf->enctype = 8; /* default is UTF-8 */
/* Read and process BOM (FEFF) */
int c;
<<Read and store a char from [[uf]]>>
if(c == 0xff) {
  <<Read and store a char from [[uf]]>>
  if(c == 0xfe) { /* FFFE UTF-16BE */
    uf->bige = 1;
    uf->enctype = 16;
  }
} else if(c == 0xfe) {
  <<Read and store a char from [[uf]]>>
  if(c == 0xff) {
    <<Read and store a char from [[uf]]>>
    if(!c) {
      <<Read and store a char from [[uf]]>>
      if(!c) /* FEFF0000 UTF-32LE */
        uf->enctype = 32;
      else /* FEFF */
        uf->enctype = 16;
    } else /* FEFF UTF-16LE */
      uf->enctype = 16;
  }
} else if(!c) {
  <<Read and store a char from [[uf]]>>
  if(!c) {
    <<Read and store a char from [[uf]]>>
    if(c == 0xfe) {
      <<Read and store a char from [[uf]]>>
      if(c == 0xff) { /* 0000FEFF UTF-32BE */
        uf->enctype = 32;
	uf->bige = 1;
      }
    }
  }
}
@

<<Read and store a char from [[uf]]>>=
c = getc(uf->f);
if(c == EOF)
  uf->eof = 1;
else
  uf->readahead[uf->raptr++] = c;
@

The reader then uses the appropriate method to read a character based
on the [[enctype]].

<<Unicode I/O Exports>>=
int uni_fgetc(unifile_t *uf);
@

<<Unicode I/O functions>>=
int uni_fgetc(unifile_t *uf)
{
  if(!uf->enctype) {
    int c = getc(uf->f);
    if(c == EOF)
      uf->eof = 1;
    else
      uf->fpos++;
    return c;
#ifdef UNI_USE_ICONV
  } else if(uf->enctype == 1) {
    <<Return a character using iconv>>
#endif
  }
  <<Return a Unicode character>>
}
@

Rather than using the potentially flawed direct I/O routines, the
Unicode reader always reads raw bytes and then calls the decoder
instead.  That way, the readahead issue disappears.  The readahead
buffer is always flushed to four characters every time, since UTF-32
and UTF-16 both may require that much.  UTF-8 only requires three, but
there is little point in making that a special case.

<<Return a Unicode character>>=
while(!uf->eof && uf->raptr < 4) {
  int nr = fread(uf->readahead + uf->raptr, 1, 4 - uf->raptr, uf->f);
  if(nr <= 0)
    uf->eof = 1;
  else
    uf->raptr += nr;
}
if(uf->eof && !uf->raptr)
  return -1;
int cp, l;
if(uf->enctype == 32) {
  cp = utf32_decode((uint32_t *)uf->readahead, uf->bige);
  l = 4;
} else if(uf->enctype == 16) {
  cp = utf16_decode((uint16_t *)uf->readahead, &l, uf->bige);
  l *= 2;
} else
  cp = utf8_decode(uf->readahead, &l);
if(l > uf->raptr) {
  fprintf(stderr, "Ill-formed character @ %lu\n", uf->fpos);
  uf->raptr = 0;
  return -2;
}
if(l < 4)
  memmove(uf->readahead, uf->readahead + l, 4 - l);
if(cp < 0)
  fprintf(stderr, "Ill-formed character @ %lu\n", uf->fpos);
uf->raptr -= l;
uf->fpos += l;
return cp < 0 ? -2 : cp;
@

The [[iconv]] conversion proceeds much differently.  If a character is
availble in the output buffer, it validates and returns that character.
Otherwise, it fills up the input buffer from the file, and then
attempts to fill the output buffer using [[iconv]].  Any unprocessed
input characters are then moved to the start of the input buffer for
the next pass.

<<Return a character using iconv>>=
iconv_supt_t *ics = uf->enchelp;
while(1) {
  if(ics->olen) {
    --ics->olen;
    return utf32_decode(&ics->obuf[ics->optr++], __BYTE_ORDER == __BIG_ENDIAN);
  }
  char *ibuf = ics->ibuf;
  size_t ilen = ics->ilen;
  char *obuf = (char *)ics->obuf;
  size_t olen = ICBUF_SIZE * 4;
  while(!uf->eof && ilen < ICBUF_SIZE) {
    int nr = fread(ibuf, 1, ICBUF_SIZE - ilen, uf->f);
    if(nr <= 0)
      uf->eof = 1;
    else
      ilen += nr;
  }
  while(1) {
    iconv(ics->ic, &ibuf, &ilen, &obuf, &olen);
    if(ibuf != ics->ibuf)
      break;
    /* ignore bad chars */
    if(!--ilen)
      break;
    memmove(ics->ibuf, ics->ibuf + 1, ilen);
  }
  if(ilen)
    memmove(ics->ibuf, ibuf, ilen);
  else if(uf->eof)
    iconv(ics->ic, NULL, NULL, &obuf, &olen);
  ics->olen = olen / 4;
  ics->optr = 0;
  ics->ilen = ilen;
}
@

\subsection{Generic Normalized Unicode Input}

To make things even easier, we can make a function which returns a
normalized character.  Any normalization type we have code for, we
support.

<<Unicode normalization support exports>>=
typedef enum {
  UN_NONE, UN_NFD, UN_NFKD, UN_NFC, UN_NFKC, UN_NFKC_CF
} uni_normtype_t;
@

<<Known Data Types>>=
uni_normtype_t,%
@

<<Unicode normalization support exports>>=
#include "uni_io.h"

int uni_fgetc_norm(unifile_t *uf, uni_normtype_t nt);
@

<<Unicode normalization support functions>>=
int uni_fgetc_norm(unifile_t *uf, uni_normtype_t nt)
{
  <<Get normalized character>>
}
@

A simple function which returns a normalized character requires that
the current decomposition and composition state be kept.  It would be
ideal to extend the [[unifile_t]] structure with the extra information
rather than allocating yet another wrapper, but for now I would prefer
[[unifile_t]] to stay isolated to unnormalized I/O.

In order to make this transparent, the extra support structure is
automatically created and placed on a linked list.  It is assumed that
the file will not be closed and reopened without an intervening EOF.
It is also assumed that the function will not be called any more after
it returns EOF.

<<Unicode normalization support local definitions>>=
typedef struct unifile_norm {
  struct unifile_norm *next;
  unifile_t *uf;
  <<Normalized input buffer state>>
} unifile_norm_t;
static unifile_norm_t *norm_files = NULL;
@

<<Known Data Types>>=
unifile_norm_t,%
@

<<Get normalized character>>=
unifile_norm_t *ufn = norm_files;

while(ufn && ufn->uf != uf)
  ufn = ufn->next;
if(!ufn) {
  inisize(ufn, 1);
  clearbuf(ufn, 1);
  ufn->next = norm_files;
  norm_files = ufn;
}

<<Return normalized character if not EOF>>

unifile_norm_t **bufp;
for(bufp = &norm_files; *bufp != ufn; bufp = &(*bufp)->next);
*bufp = ufn->next;
free(ufn);

return -1;
@

For support, we'll need to store at least the maximal decomposition of
a character.  Technically, it is possible to require an infinite
buffer to support canonical ordering and composition, but we'll only
support a finite number and hope for the best.  Some future revision
should make the buffer automatically expand when necessary.

<<Normalized input buffer state>>=
/* maximal decomposition of a single char is 18 chars */
/* leave room in buf for at least 2 */
int buf[128];
@

We'll also need to know how many characters are in the buffer, and how
many of those are good to go.

<<Normalized input buffer state>>=
int blen, oklen;
@

To return a single character, we'll need to normalize until at least
one character could not possibly be modified any more.

<<Return normalized character if not EOF>>=
int oklen = ufn->oklen;
int *buf = ufn->buf;
while(!oklen && !uf->eof) {
  /* read a char into end of buf, if needed & possible */
  <<Read and normalize a character>>
}
if(oklen) {
  /* now we have oklen chars in buf that are ready to return */
  int c = buf[0];
  memmove(buf, buf + 1, (--ufn->blen) * sizeof(*buf));
  ufn->oklen = oklen - 1;
  return c;
}
@

To add more characters, append a character to the buffer and
decompose it.  The Unicode standard states that case folding
requires an additional NFD step beforehand, but reading of the data
suggests that instead, it needs canonical ordering and composition
afterwards, just like the NFC and NFKC.  This makes case folding just
another decomposition method.

int nread;

<<Read and normalize a character>>=
int c;
int bp = ufn->blen;
c = buf[bp] = uni_fgetc(uf);
if(nt == UN_NONE) {
  oklen = ufn->blen = ++bp;
  break;
}
if(c != -1)
  switch(nt) {
    case UN_NONE: /* here to remove warning */
    case UN_NFD:
    case UN_NFC:
      bp += NFD_dec(buf + bp, 1);
      break;
    case UN_NFKD:
    case UN_NFKC:
      bp += NFKD_dec(buf + bp, 1);
      break;
    case UN_NFKC_CF:
      bp += NFKC_Casefold(buf + bp, 1);
  }
@

Then, the entire set of characters already read in can be ordered.  If
no characters can be obtained from canonical ordering, try to read
more right away.

<<Read and normalize a character>>=
int oblen = Canon_Order(buf, bp, uf->eof);
if(!oblen) {
  ufn->blen = bp;
  continue;
}
@

Then, all characters which have been ordered can be composed.  If
composition might need more characters, the next trip around the loop
will get them.

<<Read and normalize a character>>=
switch(nt) {
  case UN_NONE: /* here to remove warning */
  case UN_NFD:
  case UN_NFKD:
    /* no composition */
    oklen = oblen;
    break;
  case UN_NFC:
  case UN_NFKC:
  case UN_NFKC_CF: {
    int cblen = NFC_comp(buf, oblen, uf->eof ? NULL : &oklen);
    if(uf->eof)
      oklen = cblen;
    /* move unordered characters down if composition removed chars */
    if(cblen < oblen) {
      if(bp > oblen)
        memmove(buf + cblen, buf + oblen, (bp - oblen) * sizeof(*buf));
      bp -= oblen - cblen;
    }
    break;
  }
}
ufn->blen = bp;
@

\section{Testing}

Here is a master driver to call some of the above code for testing.

<<C Test Support Executables>>=
tst \
@

<<tst.c>>=
<<Common C Header>>
#include "uni_all.h"

@

For I/O, I'll just use a decent size fixed buffer, and in fact use it
as both a UCS-32 buffer and a character buffer, allowing both at the
same time by marking the divider with [[blen]].

<<tst.c>>=
#define BUF_SIZE 1024
int buf[BUF_SIZE];
int blen;

#define cbuf ((char *)(buf+blen))
#define cblen ((BUF_SIZE - blen) * sizeof(int))
@

The purpose of this program is to take Unicode characters as input,
and do things with them.

<<tst.c>>=
int main(int argc, const char **argv)
{
  <<Get and show characters for [[tst]]>>
  <<Manipulate characters for [[tst]]>>
  return 0;
}
@

For this program, input means command line arguments.  Characters are
specified by either a hexadecimal code point or a Unicode character
name.  They are all appended to [[buf]].

<<Get and show characters for [[tst]]>>=
int cp;

while(--argc > 0) {
  ++argv;
  int l = strlen(*argv);
  const int *mcp;
  cp = cp_of_uni(*argv, l, &mcp);
  if(cp >= 0) {
    /* valid name */
    if(!mcp)
      buf[blen++] = cp;
    else {
      int i;
      for(i = 0; i < cp; i++)
        buf[blen++] = mcp[i];
      cp = mcp[0];
    }
  } else {
    const char *s;
    cp = strtol(*argv, (char **)&s, 16);
    if(!*s) {
      /* valid hex code point */
      buf[blen++] = cp;
    } else {
      /* not valid anything */
      fprintf(stderr, "Can't parse argument %s\n", *argv);
      exit(1);
    }
  }
}
@

Next, we'll print some more information about each character.  This
includes characters artificially added by multi-character sequences.

<<Get and show characters for [[tst]]>>=
int i;
for(i = 0; i < blen; i++) {
  cp = buf[i];
  <<Show character for [[tst]]>>
}
@

First, I'd like to see the Unicode names, as well as the hexadecimal
code points.

<<Show character for [[tst]]>>=
printf("Character %04X\n", cp);
if(uni_gen_name_for(cp, cbuf, cblen))
  printf("Unicode: %s\n", cbuf);
@

Then, maybe some of the character classifications.  These are all
printed out by the [[chartypes]] mainline, but there is no harm in
repeating it.

<<Show character for [[tst]]>>=
fputs("Character class: ", stdout);
#define i cp
<<Print flags and classes of cp [[i]]>>
#undef i
putchar('\n');
@

Next, we'll try some normalization.  But first, we'll print the entire
buffer out before starting, and afer each step.

<<Manipulate characters for [[tst]]>>=
#define prbuf(s) do { \
  fputs(s":", stdout); \
  for(i = 0; i < blen; i++) \
    printf(" %04X", buf[i]); \
  putchar('\n'); \
  putchar('\''); \
  utf8_fputs(buf, blen, stdout); \
  putchar('\''); \
  putchar('\n'); \
} while(0)
prbuf("Start string");
@

Of course these are all destructive.  Another test program should
probably do normalization on copies so that the original can be reused
for more tests.

<<Manipulate characters for [[tst]]>>=
blen = NFD_dec(buf, blen);
prbuf("After decomp");
Canon_Order(buf, blen, 1);
prbuf("After canon-order (NFD)");
blen = NFC_comp(buf, blen, NULL);
prbuf("After canon-comp (NFC)");
blen = NFKD_dec(buf, blen);
prbuf("After compat-decomp");
Canon_Order(buf, blen, 1);
prbuf("After canon-order (NFKD)");
blen = NFC_comp(buf, blen, NULL);
prbuf("After canon-comp (NFKC)");
blen = NFKC_Casefold(buf, blen);
prbuf("After case folding (NFKC_CF)");
@

Since the normalization test above is inadequate, here is the full
normalization test suite from the UCD.

\lstset{language=make}
<<C Test Support Executables>>=
tstnorm \
@

<<makefile.vars>>=
UNINORM_TEST = $(UCD_LOC)/NormalizationTest.txt
@

<<Additional Tests>>=
./tstnorm <$(UNINORM_TEST)
@

\lstset{language=C}
<<tstnorm.c>>=
<<Common C Header>>
#include "uni_all.h"

/* longest line == 587 chars */
char lbuf[1024];

/* longest string == 18 chars */
int ibuf[5][128], ilen[5];
int obuf[128];

int main(void)
{
  int ret = 0;
  <<Read and process NormalizationTest.txt>>
  return ret;
}
@

First, we need to track when we're in part 1, and mark every code
point encountered there as having been processed.  According to the
test, after exiting part 1, we can run all four standard
normalizations on any code point not encountered and get no effect.

In general, when entering a new section, print the section name.
Otherwise, print a dot after every 50 tests.

<<Read and process NormalizationTest.txt>>=
char *didnorm;
inisize(didnorm, 0x110000);
clearbuf(didnorm, 0x110000);
int inpart1 = 0;
int ntests = 0;
while(fgets(lbuf, sizeof(lbuf), stdin)) {
  if(*lbuf == '#')
    continue;
  if(*lbuf == '@') {
    if(ntests)
      putchar('\n');
    if(lbuf[5] == '1')
      inpart1 = 1;
    else if(inpart1) {
      inpart1 = 0;
      int i;
      for(i = 0; i < 0x110000; i++)
        if(!didnorm[i]) {
	  <<Test normalization does not affect [[i]]>>
	}
    }
    fputs(lbuf, stdout);
    continue;
  }
  if(inpart1) {
    int cp = strtol(lbuf, NULL, 16);
    didnorm[cp] = 1;
  }
  <<Run normalization test for [[lbuf]]>>
  if(!(++ntests % 50)) {
    putchar('.');
    fflush(stdout);
  }
}
putchar('\n');
@

First, let's parse a line into the input buffers.  Each line has five
semicolon-separated fields, terminated by a semicolon.  Each field has
space-separated hexadecimal numbers.

<<Run normalization test for [[lbuf]]>>=
int i;
char *s = lbuf;
for(i = 0; i < 5; i++) {
  int l = 0;
  while(1) {
    ibuf[i][l++] = strtol(s, &s, 16);
    while(isspace(*s))
      s++;
    if(*s == ';')
      break;
  }
  ilen[i] = l;
  if(*s == ';')
    s++;
  else {
    fprintf(stderr, "bad line %s\n", lbuf);
    exit(1);
  }
}
@

Then, run the conformance tests.

<<Run normalization test for [[lbuf]]>>=
#define dotst(t, i0, il, r) do { \
  for(i = i0; i < il; i++) { \
    memcpy(obuf, ibuf[i], ilen[i] * sizeof(obuf[0])); \
    int l = t(obuf, ilen[i]); \
    if(l != ilen[r] || memcmp(obuf, ibuf[r], l * sizeof(obuf[0]))) { \
      ret = 1; \
      fprintf(stderr, "Failed "#t" test %d/%d on %s\nGot: ", r + 1, i + 1, lbuf); \
      int j; \
      for(j = 0; j < l; j++) \
        fprintf(stderr, " %04X", obuf[j]); \
      fputs("\nExpected: ", stderr); \
      for(j = 0; j < ilen[r]; j++) \
        fprintf(stderr, " %04X", ibuf[r][j]); \
      fputc('\n', stderr); \
      continue; \
    } \
  } \
} while(0)
@

<<Run normalization test for [[lbuf]]>>=
dotst(NFC, 0, 3, 1);
dotst(NFC, 3, 5, 3);
@

<<Run normalization test for [[lbuf]]>>=
dotst(NFD, 0, 3, 2);
dotst(NFD, 3, 5, 4);
@

<<Run normalization test for [[lbuf]]>>=
dotst(NFKC, 0, 5, 3);
@

<<Run normalization test for [[lbuf]]>>=
dotst(NFKD, 0, 5, 4);
@

<<Test normalization does not affect [[i]]>>=
obuf[0] = i;
if(NFC(obuf, 1) != 1 || obuf[0] != i ||
   NFD(obuf, 1) != 1 || obuf[0] != i ||
   NFKC(obuf, 1) != 1 || obuf[0] != i ||
   NFKD(obuf, 1) != 1 || obuf[0] != i) {
  fprintf(stderr, "Failed Part1 end check at %04x\n", i);
  ret = 1;
}
@

As a more thorough character name test, we can run every single known
character through the [[tst]] program.  This does not guarantee that
the lists themselves are OK, or that bad names will not trigger false
positives, but it's better than nothing.

<<Test Scripts>>=
tstchars \
@

\lstset{language=sh}
<<tstchars>>=
#!/bin/sh
<<Common NoWeb Warning>>
@

First, we'll iterate through all of the Unicode names mapping to one
character, both forward and reverse.

<<tstchars>>=
cat charnames.uni{a,} | while IFS=, read a b; do
  b=$(($b))
  hb=`printf %04X $b`
  tr=$(./tst "$a" | head -n 1)
  tr="${tr#* }"
  test $hb = "$tr" || echo "$a,$b $hb"
  tr="`./tst $hb | grep -a '^Unicode:'`"
  tr="${tr#* }"
  test "$a" = "$tr" && continue
  ab=`grep "^$a," charnames.uni{a,} | cut -d: -f2 | cut -d, -f2`
  test "$ab" = "$b" || echo "$hb $a $tr"
done
@

Next, all of the multi-character Unicoded characters.  They can only
be tested for name interpretation, since reverse lookups on
multi-character sequences are impossible.

<<tstchars>>=
cat charnames.uniseq | while IFS=, read a b; do
  tr=
  for x in $(./tst "$a" | grep -a '^Character [0-9A-F]' | cut -d\  -f2); do
    tr=$tr,$((0x$x))
  done
  test ,$b = "$tr" || echo "$a,$b $tr"
done
@

Another useful check is to ensure that the string table's strings are
all actually unique.  This is done by traversing each hash bucket to
see if there are any duplicates.

<<makefile.rules>>=
# explicit strs.gen dep should not be necessary, but gmake flakes on it
hashcheck: uni_strs.o strs.gen
	$(CC) -o $@ $(CFLAGS) $(LDFLAGS) $(EXTRA_CFLAGS) -DHASHCHECK uni_strs.c
@

<<C Test Executables>>=
hashcheck \
@

\lstset{language=C}
<<uni_strs.c>>=
#ifdef HASHCHECK
int main(void)
{
    int i;
    int err = 0;
    for(i = 0; i < HTAB_SIZE; i++) {
        if(!strhash[i])
            continue;
        int j, k;
        for(j = strhash[i]; builtin_hashe[j - 1].nextent; j = builtin_hashe[j - 1].nextent) {
            const char *js = builtin_hashe[j - 1].off + builtin_str;
            int jl = num_before(js, NULL);
            for(k = builtin_hashe[j - 1].nextent; k; k = builtin_hashe[k - 1].nextent) {
                const char *ks = builtin_hashe[k - 1].off + builtin_str;
                if(num_before(ks, NULL) == jl && !memcmp(js, ks, jl)) {
                    fprintf(stderr, "duplicate string %d %d %.*s\n",
                            builtin_hashe[j - 1].off, builtin_hashe[k - 1].off,
                            jl, js);
                    err = 1;
                }
            }
        }
    }
    printf("%lu string space; %lu hash entries\n",
           (unsigned long)builtin_str_size,
           (unsigned long)builtin_hashe_size);
    return err;
}
#endif
@

\section{Code Index}
\nowebchunks

\end{document}
