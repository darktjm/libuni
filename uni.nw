% -*- mode: Noweb; noweb-code-mode: c-mode; -*-
% Build with noweb:
%  notangle -t8 build.nw > makefile
%  make
\documentclass[twoside,english]{report}
\usepackage[letterpaper,rmargin=1.5in,bmargin=1in]{geometry}
%%% latex preamble
% Begin-doc uni-rcs
\RCS $Id$
\RCS $Revision$
\RCS $Date$
% End-doc uni-rcs

%%% requires tjm-ext

% shut l2h up about the font changes
% l2h ignore newfontfamily A[{
% l2h ignore unimono

% There are very few monospace Unicode fonts, and even fewer
% that support U+2620 SKULL AND CROSSBONES.
% GNU Unifont isn't pretty, but supports it: http://unifoundry.com/
\newfontfamily\unimonox[Path = /usr/share/fonts/unifont/]{unifont.ttf}
% DejaVu Sans Mono looks OK: http://dejavu-fonts.org/
% It's installed by texlive by fontsextra, so it doesn't need a path.
\newfontfamily\unimono[Scale = 0.8]{DejaVuSansMono}

% You could just replace LMMono with unimono globally for consistency:
%\let\Tt\unimono

% or even replace all 3:
%\setmainfont{DejaVuSerif}
%\setsansfont{DejaVuSans}
%\setmonofont{DejaVuSansMono}

\begin{document}

\title{Unicode Support Routines}
\author{Thomas J. Moore}
% Begin-doc uni-version
\date{Version 0.\RCSRevision\\\RCSDate}
% End-doc uni-version
\maketitle

\begin{rawhtml}
<!-->
\end{rawhtml}
\iffalse
<<Sources>>=
$Id$
@

<<Version Strings>>=
"$Id$\n"
@

<<Common noweb Warning>>=
# $Id$
@
\fi
\begin{rawhtml}
<-->
\end{rawhtml}

\begin{abstract}

This document describes and implements a library which provides
basic Unicode (\url{http://www.unicode.org}) support in a convenient
manner.  Libraries already exist that do this:  GLib, ICU, and others.
However, this library does things my way, which allows me to e.g.
build a regular expression library and a compiler using these
routines.  A large number of properties can be queried, and selected
properties have additional support routines.  All algorithms listed in
Table 3-1, Named Unicode Algorithms, in version 7.0 of the Unicode
standard are implemented except for the Bidirectional Algorithm (UBA)
and the Standard Compression Scheme for Unicode (SCSU).  Several of
these have additional modifications for localization using the Common
Locale Data Repository (CLDR).

\vspace{0.75in}

This document is \copyright{} 2012--2015 Thomas J. Moore.  This
document is licensed under the Apache License, Version 2.0 (the
``License''); you may not use this document except in compliance with
the License. You may obtain a copy of the License at
\url{http://www.apache.org/licenses/LICENSE-2.0}.  Unless required by
applicable law or agreed to in writing, software distributed under the
License is distributed on an ``AS IS'' BASIS, WITHOUT WARRANTIES OR
CONDITIONS OF ANY KIND, either express or implied. See the License for
the specific language governing permissions and limitations under the
License. 

\vspace{0.25in}

This document was generated from the following sources, all of which are
attached to the original electronic forms of this document:
\input{Sources.tex} % txt

\end{abstract}

% Begin-doc Introduction
\tableofcontents
\listoftables

\chapter{Introduction}

The Unicode standard is a freely available standard describing a large
character set, and the features of those characters.  It also
describes several common transformations and processing methods for
those characters.  The ISO 10646 standard is also freely available at
this time, but like most ISO standards, this was not always so.  The
ISO standard describes the same character set, but has a much smaller
scope overall.  The Unicode Consortium also provides an electronically
readable form of its attribute information: the Unicode Character
Database.%
\footnote{\url{http://www.unicode.org/Public/zipped}}
The purpose of this library is to provide convenient access to that
information in C programs.  The ISO 10646 standard also provides such
information, but again, it is of more limited scope.  To the most
part, the ISO 10646 standard is completely ignored from this point on.

The Unicode consortium also provides a set of rules for interpreting
characters in different locales.  ISO has standards to this effect, as
well.  Both sets of standards and their data files are freely
available.  However, once again, the ISO data is ignored in favor of
the Unicode data.  The Unicode locale information is available in
electronically processable form in the Unicode Common Locale Data
Repository (CLDR).%
\footnote{\url{http://cldr.unicode.org},
\url{http://unicode.org/Public/cldr/}}
In addition to the regular UCD data, this library also provides access
to selected parts of the CLDR data to C programs.

% End-doc Introduction
<<uni_all.h>>=
<<Common C Warning>>
#ifndef UNI_ALL_H
#define UNI_ALL_H
/** \file uni_all.h Includes all libuni-related headers */

<<Library [[uni]] headers>>

#endif /* UNI_ALL_H */
@

<<Headers to Install>>=
uni/uni_all.h \
@

% Begin-doc Introduction
This document does not contain the UCD or CLDR itself; it is expected
that the user download them manually and place them where they can be
found.  Some Linux distributions may already provide at least the UCD
as a package, although the CLDR is probably less common.  This package
was tested with versions 6.0.0, 6.2.0, 6.3.0 and 7.0.0 of the UCD, but
the file formats are relatively stable and it should work with later
versions as well, other than algorithm changes.  I recommend at least
version 6.1.0, as that is the first version where the control
character names were finally made official.  An earlier version of
this library actually had hard-coded names for ASCII control
characters optionally added to the name database.  This package was
tested against CLDR version 24; I recommend always getting the
latest version available.  In particular, care must be taken that the
CLDR and UCD versions are compatible.

Originally, I didn't care what version was used, but 6.3.0 introduces
some incompatible algorithm changes.  Thus, the version must be known
as well.  The UCD version can be extracted from the first line of many
of the data files (e.g. the first line of \texttt{PropertyAliases.txt}
is \texttt{\# PropertyAliases-}\emph{ver}\texttt{.txt}), but there is
no guarantee that it is always present, so an explicit value must be
set.  The CLDR has nothing which indicates the current version, so
again, an explicit value must be set.  Note that the CLDR version is
currently ignored.

% End-doc Introduction

\lstset{language=make}
<<makefile.config>>=
# The location of the Unicode Character Database (unzipped)
# http://www.unicode.org/Public/zipped/<version>/UCD.zip
UCD_LOC = /usr/share/unicode-data

# The version of the UCD/Unihan data: major*100+minor*10+sub
# Leave blank to automatically determine from data
#UCD_VER = 700

# The location of the Unicode Han Database (unzipped)
# http://www.unicode.org/Public/zipped/<version>/Unihan.zip
UNIHAN_LOC = $(UCD_LOC)

# The location of the Unicode Common Locale Data Repository (unzipped)
# http://www.unicode.org/Public/cldr/latest/core.zip
CLDR_LOC = /usr/share/unicode-data/cldr

# The version of the CLDR data: major*10+minor
# Leave blank to automatically determine from data
#CLDR_VER=270

@

% Begin-doc Introduction
Speaking of names, the Unicode character names are rather verbose.%
\footnote{It is recommended in some places that common names be
permitted in a less verbose manner, but that is locale-dependent and
will not be supported by this library.}
The XML standard, on the other hand, has fairly short names.  This
library can use either, or even both (where they do not conflict).  To
do this, the XML entity database is needed.  This is defined by the
W3C XML Entity Definitions for Characters.%
\footnote{\url{http://www.w3.org/TR/xml-entity-names/}}
This was tested with version 01 April 2010.%
\footnote{\url{http://www.w3.org/TR/2010/REC-xml-entity-names-20100401/}}
The only data file needed from that standard is unicode.xml,%
\footnote{\url{http://www.w3.org/2003/entities/2007xml/unicode.xml}}
which is linked from the standard.

% End-doc Introduction
<<makefile.config>>=
# The XML entity database http://www.w3.org/2003/entities/2007xml/unicode.xml
XMLUNI = /usr/share/unicode-data/unicode.xml

@

% Begin-doc Introduction
Other libraries exist which provide similar functionality; I have
looked at GLib\footnote{\url{http://www.gtk.org/}} and
ICU\footnote{\url{http://www.icu-project.org/}} in particular.  The
GLib library provides a number of utility functions for Unicode
character strings.  It provides normalization, classification, and
general string utilities. Of these, this library does not provide
general utf-8 string utilities.  It might have been useful to make
this a GLib extension instead, providing the missing properties (in
particular, the string properties such as character names).  However,
providing all functionality in a consistent way is easier than making
a GLib extension.

The ICU library is primarily C++, with a C wrapper.  While there is
nothing inherently wrong with this, I prefer C with a C++ wrapper over
C++ with a C wrapper in general.  The ICU does everything this library
does, and more.  Unfortunately, it is not very well documented.  Like
many modern library projects, it simply provides a basic API document
auto-generated from source.  This sort of documentation is necessary,
but insufficient.  Another part of the reason for not just using ICU
is that some operations I want to do in other code requires too much
overhead in ICU.

A lesser reason for not using either of these libraries is that I want
to use this library to support my own regular expression library, and
it seems redundant when both provide their own regular expression
implementation as well.  In addition, this library can be trimmed to
the minimal necessary code with static linking, and does not include
heavy wrapper code and additional utilities that the other libraries
include.

% End-doc Introduction
\chapter{Unicode Character Processing}

% Begin-doc unitypes
The supported character types (encoding forms) are UTF-8, UTF-16, and
UTF-32.  Technically, a UTF-32 character can be stored in 21 bits.
The other two require an unsigned storage type of the given width.
Instead of providing aliases for standard types, the standard sized C
types from \texttt{stdint.h} are used: [[uint8_t]], [[uint16_t]], and
[[uint32_t]], respectively.  All functions which take or return a
single character use the UTF-32 type.  If an error could be returned
for UTF-32 returning functions, a signed integer ([[int32_t]]) may be
used instead.

The stream encodings (encoding schemes) for these types are named
similarly.  The 8-bit encoding is UTF-8.  The 16-bit big-endian
encoding is UTF-16BE, and the little-endian encoding is UTF-16LE.  The
32-bit big-endian encoding is UTF-32BE, and the 32-bit little-endian
encoding is UTF-32LE.  UTF-16 and UTF-32 character streams must either
agree on a byte order ahead of time, or begin the stream with a Byte
Order Mark (U+FEFF).

% End-doc unitypes

\section{File I/O}

Basic I/O support is provided in the form of UTF decoders and
encoders.  More advanced I/O is provided in the form of arbitrary
encoding input.

<<Library [[uni]] Members>>=
uni_io.o
@

<<Libraries to Install>>=
uni \
@

\lstset{language=C}
<<uni_io.c>>=
<<Common C Header>>
#include "uni_io.h"

<<Unicode I/O local definitions>>

<<Unicode I/O functions>>
@

<<Library [[uni]] headers>>=
#include "uni_io.h"
@

<<uni_io.h>>=
<<Common C Warning>>
#ifndef UNI_IO_H
#define UNI_IO_H
/** \file uni_io.h UTF I/O and parsing */

#include <stdint.h> /* for basic data types */
#include <stdio.h> /* it's I/O, right? */

/** \addtogroup uni_io Unicode Encodings
    @{ */
<<Unicode I/O Exports>>
/** @} */
#endif
@

<<Headers to Install>>=
uni/uni_io.h \
@

\section{Basic Unicode I/O}

Unicode defines several file formats:  UTF-8, UTF-16, and UTF-32.  No
special conversion needs to be made from UTF-32 to integer code
points, other than to filter out invalid values and possibly byte-swap
the code point.

<<Common C Includes>>=
#include <stdint.h>
/* FIXME: this is glibc-specific */
/* BSD apparently uses <sys/endian.h> */
/* OpenBSD additionally uses different fn names */
/* others may not even have any equivalent functions */
/* at least non-glibc users should get compilation errors that point here */
#include <endian.h>
@

\lstset{language=make}
<<makefile.vars>>=
# for endian (was _BSD_SOURCE, but recent GNU libc hates that)
EXTRA_CFLAGS += -D_DEFAULT_SOURCE
@

\lstset{language=C}
<<Unicode I/O Exports>>=
/** Returns UTF-32 encoding length of \p cp.
  * Note that this is a preprocessor macro */
int uni_utf32_enclen(uint32_t cp);
#define uni_utf32_enclen(cp) 1
/** Encodes Unicode code point \p cp into \p buf.
 *  Returns number of 32-bit words generated.  Encoding in \p buf is
 *  UTF-32BE if \p bige is true, and UTF-32LE otherwise.   Invalid
 *  Unicode code points will not be encoded */
int uni_utf32_encode(uint32_t *buf, uint32_t cp, int bige);
@

<<Unicode I/O functions>>=
int uni_utf32_encode(uint32_t *buf, uint32_t cp, int bige)
{
  if(cp > 0x10ffff || (cp >= 0xd800 && cp < 0xe000))
    return 0;
  *buf = bige ? htobe32(cp) : htole32(cp);
  return 1;
}
@

<<Unicode I/O Exports>>=
/** Returns decoded Unicode code point at \p s.
 *  The encoding at \p s is UTF-32BE if \p bige is true, and UTF-32LE
 *  otherwise.  Returns -1 if \p s does not point to valid UTF-32 */
int32_t uni_utf32_decode(const uint32_t *s, int bige);
@

<<Unicode I/O functions>>=
int32_t uni_utf32_decode(const uint32_t *s, int bige)
{
  uint32_t c = bige ? be32toh(*s) : le32toh(*s);
  if(c > 0x10ffff || (c >= 0xd800 && c < 0xe000))
    return -1;
  return c;
}
@

<<Unicode I/O Exports>>=
/** Outputs Unicode code point \p c to file \p f.
 *  The enconding is UTF-32BE if \p bige is true, and UTF-32LE otherwise.
 *  Invalid Unicode code points will not be written out.  Returns number
 *  of 32-bit words written */
int uni_utf32_putc(uint32_t c, FILE *f, int bige);
@

<<Unicode I/O functions>>=
int uni_utf32_putc(uint32_t c, FILE *f, int bige)
{
  uint32_t w;

  if(!uni_utf32_encode(&w, c, bige))
    return 0;
  return fwrite(&w, 4, 1, f);
}
@

<<Unicode I/O Exports>>=
/** Returns Unicode code point read from UTF-32 file \p f.
 *  The encoding is UTF-32BE if \p bige is true, and UTF-32LE otherwise.
 *  This function always reads 4 bytes, and returns less than zero on
 *  errors:  -1 if no characters could be read, -2 on short read, and
 *  -3 on decoding failure.  Zero is returned on success */
int32_t uni_utf32_getc(FILE *f, int bige); /* always reads 4 bytes */
@

<<Unicode I/O functions>>=
int32_t uni_utf32_getc(FILE *f, int bige)
{
  uint32_t w;
  int32_t ret;

  if((ret = fread(&w, 1, 4, f)) != 4)
    return ret ? -2 : -1;
  ret = uni_utf32_decode(&w, bige);
  return ret < 0 ? -3 : ret;
}
@

For UTF-16, a slightly more complex scheme is used, involving the
surrogate code points (D800 through DFFF) in pairs.  The first half of
the surrogate code points (D800 through DBFF) is always used for the
first member of the pair, and the second half is used for the other.
Each member gives 10 bits.  The maximum code point is 10FFFF,
requiring 21 bits, but since the extensions start at 10000, the
maximum etension is 10FFFF$-$10000 = FFFFF, requiring 20 bits.

<<Unicode I/O Exports>>=
/** Returns UTF-16 encoding length of \p cp */
int uni_utf16_enclen(uint32_t cp);
#define uni_utf16_enclen(cp) (1 + ((cp) >= 0x10000))
/** Encodes Unicode code point \p cp into \p buf.
  * Returns number of 16-bit words generated.  Encoding in \p buf is
  * UTF-16BE if \p bige is true, and UTF-16LE otherwise.  Ensure that at
  * least 2 words are available in \p buf, or use \ref uni_utf16_enclen to
  * size the buffer first.  Invalid code points will not be encoded */
int uni_utf16_encode(uint16_t *buf, uint32_t cp, int bige);
@

<<Unicode I/O functions>>=
int uni_utf16_encode(uint16_t *buf, uint32_t cp, int bige)
{
  if((cp >= 0xD800 && cp < 0xE000) || cp > 0x10FFFF)
    return 0;
  if(cp < 0x10000) {
    *buf = bige ? htobe16(cp) : htole16(cp);
    return 1;
  }
  cp -= 0x10000;
  uint16_t w = 0xD800 + (cp >> 10);
  *buf++ = bige ? htobe16(w) : htole16(w);
  w = 0xDC00 + (cp & 0x3ff);
  *buf = bige ? htobe16(w) : htole16(w);
  return 2;
}
@

<<Unicode I/O Exports>>=
/** Returns decoded Unicode code point at \p s.
 *  The encoding at \p s is UTF-16BE if \p bige is true, and UTF-16LE
 *  otherwise.  At most \p len 16-bit words will be read, and the actual
 *  number of words read are returned in \p nread, if non-NULL.
 *  Returns -1 if \p s does not point to valid UTF-16, or \p len is
 *  too short */ 
int32_t uni_utf16_decode(const uint16_t *s, unsigned int len,
                         unsigned int *nread, int bige);
@

<<Unicode I/O functions>>=
int32_t uni_utf16_decode(const uint16_t *s, unsigned int len,
                         unsigned int *nread, int bige)
{
  if(!len) {
    if(nread)
      *nread = 0;
    return -1;
  }
  uint16_t w = bige ? be16toh(*s) : le16toh(*s);
  if(nread)
    *nread = 1;
  if(w < 0xd800 || w >= 0xe000)
    return w;
  if(w >= 0xdc00)
    return -1;
  if(len < 2) {
    if(nread)
      *nread = 0;
    return -1;
  }
  uint16_t w2 = bige ? be16toh(s[1]) : le16toh(s[1]);
  if(w2 >= 0xe000 || w2 < 0xdc00)
    return -1;
  if(nread)
    *nread = 2;
  return (w2 & 0x3ff) + ((uint32_t)(w & 0x3ff) << 10) + 0x10000;
}
@

<<Unicode I/O Exports>>=
/** Outputs Unicode code point \p c to file \p f.
 *  The enconding is UTF-16BE if \p bige is true, and UTF-16LE otherwise.
 *  Invalid Unicode code points will not be written out.  Returns number
 *  of 16-bit words written on success, and the negative of the number
 *  of 16-bit words written (or zero) on failure.  */
int uni_utf16_putc(uint32_t c, FILE *f, int bige);
@

<<Unicode I/O functions>>=
int uni_utf16_putc(uint32_t c, FILE *f, int bige)
{
  unsigned char buf[4];
  uint16_t enc[2], encl;
  int wlen;
  
  encl = uni_utf16_encode(enc, c, bige);
  if(!encl)
    return 0;
  if(bige) {
    buf[0] = enc[0] >> 8;
    buf[1] = enc[0];
    if(encl > 1) {
      buf[2] = enc[1] >> 8;
      buf[3] = enc[1];
    }
  } else {
    buf[1] = enc[0] >> 8;
    buf[0] = enc[0];
    if(encl > 1) {
      buf[3] = enc[1] >> 8;
      buf[2] = enc[1];
    }
  }
  wlen = fwrite(buf, 2, encl, f);
  return wlen == encl ? wlen : -wlen;
}
@

The input function needs to read ahead to get the second member of a
pair.  If that character is not what was expected, the correct
behavior is to undo the readahead and return the current code point as
an error.  However, it is not possible in C standard I/O to push more
than one character back into the stream.  For now, this is going to
have to be erroneous behavior.

<<Unicode I/O Exports>>=
/** Returns Unicode code point read from UTF-16 file \p f.
 *  The encoding is UTF-16BE if \p bige is true, and UTF-16LE otherwise.
 *  The number of 16-bit words actually read from the file are returned in
 *  \p nread, if non-NULL.  Zero is returned on success, and less than zero
 *  on errors:  -1 if no characters could be read, -2 on short read,
 *  -3 on decoding failure */
int32_t uni_utf16_getc(FILE *f, int bige, unsigned int *nread);
@

<<Unicode I/O functions>>=
int32_t uni_utf16_getc(FILE *f, int bige, unsigned int *nread)
{
  int c = fgetc(f), c2;
  if(c == EOF) {
    if(nread)
      *nread = 0;
    return -1;
  }
  c2 = fgetc(f);
  if(c == EOF) {
    if(nread)
      *nread = 1;
    return -2;
  }
  if(bige)
    c = c2 + (c << 8);
  else
    c += c2 << 8;
  if(c < 0xd800 || c >= 0xE000) {
    if(nread)
      *nread = 2;
    return c;
  }
  if(c >= 0xdc00) {
    if(nread)
      *nread = 2;
    return -3;
  }
  uint32_t res = 0x10000 + ((c & 0x3ff) << 10);
  if((c = fgetc(f)) == EOF) {
    if(nread)
      *nread = 2;
    return -2;
  }
  if((c2 = fgetc(f)) == EOF) {
    if(nread)
      *nread = 3;
    return -2;
  }
  if(bige)
    c = c2 + (c << 8);
  else
    c += c2 << 8;
  if(c < 0xdc00 || c >= 0xe000) {
    ungetc(bige ? c >> 8 : c & 0xff, f);
    /* POSIX only guarantees 1 ungetc */
    if(ungetc(c2, f) == EOF) {
      c2 = getc(f);
      if(nread)
        *nread = 4;
    } else if(nread)
      *nread = 2;
    return -3;
  }
  if(nread)
    *nread = 4;
  return res + (c & 0x3ff);
}
@

For UTF-8, an even more complex encoding scheme is used.  Again, a
standalone memory codec is provided.  All code points over 007F are
encoded using a multi-byte sequence.  All characters in a multi-byte
sequence have their high bit set; the first non-zero bit determines
the byte's role.  All but the first byte have only one high bit set,
and encode 6 bits.  The first byte determines how many trailing bytes
there are, and also encode 3--5 bits.  The number of high bits set in
the first byte is the total number of bytes in the sequence.

<<Unicode I/O Exports>>=
/** Returns UTF-8 encoding length of \p cp.
  * Note that this is a preprocessor macro, and may evaluate \p cp more than
  * once */
int uni_utf8_enclen(uint32_t cp);
#define uni_utf8_enclen(cp) \
  (1 + ((cp) >= 0x80) + ((cp) >= 0x800) + ((cp) >= 0x10000))
/** Encodes Unicode code point \p cp into \p buf using UTF-8.
  * Returns number of bytes generated.  Ensure that at least 3 bytes are
  * available in \p buf, or use \ref uni_utf8_enclen to size the buffer
  * first.  Invalid Unicode code points will not be encoded */
int uni_utf8_encode(uint8_t *buf, uint32_t cp);
@

<<Unicode I/O functions>>=
<<Encode code point as UTF-8>>
@

<<Encode code point as UTF-8>>=
int uni_utf8_encode(uint8_t *buf, uint32_t cp)
{
    if((cp >= 0xD800 && cp < 0xE000) || cp > 0x10FFFF)
        return 0;
    if(cp < 128) {
        *buf = cp;
        return 1;
    } else if(cp < 0x800) {
        *buf = 0xc0 + (cp >> 6);
        buf[1] = 0x80 + (cp & 0x3f);
        return 2;
    } else if(cp < 0x10000) {
        *buf = 0xe0 + (cp >> 12);
        buf[1] = 0x80 + ((cp >> 6) & 0x3f);
        buf[2] = 0x80 + (cp & 0x3f);
        return 3;
    } else {
        *buf = 0xf0 + (cp >> 18);
        buf[1] = 0x80 + ((cp >> 12) & 0x3f);
        buf[2] = 0x80 + ((cp >> 6) & 0x3f);
        buf[3] = 0x80 + (cp & 0x3f);
        return 4;
    }
}
@

<<Unicode I/O Exports>>=
/** Returns decoded UTF-8 Unicode code point at \p s.
  * At most \p len bytes will be read, and the actual number of bytes
  * read are returned in \p nread, if non-NULL.  Returns -1 if \p s does
  * not point to valid UTF-8, or if \p len is too short */
int32_t uni_utf8_decode(const uint8_t *s, unsigned int len, unsigned int *nread);
@

<<Unicode I/O functions>>=
int32_t uni_utf8_decode(const uint8_t *s, unsigned int len, unsigned int *nread)
{
  if(!len) {
    if(nread)
      *nread = 0;
    return -1;
  }
  if(*s < 0x80) {
    if(nread)
      *nread = 1;
    return *s;
  }
  uint32_t c = *s++, ec;
  if((c & 0xf8) == 0xf0) {
    if(len < 4) {
      if(nread)
        *nread = 0;
      return -1;
    }
    c &= 0x07;
    ec = *s++;
    if((ec & 0xc0) != 0x80) {
      if(nread)
        *nread = 1;
      return -1;
    }
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if((ec & 0xc0) != 0x80) {
      if(nread)
        *nread = 2;
      return -1;
    }
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if((ec & 0xc0) != 0x80) {
      if(nread)
        *nread = 3;
      return -1;
    }
    if(nread)
      *nread = 4;
    c = (c << 6) + (ec & 0x3f);
    if(c < 0x10000 || c > 0x10FFFF)
      return -1;
    return c;
  } else if((c & 0xf0) == 0xe0) {
    if(len < 3) {
      if(nread)
        *nread = 0;
      return -1;
    }
    c &= 0x0f;
    ec = *s++;
    if((ec & 0xc0) != 0x80) {
      if(nread)
        *nread = 1;
      return -1;
    }
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if((ec & 0xc0) != 0x80) {
      if(nread)
        *nread = 2;
      return -1;
    }
    if(nread)
      *nread = 3;
    c = (c << 6) + (ec & 0x3f);
    if(c < 0x800 || (c >= 0xD800 && c < 0xE000))
      return -1;
    return c;
  } else if((c & 0xe0) == 0xc0) {
    if(len < 2) {
      if(nread)
        *nread = 0;
      return -1;
    }
    c &= 0x1f;
    ec = *s++;
    if((ec & 0xc0) != 0x80) {
      if(nread)
        *nread = 1;
      return -1;
    }
    if(nread)
      *nread = 2;
    c = (c << 6) + (ec & 0x3f);
    if(c < 0x80)
      return -1;
    return c;
  } else {
    if(nread)
      *nread = 1;
    return -1;
  }
}
@

<<Unicode I/O Exports>>=
/** Outputs Unicode code point \p c to file \p f using UTF-8 encoding.
 *  Invalid Unicode code points will not be written out.  Returns number
 *  of bytes written on success, and the negative of the number
 *  of bytes written (or zero) on failure.  */
int uni_utf8_putc(uint32_t c, FILE *f);
@

<<Unicode I/O functions>>=
int uni_utf8_putc(uint32_t c, FILE *f)
{
    uint8_t obuf[4];
    int nout = uni_utf8_encode(obuf, c);
    int nwr = fwrite(obuf, 1, nout, f);
    return nout == nwr ? nwr : -nwr;
}
@

UTF-8 output is common enough to warrant a string version of the
output function as well.

<<Unicode I/O Exports>>=
/** Outputs string of Unicode code points \p buf to file \p f using UTF-8
 ** encoding.
 *  The number of code points written is \p len (i.e., \p buf may contain
 *  zeroes).  Invalid Unicode code points will not be written out.
 *  Returns number of bytes written on success, and the negative of
 *  the number of bytes written (or zero) on failure.  */
int uni_utf8_fputs(uint32_t *buf, int len, FILE *f);
@

<<Unicode I/O functions>>=
int uni_utf8_fputs(uint32_t *buf, int len, FILE *f)
{
  int ret = 0;

  while(len-- > 0) {
    int ret1 = uni_utf8_putc(*buf++, f);
    if(ret1 <= 0)
      return -ret + ret1;
    ret += ret1;
  }
  return ret;
}
@

Like UTF-16 input, UTF-8 input requires some readahead.  For anything
more than one character, C once again disallows returning the
characters to the stream.  However, it is relatively easy to abort
input as soon as an invalid character occurs.  The price to pay is the
extremely poor performance of [[getc]], but using the direct
decoder function above can be used to get around that.

<<Unicode I/O Exports>>=
/** Returns Unicode code point read from UTF-8 file \p f.
 *  The number of bytes actually read from the file are returned in
 *  \p nread, if non-NULL.  Zero is returned on success, and less than zero
 *  on errors:  -1 if no characters could be read, -2 on short read,
 *  -3 on decoding failure */
int32_t uni_utf8_getc(FILE *f, unsigned int *nread);
@

<<Unicode I/O functions>>=
int32_t uni_utf8_getc(FILE *f, unsigned int *nread)
{
  int c;
  int chrread = 1;

  if((c = getc(f)) == EOF) {
    c = -1;
    chrread = 0;
  } else if(c >= 0x80) {
    int ec;
    chrread = 1;
    if((c & 0xf8) == 0xf0) {
      <<Read 4-char utf-8>>
      if(c < 0x10000 || c > 0x10ffff)
        c = -3;
    } else if((c & 0xf0) == 0xe0) {
      <<Read 3-char utf-8>>
      if(c < 0x800 || (c >= 0xD800 && c < 0xE000))
        c = -3;
    } else if((c & 0xe0) == 0xc0) {
      <<Read 2-char utf-8>>
      if(c < 0x80)
        c = -3;
    } else
      c = -3;
  }
  if(nread)
    *nread = chrread;
  return c;
}
@

<<Read 4-char utf-8>>=
c &= 0x07;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
  if((ec = getc(f)) == EOF)
    c = -2;
  else if((ec & 0xc0) != 0x80) {
    ungetc(ec, f);
    c = -3;
  } else {
    chrread = 3;
    c = (c << 6) + (ec & 0x3f);
    if((ec = getc(f)) == EOF)
      c = -2;
    else if((ec & 0xc0) != 0x80) {
      ungetc(ec, f);
      c = -3;
    } else {
      chrread = 4;
      c = (c << 6) + (ec & 0x3f);
    }
  }
}
@

<<Read 3-char utf-8>>=
c &= 0x0f;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
  if((ec = getc(f)) == EOF)
    c = -2;
  else if((ec & 0xc0) != 0x80) {
    ungetc(ec, f);
    c = -3;
  } else {
    chrread = 3;
    c = (c << 6) + (ec & 0x3f);
  }
}
@

<<Read 2-char utf-8>>=
c &= 0x1f;
if((ec = getc(f)) == EOF)
  c = -2;
else if((ec & 0xc0) != 0x80) {
  ungetc(ec, f);
  c = -3;
} else {
  chrread = 2;
  c = (c << 6) + (ec & 0x3f);
}
@

\section{General File Input}

A more general UTF reader would have to scan for a byte order mark and
remember what it said.  This requires retaining state.

<<Unicode I/O Exports>>=
/** Opaque type used for reading Unicode files */
typedef struct uni_file_t uni_file_t;
@

<<[[uni_file_t]]>>=
struct uni_file_t {
  FILE *f;
  char *name;
  <<Unicode file buffer state>>
};
@

<<Unicode I/O local definitions>>=
<<Unicode file buffer state deps>>
<<[[uni_file_t]]>>
@

<<Known Data Types>>=
uni_file_t,%
@

Rather than automatically attaching state to a file on first access
and never really knowing when to free it, explicit routines are
provided to create and remove the state.   

<<Unicode I/O Exports>>=
/** Open file \p name for reading Unicode code points.
  * The encoding is specified by the string \p encoding.  If the library
  * was compiled with iconv support, this supports all encodings supported
  * by iconv(3), and NULL attempts to use current locale's encoding.
  * Whether or not iconv support is included, some encodings are processed
  * internally.  The special encoding "UTF" defaults to UTF-8, but can
  * switch encoding based on an initial Byte-Order-Mark.  Similarly,
  * UTF-16 and UTF-32 can switch endianness based on a Byte-Order-Mark.
  * Note that the Byte-Order-Mark is returned as the first code point if
  * present.  The other non-iconv encodings are UTF-16LE, UTF-16BE,
  * UTF-32LE, UTF-32BE, and raw bytes (ISO-8859-1 and ANSI_X3.4-1968).
  * The return value is NULL on error.  */
uni_file_t *uni_fopen(const char *name, const char *encoding);
/** Close file \p uf opened by \ref uni_fopen */
void uni_fclose(uni_file_t *uf);
@

<<Unicode I/O functions>>=
uni_file_t *uni_fopen(const char *name, const char *encoding)
{
  uni_file_t *uf = calloc(sizeof(*uf), 1);

  if(!uf)
    return NULL;
  uf->name = strdup(name);
  if(!uf->name) {
    uni_fclose(uf);
    return NULL;
  }
  uf->f = fopen(name, "rb");
  if(!uf->f) {
    uni_fclose(uf);
    return NULL;
  }
  <<Initialize unicode file buffer state>>
  return uf;
}
@

<<Unicode I/O functions>>=
void uni_fclose(uni_file_t *uf)
{
  /* all frees are conditional so uni_fopen can uni_fclose partial progress */
  if(uf->name)
    free(uf->name);
  if(uf->f)
    fclose(uf->f);
  <<Free unicode file buffer state>>
  free(uf);
}
@

One limitation of standard I/O is that only one character may be
pushed back into the stream.  To correct this, a readahead buffer is
kept.  While C provides its own large buffers, preventing I/O overhead
on every read, the read itself can be expensive anyway if called too
often.  To avoid this, the buffer is much larger than needed for
simple UTF lookahead.

<<Unicode file buffer state deps>>=
#define UNIF_BUF_LEN 16384 /* about 4x my old usual buf size */
@

<<Unicode file buffer state>>=
char readahead[UNIF_BUF_LEN];
unsigned short raptr, ralen;
@

The first thing to do is to decide the file's encoding.  ASCII and
ISO-Latin-1 can be read raw, since they map directly to Unicode.
UTF is taken to mean automatic Unicode detection, with UTF-8 as a
default.  UTF-8, UTF-16, UTF-32, UTF-16LE, UTF-16BE, UTF-32LE, and
UTF-32BE are processed directly, with UTF-16 and UTF-32 using native
encoding if a byte order mark is not found.  For all other input
types (including the above-mentioned ones which are in other formats,
such as lower-case), the [[iconv]] function should be used.%
\footnote{The C99 library provides something similar in the
[[mbstowcs]] function, but the POSIX [[iconv]] function is commonly
available (perhaps even as a third party library), and could be
provided as a wrapper around [[mbstowcs]] if all else fails.  Unlike
[[mbstowcs]], the type of the output can always be set to Unicode
encodings, and the input type can be controlled more easily as well.}
This function should be in the standard C library, but may be provided
externally as well.  If so, [[CFLAGS]] and [[LDFLAGS]] may need to be
modified appropriately.  In order to accomodate the case where a
minimal C library has no working [[iconv]], or to otherwise avoid code
bloat, this can be disabled with a configuration variable.

\lstset{language=make}
<<makefile.config>>=
# Set to non-empty to enable iconv input support
#USE_ICONV=y

@

<<makefile.vars>>=
ifneq ($USE_ICONV,)
EXTRA_CFLAGS += -DUNI_USE_ICONV
endif
@

\lstset{language=C}
<<Unicode file buffer state deps>>=
#ifdef UNI_USE_ICONV
#include <langinfo.h>
#include <locale.h>
#include <iconv.h>
#endif
@

<<Unicode file buffer state>>=
/*
 * 0 = ASCII/Latin-1
 * 1 = iconv (if UNI_USE_ICONV)
 * 8/16/32 = utf-8/16/32
 */
unsigned char enctype;
unsigned char bige;
@

<<Initialize unicode file buffer state>>=
#ifdef UNI_USE_ICONV
if(!encoding) {
  setlocale(LC_CTYPE, "");
  encoding = nl_langinfo(CODESET);
}
#endif
if(encoding && (!strcmp(encoding, "ISO-8859-1") || /* Latin-1 */
                !strcmp(encoding, "ANSI_X3.4-1968"))) /* ASCII */
  return uf; /* enctype == 0 -> raw bytes */
if(encoding && !strncmp(encoding, "UTF", 3)) {
  if(!encoding[3])
    encoding = NULL;
  else if(encoding[3] == '-') {
    if(encoding[4] == '8' && !encoding[5]) {
      uf->enctype = 8;
      return uf;
    } else if(encoding[4] == '1' && encoding[5] == '6') {
      uf->enctype = 16;
      <<Check if [[encoding]] is has [[BE]] or [[LE]] suffix>>
    } else if(encoding[4] == '3' && encoding[5] == '2') {
      uf->enctype = 32;
      <<Check if [[encoding]] is has [[BE]] or [[LE]] suffix>>
    }
  }
}
@

<<Check if [[encoding]] is has [[BE]] or [[LE]] suffix>>=
if(encoding[6] == 'B' && encoding[7] == 'E' && !encoding[8]) {
  uf->bige = 1;
  return uf;
} else if(encoding[6] == 'L' && encoding[7] == 'E' && !encoding[8])
  return uf;
else if(!encoding[6]) {
  uf->bige = __BYTE_ORDER == __BIG_ENDIAN;
  encoding = NULL; /* need to determine endianness still */
} /* else use iconv */
@

<<Unicode file buffer state>>=
#ifdef UNI_USE_ICONV
iconv_t ic;
uint32_t *obuf; /* only allocated when needed */
uint16_t optr, olen;
#endif
@

<<Initialize unicode file buffer state>>=
#ifdef UNI_USE_ICONV
if(encoding) {
  uf->obuf = malloc(UNIF_BUF_LEN); /* max len == UNIF_BUF_LEN / 4 */
  if(!uf->obuf) {
    uni_fclose(uf);
    return NULL;
  }
#if __BYTE_ORDER == __BIG_ENDIAN
#define bo "BE"
#else
#define bo "LE"
#endif
  uf->ic = iconv_open("UTF-32" bo, encoding);
  if(uf->ic == (iconv_t)-1) {
    uni_fclose(uf);
    return NULL;
  }
  uf->enctype = 1;
  return uf;
}
#else
if(encoding) {
  errno = EINVAL;
  uni_fclose(uf);
  return NULL;
}
#endif
@

<<Free unicode file buffer state>>=
#ifdef UNI_USE_ICONV
if(uf->obuf)
  free(uf->obuf);
if(uf->enctype == 1)
  iconv_close(uf->ic);
#endif
@

The first thing to do on opening a new Unicode file is to check for
the byte order mark (U+FEFF).  Flags are kept to indicate what was
found.  Note that the byte order mark is still returned as the first
character, so all read characters must be backed out.

<<Initialize unicode file buffer state>>=
if(!uf->enctype)
  uf->enctype = 8; /* default is UTF-8 */
/* Read and process BOM (FEFF) */
int c;
<<Read and store a char from [[uf]]>>
if(c == 0xfe) {
  <<Read and store a char from [[uf]]>>
  if(c == 0xff && uf->enctype != 32) { /* FEFF UTF-16BE */
    uf->bige = 1;
    uf->enctype = 16;
  }
} else if(c == 0xff) {
  <<Read and store a char from [[uf]]>>
  if(c == 0xfe) { /* FFFE UTF-16LE or UTF-32LE (or native UTF-32) */
    if(uf->enctype == 16)
      uf->bige = 0;
    else {
      <<Read and store a char from [[uf]]>>
      if(!c) {
        <<Read and store a char from [[uf]]>>
        if(!c) { /* FFFE0000 UTF-32LE */
          uf->enctype = 32;
	  uf->bige = 0;
        } else if(!uf->enctype)
          uf->enctype = 16;
	/* else UTF-32 native-endian */
      } else if(!uf->enctype)
        uf->enctype = 16;
      /* else UTF-32 native-endian */
    }
  }
} else if(!c && uf->enctype != 16) {
  <<Read and store a char from [[uf]]>>
  if(!c) {
    <<Read and store a char from [[uf]]>>
    if(c == 0xfe) {
      <<Read and store a char from [[uf]]>>
      if(c == 0xff) { /* 0000FEFF UTF-32BE */
        uf->enctype = 32;
	uf->bige = 1;
      }
    }
  }
}
<<Unread all chars read from [[uf]]>>
@

<<Read and store a char from [[uf]]>>=
if(uf->raptr < uf->ralen)
  c = uf->readahead[uf->raptr++];
else if(!uf->ralen) {
  uf->ralen = fread(uf->readahead, 1, UNIF_BUF_LEN, uf->f);
  if(uf->raptr)
    c = uf->readahead[uf->raptr++];
  else
    c = EOF;
} else
  c = EOF;
@

<<Unread all chars read from [[uf]]>>=
uf->raptr = 0;
@

The reader then uses the appropriate method to read a character based
on the [[enctype]].

<<Unicode I/O Exports>>=
/** Returns next Unicode code point from file \p uf opened by \ref uni_fopen */
int32_t uni_fgetc(uni_file_t *uf);
@

<<Unicode I/O functions>>=
int32_t uni_fgetc(uni_file_t *uf)
{
  if(!uf->enctype) {
    return getc(uf->f);
#ifdef UNI_USE_ICONV
  } else if(uf->enctype == 1) {
    <<Return a character using iconv>>
#endif
  }
  <<Return a Unicode character>>
}
@

Rather than using the potentially flawed direct I/O routines, the
Unicode reader always reads raw bytes and then calls the decoder
instead.  That way, the readahead issue disappears.  The readahead
buffer is refilled whenever it has less than 4 characters, since all
encodings require at most that many.

<<Return a Unicode character>>=
if(uf->ralen < uf->raptr + 4 && (!uf->ralen || uf->ralen == UNIF_BUF_LEN)) {
  memcpy(uf->readahead, uf->readahead + uf->raptr, uf->ralen - uf->raptr);
  uf->ralen -= uf->raptr;
  uf->raptr = 0;
  uf->ralen += fread(uf->readahead + uf->ralen, 1, UNIF_BUF_LEN - uf->ralen, uf->f);
}
if(uf->raptr == uf->ralen)
  return -1;
int cp;
unsigned int l;
uint8_t *ra = (uint8_t *)uf->readahead + uf->raptr;
if(uf->enctype == 32) {
  cp = uni_utf32_decode((uint32_t *)ra, uf->bige);
  l = 4;
} else if(uf->enctype == 16) {
  cp = uni_utf16_decode((uint16_t *)ra, (uf->ralen - uf->raptr) / 2, &l, uf->bige);
  l *= 2;
} else
  cp = uni_utf8_decode(ra, uf->ralen - uf->raptr, &l);
if(l > uf->ralen - uf->raptr) {
  uf->ralen = uf->raptr = 0;
  return -2;
}
if(l < 4)
  movebuf(uf->readahead, uf->readahead + l, 4 - l);
uf->raptr += l;
return cp < 0 ? -2 : cp;
@

The [[iconv]] conversion proceeds much differently.  If a character is
availble in the output buffer, it validates and returns that character.
Otherwise, it fills up the input buffer from the file, and then
attempts to fill the output buffer using [[iconv]].

The [[iconv]] routine needs to be called one more time at end-of-file.
Detection of end-of-file is easy, but detection of whether or not this
last step has been done is not.  A flag is used to indicate this.

<<Unicode file buffer state>>=
#ifdef UNI_USE_ICONV
uint8_t did_ic_eof;
#endif
@

<<Return a character using iconv>>=
while(1) {
  if(uf->olen) {
    --uf->olen;
    uint32_t cp = uf->obuf[uf->optr++];
    if(cp > 0x10FFFF || (cp >= 0xD800 && cp < 0xE000))
      return -3;
    return cp;
  }
  /* if not at end of input, refill buffer */
  if(!uf->ralen || uf->ralen == UNIF_BUF_LEN) {
    /* if more than UNIF_BUF_LEN needed for a pass of iconv, this is screwed */
    if(uf->ralen && !uf->raptr) {
      errno = ENOSPC;
      return -2;
    }
    memmove(uf->readahead, uf->readahead + uf->raptr, uf->ralen - uf->raptr);
    uf->ralen += fread(uf->readahead + uf->ralen, 1, UNIF_BUF_LEN - uf->ralen, uf->f);
    if(!uf->ralen) /* len was 0, and 0 additional bytes read: EOF @start */
      return -1; /* assume iconv wouldn't return anything either */
    uf->ralen -= uf->raptr;
    uf->raptr = 0;
  }
  char *obuf = (char *)uf->obuf;
  size_t olen = UNIF_BUF_LEN;
  if(uf->ralen == uf->raptr) {
    if(uf->did_ic_eof)
      return -1;
    uf->did_ic_eof = 1;
    if(!uf->ralen)
      uf->ralen = uf->raptr = 1; /* avoid reading again */
    if(iconv(uf->ic, NULL, NULL, &obuf, &olen) < 0)
      return -2;
  } else {
    char *ibuf = uf->readahead + uf->raptr;
    size_t ilen = uf->ralen - uf->raptr;
    if(iconv(uf->ic, &ibuf, &ilen, &obuf, &olen) < 0 &&
       ibuf == uf->readahead + uf->raptr) {
      uf->raptr++;
      return -2;
    }
    uf->raptr = uf->ralen - ilen;
  }
  uf->optr = 0;
  uf->olen = UNIF_BUF_LEN - olen;
}
@

\section{Internal Storage}

The previous section assumed that internal storage is in 32-bit words,
or raw UTF-32.  However, this is not the best storage method for all
applications.  The other two encoding techniques have their advantages
as well.

UTF-32 is easy to index and compute the length.  However, it always
wastes at least 11 bits of storage per character.  UTF-16 and UTF-8
require a scan of the entire string to find an index or compute the
length.  However, they only waste 11 bits in the worst case, and more
commonly (especially in western scripts) waste much less.  In order to
compensate for the length problem, two separate lengths could be
stored for every string: the number of words stored, and the actual
string length.  In order to compensate for the index problem, if the
index is required frequently, conversion to UTF-32 is recommended. Use
of a side array storing indices would take just as much space, and
offer no real advantage other than slightly less cost in computing
each entry; that cost would likely be repaid every time the character
at a particular index is actually retrieved.  It would be possible to
save space in a side array by storing only a single bit per word
flagging all but the last word of each multi-word sequence, thus
reducing the scan of the entire string down to a scan of the bit
array, but again, just converting to UTF-32 is probably best:

<<Scan continuation array>>=
/* pseudo code */
int find_offset(s, n, flags)
{
  int m = 0, c = 0;
  while(1) {
    int b = count_bits(flags, m .. n - 1);
    if(!b)
      return n;
    m = n + 1;
    n += b;
  }
}

int offset_of(s, n, flags)
{
  return n - count_bits(flags, 0 .. n - 1);
}
@

Nonetheless, for raw storage, functions are provided to compute the
offset of the first word of a character, and the character offset of a
given word offset.  No checks are made to see if the string is valid
is intended for internally-generated strings.  The UTF-32 versions are
pointless, except when creating generic code that constructs function
names based on size.

<<Unicode I/O Exports>>=
/** Return the offset in \p buf, in raw words, of Unicode code point \p n.
  * The encoding of \p buf is UTF-32.  Note that this is a
  * preprocessor macro */
unsigned int uni_utf32_offset_of(const uint32_t *buf, unsigned int n);
#define uni_utf32_offset_of(b, n) (n)
/** Return the number of valid Unicode code points before \p buf[\p n].
  * The encoding of \p buf is UTF-32.  Note that this is a preprocessor
  * macro */
unsigned int uni_utf32_index_of(const uint32_t *buf, unsigned int n);
#define uni_utf32_index_of(b, n) (n)
/** Return the offset in \p buf, in raw words, of Unicode code point \p n.
  * The encoding of \p buf is native-endian UTF-16.  Results are
  * undefined if the first \p n - 1 code points in \p buf are not valid
  * UTF-16 */
unsigned int uni_utf16_offset_of(const uint16_t *buf, unsigned int n);
/** Return the number of valid Unicode code points before \p buf[\p n].
  * The encoding of \p buf is native-endian UTF-16.  Results are
  * undefined if the first \p n - 1 members of \p buf are not valid
  * UTF-16 */
unsigned int uni_utf16_index_of(const uint16_t *buf, unsigned int n);
/** Return the offset in \p buf, in raw words, of Unicode code point \p n.
  * The encoding of \p buf is UTF-8.  Results are undefined if the first
  * \p n - 1 code points in \p buf are not valid UTF-8 */
unsigned int uni_utf8_offset_of(const uint8_t *buf, unsigned int n);
/** Return the number of valid Unicode code points before \p buf[\p n].
  * The encoding of \p buf is native-endian UTF-16.  Results are
  * undefined if the first \p n - 1 members of \p buf are not valid
  * UTF-8 */
unsigned int uni_utf8_index_of(const uint8_t *buf, unsigned int n);
@

<<Unicode I/O functions>>=
unsigned int uni_utf16_offset_of(const uint16_t *buf, unsigned int n)
{
  unsigned int i, j;
  for(i = j = 0; i < n; i++, j++)
    if(buf[j] >= 0xd800 && buf[j] < 0xdc00)
      j++;
  return j;
}

unsigned int uni_utf16_index_of(const uint16_t *buf, unsigned int n)
{
  unsigned int i, j;
  for(i = j = 0; j < n; i++, j++)
    if(buf[j] >= 0xd800 && buf[j] < 0xdc00)
      j++;
  return i;
}

unsigned int uni_utf8_offset_of(const uint8_t *buf, unsigned int n)
{
  unsigned int i, j;
  for(i = j = 0; i < n; i++, j++)
    if(buf[j] >= 0x80)
      j += !(buf[j] & 0x20) ? 1 : !(buf[j] & 0x10) ? 2 : 3;
  return j;
}

unsigned int uni_utf8_index_of(const uint8_t *buf, unsigned int n)
{
  unsigned int i, j;
  for(i = j = 0; j < n; i++, j++)
    if(buf[j] >= 0x80)
      j += !(buf[j] & 0x20) ? 1 : !(buf[j] & 0x10) ? 2 : 3;
  return i;
}
@

A modified version of the above index functions can be used to compute
the length of a zero-terminated string.  Only the loop end condition
needs to change.  Unlike the above routines, a 32-bit version is
useful as well.

<<Unicode I/O Exports>>=
/** Compute the number of Unicode code points in zero-terminated \p buf */
unsigned int uni_utf32_strlen(const uint32_t *buf);
/** Compute the number of Unicode code points in zero-terminated \p buf.
  * Results are undefined if any code code points are not valid
  * native-endian UTF-16 */
unsigned int uni_utf16_strlen(const uint16_t *buf);
/** Compute the number of Unicode code points in zero-terminated \p buf.
  * Results are undefined if any code points are not valid UTF-8 */
unsigned int uni_utf8_strlen(const uint8_t *buf);
@

<<Unicode I/O functions>>=
unsigned int uni_utf32_strlen(const uint32_t *buf)
{
  unsigned int i;
  for(i = 0; buf[i]; i++);
  return i;
}

unsigned int uni_utf16_strlen(const uint16_t *buf)
{
  unsigned int i, j;
  for(i = j = 0; buf[j]; i++, j++)
    if(buf[j] >= 0xd800 && buf[j] < 0xdc00)
      j++;
  return i;
}

unsigned int uni_utf8_strlen(const uint8_t *buf)
{
  unsigned int i, j;
  for(i = j = 0; buf[j]; i++, j++)
    if(buf[j] >= 0x80)
      j += !(buf[j] & 0x20) ? 1 : !(buf[j] & 0x10) ? 2 : 3;
  return i;
}
@

To skip around in a string, simply avoid all continuation characters.
Macros are provided to make this as simple and efficient as possible. 
The UTF-16 macros are not much less efficient, if at all, if called
either at the start or the end of a multi-word character.  However,
the utf-8 macro for the possibility of a middle of a word is more
complex, and I prefer things to be orthogonal, so the next/prev macros
assume the argument points to the start of a character, and an extra
macro finds the start of a character.  Each takes a buffer pointer,
and returns the number to add to the buffer pointer to get to the
chosen destination.  For consistency, UTF-32 functions are provided as
well.

<<Unicode I/O Exports>>=
/** Find the offset to the start of a Unicode code point pointed to by \p buf.
  * The encoding of \p buf is UTF-32.  This is a preprocessor macro */
int uni_utf32_startc(const uint32_t *buf);
#define uni_utf32_startc(buf) 0
/** Find the offset to the next Unicode code point pointed to by \p buf.
  * The encoding of \p buf is UTF-32.  This is a preprocessor macro */
int uni_utf32_nextc(const uint32_t *buf);
#define uni_utf32_nextc(buf) 1
/** Find the offset to the Unicode code point prior to that pointed to by \p buf.
  * The encoding of \p buf is UTF-32.  This is a preprocessor macro */
int uni_utf32_prevc(const uint32_t *buf);
#define uni_utf32_prevc(buf) -1
/** Find the offset to the start of a Unicode code point pointed to by \p buf.
  * The encoding of \p buf is native-endian UTF-16.  This is a preprocessor macro;
  * \p buf may be evaluated more than once */
int uni_utf16_startc(const uint16_t *buf);
#define uni_utf16_startc(buf) (*(buf) >= 0xdc00 && *(buf) < 0xe000 ? -1 : 0)
/** Find the offset to the next Unicode code point pointed to by \p buf.
  * The encoding of \p buf is native-endian UTF-16.  This is a preprocessor macro;
  * \p buf may be evaluated more than once */
int uni_utf16_nextc(const uint16_t *buf);
#define uni_utf16_nextc(buf) (*(buf) < 0xd800 || *(buf) >= 0xdc00 ? 1 : 2)
/** Find the offset to the Unicode code point prior to that pointed to by \p buf.
  * The encoding of \p buf is native-endian UTF-16.  This is a preprocessor macro;
  * \p buf may be evaluated more than once */
int uni_utf16_prevc(const uint16_t *buf);
#define uni_utf16_prevc(buf) ((buf)[-1] < 0xd800 || (buf)[-1] >= 0xe000 ? -1 : -2)
/** Find the offset to the start of a Unicode code point pointed to by \p buf.
  * The encoding of \p buf is UTF-8.  This is a preprocessor macro;
  * \p buf may be evaluated more than once */
int uni_utf8_startc(const uint8_t *buf);
#define uni_utf8_startc(buf) (*(buf) < 0x80 || (*(buf) & 0x40) ? 0 : \
                               ((buf)[-1] & 0x40) ? -1 : \
			       ((buf)[-2] & 0x40) ? -2 : -3)
/** Find the offset to the next Unicode code point pointed to by \p buf.
  * The encoding of \p buf is UTF-8.  This is a preprocessor macro;
  * \p buf may be evaluated more than once */
int uni_utf8_nextc(const uint8_t *buf);
#define uni_utf8_nextc(buf) (*(buf) < 0x80 ? 1 : !(*(buf) & 0x20) ? 2 : \
                              !(*(buf) & 0x10) ? 3 : 4)
/** Find the offset to the Unicode code point prior to that pointed to by \p buf.
  * The encoding of \p buf is UTF-8.  This is a preprocessor macro;
  * \p buf may be evaluated more than once */
int uni_utf8_prevc(const uint8_t *buf);
#define uni_utf8_prevc(buf) (uni_utf8_startc((buf) - 1) - 1)
@

There is some performance benefit to decoding UTF-16 and UTF-8 without
error checking.  To emphasize that these are only for internally
generated, guaranteed valid strings, the functions to do this have
``valid'' in their name.  In addition, since errors are never
returned, the return value is unsigned.

<<Unicode I/O Exports>>=
/** Returns decoded Unicode code point at \p s.
 *  The encoding at \p s is UTF-16BE if \p bige is true, and UTF-16LE
 *  otherwise.  The number of words read are returned in \p nread, if non-NULL.
 *  Results are undefined if \p s does not point to valid UTF-16.
 *  Use \ref uni_utf16_decode if this is in question */
uint32_t uni_valid_utf16_decode(const uint16_t *s, unsigned int *nread, int bige);
/** Returns decoded Unicode code point at \p s.
 *  The encoding at \p s is UTF-8.  The number of bytes read are returned
 *  in \p nread, if non-NULL.  Results are undefined if \p s does not
 *  point to valid UTF-16. Use \ref uni_utf8_decode if this is in question */
uint32_t uni_valid_utf8_decode(const uint8_t *s, unsigned int *nread);
@

<<Unicode I/O functions>>=
uint32_t uni_valid_utf16_decode(const uint16_t *s, unsigned int *nread, int bige)
{
  uint16_t w = bige ? be16toh(*s) : le16toh(*s);
  if(w < 0xd800 || w >= 0xe000) {
    if(nread)
      *nread = 1;
    return w;
  }
  if(nread)
    *nread = 2;
  uint16_t w2 = bige ? be16toh(s[1]) : le16toh(s[1]);
  return (w2 & 0x3ff) + ((uint32_t)(w & 0x3ff) << 10) + 0x10000;
}

uint32_t uni_valid_utf8_decode(const uint8_t *s, unsigned int *nread)
{
  if(*s < 0x80) {
    if(nread)
      *nread = 1;
    return *s;
  }
  uint32_t c = *s++, ec;
  if((c & 0xf8) == 0xf0) {
    c &= 0x07;
    ec = *s++;
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if(nread)
      *nread = 4;
    c = (c << 6) + (ec & 0x3f);
    return c;
  } else if((c & 0xf0) == 0xe0) {
    c &= 0x0f;
    ec = *s++;
    c = (c << 6) + (ec & 0x3f);
    ec = *s++;
    if(nread)
      *nread = 3;
    c = (c << 6) + (ec & 0x3f);
    return c;
  } else /* if((c & 0xe0) == 0xc0) */ {
    c &= 0x1f;
    ec = *s++;
    if(nread)
      *nread = 2;
    c = (c << 6) + (ec & 0x3f);
    return c;
  }
}
@

Actually, since internal UTF-32 has the underlying architecture's
natural endianness, the UTF-16 internal routines should use this as
well.  These have ``int'' in the name to distinguish themselves.  For
consistency, UTF-8 and UTF-32 versions of these functions are
provided, as well.  Since all but the UTF-8 routines are simple, they
are all in-lined.  The UTF-16 versions do this by being a static
function.

<<Unicode I/O Exports>>=
/** Encodes Unicode code point \p cp in native-endian UTF-32 at \p buf.
 *  Returns number of words written.  This is a preprocessor macro */
int uni_int_utf32_encode(uint32_t *buf, uint32_t cp);
#define uni_int_utf32_encode(buf, cp) ((*(buf) = cp), 1)
#ifdef __GNUC__
__attribute__((unused))
#endif
static int uni_int_utf16_encode(uint16_t *buf, uint32_t cp)
{
  if(cp < 0x10000) {
    *buf = cp;
    return 1;
  }
  cp -= 0x10000;
  *buf++ = 0xd800 + (cp >> 10);
  *buf = 0xdc00 + (cp & 0x3ff);
  return 2;
}
/** Encodes Unicode code point \p cp in native-endian UTF-16 at \p buf.
  * Ensure that at least 2 words are available in \p buf, or use
  * \ref uni_utf16_enclen to size the buffer first.  Returns number
  * of words written */
int uni_int_utf16_encode(uint16_t *buf, uint32_t cp);
/** Encodes Unicode code point \p cp into \p buf using UTF-8.
  * Returns number of bytes generated.  Ensure that at least 3 bytes are
  * available in \p buf, or use \ref uni_utf8_enclen to size the buffer
  * first.  Invalid Unicode code points will not be encoded */
int uni_int_utf8_encode(uint8_t *buf, uint32_t cp);
#define uni_int_utf8_encode uni_utf8_encode
@

<<Unicode I/O Exports>>=
#ifdef __GNUC__
__attribute__((unused))
#endif
static uint32_t uni_int_utf16_decode(const uint16_t *s, unsigned int *nread)
{
  uint16_t w = *s;
  if(w < 0xd800 || w >= 0xe000) {
    if(nread)
      *nread = 1;
    return w;
  }
  if(nread)
    *nread = 2;
  return (s[1] & 0x3ff) + ((uint32_t)(w & 0x3ff) << 10) + 0x10000;
}
/** Returns decoded Unicode code point at \p s.
 *  The encoding at \p s is native-endian UTF-16.  Results are undefined
 *  if \p s does not point to valid UTF-16.  The number of words read
 *  are returned in \p nread if non-NULL */
uint32_t uni_int_utf16_decode(const uint16_t *s, unsigned int *nread);
/* gcc warns (-Waddress) if nr is constant */
/* #define uni_int_utf32_decode(buf, nr) ((nr) ? (*(int *)(nr) = 1), *(buf) : *(buf)) */
#ifdef __GNUC__
__attribute__((unused))
#endif
static uint32_t uni_int_utf32_decode(const uint32_t *s, unsigned int *nread)
{
  if(nread)
    *nread = 1;
  return *s;
}
/** Returns decoded Unicode code point at \p s.
 *  The encoding at \p s is native-endian UTF-32.  The number of words read
 *  are returned in \p nread if non-NULL */
uint32_t uni_int_utf32_decode(const uint32_t *s, unsigned int *nread);
/** Returns decoded Unicode code point at \p s.
 *  The encoding at \p s is UTF-8.  The number of bytes read are returned
 *  in \p nread, if non-NULL.  Results are undefined if \p s does not
 *  point to valid UTF-16. Use \ref uni_utf8_decode if this is in question */
uint32_t uni_int_utf8_decode(const uint8_t *s, unsigned int *nread);
#define uni_int_utf8_decode uni_valid_utf8_decode
@

<<C Prototypes>>=
/* Repeat, but necessary since build.nw doesn't skip statics */
int uni_int_utf16_encode(uint16_t *buf, uint32_t cp);
uint32_t uni_int_utf16_decode(const uint16_t *s, unsigned int *nread);
uint32_t uni_int_utf32_decode(const uint32_t *s, unsigned int *nread);
@

All functions developed from this point forward which take only one
character for input always take UTF-32; since they would need to do
the translation anyway, it is no burden to require the calling of one
of the above two functions to decode the other formats.  Functions
which take more than one character for input should also take
(guaranteed) valid UTF-16 and UTF-8.  Similarly, functions which
return just one code point always return UTF-32, but those which
return more than one can return UTF-16 or UTF-8 as well.  Returning
UTF-32 for single characters eliminates the need for output buffer
management.

There are at least three different ways to return the potentially
multi-character results: return a pointer to a constant string, fill
in a provided return buffer of fixed size (up to its length), or fill
in a provided buffer of variable size (expanding using [[realloc]] if
necessary).  Since constant strings are only one format, they can
really only be used internally.  The choice between the other two
depends largely on how much effort is required to compute the results
repeatedly:  the function must be called twice if the return buffer is
too small.  Of course if memory management makes the use of
[[realloc]] inappropriate, forcing the third method is not a good
idea, either.  So, for consistency, a hybrid of the latter two methods
is used.  Each output buffer is passed in using three parameters: a
pointer to a pointer to the buffer, an integer offset to where the
results should be placed (appended), and a pointer to the current
buffer length.  If the offset is less than zero, the buffer is never
expanded.  As a special shortcut, a fixed-length buffer of size zero
can be indicated with a buffer length pointer of [[NULL]], and the
zero-length buffer need not be a valid pointer, either.  Rather than
use my [[resize]] macro for this, the function sets the output buffer
to [[NULL]] on memory allocation errors.  The number of words which
would have been returned, had the buffer been large enough, is
returned. The following helpers are provided to implement this policy.

<<Buffer return parameters for UTF-(@sz)>>=
uint<<@sz>>_t **buf, int off, unsigned int *buf_len
@

<<[[uni_returnN_buf]](@sz) proto>>=
/** Helper function to return UTF-<<@csz>> data \p str/\p len into a UTF-<<@sz>> buffer.
  * If \p off is less than zero, \p buf points to a pointer to a buffer of length
  * \p *buf_len.  The buffer need not be long enough to hold the results.
  * If \p buf or \p buf_len are NULL, a zero-length buffer is assumed.
  * If \p off is greater than or equal to zero, \p buf points to a pointer
  * to a dynamically resizable buffer whose current size is \p *buf_len.
  * In the latter case, \p *buf will be made at least large enough to hold
  * the result (possibly larger), and the result will be placed at offset
  * \p off from the start of the buffer.  If there is insufficient memory,
  * \p *buf will be set to NULL (and freed if it was not already NULL).
  * This function returns the number of <<@sz>>-bit words it would have
  * returned had the buffer been large enough */
uint32_t uni_return<<@csz>>_buf<<@sz>>(const uint<<@csz>>_t *str, uint32_t len,
                                <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<[[uni_return]](@csz)[[_bufN]] protos>>=
<<[[uni_returnN_buf]][[32]] proto>>;
<<[[uni_returnN_buf]][[16]] proto>>;
<<[[uni_returnN_buf]][[8]] proto>>;
@

<<Unicode I/O Exports>>=
<<[[uni_return]][[32]][[_bufN]] protos>>
<<[[uni_return]][[16]][[_bufN]] protos>>
<<[[uni_return]][[8]][[_bufN]] protos>>
@

<<Unicode I/O functions>>=
<<[[uni_return]][[32]][[_bufN]]>>
<<[[uni_return]][[16]][[_bufN]]>>
<<[[uni_return]][[8]][[_bufN]]>>
@

<<[[uni_return]](@csz)[[_bufN]]>>=
<<[[uni_returnN_buf]][[32]]>>
<<[[uni_returnN_buf]][[16]]>>
<<[[uni_returnN_buf]][[8]]>>
@

<<[[uni_returnN_buf]](@sz)>>=
<<[[uni_returnN_buf]][[<<@sz>>]] proto>>
{
  unsigned int rlen;
  
  /* not very efficient, I guess: always calculate output len first */
  <<Calculate result length [[rlen]] for [[str]] of length [[len]]>>
  /* now, store into output buffer */
  if(off < 0) {
    <<Store result into constant-length [[*buf]] of length [[*buf_len]]>>
    return rlen;
  }
  <<Store result into resizable [[*buf]] of length [[*buf_len]] at [[off]]>>
  return rlen;
}
@

<<Calculate result length [[rlen]] for [[str]] of length [[len]]>>=
#if <<@csz>> == <<@sz>>
rlen = len;
#else
unsigned int i;
for(i = rlen = 0; i < len; ) {
  unsigned int clen;
#ifdef __GNUC__ /* shut up gcc warning for UTF-32 */
  __attribute__((unused))
#endif
  uint32_t c = uni_int_utf<<@csz>>_decode(str + i, &clen);
  i += clen;
  rlen += uni_utf<<@sz>>_enclen(c);
}
#endif
@

<<Store result into constant-length [[*buf]] of length [[*buf_len]]>>=
unsigned int blen = buf_len ? *buf_len : 0;
len = blen;
if(len > rlen)
  len = rlen;
if(!len)
  return rlen;
uint<<@sz>>_t *bptr = *buf;
#if <<@csz>> == <<@sz>>
cpybuf(bptr, str, len);
#else
while(len) {
  unsigned int clen;
  uint32_t c = uni_int_utf<<@csz>>_decode(str, &clen);
  str += clen;
  if(blen >= 4) {
    int csz = uni_int_utf<<@sz>>_encode(bptr, c);
    bptr += csz;
    len -= csz;
    blen -= csz;
  } else {
    uint<<@sz>>_t cbuf[4];
    int csz = uni_int_utf<<@sz>>_encode(cbuf, c);
    if(csz > len)
      csz = len;
    cpybuf(bptr, cbuf, csz);
    bptr += csz;
    len -= csz;
    blen -= csz;
  }
}
#endif
@

<<Store result into resizable [[*buf]] of length [[*buf_len]] at [[off]]>>=
/* allocate space if needed */
if(!*buf)
  *buf_len = 0; /* shouldn't be necessary */
else if(!*buf_len) { /* should never happen */
  free(*buf); /* will probably corrupt memory */
  *buf = NULL;
}
if(rlen + off > *buf_len) {
  if(!*buf_len) {
    unsigned int asize = off + rlen;
    if(asize < 5)
      asize = 5;
    *buf = malloc(asize * sizeof(**buf));
    if(!*buf)
      return rlen;
    *buf_len = asize;
  } else {
    while(*buf_len < rlen + off)
      *buf_len *= 2;
    uint<<@sz>>_t *nbuf = realloc(*buf, *buf_len * **buf);
    if(!nbuf) {
      free(*buf);
      *buf = NULL;
      return rlen;
    }
    *buf = nbuf;
  }
}
/* store result; sufficient space guaranteed */
uint<<@sz>>_t *bptr = *buf + off;
#if <<@sz>> == <<@csz>>
cpybuf(bptr, str, rlen);
#else
len = rlen;
while(len) {
  unsigned int clen;
  uint32_t c = uni_int_utf<<@csz>>_decode(str, &clen);
  str += clen;
  int csz = uni_int_utf<<@sz>>_encode(bptr, c);
  bptr += csz;
  len -= csz;
}
#endif
@

\chapter{Properties}

This library's primary purpose is to preparse the Unicode information
and query it efficiently.  The Unicode Character Database defines
numerous properties for characters.  Since there are many properties,
and any particular application may only need a few of them, an attempt
is made to keep each property in a separate object file.  Static
linking will only pull in the required properties, and there is little
penalty for shared libraries which include everything.  Some code is
shared by all properties, though; this is placed in a common file.

<<Library [[uni]] headers>>=
#include "uni_prop.h"
@

<<uni_prop.h>>=
<<Common C Warning>>
#ifndef UNI_PROP_H
#define UNI_PROP_H
/** \file uni_prop.h Unicode Properties */

#include "uni_io.h"
/** \addtogroup uni_prop Unicode Property Queries
    @{ */
<<Unicode property exports>>
/** @} */
#endif /* UNI_PROP_H */
@

<<Headers to Install>>=
uni/uni_prop.h \
@

\lstset{language=make}
<<makefile.rules>>=
uni_prop.h: uni_io.h
@

<<Library [[uni]] Members>>=
uni_prop.o
@

\lstset{language=C}
<<uni_prop.c>>=
<<Common C Header>>
#include "uni_prop.h"
// static_proto

<<Unicode property functions>>
@

Some Unicode property exports are generated, and some are hand-coded.
Some of the hand-coded exports may be useful for generation, so they
are in a separate code chunk, which can be included in the generator
without depending on the generated output. 

<<Unicode property exports>>=
<<Unicode property exports for generator>>
@

<<Unicode property functions>>=
<<Unicode property functions for generator>>
@

These properties come in a variety of formats, but for the purpose of
this library, they may be generally classified according to the type
of value:  Boolean, String, Enumerated, or Numeric.  Each class of
values has a particular set of desirable operations related to a
property of that class.

\section{Boolean Properties}

Boolean properties are either true or false.  They define a subset of
characters: those for which the value is true.  As with any set, the
desirable operations are:

% Begin-doc op-table
%% l2h substitution ominus &ominus;
% tidy doesn't understand &ominus
%% l2h substitution ominus &#8854;
% substitution replaces # with space, so use 0-arg macro instead
% l2h macro ominus 0 &##8854;
% l2h substitution oplus &oplus;
% l2h substitution wedge &and;
% l2h substitution vee &or;
% l2h substitution neg &not;
% l2h macro overline 1 <span style="text-decoration:overline">#1</span>
% l2h substitution emptyset &empty;
% l2h substitution alpha &alpha;
% l2h substitution in &isin;
% l2h substitution notin &notin;
% l2h substitution cap &cap;
% l2h substitution cup &cup;
% End-doc op-table

\begin{itemize}
\item Finding out if a character \emph{is} or is not an element of the
set.
\item Querying the \emph{members} of a set.
\item Applying set \emph{operators} on a pair of sets.  There are
16 possible set operations between two sets (each potential member is
either in or not in each of the 2 sets = $2^2$ membership
combinations; member is either in or not in result = $4^2$ total
combinations).  See table \ref{tab:setop} for a complete list of
operations.

% Begin-doc op-table
\begin{table}
\begin{quote}
\begin{tabular}{lllllcrll}
$x\in A$&false&false&true&true&&~&\\
$x\in B$&false&true&false&true&&&\\
\emph{op}&\multicolumn{4}{c}{$x\in A\textrm{ \emph{op} }B$}&\emph{when}&\multicolumn{2}{l}{\emph{\#}}&\emph{Op Name}\\
$\emptyset$&false&false&false&false&never&0&&NIL\\
$\alpha$&true&true&true&true&always&1&&ALL\\
$A$&false&false&true&true&$x\in A$&2&&A\\
$\overline A$&true&true&false&false&$x\notin A$&3&&INV\_A NOT\_A\\
$B$&false&true&false&true&$x\in B$&4&&B\\
$\overline B$&true&false&true&false&$x\notin B$&5&&INV\_B NOT\_B\\
$A\ominus B$&false&true&true&false&$(x\in A)\oplus (x\in B)$&6&&SYM\_DIFF XOR\\
$\overline{A\ominus B}$&true&false&false&true&$(x\in A)\overline\oplus (x\in B)$&7&&INV\_SYM\_DIFF XNOR\\
$A\cap B$&false&false&false&true&$(x\in A)\wedge (x\in B)$&8&&INTER AND\\
$\overline{A\cap B}$&true&true&true&false&$(x\in A)\overline\wedge (x\in B)$&9&&INV\_INTER NAND\\
$A-B$&false&false&true&false&$(x\in A)\wedge (x\notin B)$&10&&A\_MINUS\_B\\
$\overline{A-B}$&true&true&false&true&$(x\notin A)\vee (x\in B)$&11&&INV\_A\_MINUS\_B\\
$B-A$&false&true&false&false&$(x\notin A)\wedge (x\in B)$&12&&B\_MINUS\_A\\
$\overline{B-A}$&true&false&true&true&$(x\in A)\vee (x\notin B)$&13&&INV\_B\_MINUS\_A\\
$A\cup B$&false&true&true&true&$(x\in A)\vee (x\in B)$&14&&UNION OR\\
$\overline{A\cup B}$&true&false&false&false&$(x\in A)\overline\vee (x\in B)$&15&&INV\_UNION NOR\\
\end{tabular}

{\small Operation numbers are assigned using the formula $o_0 \oplus
(x \in A)\wedge o_1 \oplus (x \in B)\wedge o_2 \oplus (x \in A\cap
B)\wedge o_3$, where $o_n$ is true if bit $n$ is one in the operation
number.  Operation names are of the enumeration type [[uni_set_op_t]],
and are prefixed with [[UNI_SOP_]].  Where two operation names are
given, either will work.}
\end{quote}
% End-doc op-table

\caption{\label{tab:setop}Set Operations}
\end{table}
\end{itemize}

\subsection{Storage Methods}

All of the above require that every member be stored.  The \emph{is}
test can be stored in a single bit.  However, there are over a million
valid character values, so the total storage for a simple bit array
would still be over 128 kilobytes, even if the array is limited to the
valid code point range.  Some properties only apply to a limited range
of characters, so these might use a little less space.  Even so, with
numerous properties, this adds up quickly, and may even slow things
down by keeping everything out of the cache.  The set
\emph{operations} are easy to perform, but require manipulating up to
384 kilobytes of data.

<<Unicode property exports for generator>>=
#define UNI_MAX_CP 0x10ffff /**< Maximum valid Unicode code point */
/** \addtogroup uni_prop_bitarrays Generic Bit Array Access
    @{ */
/** Find bit array \p a's element index from bit number \p i */
#define UNI_BSET_ELT(a, i) ((i)/(sizeof(*(a))*8))
/** Find bit array \p a's element from bit number \p i.
  * This is an lval with an address */
#define UNI_BSET_AELT(a, i) (a)[UNI_BSET_ELT(a, i)]
/** Find bit array \p a's element's bit number from full bit number \p i */
#define UNI_BSET_BIT(a, i) ((i)%(sizeof(*(a))*8))
/** Find full bit number given bit array \p a's element index \p e and
 ** element bit number \p b */
#define UNI_BSET_ENTRY(a, e, b) ((e)*(sizeof(*(a))*8) + b)
/** Convert full bit number \p i into single-bit mask for bit array \p a's element */
#define UNI_BSET_MASK(a, i) (1ULL << UNI_BSET_BIT(a, i))
/** True if full bit number \p i is set in bit array \p a */
#define UNI_BSET_IS_SET(a, i) ((UNI_BSET_AELT(a, i) & UNI_BSET_MASK(a, i)) != 0)
/** Set bit number \p i in bit array \p a */
#define UNI_BSET_SET(a, i) UNI_BSET_AELT(a, i) |= UNI_BSET_MASK(a, i)
/** Clear bit number \p i in bit array \p a */
#define UNI_BSET_CLEAR(a, i) UNI_BSET_AELT(a, i) &= ~UNI_BSET_MASK(a, i)
@

<<C Prototypes>>=
uint_ei_type UNI_BSET_ELT(uint_e_type *a, uint_i_type i);
lval uint_e_type UNI_BSET_AELT(uint_e_type *a, uint_i_type i);
uint_b_type UNI_BSET_BIT(uint_e_type *a, uint_i_type i);
uint_i_type UNI_BSET_ENTRY(uint_e_type *a, uint_ei_type e, uint_b_type b);
uint_e_type UNI_BSET_MASK(uint_e_type *a, uint_i_type i);
int UNI_BSET_IS_SET(uint_e_type *a, uint_i_type i);
void UNI_BSET_SET(uint_e_type *a, uint_i_type i);
void UNI_BSET_CLEAR(uint_e_type *a, uint_i_type i);
@

<<[[uni_setop_t]]>>=
/** Logical operation names for \ref UNI_BIT_SET_OP */
typedef enum {
  UNI_SOP_NIL, /**< \f$\emptyset\f$ */ UNI_SOP_ALL, /**< \f$\alpha\f$ */
  UNI_SOP_A, /**< \f$A\f$ */ UNI_SOP_INV_A, /**< \f$\overline A\f$ */
  UNI_SOP_NOT_A = UNI_SOP_INV_A, /**< \f$\overline A\f$ */
  UNI_SOP_B, /**< \f$B\f$ */ UNI_SOP_INV_B, /**< \f$\overline B\f$ */
  UNI_SOP_NOT_B = UNI_SOP_INV_B, /**< \f$\overline B\f$ */
  UNI_SOP_SYM_DIFF, /**< \f$A\ominus B\f$ **/
  UNI_SOP_XOR = UNI_SOP_SYM_DIFF, /**< \f$A\ominus B\f$ **/
  UNI_SOP_INV_SYM_DIFF, /**< \f$\overline{A\ominus B}\f$ **/
  UNI_SOP_XNOR = UNI_SOP_INV_SYM_DIFF, /**< \f$\overline{A\ominus B}\f$ **/
  UNI_SOP_INTER, /**< \f$A\cap B\f$ */ UNI_SOP_AND = UNI_SOP_INTER, /**< \f$A\cap B\f$ */
  UNI_SOP_INV_INTER, /**< \f$\overline{A\cap B}\f$ */
  UNI_SOP_NAND = UNI_SOP_INV_INTER, /**< \f$\overline{A\cap B}\f$ */
  UNI_SOP_A_MINUS_B, /**< \f$A-B\f$ */
  UNI_SOP_INV_A_MINUS_B, /**< \f$\overline{A-B}\f$ */
  UNI_SOP_B_MINUS_A, /**< \f$B-A\f$ */
  UNI_SOP_INV_B_MINUS_A, /**< \f$\overline{B-A}\f$ */
  UNI_SOP_UNION, /**< \f$A\cup B\f$ */
  UNI_SOP_OR = UNI_SOP_UNION, /**< \f$A\cup B\f$ */
  UNI_SOP_INV_UNION, /**< \f$\overline{A\cup B}\f$ */
  UNI_SOP_NOR = UNI_SOP_INV_UNION /**< \f$\overline{A\cup B}\f$ */
} uni_set_op_t;
@

<<Unicode property exports for generator>>=
<<[[uni_setop_t]]>>
#define UNI_BIT_SET_OP_BIT_(op, n) (~((((op) >> n) & 1) - 1))
/** Perform a set operation between unsigned integers \p A and \p B.
  * The operations (\p op) are defined by \ref uni_set_op_t */
#define UNI_BIT_SET_OP(A, op, B) \
  (((A) & UNI_BIT_SET_OP_BIT_(op, 1)) ^ ((B) & UNI_BIT_SET_OP_BIT_(op, 2)) ^ \
   ((A) & (B) & UNI_BIT_SET_OP_BIT_(op, 3)) ^ UNI_BIT_SET_OP_BIT_(op, 0))
/** @} */
@

<<C Prototypes>>=
uint_t UNI_BIT_SET_OP(uint_t A, uni_set_op_t op, uint_t B);
@

<<Known Data Types>>=
uni_set_op_t,%
@

The set \emph{operations} must be performed on arrays of a particular
type.  Since C does not really support generics, and using the
preprocessor would be impractical, the noweb parameterized macro
facility is used.  Previous versions of this document wrote the
function definition to an include file, and used the preprocessor to
redefine types and names for four separate [[#include]]s.  That is
probably the only way to get debugable code using the preprocessor.

<<Unicode property exports for generator>>=
#include <stdint.h>
@

<<Unicode property exports>>=
/** \addtogroup uni_prop_bitarrays
    @{ */
<<[[uni_setop_bit]] proto for [[64]]-bit integers>>; /* uint64_t is not standard, but common */
<<[[uni_setop_bit]] proto for [[32]]-bit integers>>;
<<[[uni_setop_bit]] proto for [[16]]-bit integers>>;
<<[[uni_setop_bit]] proto for [[8]]-bit integers>>;
/** @} */
@

<<[[uni_setop_bit]] proto for (@sz)-bit integers>>=
/** Perform set operations on sets described by bit arrays.
  * The bit arrays \p a and \p b map bit 0 to \p a_low and \p b_low,
  * respectively.  The array lengths are \p a_len and \p b_len,
  * respectively, making their bit lengths \f$<<@sz>>\cdot\texttt{a\_len}\f$
  * and  \f$<<@sz>>\cdot\texttt{b\_len}\f$, respectively.  In each set,
  * a one bit indicates membership, and a zero does not.  The result is
  * stored in dynamically resizable \p *res, whose currently allocted size
  * is \p *res_max.  The result's starting bit is stored in \p *res_low
  * and its array size, in <<@sz>>-bit words, is in \p *res_len */
void uni_setop_bit<<@sz>>(uint<<@sz>>_t *a, uint32_t a_low, uint32_t a_len,
                        uni_set_op_t op,
		        uint<<@sz>>_t *b, uint32_t b_low, uint32_t b_len,
		        uint<<@sz>>_t **res, uint32_t *res_low, uint32_t *res_len,
		        uint32_t *res_max)
@

<<Unicode property functions>>=
<<[[uni_setop_bit]] for [[64]]-bit integers>>
<<[[uni_setop_bit]] for [[32]]-bit integers>>
<<[[uni_setop_bit]] for [[16]]-bit integers>>
<<[[uni_setop_bit]] for [[8]]-bit integers>>
@

<<[[uni_setop_bit]] for (@sz)-bit integers>>=
<<[[uni_setop_bit]] proto for [[<<@sz>>]]-bit integers>>
{
  <<Perform set operation on bit strings>>
}
@

First, there are few degenerate cases which do not involve [[a]]
and/or [[b]].  While the general routine will return the same result,
the logic for the degenerate cases is much simpler, so a value can be
returned immediately.  The $\overline A$ and $\overline B$ operations
are degenerate cases by this definition, but they require a bit of
finesse that is provided by the main routine, anyway.  So, instead of
duplicating that code here, the unused input is simply zeroed to
improve its performance.

<<Perform set operation on bit strings>>=
if(op == UNI_SOP_INV_A)
  b_len = 0;
else if(op == UNI_SOP_INV_B)
  a_len = 0;
@

If both inputs are empty, the operation is determined by applying the
operator to two zeroes.  This will always match bit zero of the
operation number, and will be equivalent to applying $\emptyset$ or
$\alpha$.  If just one is empty, then that reduces the operations to
four: the above two, the non-empty set, or the inverse of the
non-empty set.  The operation number is adjusted to reflect this.

<<Perform set operation on bit strings>>=
int zz_op = /* UNI_BIT_SET_OP(0, op, 0) */ op & UNI_SOP_ALL;
if(!a_len && !b_len)
  op &= UNI_SOP_ALL;
else if(!a_len)
  op &= UNI_SOP_B | UNI_SOP_ALL;
else if(!b_len)
  op &= UNI_SOP_A | UNI_SOP_ALL;
@

The $\emptyset$ operation returns an empty result.  The $\alpha$
operation returns a result with all bits set.  The $A$ and $B$
operations just return copies of the requested input.

<<Perform set operation on bit strings>>=
if(op == UNI_SOP_NIL) {
  *res_len = 0;
  *res_low = 0;
  if(*res_max) {
    *res_max = 0;
    free(*res);
  }
  return;
}
if(op == UNI_SOP_ALL) {
  *res_len = UNI_MAX_CP / <<@sz>> + 1;
  *res_low = 0;
  check_size(*res, *res_max, *res_len);
  memset(*res, ~0, *res_len * <<@sz>> / 8);
  return;
}
if(op == UNI_SOP_A) {
  *res_len = a_len;
  *res_low = a_low;
  check_size(*res, *res_max, a_len);
  if(*res != a)
    cpybuf(*res, a, *res_len);
  return;
}
if(op == UNI_SOP_B) {
  *res_len = b_len;
  *res_low = b_low;
  check_size(*res, *res_max, b_len);
  if(*res != b)
    cpybuf(*res, b, *res_len);
  return;
}
@

The bit strings do not necessarily start at the same position.  For
convenience, [[a]] will be reassigned to be the set with the earlier
start position, or only start position if one of the inputs is empty.
If a swap occurs, the operation number must also be adjusted by
swapping the $A$ and $B$ bits.  In addition, if one input is empty, it
is moved past the end of the first so it is not involved in
calculations until the end.

<<Perform set operation on bit strings>>=
if(!a_len || b_low < a_low) {
  uint<<@sz>>_t *t = a;
  a = b;
  b = t;
  a_len ^= b_len;
  b_len ^= a_len;
  a_len ^= b_len;
  a_low ^= b_low;
  b_low ^= a_low;
  a_low ^= b_low;
  op = (op & ~(UNI_SOP_A|UNI_SOP_B)) | ((op & UNI_SOP_A) << 1) |
                                       ((op & UNI_SOP_B) >> 1);
}
if(!b_len)
  b_low = a_low + a_len * <<@sz>>;
@

The bit strings do not necessarily start at zero.  If neither does,
then the region before the bit strings is all zeroes, meaning that the
result will be all zeroes or all ones depending on [[zz_op]].  If it
is all zeroes, the start of the result can be moved up.  In fact, the
start of the result only needs to be set once ones start rolling in,
so a flag is kept to determine if a one has been encountered yet.

<<Perform set operation on bit strings>>=
uint32_t i;
int did_start = 0;

if(a_low > 0 && zz_op) {
  uint32_t need = a_low / <<@sz>>;
  did_start = 1;
  *res_low = 0;
  check_size(*res, *res_max, need);
  if(need > 0)
    memset(*res, ~0, need * sizeof(**res));
}
@

If the inputs do not start at the same place, there is an initial
region where only [[a]] has values other than zero.  Rather than
complicate the case where [[b]] is involved by checking for this case,
a separate loop is performed for this region.  If the result start was
set to zero above, the result may be misaligned with [[a]].  If so,
the bits from the previous operation are shifted into place.  Since
the above set all ones, the first previous value is also all ones.
While it is setting values, it also does not actually increase the
result size or set the value if it is zero.  Instead, it keeps track
of the last position that was set to zero after a non-zero set, so
that the result can be truncated with no excess zeroes.  Naturally,
when a new non-zero value comes along, the zeroes need to be filled in
after all.  The first non-zero value also sets the start position, if
it was not already set above.

<<Perform set operation on bit strings>>=
uint<<@sz>>_t cur_a = 0, prev_a, r;
uint32_t last_zero = 0;
int misalign_a = did_start ? (a_low - *res_low) % <<@sz>> : 0;
uint32_t end = a_low + a_len * <<@sz>>;
if(end < b_low)
  end = b_low;
for(; (i = a_low) < end - <<@sz>> + 1; a_low += <<@sz>>, a_len--, a++) {
  <<Obtain next aligned bits from source [[a]]>>
  r = UNI_BIT_SET_OP(prev_a, op, 0);
  <<Set res if non-zero or mark if zero>>
}
@

<<Obtain next aligned bits from source (@which)>>=
prev_<<@which>> = cur_<<@which>>;
cur_<<@which>> = *<<@which>>;
if(misalign_<<@which>>) {
  prev_<<@which>> |= cur_<<@which>> @<< misalign_<<@which>>;
  cur_<<@which>> >>= <<@sz>> - misalign_<<@which>>;
} else
  prev_<<@which>> = cur_<<@which>>;
@

<<Set res if non-zero or mark if zero>>=
if(!r) {
  if(!last_zero && did_start)
    last_zero = i - *res_low;
} else {
  if(!did_start) {
    did_start = 1;
    *res_low = i;
  }
  check_size(*res, *res_max, (i - *res_low) / <<@sz>>);
  if(last_zero) {
    memset(&UNI_BSET_AELT(*res, last_zero), 0, (i - *res_low - last_zero) / 8);
    last_zero = 0;
  }
  UNI_BSET_AELT(*res, i - *res_low) = r;
}
@

Now, either [[a]] is pointing past the end of the lower input, or it
is pointing at a word which contains the position [[b_low]].  In the
former case, there may be a(nother) region where both [[a]] and [[b]]
are not present.  If so, the region can be set to [[zz_op]] as before,
except for the first word if there was a misalignment.  In either
case, if there was a misalignment, [[cur_a]] contains some of the bits
to be applied to the next word.  Only full words which do not contain
[[b_low]] are processed here.

<<Perform set operation on bit strings>>=
if(misalign_a && i < b_low - <<@sz>> - 1) {
  r = UNI_BIT_SET_OP(cur_a, op, 0);
  <<Set res if non-zero or mark if zero>>
  i += <<@sz>>;
  cur_a = 0; /* a_len must be 0 */
}
if(!a_len) {
  misalign_a = 0; /* cur_a is aligned, and remaining 0s do not need align */
  if(did_start)
    i -= (i - *res_low) % <<@sz>>; /* align i with result instead of a */
}
if(i < b_low - <<@sz>> - 1) {
  uint32_t skip = (b_low - i) / <<@sz>>;
  skip *= <<@sz>>;
  if(!zz_op) {
    if(!last_zero && did_start)
      last_zero = i - *res_low;
    if(!did_start)
      i = b_low; /* if b causes a start, *res_low should be b_low */
    else
      i += skip;
  } else {
    <<Set gap to all ones>>
  }
}
@

<<Set gap to all ones>>=
if(!did_start) {
  did_start = 1;
  *res_low = i;
}
uint32_t tot = i - *res_low + skip;
check_size(*res, *res_max, tot / <<@sz>>);
if(last_zero) {
  memset(&UNI_BSET_AELT(*res, last_zero), 0, (i - *res_low - last_zero) / 8);
  last_zero = 0;
}
memset(&UNI_BSET_AELT(*res, i - *res_low), ~0, skip / 8);
i += skip;
@

Now the position is at the word containing [[b_low]], if [[b]] is
non-empty.  If [[a]] has not yet ended, or ended in the last word,
bits remain in [[cur_a]] for this word.  If it ended earlier than
that, [[cur_a]] is zero, so either way [[cur_a]] needs to be applied. 
Both [[b]] and [[a]] need to be aligned to the result before use.  If
the result has not yet started, it will start aligned with [[a]] if
[[a]] has not yet ended (i.e., [[i]] is an integral [[<<@sz>>]]
multiple following [[a_low]]); otherwise, due to the above code, it
will start aligned with [[b]] (i.e., [[i]] was set to [[b_low]]).
First any full words in common with both [[a]] and [[b]] are
processed; then whichever one has words left is processed.  Note that
it is not possible for [[a]] to have data left to process if [[b]] is
absent.

<<Perform set operation on bit strings>>=
if(b_len) {
  uint32_t misalign_b = did_start ? (b_low - *res_low) % <<@sz>> : 
                                    (b_low - i) % <<@sz>>;
  uint<<@sz>>_t cur_b = 0, prev_b;
  for(; a_len && b_len; b++, b_len--, b_low += <<@sz>>, i += <<@sz>>,
                        a++, a_len--, a_low += <<@sz>>) {
    <<Obtain next aligned bits from source [[b]]>>
    <<Obtain next aligned bits from source [[a]]>>
    r = UNI_BIT_SET_OP(prev_a, op, prev_b);
    <<Set res if non-zero or mark if zero>>
  }
  for(; b_len; b++, b_len--, b_low += <<@sz>>, i += <<@sz>>) {
    <<Obtain next aligned bits from source [[b]]>>
    r = UNI_BIT_SET_OP(cur_a, op, prev_b);
    <<Set res if non-zero or mark if zero>>
    cur_a = 0;
  }
  for(; a_len; a++, a_len--, a_low += <<@sz>>, i += <<@sz>>) {
    <<Obtain next aligned bits from source [[a]]>>
    r = UNI_BIT_SET_OP(prev_a, op, cur_b);
    <<Set res if non-zero or mark if zero>>
    cur_b = 0;
  }
  if(cur_a || cur_b) {
    r = UNI_BIT_SET_OP(cur_a, op, cur_b);
    <<Set res if non-zero or mark if zero>>
    i += <<@sz>>;
  }
}
@

Finally, if the upper set did not end at the last valid code point,
the upper region needs to be set the same way as the initial region.

<<Perform set operation on bit strings>>=
if(zz_op) {
  if(did_start)
    i -= (i - *res_low) % <<@sz>>; /* align i with result */
  if(i <= UNI_MAX_CP) {
    uint32_t skip = (UNI_MAX_CP - i) / <<@sz>> + 1;
    skip *= <<@sz>>;
    <<Set gap to all ones>>
  }
}
@

The only thing remaining is to update the length of the result.

<<Perform set operation on bit strings>>=
if(did_start)
  *res_len = ((last_zero ? last_zero : i) - *res_low) / <<@sz>>;
else
  *res_len = 0;
@

A plain bit array also makes it difficult to perform the
\emph{members} operation: every single potential member needs to be
tested.  Other storage strategies are needed.

<<Unicode property exports>>=
/* List members enumeration not directly supported */
@

<<Unicode property functions>>=
#if 0
  <<List bit array members>>
#endif
@

<<List bit array members>>=
int i; elt_t e;
for(i = 0; i < tab_size; i++) {
  e = tab[i];
  while(e) {
    /* strings.h usually selects optimal built-in implementation of ffs() */
    int b = ffs(e); /* or ffsl, or ffsll if glibc */
    add_ent(UNI_BSET_ENTRY(tab, i, b - 1));
    e &= ~(1 << (b - 1));
  }
}
@

One of the simplest strategies is to provide a sorted table of
members.  Membership is tested by binary search, so the speed of
lookup is proportional to $\log_2(N)$, where $N$ is the number of set
members.  Querying the \emph{members} of the set is trival: the table
is already the list.  Applying set \emph{operations} only requires
looking at the members already present, with the exception of the
odd-numbered ones:  inversion may be more expensive than with bit
arrays.  This does not save much space for properties which apply to
a large number of characters; in fact, it might even take more space.

<<Unicode property exports for generator>>=
/** Compare 32-bit signed integers pointed to by \p a and \p b.
  * Intended for use with qsort and bsearch */
int uni_cmp_cp(const void *a, const void *b);
@

<<Unicode property exports>>=
/** Returns true if Unicode code point \p c is in sorted array \p tab.
  * The length of \p tab is \p tab_len.  It is assumed that the
  * code point is contained in the first 32-bit signed integer in
  * \p tab's structure, and that \p tab is sorted by that element
  * as per \ref uni_cmp_cp */
#define uni_is_cp(c, tab, tab_len) \
  bsearch(&c, tab, tab_len, sizeof(tab[0]), uni_cmp_cp)

/* members are already in list */
/* set operations not directly supported */
@

<<C Prototypes>>=
int uni_is_cp(elt_t c, elt_t *tab, size_t tab_len);
@

<<Unicode property functions for generator>>=
int uni_cmp_cp(const void *a, const void *b)
{
  return *(int32_t *)a - *(int32_t *)b;
}
@

<<Unicode property functions>>=
#if 0
  <<Perform set ops on cp array>>
#endif
@

<<Perform set ops on cp array>>=
uint32_t i = 0;
uint32_t tab[max], tab_len = 0;
int invert = /* UNI_BIT_SET_OP(0, op, 0) */ op & 1;
uint32_t aptr, bptr;
for(aptr = bptr = 0; aptr < a_len || bptr < b_len; ) {
  int res;
  uint32_t cp;
  if(bptr == b_len || (aptr < a_len && a[aptr] < b[bptr])) {
    res = UNI_BIT_SET_OP(~0, op, 0);
    cp = a[aptr++];
  } else if(aptr == a_len || (bptr < b_len && a[aptr] > b[bptr])) {
    res = UNI_BIT_SET_OP(0, op, ~0);
    cp = b[bptr++];
  } else {
    res = UNI_BIT_SET_OP(~0, op, ~0);
    cp = a[aptr++];
    bptr++;
  }
  if(invert)
    while(i < cp)
      tab[tab_len++] = i++;
  if(res)
    tab[tab_len++] = cp;
}
if(invert)
  while(i < max)
    tab[tab_len++] = i++;
@

A possible improvement on that strategy is to store ranges instead of
individual code points.  This relies on the fact that most properties
apply to large consecutive ranges; if this is not true, then this is
actually worse than storing individual code points.  Whenever the
number of ranges is less than half of the number of individual code
points, this table will be smaller, requiring less time to search, and
less time to perform set \emph{operations}.  In particular, negation
is usually much faster with a range table than with an individual code
point table.  Not only can absent members be skipped, but present
members within a range can be skipped as well.

<<[[uni_chrrng_t]]>>=
/** A simple 64-bit range of 32-bit integers (Unicode code points) */
typedef struct {
    uint32_t low, /**< Low end of range */ high; /**< High end of range */
} uni_chrrng_t;
@

<<Unicode property exports for generator>>=
<<[[uni_chrrng_t]]>>
/** Compare \ref uni_chrrng_t code point ranges pointed to by \p a and \p b.
  * Intended for use with qsort and bsearch */
int uni_cmprng(const void *a, const void *b);
/** Returns true if \p cp is within one of the ranges in \p tab.
  * The \p tab array must be sorted as per \ref uni_cmprng.  Its length
  * is \p tab_len */
int uni_is_cp_chrrng(uint32_t cp, const uni_chrrng_t *tab, uint32_t tab_len);
@

<<Known Data Types>>=
uni_chrrng_t,%
@

<<Unicode property functions for generator>>=
int uni_cmprng(const void *a, const void *b)
{
    const uni_chrrng_t *_a = a, *_b = b;

    if(_a->high < _b->low)
        return -1;
    else if(_b->high < _a->low)
        return 1;
    else
        return 0;
}

int uni_is_cp_chrrng(uint32_t cp, const uni_chrrng_t *tab, uint32_t tab_len)
{
#if 0
  uni_chrrng_t cr = {cp, cp};
  return bsearch(&cr, tab, tab_len, sizeof(uni_chrrng_t), uni_cmprng) ? 1 : 0;
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > tab[j].high)
      l = j + 1;
    else
      return 1;
  }
  return 0;
#endif
}
@

<<Unicode property exports for generator>>=
/** Perform set operations on sets described by sorted code point range arrays.
  * Each array describes a set containing all integers within the array's
  * ranges.  Each array must be sorted as per \ref uni_cmprng.  The result
  * range is newly allocated if non-empty */
void uni_chrrng_setop(const uni_chrrng_t *a, uint32_t a_len, uni_set_op_t op,
                      const uni_chrrng_t *b, uint32_t b_len,
		      uni_chrrng_t **rtab, uint32_t *r_len);
@

While it would be possible to invert in place (and even use the same
array as the input), if there is enough room, it's easier to just
generate a new array.  At most one range is added (all the ranges
below the current ranges plus one above the top-most range), and at
most one range is removed (replaced by ranges below if top-most is at
top, except if one is already at the bottom).

<<Unicode property functions for generator>>=
static void uni_chrrng_invert(const uni_chrrng_t *a, uint32_t a_len,
		              uni_chrrng_t **rtab, uint32_t *r_len)
{
  uni_chrrng_t *tab;
  uint32_t tab_len = 0;
  inisize(tab, a_len + 1);
  uint32_t i;
  if(a_len && a[0].low) {
    tab[0].low = 0;
    tab[0].high = a[0].low - 1;
    tab_len++;
  }
  for(i = 0; i < a_len - 1; i++, tab_len++) {
    tab[tab_len].low = a[i].high + 1;
    tab[tab_len].high = a[i + 1].low - 1;
  }
  if(a_len && a[a_len - 1].high < UNI_MAX_CP) {
    tab[tab_len].low = a[a_len - 1].high + 1;
    tab[tab_len].high = UNI_MAX_CP;
    tab_len++;
  }
  *rtab = tab;
  *r_len = tab_len;
}
@

For the general operation, as with ordinary bit sets, a few degenerate
cases are handled immediately.  Again, rather than supporting a
previously allocated result structure, a new one is always allocated.
Unlike inversion, it is very likely that this is necessary in order to
avoid overwriting either one of the inputs when they are misaligned.

The generic routine operates by finding points where either input
changes state.  Then, the entire range where the the states where the
same get the result from a single operation call whose result is
almost staticaly calculated using constant inputs.

<<Unicode property functions for generator>>=
void uni_chrrng_setop(const uni_chrrng_t *a, uint32_t a_len, uni_set_op_t op,
                      const uni_chrrng_t *b, uint32_t b_len,
		      uni_chrrng_t **rtab, uint32_t *r_len)
{
  uni_chrrng_t *tab;
  uint32_t tab_len = 0, max_tab = 8;
  <<Shortcut all ops that don't involve both range table sets>>
  inisize(tab, max_tab);
  int invert = /* UNI_BIT_SET_OP(0, op, 0); */ op & 1;
  uint32_t aptr = 0, bptr = 0;
  uint32_t alow, ahigh, blow, bhigh;
  <<Set low and high for [[a]]>>
  <<Set low and high for [[b]]>>
  int32_t lasthigh = -1, curlow = -1, curhigh = -1;
  while(alow <= UNI_MAX_CP || blow <= UNI_MAX_CP) {
    uint32_t nextlow, nexthigh, res;
    <<Find next range where either [[a]] or [[b]] is different>>
    /* into nextlow-nexthigh and compute bit op as well (into res) */
    /* Invert the skipped range (where a=b=0) if necessary */
    if(invert && nextlow > lasthigh + 1) {
      if(curlow < 0)
        curlow = lasthigh + 1;
      curhigh = nextlow - 1;
    }
    /* store res into result */
    if(res) {
      /* extend or create current range */
      if(curlow < 0)
        curlow = nextlow;
      curhigh = nexthigh;
    } else if(curlow >= 0) {
      /* store & finish current range */
      check_size(tab, max_tab, tab_len + 1);
      tab[tab_len].low = curlow;
      tab[tab_len++].high = curhigh;
      curlow = -1;
    }
    lasthigh = nexthigh;
  }
  /* Invert the final range (where a=b=0) if necessary */
  if(invert && lasthigh < UNI_MAX_CP) {
    if(curlow < 0)
      curlow = lasthigh + 1;
    curhigh = UNI_MAX_CP;
  }
  /* store & finish the current range */
  if(curlow >= 0) {
    check_size(tab, max_tab, tab_len + 1);
    tab[tab_len].low = curlow;
    tab[tab_len++].high = curhigh;
  }
  *rtab = tab;
  *r_len = tab_len;
}
@

<<Set low and high for (@x)>>=
<<@x>>low = <<@x>>ptr < <<@x>>_len ? <<@x>>[0].low : UNI_MAX_CP + 1;
<<@x>>high = <<@x>>ptr < <<@x>>_len ? <<@x>>[0].high : UNI_MAX_CP + 1;
@

<<Find next range where either [[a]] or [[b]] is different>>=
if(alow < blow) {
  nextlow = alow;
  res = UNI_BIT_SET_OP(~0, op, 0);
  if(ahigh < blow) {
    nexthigh = ahigh;
    aptr++;
    <<Set low and high for [[a]]>>
  } else {
    nexthigh = blow - 1;
    alow = blow;
  }
} else if(alow > blow) {
  nextlow = blow;
  res = UNI_BIT_SET_OP(0, op, ~0);
  if(bhigh < alow) {
    nexthigh = bhigh;
    bptr++;
    <<Set low and high for [[b]]>>
  } else {
    nexthigh = alow - 1;
    blow = alow;
  }
} else { /* alow == blow */
  nextlow = alow;
  res = UNI_BIT_SET_OP(~0, op, ~0);
  nexthigh = ahigh;
  if(ahigh <= bhigh) {
    aptr++;
    <<Set low and high for [[a]]>>
  } else
    alow = bhigh + 1;
  if(bhigh <= nexthigh) {
    nexthigh = bhigh;
    bptr++;
    <<Set low and high for [[b]]>>
  } else
    blow = nexthigh + 1;
}
@

Any operations involving just one set are done immediately.  Although
it would be possible to shortcut operations against empty (or full)
sets as well, the slower generic routines are used instead of trying
to test for those cases.

<<Shortcut all ops that don't involve both range table sets>>=
if(op == UNI_SOP_NIL) {
  inisize(tab, 1); /* so free always works */
  *rtab = tab;
  *r_len = 0;
  return;
} else if(op == UNI_SOP_ALL) {
  inisize(tab, 1);
  tab[0].low = 0;
  tab[0].high = UNI_MAX_CP;
  *rtab = tab;
  *r_len = 1;
  return;
} else if(op == UNI_SOP_A) {
  inisize(tab, a_len);
  cpybuf(tab, a, a_len * sizeof(*a));
  *rtab = tab;
  *r_len = a_len;
  return;
} else if(op == UNI_SOP_INV_A) {
  uni_chrrng_invert(a, a_len, rtab, r_len);
  return;
} else if(op == UNI_SOP_B) {
  inisize(tab, b_len);
  cpybuf(tab, b, b_len);
  *rtab = tab;
  *r_len = b_len;
  return;
} else if(op == UNI_SOP_INV_B) {
  uni_chrrng_invert(b, b_len, rtab, r_len);
  return;
}
@

Another possible simple improvement is to store fixed ranges, for
example 32 characters at at time, with a bit array for that range.
Again, two words are required per entry (start and bit mask), so this
only saves storage space and time if each block has on average two or
more bits set.  This loses the range advantage of being able to skip
large contiguous groups of present members, but also partly loses the
range disadvantage of requiring contiguous members for efficiency.  It
also partly gains the disadvantage of requiring scanning of bit arrays
to produce the \emph{members} list.  No sample implementation is
provided here.

A more complex possible improvement is to split the original bit array
into a multi-level table, with duplicate subtables shared.  The bit
array is partitioned into chunks; these are stored at the lowest
level.  Higher levels are just tables of pointers to lower levels.
This increased indirection is less efficient than the raw bit array,
but having two pointers to the same subtable can significantly reduce
storage requirements.  Note that another name for this structure is an
optimized prefix tree (aka optimized trie) on the code point bit string.

There are other trie-style data structures.  For example, ICU uses
what they call a folding trie.  This structure uses two levels in a
way that optimizes characteristics of the Unicode character set
somewhat, but it does not appear to do a very good job of compressing
data.  On the other hand, with UTF-16 input, it can look up data with
only one table access for BMP code points, and two (one for each
UTF-16 character) for non-BMP code points, making lookup time almost
optimal.  However, even using 64k tables for each level is not very
space or cache efficient.  Another possibility is the Judy array.  It
might be worth testing it, but its inner structure is complex, and it
still doesn't do like node merging for less space usage.

The multi-level table approach requires fewer lookups than the code
point arrays (just as many table lookups as there are levels, vs. a
binary search against a long range list that might take, say, 10 or
more table probes), but even when using space-saving measures will
likely still produce larger tables than even unoptimized range tables
(range tables could be optimized for space by using only 24 bits per
code point, for example.) To share duplicate subtables, additional
management of a list of unique lower-level tables at each level, along
with their users, is required. Making a change of any kind is a
non-trivial operation.  This additional storage and computational
expense makes these more suited to static one-time generation.  On the
other hand, their straight lookup performance benefit over ranged
tables is three-fold or more for many Unicode tables, without a need
to do any special optimizations.

<<FIXME>>=
Make them more mergable.  Plan: align booleans' ranges to
multiples of 32 (so low-level chunks don't need shifting for merging),
store a checksum (CRC-32 or simple shifting sum) in a side table,
along with a duplicate count (maybe 24 bits of cksum plus 8 bits of
count).  Never bother resizing lowest level; use largest chunk size
instead.  Likewise for each level, maybe.  Provide single merging
function which takes 2 tables and a bitop.  Similar function also for
range tables, maybe.
@

There are probably very efficient algorithms to generate good
multi-level tables, but I am not aware of them.  The optimal number of
levels and level size is difficult to find, but a pretty good number
can be found by taking each level in turn, and finding the size which
produces the minimum total length taking into account all uniques and
the length of the array at the next level.  Of course the size of the
next level is determined by redundancy elimination as well, so this
requires a recursive search with backtracking.  In addition, having
too many levels eliminates some of the performance benefit, so the
search depth cannot be infinite.%
\footnote{Then again, a specific structure could be imposed,
eliminating the need for a search, but not always selecting the
optimal size.  For example, choosing a 3-level X/16/8 table might be
good enough for most cases.  However, in order to gain the space
savings, the routine would still need to perform a lot of block
comparisons to eliminate redundancy.  If it's going to take a lot of
time and memory anyway, why not go all the way?}
The initial bit array may be a subset of the desired values, in which
case an initial range can (actually, must) be specified as well.  When
a value lies outside of that range, it can be given a default;
currently only all zeroes and all ones are supported. The structure
may also store multiple bit arrays at once, in which case it may be
necessary to keep more than one byte together contiguously at the
lowest level; this may be specified using [[minwidth]].  By default,
[[minwidth]] can never be lower than 4, since 32-bit words are used.

<<Unicode property exports for generator>>=
/** Convert bit array \p bits into a multi-level table.
  * The first bit of \p bits is mapped to \p low, and its last bit is
  * \p high.  All bits outside of that range have the value zero if \p def
  * is zero, and one otherwise.  The lowest level of the table is broken
  * into chunks whose minimum number of bytes is \p minwidth (or 4 if
  * \p minwidth is too small).  The returned table is placed in \p ml,
  * and its size is \p ml_len words */
void uni_bits_to_multi(const uint8_t *bits, uint32_t len, uint32_t low,
                       uint32_t high, uint8_t def, uint32_t minwidth,
		       <<Multi-level table type>> **ml, uint32_t *ml_len);
/** Look up bit array segment corresponding to \p val in multi-level table
  * Returns 0 if segment is all zeroes, ~0 if all ones, and 1 if \p ret
  * points to a bit array segment.  Since the lowest granularity of return
  * is a byte, a bit must be looked up using the index divided by 8, followed
  * by a bit mask.  Note that \p ret is NULL for all zeroes and all ones
  * returns as well */
int uni_multi_tab_lookup(const <<Multi-level table type>> *dat, uint32_t val,
                         const uint8_t **ret, uint8_t def);
@

<<Unicode property functions for generator>>=
#define UNI_MAX_MULTILEV 3
void uni_bits_to_multi(const uint8_t *bits, uint32_t len, uint32_t low,
                       uint32_t high, uint8_t def, uint32_t minwidth,
		       <<Multi-level table type>> **ml, uint32_t *ml_len)
{
  <<Split [[bits]] into multi-level table [[*ml]]>>
}
@

<<Unicode property functions>>=
int uni_multi_tab_lookup(const <<Multi-level table type>>*dat, uint32_t val,
                         const uint8_t **ret, uint8_t def)
{
  <<Find multi-level table entry [[val]]>>
}
@

Recursion is actually accomplished using a backtracking stack.

<<Split [[bits]] into multi-level table [[*ml]]>>=
uint32_t i;
<<Saved multi-level table information>>
int curlev = 0;
/* The block size of the block currently being worked on */
uint32_t curblk[UNI_MAX_MULTILEV];

curblk[0] = 0;
while(1) {
  <<Unsaved multi-level table information>>

  <<Compute next [[curblk[curlev]]]>>
  if(!curblk[curlev]) {
    if(!curlev--)
      break;
    <<Free [[curlev]]>>
    continue;
  }
  <<Compute level [[curlev]]>>
  <<Create next level bit array>>
  ++curlev;
  curblk[curlev] = 0;
  <<Save or free [[curlev]]>>
  if(curlev == UNI_MAX_MULTILEV - 1) {
    curlev--;
    <<Free [[curlev]]>>
  }
}
@

The minimum block size is twice the pointer size at the next level.
Any smaller, and the next level could just as well be a copy.
Actually, any smaller than the architecture's memory alignment may
cause performance issues.  For now, this is set to a minimum of 4. 
The pointer size at the next level is 1, 2, or 4 depending on the
number of blocks at this level.  The maximum block size is one half
the total size; if the total size is not binary, the data is padded
with zeroes.

<<Saved multi-level table information>>=
/* raw data array for current set of levels */
uint8_t *lev[UNI_MAX_MULTILEV] = { (uint8_t *)bits };
uint32_t curlen[UNI_MAX_MULTILEV] = { len };
@

<<Unsaved multi-level table information>>=
uint32_t blks;
@

<<Compute next [[curblk[curlev]]]>>=
if(!curblk[curlev]) {
  if(curlen[curlev] <= 256 * 2)
#if 0 /* too small, really */
    curblk[curlev] = 2;
#else
    curblk[curlev] = 4;
#endif
  else if(curlen[curlev] <= 256 * 256 * 4)
    curblk[curlev] = 4;
  else
    curblk[curlev] = 8;
  if(!curlev && curblk[curlev] < minwidth)
    curblk[curlev] = minwidth;
} else {
  curblk[curlev] *= 2;
  if(curblk[curlev] >= curlen[curlev])
    curblk[curlev] = 0;
}
if(curblk[curlev])
  blks = (curlen[curlev] + curblk[curlev] - 1) / curblk[curlev];
@

For each pass, a new pointer array is created.  While this will
eventually be reduced to the pointer size, 32-bit pointers are used
during the search.  In addition, a side array is created to store only
the unique block indices.  This alone prevents having to scan the
entire return array for block matches, speeding things up orders of
magnitude with sparse data.  Additionally, this array is kept sorted,
allowing binary searching to further reduce the number of comparisons
needed.  Using a hash table for this would require extra storage, and
may speed things up further.  In any case, each block is simply added
to the pointer array, checking first for duplicates.  As another
special compensation for sparse (or dense) arrays, all-zero entries
are always encoded as the pointer zero, and all-one entries are always
encoded as the maximum pointer value (all ones).  Not only does this
save a tiny bit of space, but scanning through dense or sparse
sections of the array can be much faster.  Care must be taken during
the comparisons to not access data past the end of the actual bit
array, instead treating them as zeroes.

<<FIXME>>=
Maybe add hash to block before sort for dup detect; maybe also store
hash with mtab.  See note above about making this more mergable.
@

<<Unsaved multi-level table information>>=
uint32_t j;
uint32_t *ptr, *ublocks, nublocks = 0;
uint8_t *data = lev[curlev];
uint32_t blklen, shortlen;
@

<<Compute level [[curlev]]>>=
blklen = shortlen = curblk[curlev];
ptr = malloc(blks * sizeof(*ptr));
ublocks = malloc(blks * sizeof(*ublocks));
for(i = 0; i < blks; i++) {
  int h, l;
  /* first, check for 0 or 1 */
  int is0 = 1, is1 = 1;
  if(i == blks - 1) {
    shortlen = curlen[curlev] % blklen;
    if(!shortlen)
      shortlen = blklen;
  }
  for(l = 0; l < shortlen && (is0 || is1); l++) {
    if(data[blklen * i + l])
      is0 = 0;
    if(data[blklen * i + l] != (uint8_t)~0)
      is1 = 0;
  }
  if(is0 && (!def || shortlen == blklen)) {
    ptr[i] = 0;
    continue;
  }
  if(is1 && (def || shortlen == blklen)) {
    ptr[i] = ~0;
    continue;
  }
  h = nublocks - 1;
  l = 0;
  while(l <= h) {
    j = (l + h) / 2;
    /* comparing in reverse order to make shortlen cmp easier */
    int c = memcmp(data + blklen * ublocks[j], data + blklen * i, shortlen);
    if(!c) {
      uint32_t k;
      if(shortlen == blklen)
        break;
      /* make c > 0 if there is any non-0 element in ublock */
      for(k = shortlen; k < blklen && !c; k++)
        c = data[blklen * ublocks[j] + k];
      if(!c)
        break;
    }
    if(c > 0)
      h = j - 1;
    else
      l = j + 1;
  }
  if(l > h) {
    if(++h == nublocks)
      ublocks[nublocks++] = i;
    else {
      movebuf(ublocks + h + 1, ublocks + h, nublocks - h);
      ++nublocks;
      ublocks[h] = i;
    }
    j = h;
  }
  ptr[i] = ublocks[j] + 1;
}
@

Now that the search is complete, the pointers can be compressed to the
minimum word size required.  The data will eventually be shifted down,
but it is not necessary (or safe) yet.  However, some preparation must
be done here, so that the pointers will eventually point to the
shifted blocks.  The actual pointers need to be adjusted to use the
offset of that pointer in the unique blocks array.  This could not
have been done above, because the array was having members inserted in
the middle, invalidating all of the indices.  To look up the index
more quickly, the block array is stored by block number first (it was
sorted by block contents, instead).

<<Saved multi-level table information>>=
/* unique block pointers */
uint32_t *curublk[UNI_MAX_MULTILEV] = {NULL};
uint32_t curnublk[UNI_MAX_MULTILEV];
@

<<Create next level bit array>>=
curublk[curlev] = ublocks;
curnublk[curlev] = nublocks;
qsort(ublocks, nublocks, sizeof(uint32_t), uni_cmp_cp);
/* for comparison with bsearch, adjust to actual stored value */
for(i = 0; i < nublocks; i++)
  ++ublocks[i];
for(i = 0; i < blks; i++) {
  if(!ptr[i] || ptr[i] == (uint32_t)~0)
    continue;
  uint32_t *p = bsearch(ptr + i, ublocks, nublocks, sizeof(uint32_t), 
                        uni_cmp_cp);
  ptr[i] = p - ublocks + 1;
}
lev[curlev + 1] = (uint8_t *)ptr;
/* note: 0 and ~0 are reserved values, so space for 2 must be reserved */
if(nublocks <= 254) {
  uint8_t *p = (uint8_t *)ptr;
  for(i = 0; i < blks; i++)
    p[i] = ptr[i];
  curlen[curlev + 1] = blks;
} else if(nublocks <= 65534) {
  uint16_t *p = (uint16_t *)ptr;
  for(i = 0; i < blks; i++)
    p[i] = ptr[i];
  curlen[curlev + 1] = blks * 2;
} else
  curlen[curlev + 1] = blks * 4;
@

To retain the minimum-sized level, the current set of minima is
stored.  If the current level is either smaller or the same size, but
with fewer levels, it is saved.  Otherwise, it and all of its children
are freed.

<<Saved multi-level table information>>=
uint32_t cursize[UNI_MAX_MULTILEV];
uint8_t *minlev[UNI_MAX_MULTILEV] = { (uint8_t *)bits };
uint32_t *minublk[UNI_MAX_MULTILEV] = { NULL };
uint32_t minlen[UNI_MAX_MULTILEV] = { len }, minnublk[UNI_MAX_MULTILEV];
uint32_t minblk[UNI_MAX_MULTILEV] = { 0 }, minsize = len;
@

<<Create next level bit array>>=
cursize[curlev] = nublocks * curblk[curlev];
cursize[curlev + 1] = curlen[curlev + 1];
@

<<Save or free [[curlev]]>>=
uint32_t save_size = 0;
for(i = 0; curblk[i]; i++)
  save_size += cursize[i];
save_size += cursize[i];
int save_lev = save_size < minsize;
if(save_size == minsize) {
  for(j = 0; minblk[j]; j++);
  save_lev = i < j;
}
if(save_lev) {
  for(i = 0; i < UNI_MAX_MULTILEV; i++) {
    if(minlev[i] != lev[i] && minlev[i])
      free(minlev[i]);
    if(minublk[i] != curublk[i] && minublk[i])
      free(minublk[i]);
    minblk[i] = curblk[i];
    minlev[i] = lev[i];
    minlen[i] = curlen[i];
    minublk[i] = curublk[i];
    minnublk[i] = curnublk[i];
  }
  minsize = save_size;
}
@

<<Free [[curlev]]>>=
if(curublk[curlev] != minublk[curlev])
  free(curublk[curlev]);
curublk[curlev] = NULL;
if(lev[curlev + 1] != minlev[curlev + 1])
  free(lev[curlev + 1]);
lev[curlev + 1] = NULL;
@

Now, the data can be combined into a single buffer, which is parsed
for every lookup.  This buffer is an opaque buffer of 32-bit integers,
with each data chunk aligned to a 32-bit boundary.  That way, 16-bit
and 32-bit values can be read directly from the buffer without
alignment issues.  The buffer is read from top to bottom, traversing
pointer tables until the data is reached.


<<Multi-level table type>>=
uint32_t
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
#define lev_psz(l) \
  (!(l) ? 1 : minnublk[l - 1] <= 254 ? 1 : minnublk[l - 1] <= 65534 ? 2 : 4)
for(curlev = 0; curlev < UNI_MAX_MULTILEV - 1 && minlev[curlev + 1]; curlev++);
*ml_len = 0<<Multi-level table buffer length>>;
inisize(*ml, *ml_len);
uint32_t *mp = *ml;
@

To determine the size of the buffer, we need to know what the buffer
will contain.  I will approach this by developing the lookup function
at the same time.  This function returns either a pointer into the
lowest-level data arrays, or an integer indicating that this is a zero
or one entry.

<<Find multi-level table entry [[val]]>>=
*ret = NULL;
@

First, we need to filter out the main range.  If the value is out of
range, the default is returned. This can be stored in the next word.
Since it's only one bit, other information can be stored in that word
as well.

<<Find multi-level table entry [[val]]>>=
if(val < *dat || val > dat[1])
  return def ? def : dat[2] & 1 ? ~0 : 0;
val -= *dat;
dat += 3;
@

<<Multi-level table buffer length>>=
+3
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
*mp++ = low;
*mp++ = high;
*mp++ = def & 1;
@

There is no other rarely used information necessary for scanning the
table that can be stored along with the default bit.  However, for
some applications, it may be nice to know the size of the table.  So,
the remaining bits of that word are filled with that size (in words).

<<FIXME>>=
Provide function that returns mt[2] >> 1 ("storage_len")
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
mp[-1] |= *ml_len << 1;
@

After looking up the value at a particular level, the level needs to
be skipped.  The size of the table can be stored as a compressed
pointer beyond the end of the next level's data.  This is not possible
for the first level, though, so the length needs to be stored
separately.  There is no point in storing a compression length as
well, so this is stored as a 32-bit integer.

<<Find multi-level table entry [[val]]>>=
uint32_t skip = *dat++;
@

<<Multi-level table buffer length>>=
+1
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
*mp++ = minlen[curlev] / lev_psz(curlev);
@

Following this is a list of levels.  The first word of the last level
(i.e., the data level) is zero to terminate the list.

<<Find multi-level table entry [[val]]>>=
<<Prepare to scan multi-level table pointers>>
while(*dat) {
  <<Scan multi-level table pointers>>
}
@

<<Multi-level table buffer length>>=
+1;
for(i = 1; i <= curlev; i++)
  *ml_len += 0<<Multi-level table buffer pointer [[i]] length>>;
*ml_len += 0<<Multi-level table buffer data length>>
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
while(curlev > 0) {
  <<Dump multi-level table pointers>>
  --curlev;
}
*mp++ = 0;
@

Next, we need information regarding this level of the array.  First,
each array corresponds to a particular set of bits; all bits below
that are ignored.  The bit number below which the rest may be ignored
is stored next.  For the lowest level of the array, this is always
zero, so this corresponds with the loop terminator.  Rather than store
the number of bits in an entire 32-bit word, it is stored in a byte.
The other three bytes are available for other information that is only
needed for a pointer level.

<<Scan multi-level table pointers>>=
uint32_t desc = *dat++;
uint8_t shift = desc;
uint32_t idx = val >> shift;

val -= idx << shift;
@

<<Multi-level table buffer pointer [[i]] length>>=
+1
@

<<Dump multi-level table pointers>>=
uint32_t desc;
<<Compute pointer level descriptor>>
*mp++ = desc;
@

<<Compute pointer level descriptor>>=
for(i = 0, desc = 0; i < curlev; i++)
  desc += lg2(minblk[i] / lev_psz(i));
@

Now that the index into the subarray is known, to actually look
something up in the table requires the size of the table entries.
This is stored next.

<<Scan multi-level table pointers>>=
desc >>= 8;
uint8_t psz = desc;
@

<<Compute pointer level descriptor>>=
uint8_t psz = lev_psz(curlev);
desc |= psz << 8;
@

It also needs the offset of the subtable.  Since that is what we are
looking up, it can be obtained from the previous pass.  For the first
pass, the offset is always zero.

<<Prepare to scan multi-level table pointers>>=
uint32_t toff = 0;
@

Now the lookup can be performed.  First, the offset beyond the table
is read, and then the pointer itself.  Then, the data pointer is
advanced past the end of the entire level.

<<Scan multi-level table pointers>>=
switch(psz) {
  case 1: {
    const uint8_t *p = (const uint8_t *)dat;
    toff = p[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint8_t)~0)
      return ~0;
    dat += skip / 4 + 1; /* aka (skip + 1 + 3) / 4 */
    skip = *p;
    break;
  }
  case 2: {
    const uint16_t *p = (const uint16_t *)dat;
    toff = p[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint16_t)~0)
      return ~0;
    dat += skip / 2 + 1; /* aka (skip + 1 + 1) / 2 */
    skip = *p;
    break;
  }
  case 4:
    toff = dat[idx + toff + 1];
    if(!toff)
      return 0;
    if(toff == (uint32_t)~0)
      return ~0;
    toff = *dat;
    dat += skip + 1;
    break;
}
@

<<Multi-level table buffer pointer [[i]] length>>=
+((i != curlev ? minnublk[i] * minblk[i] : minlen[i]) + lev_psz(i) + 3) / 4
@

<<Dump multi-level table pointers>>=
uint8_t *mpp = (uint8_t *)mp;
switch(psz) {
 case 1:
   *mpp++ = minnublk[curlev - 1];
   break;
 case 2:
   *(uint16_t *)mpp = minnublk[curlev - 1];
   mpp += 2;
   break;
 case 4:
   *(uint32_t *)mpp = minnublk[curlev - 1];
   mpp += 4;
   break;
}
<<Copy table data out>>
free(minlev[curlev]);
@

<<Copy table data out>>=
if(minblk[curlev]) {
  /* copy all but last blindly */
  for(i = 0; i < minnublk[curlev] - 1; i++) {
    cpybuf(mpp, minlev[curlev] + minblk[curlev] * (minublk[curlev][i] - 1),
           minblk[curlev]);
    mpp += minblk[curlev];
  }
  /* the last may be short */
  uint32_t shortlen;
  if(minublk[curlev][i] == (minlen[curlev] + minblk[curlev] - 1) / minblk[curlev]) {
    shortlen = minlen[curlev] % minblk[curlev];
    if(!shortlen)
      shortlen = minblk[curlev];
  } else
    shortlen = minblk[curlev];
  cpybuf(mpp, minlev[curlev] + minblk[curlev] * (minublk[curlev][i] - 1),
         shortlen);
  /* silence valgrind, even though random garbage is perfectly safe */
  if(shortlen < minblk[curlev])
    memset(mpp + shortlen, 0, minblk[curlev] - shortlen);
  mpp += minblk[curlev];
  free(minublk[curlev]);
} else {
  cpybuf(mpp, minlev[curlev], minlen[curlev]);
  mpp += minlen[curlev];
}
/* align mpp */
if((mpp - (uint8_t *)mp) & 3) {
  /* silence valgrind here as well */
  memset(mpp, 0, 4 - ((mpp - (uint8_t *)mp) & 3));
  mpp += 4 - ((mpp - (uint8_t *)mp) & 3);
}
mp = (uint32_t *)mpp;
@

The table offset just acquired is a block number (actually, plus one
to allow for zero).  To convert it to a word offset, it needs to be
multiplied by the next block size, in pointer size units.  This is
stored next in [[desc]].  Since there are two bytes left, it is stored
as a 16-bit integer.  If another byte is ever needed, it can be
obtained by changing this to a shift value (it is always a power of
2).

<<Scan multi-level table pointers>>=
desc >>= 8;
toff = (toff - 1) * desc;
skip *= desc;
@

<<Compute pointer level descriptor>>=
desc |= (minblk[curlev - 1] / lev_psz(curlev - 1)) << 16;
@

The final lookup simply finds the byte offset of the remaining value
in the selected subtable.  Note that only bytes may be addressed;
storing more than one byte requires shifting the value left first
before finding the first byte of the value.  Similarly, storing only
one bit requires that the value be shifted right first for addressing,
and then using the shifted out bits to create a bit mask.

<<Find multi-level table entry [[val]]>>=
*ret = (const uint8_t *)dat + 4 + toff + val;
return 1;
@

<<Multi-level table buffer data length>>=
+((curlev ? minnublk[0] * minblk[0] : len) + 3) / 4
@

<<Split [[bits]] into multi-level table [[*ml]]>>=
uint8_t *mpp = (uint8_t *)mp;
<<Copy table data out>>
if(mp - *ml != *ml_len) {
  fprintf(stderr, "len mismatch: %d %d\n", (int)(mp - *ml), (int)*ml_len);
  exit(1);
}
@

Generally, even with the complexity I added by compressing pointers,
the table lookups are faster than equivalent range table lookups.  As
long as duplicates are removed, the space taken up is comparable to
ordinary range tables, and sometimes even better.  With the
suppression of zero and one blocks, it is faster to find the
\emph{members} list than with plain bit tables, but is still not very
fast.  On the other hand, making modifications, or creating new tables
from scratch, is very expensive. Even if we stick with a table's
current structure, there needs to be side storage that deals with the
shared arrays.  If a change is made to an array at any level, then all
pointers at the next level need to be checked: if they point to the
same place, the block needs to be duplicated first, and after updating
the block (duplicate or not), it needs to be compared against all
existing blocks at the same level for re-merging.  The last step is
not strictly necessary for run-time operations, but there is no way
with the current structure to detect shared blocks.  An additional bit
for each pointer could be used to indicate sharing, which would never
be turned off even if the last owner disappears.  There are many
possibilities, and I do not wish to dwell on it any more.  For now,
the multi-level tables are expected to be read-only and only provide
the \emph{in} function.

However, converting a multi-level table to a sorted list of code point
ranges adequately satisfies the [[list]] function, and is relatively
easy to implement.  First, since the entire table is to be scanned
sequentally, the structure of the table is read.  Next, a loop simply
advances counters at each level of the table to get to the data, and
parses the data (slowly) using the generic bit tester.  The only
shortcuts here are when reading all zeroes or all ones, which bypass
the lowest level scan.  The function might get a little faster if it
didn't re-read every level's pointer for every low-level block, but
doing it this way is simpler and probably not a big performance hit.
A bigger speed gain could probably be had by optimizing the bit tester
loop.

<<Unicode property exports>>=
/** Convert a multi-level bit array table to a sorted range array.
  * The multi-level table \p mt is assumed be a set defined by a
  * single-bit bit array, and is converted to a set defined by the
  * sorted range array \p *ret/\p *rlen */
void uni_multi_bit_to_range(const uint32_t *mt, uni_chrrng_t **ret,
                            uint32_t *rlen);
@

<<Unicode property functions>>=
void uni_multi_bit_to_range(const uint32_t *mt, uni_chrrng_t **ret,
                            uint32_t *rlen)
{
  <<Multi-level structure storage>>
  
  <<Gather multi-level structure>>
  <<Convert multi-level structure to range list>>
}
@

<<Multi-level structure storage>>=
const uint32_t *dat[UNI_MAX_MULTILEV + 1];
uint32_t sh[UNI_MAX_MULTILEV + 1], psz[UNI_MAX_MULTILEV + 1],
         bsz[UNI_MAX_MULTILEV + 1];
uint32_t low, high;
int inv;
uint32_t nl;
@

<<Gather multi-level structure>>=
low = mt[0] * 8;
high = mt[1] * 8 + 7;
inv = mt[2] & 1;
uint32_t desc;
uint32_t nskip = mt[3], skip;
uint32_t l;
mt += 4;
bsz[0] = nskip; /* top level is just one block */
for(l = 0; (desc = *mt++); l++) {
  dat[l] = mt;
  psz[l] = (desc >> 8) & 0xff;
  sh[l] = desc & 0xff;
  switch(psz[l]) {
    case 1:
      skip = nskip / 4 + 1;
      nskip = *(uint8_t *)mt;
      break;
    case 2:
      skip = nskip / 2 + 1;
      nskip = *(uint16_t *)mt;
      break;
    default:
      skip = nskip + 1;
      nskip = *mt;
  }
  nskip *= (bsz[l + 1] = desc >> 16);
  mt += skip;
}
dat[l] = mt;
nl = l;
@

<<Convert multi-level structure to range list>>=
uint32_t pno[UNI_MAX_MULTILEV];
uint32_t ret_max;
int32_t rno, in_r;
ret_max = 8;
inisize(*ret, ret_max);
rno = in_r = 0;
if(inv && low > 0) {
  (*ret)[0].low = 0;
  in_r = 1;
}
for(l = 0; l < nl; l++)
  pno[l] = 0;
while(1) {
  uint32_t ptr = 1;
  for(l = 0; l < nl; l++) {
    switch(psz[l]) {
      case 1:
        ptr = ((uint8_t *)dat[l] + (ptr - 1) * bsz[l])[pno[l] + 1];
	if(ptr == 0xff)
	  ptr = ~0;
	break;
      case 2:
        ptr = ((uint16_t *)dat[l] + (ptr - 1) * bsz[l])[pno[l] + 1];
	if(ptr == 0xffff)
	  ptr = ~0;
	break;
      default:
        ptr = (dat[l] + (ptr - 1) * bsz[l])[pno[l] + 1];
    }
    if(!ptr || ptr == (uint32_t)~0)
      break;
  }
  if(!ptr) {
    if(in_r) {
      (*ret)[rno++].high = low - 1;
      in_r = 0;
    }
    low += 8 << sh[l];
  } else if(ptr == (uint32_t)~0) {
    if(!in_r) {
      check_size(*ret, ret_max, rno + 1);
      (*ret)[rno].low = low;
      in_r = 1;
    }
    low += 8 << sh[l];
  } else {
    uint32_t i;
    uint8_t *rdat = (uint8_t *)dat[nl] + (ptr - 1) * bsz[nl];
    for(i = 0; i < bsz[nl] * 8 && low <= high; i++, low++)
      if(!UNI_BSET_IS_SET(rdat, i) != !in_r) {
	if(!in_r) {
	  check_size(*ret, ret_max, rno + 1);
	  (*ret)[rno].low = low;
	  in_r = 1;
	} else {
	  (*ret)[rno++].high = low - 1;
	  in_r = 0;
	}
      }
  }
  if(low > high) {
    low = high + 1;
    break;
  }
  if(l == nl)
    l--;
  while(++pno[l] == bsz[l])
    l--;
  for(l++; l < nl; l++)
    pno[l] = 0;
}
if(in_r) {
  if(!inv)
    (*ret)[rno++].high = low - 1;
  else
    (*ret)[rno++].high = UNI_MAX_CP; /* or ~0? */
} else if(inv && high < UNI_MAX_CP) { /* or ~0? */
  check_size(*ret, ret_max, rno + 1);
  (*ret)[rno].low = low;
  (*ret)[rno++].high = UNI_MAX_CP;  /* or ~0? */
}
*rlen = rno;
@

The inverse operation is not so simple, but is worth implementing for
use with the UCD parser.  The parser can simply read in a range table,
and convert it to a multi-level table.  Since the multi-level table
generation algorithm partitions the bit array, the range table
converter actually constructs the bit array first, and calls the
function already developed above to do the conversion.

This is fairly inelegant and inefficient, but it is not meant to be
called at run-time very often.  Tables are meant to be pre-generated.

<<Unicode property exports for generator>>=
/** Convert a sorted range array to a multi-level bit array table.
  * The sorted range array \p tab/\p tab_len is assumed to be a set
  * defined by inclusion in one of its ranges.  It is converted to
  * a multi-level bit array table.  The table itself is returned, and,
  * if \p ml_len is not NULL, its length is returned in \p *ml_len */
uint32_t *uni_rng_to_multi_bit(const uni_chrrng_t *tab, uint32_t tab_len,
                               uint32_t *ml_len);
@

<<Unicode property functions for generator>>=
uint32_t *uni_rng_to_multi_bit(const uni_chrrng_t *tab, uint32_t tab_len,
                               uint32_t *ml_len)
{
  uint32_t low, high, len, i;
  uint32_t *ml;
  uint8_t *bits, def;

  /* degenerate cases:  all 0, all 1 */
  if(!tab_len) {
    uni_bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  if(tab_len == 1 && !tab[0].low && tab[0].high == UNI_MAX_CP) {
    uni_bits_to_multi(NULL, 0, 1, 0, 1, 0, &ml, ml_len);
    return ml;
  }
  /* not all 0 or all 1 */
  low = tab[0].low;
  high = tab[tab_len - 1].high;
  /* if starts & ends with 1, make it an inverse array */
  /* should also do this if low == 0 && tab[0].high > UNI_MAX_CP - high */
  /* or vice-versa, but it complicates things a bit */
  def = !low && high == UNI_MAX_CP;
  if(def) {
    low = tab[0].high + 1;
    high = tab[tab_len - 1].low - 1;
  }
  /* align with byte boundary */
  low &= ~7;
  len = ((high - low + 1 + 7) / 8);
  inisize(bits, len);
  /* Optimize(maybe): only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  if(def)
    /* fill in ones if low not aligned with byte boundary */
    bits[0] = (1 << (tab[0].high + 1) % 8) - 1;
  for(i = def; i < tab_len - def; i++) {
    uint32_t l = tab[i].low - low, h = tab[i].high - low + 1;
    uint8_t hm = (1 << h % 8) - 1;
    uint8_t lm = ~((1 << l % 8) - 1);
    if(h / 8 == l / 8) {
      UNI_BSET_AELT(bits, l) |= lm & hm;
      continue;
    }
    if(lm != (uint8_t)~0) {
      UNI_BSET_AELT(bits, l) |= lm;
      l += 8 - (l & 7);
    }
    if(l < (h & ~7))
      memset(&UNI_BSET_AELT(bits, l), 0xff, h / 8 - l / 8);
    if(hm)
      UNI_BSET_AELT(bits, h) |= hm;
  }
  if(def && (tab[tab_len - 1].low - low) % 8)
    /* fill in ones if high + 1 not aligned with byte boundary */
    bits[(tab[tab_len - 1].low - low) / 8] |=
                         ~((1 << (tab[tab_len - 1].low - low) % 8) - 1);
  uni_bits_to_multi(bits, len, low / 8, high / 8, def, 0, &ml, ml_len);
  free(bits);
  return ml;
}
@

As mentioned earlier, I am not aware of any fast, efficient algorithms
for generating the multi-level table structure.  This makes general
bit operations difficult at best.  The general algorithm for unaligned
bit sets above would seem trivial in comparison to the tricks needed
to get unaligned multi-level tables together.  Right now, the only
supported method for set operations is to convert to a range table,
perform the operation, and convert back.  For less frequent
operations, simply performing the set operation on the results of
multiple lookups will do the trick.  This is extremely inefficient,
but at least it does not require creating a huge unmaintanable mess of
code for something that should rarely be done.

Another possibility would be to use more complex data structures.  I
doubt I could come up with a more space-efficient storage method than
the sorted range table, but some are much better at operations like
lookup, scanning, or modification.  An appropriately designed hash
table could be optimized off-line to provide even faster lookups, at
the cost of fragility due to the hash algorithm probably depending on
a particular Unicode release.  Prefix trees (tries, if you prefer)
have good theoretical performance, and in fact the multi-level table
is just a prefix tree of sorts.  It and many other faster tree
approaches achieve their performance by assuming a constant time
parent-to-child traversal with a large set of children, which once
again equates to large storage penalties.  I have reduced the penalty
somewhat in my own implementation by using smaller pointers, at the
expense of slowing the lookup down a bit.  For now, the multi-level
table, sorted range table, and sorted code point table are the only
supported data structures.

<<FIXME>>=
Add discussion of compressed tries:
  Liang/TeX: overlapping arrays
     build normally
     merge suffixes
     side bitmask of filled positions
     add node by computing bitmask
        find first fit pos in side mask
     each array ent stores index of child
     add node symbol itself
  ref by Liang: bitmask of structure
     e.g. 32-bit
@

\subsection{Testing}

In order to actually compare performance and size, a separate test
program is provided.

<<C Test Support Executables>>=
tsttab \
@

<<makefile.rules>>=
tsttab.o: uni_prop.h
@

<<tsttab.c>>=
<<Common C Header>>

#include "uni_prop.h"
// static_proto

<<POSIX timing support>>

<<Functions to help test generated tables>>

int main(void)
{
  <<Test generated tables>>
  return 0;
}
@

To test a boolean, a function is provided to run the entire test, and
a call to that function is done in the mainline.  The function is
passed the range table, the multi-level table, and the name of the
property.  These can all be generated from the property name with a
preprocessor macro.

<<Functions to help test generated tables>>=
volatile int32_t tres;

#define bool(x) doit_bool(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab)
static void print_mtab_info(const uint32_t *, uint32_t);
static void doit_bool(const char *name, const uni_chrrng_t *rng, uint32_t nent,
                      const uint32_t *mtab)
{
    uint32_t i;
    
    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      int rv = uni_is_cp_chrrng(i, rng, nent);
      int mv = <<range table result for [[i]]>>;
      if(!rv != !mv) {
        fprintf(stderr, "mismatch %s@%d %d %d\n", name, i, rv, mv);
	exit(1);
      }
    }
    /* check performance */
    int j;
    double tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = uni_is_cp_chrrng(i, rng, nent);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = <<range table result for [[i]]>>;
    tt = tend();
    printf("  r%.0f t%.0f %.2fx\n", tr, tt, tr / tt);
    /* check conversion */
    uni_chrrng_t *rng2;
    uint32_t nent2;
    uni_multi_bit_to_range(mtab, &rng2, &nent2);
    if(nent2 != nent || cmpbuf(rng, rng2, nent)) {
      for(j = 0; j < nent && j < nent2; j++)
        if(cmpbuf(&rng[j], &rng2[j], 1))
	  break;
      fprintf(stderr, "misconvert %d/%d @%d\n", (int)nent, (int)nent2, j);
      exit(1);
    }
}
@

<<Functions to help test generated tables>>=
static void print_mtab_info(const uint32_t *mt, uint32_t rsize)
{
  uint32_t i, l;
  uint32_t skip, desc, nskip, psz;

  fputs("  mtab: ", stdout);
  if(mt[2] & 1)
    fputs("*inv* ", stdout);
  nskip = mt[3];
  i = 4;
  printf(" %d", nskip);
  for(l = 0; (desc = mt[i]); l++) {
    printf("/%d", desc >> 16);
    psz = (desc >> 8) & 0xff;
    switch(psz) {
      case 1:
	skip = nskip / 4 + 1;
        nskip = *(uint8_t *)&mt[++i];
	break;
      case 2:
	skip = nskip / 2 + 1;
        nskip = *(uint16_t *)&mt[++i];
	break;
      default:
	skip = nskip + 1;
        nskip = mt[++i];
    }
    nskip *= desc >> 16;
    i += skip;
  }
  skip = (nskip + 3) / 4;
  i += skip;
  printf(" (%d bytes, %d lookups max) (%.2fx rng)\n", i * 4, l + 1,
         (double)(i * 4) / rsize);
}
@

\subsection{Parsing the UCD}

A parser program is used to generate these tables as static,
compilable C code from the UCD tables.  One of the advantages of the
range table method is that creating range tables is trival, and could
be done using a simple shell script.  However, during the stages of
this project where that was in use, the generation time began to
dominate compilation time, so this was converted to C.  Since it is in
C anyway, it may as well generate multi-level tables directly, as
well.  In fact, the logic outside of the scripts was becoming
cumbersome as well, so all of that is now incorporated into the C
program.

This program has a special build rule so it can be used to generate
other C code without depending on that C code itself (as with
[[cproto.h]]).  The code itself can't use the standard header macro,
either, since it needs to limit its include files.  One thing it does
need, though, is the ability to read lines of arbitrary length.  This
is provided in my support library, which is referenced rather than
built here, again to avoid building [[cproto.h]].  Since
[[uni_prop.h]] and [[uni_prop.c]] depend on the output of this
program, but this program also needs to use some of the functions
defined therein, the functions and definitions are reinserted directly
into [[parse-ucd.c]] without any dependencies on the generated data.

\lstset{language=make}
<<C Build Executables>>=
parse-ucd \
@

\lstset{language=C}
<<parse-ucd.c>>=
<<Common C Warning>>
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <ctype.h>
#include <unistd.h>
#include <stdint.h>
<<Additional parse-ucd includes>>
<<Unicode property exports for generator>>

<<Unicode property functions for generator>>

<<UCD parser local definitions>>

<<UCD parser local functions>>

int main(void)
{
  <<UCD parser variables>>

  <<Parse character data files>>
  <<Post-process property data>>
  <<Dump character information as C code>>
  <<Clean up after parsing UCD files>>
  return 0;
}
@

\lstset{language=make}
<<makefile.vars>>=
PARSE_UCD_DEPS := parse-ucd.c $(SUPT_LIB_LOC)/libtjm-supt.a \
                  <<Additional parse-ucd C files>>

PARSER_CFLAGS :=
PARSER_LDFLAGS :=
@

<<makefile.config>>=
# The location of my support library (tjm-ext.nw)
SUPT_LIB_LOC := ../build

@

<<makefile.rules>>=
parse-ucd: $(PARSE_UCD_DEPS)
	$(CC) $(CFLAGS) $(EXTRA_CFLAGS) $(PARSER_CFLAGS) \
	   -o $@ parse-ucd.c -L$(SUPT_LIB_LOC) -ltjm-supt $(LDFLAGS) $(PARSER_LDFLAGS)
@

<<Additional parse-ucd C files>>=
mfgets.h \
@

\lstset{language=C}
<<Additional parse-ucd includes>>=
#include "mfgets.h"
@

All files will be parsed by the same program.  The only necessary
parameters are the locations and versions of the UCD files, which can
just be compiled in based on the configuration parameters.  For
convenience, the known (but not enforcably standardized) format of the
version string in a UCD file can be used to extract its version.
Similarly, the version can be extracted from the DTD of the CLDR; this
is less likely to change without notice, but it is still not
enforcably standardized.

\lstset{language=make}
<<makefile.vars>>=
PARSER_CFLAGS += -DUCD_LOC=\"$(UCD_LOC)\" -DUNIHAN_LOC=\"$(UNIHAN_LOC)\"
ifeq ($(UCD_VER),)
# $(shell ...) expands $(..._LOC) incorrectly, so use shell's $() instead
UCD_VER = $$( \
     sed -n -e '1{s/.*-//;s/\.txt.*//;s/\.//g;p;}' $(UCD_LOC)/PropList.txt)
endif
ifeq ($(CLDR_VER),)
# plain versions don't have .0 attached, so attach .0 and strip if wrong
CLDR_VER = $$( \
   sed -n -e '/cldrVersion/{ \
      s/[^"]*"//;s/".*//;s/$$/.0/;s/\(\..*\)\..*/\1/;s/\.//g;p;}' \
       $(CLDR_LOC)/common/dtd/ldml.dtd)
endif
PARSER_CFLAGS += -DUCD_VER=$(UCD_VER) -DCLDR_VER=$(CLDR_VER)
@

Rather than construct file names using the provided paths, the program
just changes to the location directory before reading files, and all
files are read relative to the current directory.  In order to be able
to change back to the starting directory, once again my utility
library has a portable [[getcwd]] that does not use fixed buffer sizes.

\lstset{language=C}
<<UCD parser variables>>=
<<Parser common variables>>
char *cwd = getcwd_full();
@

<<Parser common variables>>=
FILE *f;
char *lbuf = NULL;
unsigned int lbuflen, llen;
#define open_f(fn) do { \
  if(!(f = fopen(fn, "r"))) { \
    perror(fn); \
    exit(1); \
  } \
} while(0)
#define force_chdir(d) do { \
  if(chdir(d)) { \
    perror(d); \
    exit(1); \
  } \
} while(0)
@

<<Parse character data files>>=
<<Initialize UCD files>>
<<Parse UCD files>>
<<Initialize Unihan files>>
<<Parse Unihan files>>
@

<<Initialize UCD files>>=
force_chdir(UCD_LOC);
@

<<Initialize Unihan files>>=
force_chdir(cwd); /* in case unihan_loc is relative */
force_chdir(UNIHAN_LOC);
@

<<Dump character information as C code>>=
force_chdir(cwd);
free(cwd);
@

UCD text files consist of semicolon-separated fields, with the code
point(s) in the first field.  Comments are introduced with the number
sign.  Blank lines and purely comment lines have no fields. 
Whitespace is stripped from the beginning and ending of each field.
Comments are stripped off, but kept on the side, since one particular
file stores relevant information in a line comment.  Rather than
process each field as it is encountered, like I would normally do,
each line is split into an array of fields first.  This makes the
parsing routines more readable and obvious as well.

<<Process a line of [[UnicodeData.txt]]>>=
split_line(lbuf);
if(num_fields < 15) { /* should never happen! */
  perror("UnicodeData.txt");
  exit(1);
}
@

<<UCD parser local definitions>>=
static char **fields;
static int num_fields, max_fields = 0;
static char *line_comment;
@

<<Additional parse-ucd C files>>=
mallocdef.h \
@

<<Additional parse-ucd includes>>=
#include "mallocdef.h"
@

<<UCD parser local functions>>=
static void split_line(char *buf)
{
  char fc = 0;

  if(!max_fields)
    inisize(fields, (max_fields = 16));
  num_fields = 0;
  line_comment = NULL;
  while(isspace(*buf)) buf++;
  if(!*buf || *buf == '#')
    return;
  while(1) {
    while(isspace(*buf))
      buf++;
    if(fc == '#')
      line_comment = buf;
    char *f = buf, *nf;
    for(nf = buf; *nf && (fc == '#' || (*nf != ';' && *nf != '#')); nf++);
    fc = *nf;
    buf = nf + 1;
    while(nf > f && isspace(nf[-1])) --nf;
    *nf = 0;
    if(line_comment)
      return;
    check_size(fields, max_fields, num_fields + 1);
    fields[num_fields++] = f;
    if(!fc)
      return;
  }
}
@

Since named properties are being parsed, and named variables will be
created, it is useful to have a database of all of the property names
and their aliases before starting.  For this reason, the first file to
process is the file which contains those names:  [[PropertyAliases.txt]].
This file is read in and stored in a local table, sorted by name.  The
sorting is done after the fact to reduce complexity.

<<[[uni_alias_t]]>>=
/** A name and its known aliases */
typedef struct {
  const char *short_name, /**< Primary name; usually short */
             *long_name, /**< Secondary name, usually longer;
	                  **  sometimes same as \ref short_name */
  *alt_name, /**< First alias, if non-NULL */
  *alt_name2; /**< Second alias, if non-NULL */
} uni_alias_t;
@

<<Unicode property exports for generator>>=
<<[[uni_alias_t]]>>
@

<<Known Data Types>>=
uni_alias_t,%
@

<<UCD parser local definitions>>=
static uni_alias_t *prop_aliases;
static int num_prop_aliases = 0, max_prop_aliases = 0;
@

<<Initialize UCD files>>=
open_f("PropertyAliases.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  split_line(lbuf);
  if(num_fields < 2)
    continue;
  check_size(prop_aliases, max_prop_aliases, num_prop_aliases + 1);
  prop_aliases[num_prop_aliases].short_name = strdup(fields[0]);
  prop_aliases[num_prop_aliases].long_name = strdup(fields[1]);
  /* there are no comments on alias lines, so field count is correct */
  prop_aliases[num_prop_aliases].alt_name =
                 num_fields > 2 ? strdup(fields[2]) : NULL;
  prop_aliases[num_prop_aliases].alt_name2 =
                 num_fields > 3 ? strdup(fields[3]) : NULL;
  if(num_fields > 4) {
    perror("PropertyAliases.txt");
    exit(1);
  }
  num_prop_aliases++;
}
fclose(f);
@

There are actually two fields to sort by: long name and short name.
The main table will be sorted by long name, and a side table with
pointers will be used to sort the table by short name as well.  That
way, lookups can use binary searching on either field.

<<UCD parser local functions>>=
static int cmp_longname(const void *a, const void *b)
{
  return strcmp(((uni_alias_t *)a)->long_name, ((uni_alias_t *)b)->long_name);
}
@

<<Initialize UCD files>>=
qsort(prop_aliases, num_prop_aliases, sizeof(uni_alias_t), cmp_longname);
@

<<UCD parser local definitions>>=
uni_alias_t **prop_aliases_short;
@

<<UCD parser local functions>>=
static int cmp_shortname(const void *a, const void *b)
{
  return strcmp((*(uni_alias_t **)a)->short_name,
                (*(uni_alias_t **)b)->short_name);
}
@

<<Initialize UCD files>>=
inisize(prop_aliases_short, num_prop_aliases);
for(i = 0; i < num_prop_aliases; i++)
  prop_aliases_short[i] = &prop_aliases[i];
qsort(prop_aliases_short, num_prop_aliases, sizeof(*prop_aliases_short),
      cmp_shortname);
@


Properties are added to a master property list, containing the name
and the parsed contents.  This is simply tacked on next to the
like-named property in the property name table, if applicable.  Some
properties are artificial, though, so they are appended to the end.  A
separate count is provided for this purpose.

<<UCD parser local definitions>>=
<<[[prop_t]] prerequisites>>
typedef struct {
  const char *name;
  <<Property parsed contents>>
} prop_t;
static prop_t *parsed_props;
static uint32_t nparsed, maxparsed = 0;
@

<<Known Data Types>>=
prop_t,%
@

<<UCD parser local functions>>=
static int add_prop(const char *name)
{
  uni_alias_t *pn, n;
  uint32_t i;
  static char **added_names = NULL;
  static int *added_names_p = NULL;
  static int num_added_names = 0, max_added_names = 0;

  if(!maxparsed) {
    inisize(parsed_props, (maxparsed = num_prop_aliases));
    nparsed = num_prop_aliases;
    clearbuf(parsed_props, nparsed);
  }
  n.long_name = name;
  pn = bsearch(&n, prop_aliases, num_prop_aliases, sizeof(*prop_aliases),
               cmp_longname);
  if(!pn) {
    uni_alias_t **ppn;
    pn = &n;
    n.short_name = name;
    ppn = bsearch(&pn, prop_aliases_short, num_prop_aliases,
                  sizeof(*prop_aliases_short), cmp_shortname);
    if(ppn)
      pn = *ppn;
    else
      pn = NULL;
  }
  if(pn)
    i = (int)(pn - prop_aliases);
  else {
    int h, l, m, c;
    for(l = 0, h = num_added_names - 1; l <= h; ) {
      m = (h + l) / 2;
      c = strcmp(name, added_names[m]);
      if(c < 0)
        h = m - 1;
      else if(c > 0)
        l = m + 1;
      else
        return added_names_p[m];
    }
    if(maxparsed == nparsed) {
      check_size(parsed_props, maxparsed, nparsed + 1);
      <<Resize prop-associated arrays>>
    }
    i = nparsed;
    clearbuf(&parsed_props[nparsed], 1);
    <<Clear prop-associated array[i]>>
    nparsed++;
    if(num_added_names == max_added_names) {
      check_size(added_names, max_added_names, num_added_names + 1);
      if(!added_names_p)
	inisize(added_names_p, max_added_names);
      else
	resize(added_names_p, max_added_names);
    }
    if(l < num_added_names) {
      movebuf(added_names + l + 1, added_names + l, num_added_names - l);
      movebuf(added_names_p + l + 1, added_names_p + l, num_added_names - l);
    }
    added_names[l] = strdup(name);
    added_names_p[l] = i;
    num_added_names++;
  }
  if(!parsed_props[i].name)
    parsed_props[i].name = strdup(name);
  return i;
}
@

The first file to parse for actual data is the main database file,
[[UnicodeData.txt]].

<<Parse UCD files>>=
open_f("UnicodeData.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Process a line of [[UnicodeData.txt]]>>
}
fclose(f);
@

The first field is a code point.  While it appears that each line only
deals with one code point, there are actually ranges as well.  These
are specified by two consecutive entries with names ending in \texttt{,
First>} and \texttt{, Last>}, respectively.

<<Parser common variables>>=
uint32_t low, high;
char *s;
@

<<Process a line of [[UnicodeData.txt]]>>=
low = high = strtol(fields[0], NULL, 16);
if(fields[1][0] == '<' && (s = strchr(fields[1], ',')) &&
   !strcasecmp(s, ", First>")) {
  if(!mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    perror("UnicodeData.txt");
    exit(1);
  }
  split_line(lbuf);
  if(num_fields < 15 ||
     fields[1][0] != '<' || !(s = strchr(fields[1], ',')) ||
     strcasecmp(s, ", Last>")) { /* should also never happen! */
    perror("UnicodeData.txt");
    exit(1);
  }
  high = strtol(fields[0], NULL, 16);
}
@

The parsed contents for boolean properties are the range table and the
associated multi-level table.  In order to build the range table in
place, the storage size of the range table is kept as well.

<<Property parsed contents>>=
uni_chrrng_t *rng;
uint32_t len, max_len;
uint32_t *mt;
@

<<Parser common variables>>=
uint32_t i, j;
@

<<UCD parser local functions>>=
static void add_bool_rng(prop_t *p, uint32_t low, uint32_t high)
{
  if(!p->max_len)
    inisize(p->rng, (p->max_len = 8));
  if(p->len && p->rng[p->len - 1].high == low - 1)
    p->rng[p->len - 1].high = high;
  else {
    check_size(p->rng, p->max_len, p->len + 1);
    p->rng[p->len].low = low;
    p->rng[p->len].high = high;
    ++p->len;
  }
}
@

<<UCD parser local definitions>>=
#define decl_bool(n) int prop_##n = -1
#define add_bool(n) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  add_bool_rng(&parsed_props[prop_##n], low, high); \
} while(0)
@

The only directly derived boolean properties in [[UnicodeData.txt]] are
ASSIGNED, which is true if the character is present, and
Bidi\_Mirrored, which is true if field 10 is Y%
\footnote{This property, like many boolean properties, has property
value aliases that indicate Yes, T, and True as possible truth values
as well.  For now, though, all files are parsed assuming Y, since that
is always the case in the current UCD.}%
.

<<UCD parser variables>>=
decl_bool(ASSIGNED);
decl_bool(Bidi_Mirrored);
@

<<Process a line of [[UnicodeData.txt]]>>=
add_bool(ASSIGNED);
if(fields[9][0] == 'Y')
  add_bool(Bidi_Mirrored);
@

The [[PropList.txt]] file has more boolean properties, though.  In
fact, all entries in this file specify boolean properties.

<<Parse UCD files>>=
open_f("PropList.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Process a line of [[PropList.txt]]>>
}
fclose(f);
@

Like most of the rest of the UCD files, this file specifies ranges
using double-dot-separated code points.  Also, there are numerous
blank lines and comments, which need to be skipped.

<<Process a line of [[PropList.txt]]>>=
<<Parse dotted cp>>
@

<<Parse dotted cp>>=
split_line(lbuf);
if(!num_fields || !isxdigit(*lbuf))
  continue;
low = high = strtol(lbuf, &s, 16);
if(s && *s == '.' && s[1] == '.')
  high = strtol(s + 2, &s, 16);
if(*s) { /* should never happen, but if it does: ignore */
  fprintf(stderr, "bad col 1: %s\n", lbuf);
  continue;
}
@

Each boolean property is true if field two is the name of that
property.  Since the [[add_prop]] routine finds the correct entry based
on the name (assuming the name exists), it can be called every time,
and the correct entry will be updated.  Hyphen is a deprecated
property, so it is explicitly excluded.  It would be nice if there
were some place where a list of deprecated propreties could be read.

<<Process a line of [[PropList.txt]]>>=
if(!strcmp(fields[1], "Hyphen"))
  continue;
<<Just add field two as binary property>>
@

<<Just add field two as binary property>>=
add_bool_rng(&parsed_props[add_prop(fields[1])], low, high);
@

Speaking of [[DerivedCoreProperties.txt]], this file has entirely
boolean properties as well.  The Grapheme\_Link property is
deprecated, so it is explicitly excluded.  It would be nice if there
were a file listing all deprecated properties to exclude.

<<Parse UCD files>>=
open_f("DerivedCoreProperties.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  if(!strcmp(fields[1], "Grapheme_Link"))
    continue;
  <<Just add field two as binary property>>
}
fclose(f);
@

The [[CompositionExclusions.txt]] file is simply a commmented list of
code points, one per line, each of which has the
Composition\_Exclusion property.  Since comments are processed as
fields, and all lines have trailing comments, the number of fields is
sufficient to use the same parsing technique as for the other files.

<<UCD parser variables>>=
decl_bool(Composition_Exclusion);
@

<<Parse UCD files>>=
open_f("CompositionExclusions.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_bool(Composition_Exclusion);
}
fclose(f);
@

Unfortunately, this file does not have all entries in order. Instead,
it lists multiple groups, and each group is in order.  This requires
post-processing.  In fact, since this might happen with other files as
well, this processing may as well be done for all tables.

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng)
    fixup_rng(&parsed_props[i]);
@

<<UCD parser local functions>>=
static void fixup_rng(prop_t *p)
{
  uint32_t i;
  qsort(p->rng, p->len, sizeof(uni_chrrng_t), uni_cmprng);
  /* starting at top means only optimized entries are memmove'd */
  for(i = p->len - 1; i > 0; i--) {
    uint32_t j = i;
    while(i > 0 && p->rng[i - 1].high == p->rng[i].low - 1)
      i--;
    if(i == j)
      continue;
    p->rng[i].high = p->rng[j].high;
    if(j < p->len - 1)
        movebuf(p->rng + i + 1, p->rng + j + 1, p->len - (j + 1));
    p->len -= j - i;
    if(!i)
      break;
  }
}
@

[[DerivedNormalizationProps.txt]] contains the only remaining required
boolean UCD properties.  Unlike the others, there are some non-boolean
properties in this file as well.  Luckily, some boolean values can be
detected by simply counting the fields.  The Expands\_On\_* properties
are deprecated.  Technically, some of the quick check fields could be
booleans as well, but they will be taken care of as enumerations.

<<Parse UCD files>>=
open_f("DerivedNormalizationProps.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  <<Process a line of [[DerivedNormalizationProps.txt]]>>
}
fclose(f);
@

<<Process a line of [[DerivedNormalizationProps.txt]]>>=
if(num_fields == 2) {
  if(!strncmp(fields[1], "Expands_On_", 11))
    continue;
  <<Just add field two as binary property>>
  continue;
}
@

There are no boolean fields exported from Unihan.  The only field
which might be considered boolean (kIICore) is not required by
anything I use.

\lstset{language=txt}
<<FIXME>>=
Unihan_IRGSources.txt: kIICore
@

Now, to finish up post-processing, the multi-level table is generated.

\lstset{language=C}
<<Post-process property data>>=
uint32_t ml_len;
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng)
    parsed_props[i].mt = uni_rng_to_multi_bit(parsed_props[i].rng,
                                              parsed_props[i].len,
                                              &ml_len);
@

\subsection{Generating the Static Data}

The boolean properties can now be printed, using the canonical short
name.  Declarations are all printed to the same file:
[[uni_prop.gen.h]].  However, each boolean property is printed to a
different file, so that static linking will only pull in the desired
properties and representations.  They are then linked into a separate
library from the support routines.  The names of the files are
extracted from the generated header file, and as such, [[parse-ucd]]
is a prerequisite to generating the full makefile.  This complicates
things, and makes builds much slower.  In the future, this may be
changed to use a recursive make, so that only generating the library
is dependent on the generated code.

\lstset{language=make}
<<makefile.rules>>=
uni_prop.gen.h: parse-ucd
	./parse-ucd
@

<<Headers to Install>>=
uni/uni_prop.gen.h \
@

\lstset{language=C}
<<Unicode property exports>>=
/** \file uni_prop.gen.h Automatically generated supplement to uni_prop.h */
#include "uni_prop.gen.h"
@

<<makefile.rules>>=
uni_prop.h: uni_prop.gen.h
@

<<UCD parser local definitions>>=
#define open_wf(f, fn) \
  FILE *f; \
  if(!(f = fopen(fn, "w"))) { \
    perror(fn); \
    exit(1); \
  }
@

<<Dump character information as C code>>=
open_wf(gen_h, "uni_prop.gen.h");
fputs("/** \\addtogroup uni_prop */\n"
      "/** @{ */\n"
       "/** \\addtogroup uni_prop_dat Unicode Property Data */\n"
      "/** @{ */\n", gen_h);
@

<<Clean up after parsing UCD files>>=
fputs("/** @} */\n/** @} */\n", gen_h);
fclose(gen_h);
@

\lstset{language=make}
<<makefile.vars>>=
MAKEFILES+=makefile.unidat
@

<<makefile.rules>>=
makefile.unidat: uni_prop.gen.h
	printf 'RNGDAT_NAMES := ' >$@
	fgrep '[]' $^ | <<Filter generated file names>>
	       sed -e 's/\[.*//;s/.* //' | tr \\n ' ' >>$@
-include makefile.unidat
$(RNGDAT_NAMES:%=%.gen.c): uni_prop.gen.h
	@touch "$@"
$(RNGDAT_NAMES:%=%.gen.o): uni_prop.h
@

<<Library [[uni]] Members>>=
$(RNGDAT_NAMES:%=%.gen.o)
@

<<C Headers>>=
uni_prop.gen.h \
@

Converting these to lower-case would be more consistent, but once the
name is known, casing shouldn't really matter.  In addition, a simple
query function (actually, preprocessor macro wrapper) is printed for
the multi-level table lookup functions.  Finally, a list of boolean
properties needs to be converted into calls to [[bool()]] for the test
program.  This is generated directly into a fixed-named C file,
[[tsttab.tests.gen.c]].

<<Plain Built Files>>=
tsttab.tests.gen.c \
@

\lstset{language=C}
<<Test generated tables>>=
#include "tsttab.tests.gen.c"
@

<<Dump character information as C code>>=
open_wf(tstf, "tsttab.tests.gen.c");
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    const char *lname = i < num_prop_aliases ? prop_aliases[i].long_name :
                                               parsed_props[i].name;
    char nbuf[64];
    sprintf(nbuf, "uni_%s_rng.gen.c", name);
    open_wf(of, nbuf);
    fprintf(of, "#include \"uni_prop.h\"\n\n"
                "const uni_chrrng_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].len; j++)
      fprintf(of, "\t{ 0x%04X, 0x%04X }%s\n", parsed_props[i].rng[j].low,
                                              parsed_props[i].rng[j].high,
					      j < parsed_props[i].len - 1 ? "," : "");
    fputs("};\n", of);
    fclose(of);
    fprintf(gen_h, "/** Sorted range table for Unicode boolean %s property */\n"
                   "extern const uni_chrrng_t uni_%s_rng[];\n"
		   "/** Length of \\ref uni_%s_rng */\n"
		   "#define uni_%s_rng_len %d /* %d lookups max */\n"
		   "/** True if Unicode %s property is true for \\p x */\n"
                   "#define uni_is_%s(x) uni_is_x(x, uni_%s_mtab)\n",
		   lname, name, name, name, parsed_props[i].len,
		   lg2(parsed_props[i].len + 1), lname, name, name);
    print_mtab(name, lname, parsed_props[i].mt, gen_h);
    fprintf(tstf, "bool(%s);\n", name);
  }
}
@

<<Clean up after parsing UCD files>>=
fclose(tstf);
@

<<Unicode property exports>>=
/** Determine if \p cp is present in multi-level bit array \p tab */
int uni_is_x(uint32_t cp, const uint32_t *tab);
@

<<Unicode property functions>>=
int uni_is_x(uint32_t cp, const uint32_t *tab)
{
  const uint8_t *mr;
  uint8_t mv = uni_multi_tab_lookup(tab, cp / 8, &mr, 0);
  if(mr)
    mv = *mr;
  return mv & UNI_BSET_MASK(mr, cp) ? 1 : 0;
}
@

<<range table result for [[i]]>>=
uni_is_x(i, mtab)
@

\lstset{language=sh}
<<Clean temporary files>>=
rm -f uni_*.gen.[co]
@

When printing the multi-level table, comments are added to indicate
the struture.

\lstset{language=C}
<<UCD parser local functions>>=
static void print_mtab(const char *name, const char *lname, const uint32_t *mt,
                       FILE *gen_h)
{
  if(!mt)
    return; /* mtab may be inappropriate for some tables */
  char nbuf[64];
  sprintf(nbuf, "uni_%s_mtab.gen.c", name);
  open_wf(mf, nbuf);
  uint32_t i, j, l;
  uint32_t skip, desc, nskip, psz;
  fprintf(mf, "#include <stdint.h>\n\n"
              "const uint32_t uni_%s_mtab[] = {\n", name);
  fprintf(mf, "\t/* low, high, inv/fullen, l0len */\n"
              "\t0x%X, 0x%X, (%d << 1) + %d, %d,\n",
	      mt[0], mt[1], mt[2] >> 1, mt[2] & 1, (nskip = mt[3]));
  i = 4;
  fprintf(gen_h, "/** Multi-level table for Unicode %s property */\n"
                 "extern const uint32_t uni_%s_mtab[]; /* %d", lname, name, nskip);
  for(l = 0; (desc = mt[i]); l++) {
    fprintf(mf, "\t/* level %d (pointers) */\n"
                "\t0x%X, /* shift=%d psz=%d nextblk=%d */\n",
                l, desc, desc & 0xff, (psz = (desc >> 8) & 0xff), desc >> 16);
    fprintf(gen_h, "/%d", desc >> 16);
    switch(psz) {
      case 1:
	skip = nskip / 4 + 1;
        nskip = *(uint8_t *)&mt[++i];
	break;
      case 2:
	skip = nskip / 2 + 1;
        nskip = *(uint16_t *)&mt[++i];
	break;
      default:
	skip = nskip + 1;
        nskip = mt[++i];
    }
    fprintf(mf, "\t/* raw pointers; pointer[0] = l%dlen = %d */\n\t", l + 1, nskip);
    nskip *= desc >> 16;
    for(j = 0; j < skip - 1; j++, i++)
      fprintf(mf, "0x%X,%s", mt[i], !((j + 1) % 8) ? "\n\t" : " ");
    fprintf(mf, "0x%X,\n", mt[i++]);
  }
  skip = (nskip + 3) / 4;
  fprintf(mf, "\t/* level %d (raw data) */\n"
              "\t0%s", l, skip ? ",\n\t" : "\n");
  for(j = 0, i++; j < skip; j++, i++)
    fprintf(mf, "0x%X%s", mt[i], j == skip - 1 ? "\n" :
                                                 !((j + 1) % 8) ? ",\n\t" :
						                  ", ");
  fputs("};\n", mf);
  fclose(mf);
  fprintf(gen_h, " (%d-level) */\n"
                 "/** Length (in words) of multi-level table for Unicode %s property */\n"
                 "#define uni_%s_mtab_len %d\n", l + 1, name, name, i);
}
@

\subsection{Additional Properties}

A few specific character classes that are useful for some applications
do not warrant a full set of tables.  Instead, they are implemented as
macros that compare directly to the limited set they contain.  These
are:  characters which act as line terminators, characters which
may initiate backslash-escapes, and UTF-16 surrogates.

<<Unicode property exports for generator>>=
/** Returns true if \p cp is a newline character */
int uni_is_nl(uint32_t cp);
#define uni_is_nl(x) ((x) == '\n' || (x) == '\r' || (x) == '\v' || (x) == '\f' || \
                  (x) == 0x0085 || (x) == 0x2028 || (x) == 0x2029)
/** Returns true if \p cp is a backslash character.
  * This is the equivalent to \p cp == '\\' when processing NFKC_CF text */
int uni_is_bs(uint32_t cp);
#define uni_is_bs(x) ((x) == '\\' || (x) == 0xFE68 || (x) == 0xFF3C)
/** Returns true if \p cp is a UTF-16 surrogate */
int uni_is_surrogate(uint32_t cp);
#define uni_is_surrogate(x) ((x) >= 0xD800 && (x) < 0xE000)
@

\section{Enumerated Properties}

Enumerated properties have a limited number of values, which have
names of their own.  These names may have aliases, as well.  Of course
all properties have a limited number of values, so it is a matter of
interpretation whether it qualifies as an enumeration.  The general
rule followed here is that this only includes properties with an
explicit list of aliases.  That includes one psuedo-property: the list
of properties itself.  An enumerated property can be split into sets,
each of which consists of the characters having one specific
enumerated value.  This means that each enumerated property also
defines a number of boolean properties, which are covered above.
Additional useful operations include:

\begin{itemize}
\item Find out the value \emph{of} the property for a character.  This
should be a numeric value indicating the enumeration literal index.
\item Given a value, find out its \emph{name} and primary \emph{alias}.
\item Given a name or alias, find out its \emph{value}.  This should
support Unicode sloppy matching:  case-insensitive, with hyphens and
underscores removed (except as noted).
\item Obtain a \emph{list} of all possible values and aliases, for more
general searching.
\end{itemize}

\subsection{Storage Methods}

Again, for the \emph{of} operation, the multi-level table seems
appropriate.  Rather than storing the actual string value, the
enumeration is simply converted into an integer from zero to the
number of elements, and that integer is stored.  For all enumerations
but the character name, this just one byte.  For the character name, a
different approach is required.  Since every code point has a
different name, there will never be any sharing of lower-level tables.
The only possible sharing would be in the algorithmically generated
code points.  Since this is a significant special case, it gets its
own section, later on.

To obtain a subset for boolean operations, the range list is provided
as well.  It is augmented by a data field.  No enumeration actually
requires 32 bits, so adding another separate field is wasteful.
Instead, an 8-bit bit field is added.  This does slow things down
slightly, but for performance, the multi-level table is better, anyway.

<<[[uni_chrrng_dat8_t]]>>=
/** A simple 64-bit range structure with room for 8-bit data */
typedef struct {
    uint32_t low, /**< Low end of range */ high:24, /**< High end of range */
             dat:8; /**< Value for all code points within this range */
} uni_chrrng_dat8_t;
@

<<Unicode property exports for generator>>=
#if 1
<<[[uni_chrrng_dat8_t]]>>
#else /* 32-bit dat during testing */
typedef struct {
    uint32_t low, high, dat;
} uni_chrrng_dat8_t;
#endif
/** Compare \ref uni_chrrng_dat8_t code point ranges pointed to by \p a and \p b.
  * Intended for use with qsort and bsearch */
int uni_cmprng_dat8(const void *a, const void *b); /* for bsearch/qsort */
/** Return the value associated with \p cp in a \ref uni_chrrng_dat8_t table.
  * If \p cp is not covered by any range, \p def is returned.  The 
  * table \p tab/\p tab_len must be sorted as per \ref uni_cmprng_dat8.
  * If there are overlapping ranges in the array, results are undefined */
uint32_t uni_chrrng_dat8(uint32_t cp, const uni_chrrng_dat8_t *tab,
                         uint32_t tab_len, uint32_t def);
@

<<Known Data Types>>=
uni_chrrng_dat8_t,%
@

<<Unicode property functions for generator>>=
int uni_cmprng_dat8(const void *a, const void *b)
{
    const uni_chrrng_dat8_t *_a = a, *_b = b;

    if(_a->high < _b->low)
        return -1;
    else if(_b->high < _a->low)
        return 1;
    else
        return 0;
}

uint32_t uni_chrrng_dat8(uint32_t cp, const uni_chrrng_dat8_t *tab,
                         uint32_t tab_len, uint32_t def)
{
#if 0
  uni_chrrng_dat8_t cr = {cp, cp}, *tab_el;
  tab_el = bsearch(&cr, tab, tab_len, sizeof(uni_chrrng_dat8_t), uni_cmprng_dat8);
  return tab_el ? tab_el->dat : def;
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > tab[j].high)
      l = j + 1;
    else
      return tab[j].dat;
  }
  return def;
#endif
}
@

While no enumeration requires more than 8 bits, there may be other
data types which require more.  For these, larger types are needed.
For 16 bits, the [[low]] field can be split as well.  For 32 bits, if
they are unique, it is unlikely that long ranges (if any ranges at
all) exist.  For this reason, a low+length approach is used instead;
this splits the [[low]] field to make room for a [[len]] field
(which is the length of the range minus one).

<<[[uni_chrrng_dat16_t]]>>=
/** A simple 64-bit range structure with room for 16 bits of data */
typedef struct {
    uint32_t low:24, /**< Low end of range */
             datl:8, /**< Low half of 16-bit data assigned to range */
	     high:24, /**< High end of range */
	     dath:8; /**< High half of 16-bit data assigned to range */
} uni_chrrng_dat16_t;
@

<<[[uni_chrrng_dat32_t]]>>=
/** A simple 64-bit range structure with room for 32 bits of data */
typedef struct {
    uint32_t low:24, /**< Low end of range */
             len:8, /**< Size of range - 1 (high = low + len) */
	     dat; /* 32-bit data assigned to range */
} uni_chrrng_dat32_t;
@

<<Known Data Types>>=
uni_chrrng_dat16_t,uni_chrrng_dat32_t,%
@

<<Unicode property exports for generator>>=
<<[[uni_chrrng_dat16_t]]>>
<<[[uni_chrrng_dat32_t]]>>
/** Compare \ref uni_chrrng_dat16_t code point ranges pointed to by \p a and \p b.
  * Intended for use with qsort and bsearch */
int uni_cmprng_dat16(const void *a, const void *b); /* for bsearch/qsort */
/** Return the value associated with \p cp in a \ref uni_chrrng_dat16_t table.
  * If \p cp is not covered by any range, zero is returned.  The 
  * table \p tab/\p tab_len must be sorted as per \ref uni_cmprng_dat16.
  * If there are overlapping ranges in the array, results are undefined */
uint16_t uni_chrrng_dat16(uint32_t cp, const uni_chrrng_dat16_t *tab,
                          uint32_t tab_len);
/** Compare \ref uni_chrrng_dat32_t code point ranges pointed to by \p a and \p b.
  * Intended for use with qsort and bsearch */
int uni_cmprng_dat32(const void *a, const void *b); /* for bsearch/qsort */
/** Return the value associated with \p cp in a \ref uni_chrrng_dat32_t table.
  * If \p cp is not covered by any range, zero is returned.  The 
  * table \p tab/\p tab_len must be sorted as per \ref uni_cmprng_dat32.
  * If there are overlapping ranges in the array, results are undefined */
uint32_t uni_chrrng_dat32(uint32_t cp, const uni_chrrng_dat32_t *tab,
                          uint32_t tab_len);
@

<<Unicode property functions for generator>>=
int uni_cmprng_dat16(const void *a, const void *b)
{
    const uni_chrrng_dat16_t *_a = a, *_b = b;

    if(_a->high < _b->low)
        return -1;
    else if(_b->high < _a->low)
        return 1;
    else
        return 0;
}

int uni_cmprng_dat32(const void *a, const void *b)
{
    const uni_chrrng_dat32_t *_a = a, *_b = b;

    if(_a->low + _a->len < _b->low)
        return -1;
    else if(_b->low + _b->len < _a->low)
        return 1;
    else
        return 0;
}

uint16_t uni_chrrng_dat16(uint32_t cp, const uni_chrrng_dat16_t *tab,
                          uint32_t tab_len)
{
#if 0
  uni_chrrng_dat16_t cr = {cp, cp}, *tab_el;
  tab_el = bsearch(&cr, tab, tab_len, sizeof(uni_chrrng_dat16_t), uni_cmprng_dat16);
  return tab_el ? (tab_el->dath << 8) + tab_el->datl : 0;
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > tab[j].high)
      l = j + 1;
    else
      return (tab[j].dath << 8) + tab[j].datl;
  }
  return 0;
#endif
}

uint32_t uni_chrrng_dat32(uint32_t cp, const uni_chrrng_dat32_t *tab,
                          uint32_t tab_len)
{
#if 0
  uni_chrrng_dat32_t cr = {cp, cp}, *tab_el;
  tab_el = bsearch(&cr, tab, tab_len, sizeof(uni_chrrng_dat32_t), uni_cmprng_dat32);
  return tab_el ? tab_el->dat : 0;
#else /* twice as fast! */
  int l = 0, h = tab_len - 1;
  while(l <= h) {
    int j = (l + h) / 2;
    if(cp < tab[j].low)
      h = j - 1;
    else if(cp > tab[j].low + tab[j].len)
      l = j + 1;
    else
      return tab[j].dat;
  }
  return 0;
#endif
}
@

For the \emph{name} and \emph{alias} operations, a simple lookup table
can be provided, indexed on the enumeration value (again, except for
the name property).  The table fulfulls the \emph{list} requirement as
well.

For the \emph{value} operation, either a sorted table of names or some
other structure can be provided.  The advantage of the sorted table of
names is that it takes no effort to create, search, and provides the
\emph{list} operation (specifially for searching) as well.  A
statically generated hash function might be faster, though.  Since
applications that need to look up a lot of \emph{value}s are probably
rare, only the simple sorted table is provided, and the library user
is expected to convert that to a hash table, prefix tree, or some
other structure as needed.

\subsection{Name Tables}

The first thing to read in is the list of aliases.  These are keyed on
property short name.  Since there is already a list of property names,
the array of aliases may as well correspond.  As the list of aliases
is read, the comment may be stored as an alias as well.  The group
names for the gc property are stored in this file, and the comment for
the definition line for these names indicates the fundamental values
that are combined into this group.

<<UCD parser local definitions>>=
uni_alias_t **val_aliases;
int *num_val_aliases, *max_val_aliases;
@

<<Initialize UCD files>>=
inisize(val_aliases, num_prop_aliases);
clearbuf(val_aliases, num_prop_aliases);
inisize(num_val_aliases, num_prop_aliases);
clearbuf(num_val_aliases, num_prop_aliases);
inisize(max_val_aliases, num_prop_aliases);
clearbuf(max_val_aliases, num_prop_aliases);
@

<<Resize prop-associated arrays>>=
resize(val_aliases, maxparsed);
resize(num_val_aliases, maxparsed);
resize(max_val_aliases, maxparsed);
@

<<Clear prop-associated array[i]>>=
val_aliases[i] = NULL;
num_val_aliases[i] = max_val_aliases[i] = 0;
@

Then, the alias file can be read and stored in the array.  While it
would be more efficient to only look up the destination when things
change, it's easier to just go ahead and put it in place.

<<Initialize UCD files>>=
open_f("PropertyValueAliases.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  split_line(lbuf);
  if(num_fields < 3)
    continue;
  uni_alias_t me, *mep = &me;
  me.short_name = fields[0];
  uni_alias_t **ret = bsearch(&mep, prop_aliases_short, num_prop_aliases,
                              sizeof(*prop_aliases_short), cmp_shortname);
  if(!ret) {
    perror("PropertyValueAliases.txt");
    exit(1);
  }
  int idx = (int)(*ret - prop_aliases);
  check_size(val_aliases[idx], max_val_aliases[idx], num_val_aliases[idx] + 1);
  val_aliases[idx][num_val_aliases[idx]].short_name = strdup(fields[1]);
  val_aliases[idx][num_val_aliases[idx]].long_name = strdup(fields[2]);
  /* there are no comments on alias lines, so field count is correct */
  val_aliases[idx][num_val_aliases[idx]].alt_name =
                 num_fields > 3 ? strdup(fields[3]) : NULL;
  val_aliases[idx][num_val_aliases[idx]].alt_name2 =
                 num_fields > 4 ? strdup(fields[4]) : NULL;
  if(num_fields > 5) {
    perror("PropertyValueAliases.txt");
    exit(1);
  }
  if(line_comment && !strcmp(fields[0], "gc"))
    val_aliases[idx][num_val_aliases[idx]].alt_name2 = strdup(line_comment);
  num_val_aliases[idx]++;
}
fclose(f);
@

Since we're about to scan files with the enumerations in text form,
the next logical step is to build a search mechanism.  In this case,
just a simple sorted array, using the standard loose matching.  This
is necessary because some fields do not match any of the literal
values we have gathered so far.  This table will be dumped as well,
for the \emph{value} function.

<<[[uni_valueof_t]]>>=
/** A simple structure associating a name with an integer */
typedef struct {
  const char *name; /**< The name */
  int val; /**< The value */
} uni_valueof_t;
@

<<Unicode property exports for generator>>=
<<[[uni_valueof_t]]>>
@

<<Known Data Types>>=
uni_valueof_t,%
@

<<UCD parser local definitions>>=
uni_valueof_t **enum_vals;
uint32_t *enum_vals_len;
@

<<Unicode property exports for generator>>=
/** Compare strings pointed to indirectly by \p a and \p b.
  * This is simple comparison using strcmp.  This function is intended
  * for use with bsearch and qsort for arrays of \ref uni_valueof_t, but
  * can be used with any array of structures whose first member is a
  * string pointer */
int uni_cmp_valueof(const void *a, const void *b);
@

<<Unicode property functions for generator>>=
int uni_cmp_valueof(const void *a, const void *b)
{
  return strcmp(((uni_valueof_t *)a)->name, ((uni_valueof_t *)b)->name);
}
@

<<Initialize UCD files>>=
inisize(enum_vals, num_prop_aliases);
clearbuf(enum_vals, num_prop_aliases);
inisize(enum_vals_len, num_prop_aliases);
clearbuf(enum_vals_len, num_prop_aliases);
for(i = 0; i < num_prop_aliases; i++) {
  if(!val_aliases[i])
    continue;
  /* ignore boolean properties */
  if(num_val_aliases[i] == 2 && !strcmp(val_aliases[i][0].short_name, "N") &&
     !strcmp(val_aliases[i][1].short_name, "Y"))
    continue;
  uni_valueof_t *valueof;
  int num_valueof = 0;
  inisize(valueof, num_val_aliases[i] * 4);
  for(j = 0; j < num_val_aliases[i]; j++) {
    const uni_alias_t *va = &val_aliases[i][j];
    valueof[num_valueof].name = strdup(va->short_name);
    valueof[num_valueof++].val = j;
    valueof[num_valueof].name = strdup(va->long_name);
    valueof[num_valueof++].val = j;
    if(va->alt_name) {
      valueof[num_valueof].name = strdup(va->alt_name);
      valueof[num_valueof++].val = j;
    }
    /* need to skip comments stored in alt_name2; all contain | */
    if(va->alt_name2 && !strchr(va->alt_name2, '|')) {
      valueof[num_valueof].name = strdup(va->alt_name2);
      valueof[num_valueof++].val = j;
    }
  }
  for(j = 0; j < num_valueof; j++) {
    char *n = (char *)valueof[j].name, *d = n;
    for(; *n; n++) {
      if(isupper(*n))
        *d++ = tolower(*n);
      else if(*n != '_' && *n != '-' && !isspace(*n))
        *d++ = *n;
    }
    *d = 0;
  }
  qsort(valueof, num_valueof, sizeof(*valueof), uni_cmp_valueof);
  /* remove duplicates now that they are always contiguous */
  for(j = 1; j < num_valueof; j++)
    if(!strcmp(valueof[j].name, valueof[j - 1].name)) {
      free((char *)valueof[j].name);
      if(valueof[j].val != valueof[j - 1].val) {
        perror(valueof[j - 1].name);
	exit(1);
      }
      movebuf(valueof + j, valueof + j + 1, num_valueof - (j + 1));
      --j;
      --num_valueof;
    }
  enum_vals[i] = valueof;
  enum_vals_len[i] = num_valueof;
}
@

<<Resize prop-associated arrays>>=
resize(enum_vals, maxparsed);
resize(enum_vals_len, maxparsed);
@

<<Clear prop-associated array[i]>>=
enum_vals[i] = NULL;
enum_vals_len[i] = 0;
@

For each value alias, an enumeration needs to be printed to the header
file.  Both the short and long name should be supported, but there is
little value in supporting the secondary aliases.  This excludes
binary values, which have pointless yes/no aliases, and, if possible,
deprecated values.  Special care must be taken with the version
aliases, as the numeric values have decimal points.  The CCC aliases
are numeric as well, but in this case, the short name should be
ignored and used as the value directly instead.

In order to support the \emph{name} and \emph{value} functions, tables
need to be dumped as well.  First, a table similar to the locally
stored table is printed; this does the \emph{name} operation.  Next,
value table built just previously is dumped for the \emph{value}
operation.  Unlike the enumerations, no values may be skipped when
generating the string tables.  The string tables are stored in the
same file, because both files use the same set of string constants,
and most compilers share storage for like string constants.

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  const char *pn = i < num_prop_aliases ? prop_aliases[i].short_name :
                                          parsed_props[i].name;
  const char *lname = i < num_prop_aliases ? prop_aliases[i].long_name :
                                             parsed_props[i].name;
  if(!enum_vals[i])
    continue;
  /* ignore boolean properties */
  if(parsed_props[i].rng)
    continue;
  <<Ignore unimplemented enums>>
  <<Generate C code for enumeration constants>>
}
@

<<Generate C code for enumeration constants>>=
fprintf(gen_h, "/** Enumeration constant names for property %s.\n"
               "  * Indexed by \\ref uni_%s_t enumeration literal;\n"
	       "  * length is UNI_NUM_%s */\n"
	       "extern const uni_alias_t uni_%s_nameof[];\n", lname, pn, pn, pn);
char nbuf[64];
sprintf(nbuf, "uni_%s_nameof.gen.c", pn);
open_wf(nf, nbuf);
fprintf(nf, "#include \"uni_prop.h\"\n\n"
	    "const uni_alias_t uni_%s_nameof[] = {\n", pn);
/* generating enum on gen_h */
fprintf(gen_h, "/** Enumeration constants for enumerated property %s */\n"
	       "typedef enum {\n", lname);
int last_nameof = -1;
for(j = 0; j < num_val_aliases[i]; j++) {
  const uni_alias_t *va = &val_aliases[i][j];
  /* print nameof table entry */
  char *e_nameof;
  int cur_nameof = strtol(va->short_name, &e_nameof, 10);
  if(!*e_nameof && strcmp(va->short_name, va->long_name))
    while(++last_nameof != cur_nameof)
      fputs("\t{ NULL },\n", nf);
  fputs("\t{ ", nf);
  put_string(nf, va->short_name);
  if(strcmp(va->long_name, va->short_name)) {
    fputs(", ", nf);
    put_string(nf, va->long_name);
  }
  if(va->alt_name && strcmp(va->alt_name, va->long_name)) {
    fputs(", ", nf);
    put_string(nf, va->alt_name);
  }
  /* again, | is to skip comments */
  if(va->alt_name2 && !strchr(va->alt_name2, '|') &&
     strcmp(va->alt_name, va->alt_name2)) {
    fputs(", ", nf);
    put_string(nf, va->alt_name2);
  }
  fputs(" }", nf);
  if(j < num_val_aliases[i] - 1)
    putc(',', nf);
  putc('\n', nf);
  /* filter out bad enum values */
  /* skip version short name */
  if(strchr(va->short_name, '.')) {
    fprintf(gen_h, "\tUNI_%s_%s, /**< %s */\n", pn, va->long_name, va->short_name);
    continue;
  }
  /* use numeric values directly instead of making into enum */
  int sn_bad = 0;
  if(!isdigit(va->short_name[0]) || !strcmp(va->long_name, va->short_name)) {
    const char *np;
    /* slow, but necessary with e.g. ID_Restrict_Type */
    fprintf(gen_h, "\tUNI_%s_", pn);
    for(np = va->short_name; *np; np++) {
      if(isalnum(*np))
        putc(*np, gen_h);
      else if(*np == ' ' || *np == '-' || *np == '_')
        putc('_', gen_h);
      else {
        fprintf(gen_h, "_%02X", (int)(uint8_t)*np);
	sn_bad = 1;
      }
    }
    fprintf(gen_h, ", /**< %s */", sn_bad ? "*" : va->long_name);
  }
  if(strcmp(va->long_name, va->short_name)) {
    fprintf(gen_h, "%cUNI_%s_%s = ", isdigit(va->short_name[0]) ? '\t' : ' ',
                    pn, va->long_name);
    if(!isdigit(va->short_name[0]))
      fprintf(gen_h, "UNI_%s_%s,", pn, va->short_name);
    else
      fprintf(gen_h, "%s,", va->short_name);
    fprintf(gen_h, " /**< %s */", sn_bad ? "*" : va->short_name);
  }
  /* filter out alt names with hyphens */
  if(va->alt_name && !strchr(va->alt_name, '-') &&
     strcmp(va->alt_name, va->long_name))
    fprintf(gen_h, " UNI_%s_%s = UNI_%s_%s, /**< %s */",
                   pn, va->alt_name, pn, va->long_name, va->long_name);
  /* again, | is to skip comments */
  if(va->alt_name2 && !strchr(va->alt_name2, '|') && !strchr(va->alt_name2, '-') &&
     strcmp(va->alt_name, va->long_name) && strcmp(va->alt_name, va->alt_name2))
    fprintf(gen_h, " UNI_%s_%s = UNI_%s_%s, /**< %s */",
                   pn, va->alt_name2, pn, va->long_name, va->long_name);
  fputc('\n', gen_h);
}
fputs("};\n", nf);
fprintf(gen_h, "\tUNI_NUM_%s /**< Number of enumeration values for %s */\n"
               "} uni_%s_t;\n", pn, lname, pn);
fprintf(gen_h, "/** Sorted list of all names for values of %s property\n"
               "  * Names are in their canonical form, and are associated\n"
	       "  * with the \\ref uni_%s_t enumeration literal for that\n"
	       "  * name */\n"
	       "extern const uni_valueof_t uni_%s_valueof[];\n", lname, pn, pn);
fprintf(nf, "const uni_valueof_t uni_%s_valueof[] = {\n", pn);
for(j = 0; j < enum_vals_len[i]; j++) {
  const char *name = val_aliases[i][enum_vals[i][j].val].short_name;
  if(isdigit(*name))
    name = val_aliases[i][enum_vals[i][j].val].long_name;
  fputs("\t{ ", nf);
  put_string(nf, enum_vals[i][j].name);
  fprintf(nf, ", UNI_%s_", pn);
  /* slow, but necessary with e.g. ID_Restrict_Type */
  /* also, the CLDR ones have /, +, space, etc. */
  for(; *name; name++) {
    if(isalnum(*name))
      putc(*name, nf);
    else if(*name == ' ' || *name == '-' || *name == '_')
      putc('_', nf);
    else
      fprintf(nf, "_%02X", (int)(uint8_t)*name);
  }
  fprintf(nf, " }%s\n", j < enum_vals_len[i] - 1 ? "," : "");
}
fputs("};\n", nf);
fclose(nf);
fprintf(gen_h, "/** Length of \\ref uni_%s_valueof */\n"
	       "#define uni_%s_valueof_len %d /* %d lookups max */\n",
               pn, pn, enum_vals_len[i], lg2(enum_vals_len[i] + 1));
@

<<UCD parser local functions>>=
static void put_string(FILE *f, const char *s)
{
  if(!s)
    fputs("NULL", f);
  else {
    putc('"', f);
    while(*s) {
      if(*s == '\\' || *s == '"')
        putc('\\', f);
      if(isprint(*s))
        putc(*s, f);
      else
        fprintf(f, "\\%03o", (int)(uint8_t)*s);
      s++;
    }
    putc('"', f);
  }
}
@

Since the two arrays are stored in the same file, the logic to
generate file names needs to skip the valueof array.

<<Filter generated file names>>=
fgrep -v _valueof\[ | \
@

Another useful array to print is for approximate name matching.  In
this case, approximate is defined by UAX44-LM3: case-insensitive,
optionally without any prefix of is, and without any spaces,
underscores, and hyphens.  A function to perform most of this
transformation in-place is provided as well.  If additional
transformations are required, such as compatibility decomposition, they
must be done before calling this function.  All enumerations are
ASCII-only, so no decomposition is needed, and they are simply stored
lower-case with hyphens, spaces, and underscores removed.  Initial
``is'' is left in; that should only be removed from user input.  To
compare using this function, strip, find a match, and if none is found,
search again with initial ``is'' removed if present.  A sample search
function is provided.

<<Unicode property exports for generator>>=
/** Prepare name for loose enumeration name matching.
  * Removes all underscores, spaces, and dashes, and converts upper-case
  * characters to lower-case in-place in \p n.  Does not perform normalization */
void uni_enum_name_strip(char *n);
@

<<Unicode property functions for generator>>=
void uni_enum_name_strip(char *n)
{
  char *s, *d;
  for(s = d = n; *s; s++)
    if(*s != '-' && *s != '_' && *s != ' ')
      *d++ = tolower(*s);
  *d = 0;
}
@

<<Unicode property exports>>=
/** Look up \p n in stripped name table \p tab/\p len loosely.
  * This assumes that \p tab is fully stripped as per \ref uni_enum_name_strip,
  * and then sorted using simple string compares (e.g. \ref uni_cmp_valueof).
  * The name \p n is itself stripped, and if lookup fails, and \p n is
  * prefixed with "is", that is stripped as well and lookup is repeated.
  * If lookup succeeds, the integer value is returned, reduced to 8 bits.
  * Otherwise, \p def is returned.  Note that since this routine always
  * duplicates and strips \p n, this may not be the most efficient way to
  * perform such lookups. */
uint8_t uni_x_valueof_approx(const char *n, const uni_valueof_t *tab, int len,
                             uint8_t def);
@

<<Unicode property functions>>=
uint8_t uni_x_valueof_approx(const char *n, const uni_valueof_t *tab, int len,
                             uint8_t def)
{
  /* yuck - strdup on every call */
  char *s = strdup(n);
  uni_valueof_t v, *f;
  uni_enum_name_strip(s);
  v.name = s;
  /* more yuck - bsearch is 1/2 as fast as hand-coded binary search */
  f = bsearch(&v, tab, len, sizeof(*tab), uni_cmp_valueof);
  if(!f && !strncmp(s, "is", 2) && s[2]) {
    v.name = s + 2;
    /* more yuck - bsearch is 1/2 as fast as hand-coded binary search */
    f = bsearch(&v, tab, len, sizeof(*tab), uni_cmp_valueof);
  }
  free(s);
  return f ? f->val : def;
}
@

<<Generate C code for enumeration constants>>=
sprintf(nbuf, "uni_%s_valueof_approx.gen.c", pn);
open_wf(anf, nbuf);
fprintf(gen_h, "/** Sorted list of all names for values of %s property for\n"
	       "  * approximate matching.  Names have been stripped using\n"
	       "  * \\ref uni_enum_name_strip, and are associated with the\n"
	       "  * \\ref uni_%s_t enumeration literal for that name.  The\n"
	       "  * length of this array is also \\ref uni_%s_valueof_len */\n"
	       "extern const uni_valueof_t uni_%s_valueof_approx[];\n",
	       lname, pn, pn, pn);
fprintf(anf, "#include \"uni_prop.h\"\n\n"
             "const uni_valueof_t uni_%s_valueof_approx[] = {\n", pn);
uni_valueof_t *approx_array;
inisize(approx_array, enum_vals_len[i]);
for(j = 0; j < enum_vals_len[i]; j++) {
  approx_array[j].name = strdup(enum_vals[i][j].name);
  approx_array[j].val = enum_vals[i][j].val;
  uni_enum_name_strip((char *)approx_array[j].name);
}
qsort(approx_array, enum_vals_len[i], sizeof(*approx_array), uni_cmp_valueof);
for(j = 0; j < enum_vals_len[i]; j++) {
  const char *name = val_aliases[i][approx_array[j].val].short_name;
  if(isdigit(*name))
    name = val_aliases[i][approx_array[j].val].long_name;
  fputs("\t{ ", anf);
  put_string(anf, approx_array[j].name);
  fprintf(anf, ", UNI_%s_", pn);
  /* slow, but necessary with e.g. ID_Restrict_Type */
  /* also, the CLDR ones have /, +, space, etc. */
  for(; *name; name++) {
    if(isalnum(*name))
      putc(*name, anf);
    else if(*name == ' ' || *name == '-' || *name == '_')
      putc('_', anf);
    else
      fprintf(anf, "_%02X", (int)(uint8_t)*name);
  }
  fprintf(anf, " }%s\n", j < enum_vals_len[i] - 1 ? "," : "");
  free((char *)approx_array[j].name);
}
fputs("};\n", anf);
fclose(anf);
free(approx_array);
fprintf(gen_h, "/** Look up \\ref uni_%s_t value for name \\p x using loose\n"
	       "  * matching.  See \\ref uni_x_valueof_approx for details.\n"
	       "  * Lookup failure returns ~0 */\n"
	       "#define uni_%s_lookup(n) "
                   "uni_x_valueof_approx(n, uni_%s_valueof_approx, %d, ~0)\n",
	       pn, pn, pn, enum_vals_len[i]);
@

There is still one nagging issue:  the gc aliases include some that
are actually combinations of the others.  Since each value can only
hold one value, this is not going to work.  To support this, an extra
table is generated which translates a value into a 64-bit mask that
includes all base values.  For the base values themselves, only their
own bit will be set.  The pseudo values are recognized by a comment in
their last field, which is stored in one of the aliases.  This is the
reason the vertical bar was filtered out above:  all of the aliases
have vertical bars in their last field.

<<Generate C code for enumeration constants>>=
if(!strcmp(pn, "gc")) {
  fputs("/** Convert aggregate \\ref uni_gc_t values to constituent codes.\n"
	"  * Each element of this array is a bit mask with a bit set for each\n"
	"  * base code associated with the \\ref uni_gc_t index.  For example,\n"
	"  * uni_gc_trans[uni_gc_M] has bits uni_gc_Mc, uni_gc_Me and\n"
	"  * uni_gc_Mn set.  A base code is identified by having its own\n"
	"  * bit set in the result */\n"
        "extern const uint64_t uni_gc_trans[];\n", gen_h);
  open_wf(tf, "uni_gc_trans.gen.c");
  fputs("#include \"uni_prop.h\"\n\n"
	"const uint64_t uni_gc_trans[] = {\n", tf);
  for(j = 0; j < num_val_aliases[i]; j++) {
    const char *desc;
    fprintf(tf, "\t/* %2s */ ", val_aliases[i][j].short_name);
    desc = val_aliases[i][j].alt_name2;
    if(desc && !strchr(desc, '|'))
      desc = NULL;
    if(!desc)
      fprintf(tf, "1ULL << UNI_gc_%s", val_aliases[i][j].short_name);
    else {
      const char *desce;
      while(1) {
        desce = strchr(desc, ' ');
	if(!desce)
	  desce = desc + strlen(desc);
	fprintf(tf, "(1ULL << UNI_gc_%.*s)%s", (int)(desce - desc), desc,
	                                     *desce ? " | " : "");
	if(!*desce)
	  break;
	desc = desce + 3; /* past ' | ' */
      }
    }
    if(j < num_val_aliases[i] - 1)
      putc(',', tf);
    putc('\n', tf);
  }
  fputs("};\n", tf);
  fclose(tf);
}
@

One other set of aliases has already been read, and is treated in some
contexts almost like enumeration properties: the property names
themselves.  While it defeats the purpose of placing properties in
separate files for statically linking only needed properties, a master
table of all properties and their exported symbol values will make
other projects' data file generation much easier.

<<[[uni_propdesc_t]]>>=
/** A property supported by libuni. */
typedef struct {
  uni_alias_t name; /**< The property's name and aliases */
  const uint32_t *mtab;  /**< The property's multi-level lookup table */
  uint32_t mtab_len; /**< The property's multi-level lookup table's length */
  const void *tab; /**< The property's range table; type is determined by \ref type */
  uint32_t tab_len; /**< The property's range table length */
  uni_prop_type_t type; /**< The property type; see \ref uni_prop_type_t */
  <<Additional property structure members>>
} uni_propdesc_t;
@

<<Unicode property exports for generator>>=
/** The type of a property in \ref uni_propdesc_t */
typedef enum {
  UNI_PROP_TYPE_NONE,  /**< Dummy array entry; never happens */
  <<Additional property type names>>
  /** Boolean; range table is \ref uni_chrrng_t and multi-level table is
   ** 1 bit per code point */
  UNI_PROP_TYPE_BOOL,
  /** Enumeration; range table is \ref uni_chrrng_dat8_t and multi-level
   ** is 1 byte per code point */
  UNI_PROP_TYPE_ENUM
} uni_prop_type_t;
<<[[uni_propdesc_t]]>>
@

<<Known Data Types>>=
uni_propdesc_t,uni_prop_type_t,%
@

<<UCD parser local definitions>>=
uni_valueof_t *prop_ptrs;
@

<<Dump character information as C code>>=
fprintf(gen_h, "/** A table containing a property descriptor for all generated properties */\n"
	       "extern const uni_propdesc_t uni_propdesc[];\n");
open_wf(pnf, "uni_propdesc.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_propdesc_t uni_propdesc[] = {\n", pnf);
uint32_t num_prop_ptrs = 0, max_prop_ptrs;
inisize(prop_ptrs, max_prop_ptrs = 20);
int index = 0;
for(i = 0; i < nparsed; i++) {
  uni_prop_type_t t = UNI_PROP_TYPE_NONE;
  const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                            parsed_props[i].name;
  if(!parsed_props[i].mt)
    continue;
  if(parsed_props[i].rng)
    t = UNI_PROP_TYPE_BOOL;
  <<Set prop type for export>>
  if(t == UNI_PROP_TYPE_NONE)
    continue;
  if(index)
    fputs(",\n", pnf);
  check_size(prop_ptrs, max_prop_ptrs, num_prop_ptrs + 4);
  fprintf(pnf, "\t{{\"%s\"", name);
  prop_ptrs[num_prop_ptrs].name = strdup(name);
  prop_ptrs[num_prop_ptrs++].val = index;
  if(i < num_prop_aliases) {
    fprintf(pnf, ", \"%s\"", prop_aliases[i].long_name);
    if(strcmp(prop_aliases[i].long_name, name)) {
      prop_ptrs[num_prop_ptrs].name = strdup(prop_aliases[i].long_name);
      prop_ptrs[num_prop_ptrs++].val = index;
    }
    if(prop_aliases[i].alt_name && strcmp(prop_aliases[i].alt_name, name) &&
       strcmp(prop_aliases[i].long_name, prop_aliases[i].alt_name)) {
      fprintf(pnf, ", \"%s\"", prop_aliases[i].alt_name);
      prop_ptrs[num_prop_ptrs].name = strdup(prop_aliases[i].alt_name);
      prop_ptrs[num_prop_ptrs++].val = index;
    }
    if(prop_aliases[i].alt_name2 && strcmp(prop_aliases[i].alt_name, name) &&
       strcmp(prop_aliases[i].long_name, prop_aliases[i].alt_name) &&
       strcmp(prop_aliases[i].alt_name, prop_aliases[i].alt_name2)) {
      fprintf(pnf, ", \"%s\"", prop_aliases[i].alt_name2);
      prop_ptrs[num_prop_ptrs].name = strdup(prop_aliases[i].alt_name);
      prop_ptrs[num_prop_ptrs++].val = index;
    }
  }
  fprintf(pnf, "},\n"
               "\t\tuni_%s_mtab, uni_%s_mtab_len, uni_%s_%s, uni_%s_%s_len,\n",
               name, name, name, <<Plain table name>> "rng", name,
	       <<Plain table name>> "rng");
  fprintf(pnf, "\t\t%d", (int)t);
  <<Print additional property structure members>>
  fputc('}', pnf);
  index++;
}
fprintf(gen_h, "/** Length of \\ref uni_propdesc array */\n"
	       "#define uni_propdesc_len %d\n"
	       "/** Sorted list of all generated properties' names and aliases.\n"
               "  * Names are in their canonical form, and are associated\n"
	       "  * with the index into \\ref uni_propdesc of its descriptor */\n"
               "extern const uni_valueof_t uni_propdesc_valueof[];\n",
               index);
fputs("\n};\n\n"
      "const uni_valueof_t uni_propdesc_valueof[] = {\n", pnf);
qsort(prop_ptrs, num_prop_ptrs, sizeof(*prop_ptrs), uni_cmp_valueof);
for(i = num_prop_ptrs - 1; i > 0; i--) {
  if(!strcmp(prop_ptrs[i -1].name, prop_ptrs[i].name)) {
    if(prop_ptrs[i - 1].val != prop_ptrs[i].val) {
      fprintf(stderr, "Prop name conflict %s\n", prop_ptrs[i].name);
      exit(1);
    }
    free((char *)prop_ptrs[i].name);
    num_prop_ptrs--;
    movebuf(prop_ptrs + i, prop_ptrs + i + 1, num_prop_ptrs - i);
  }
}
for(i = 0; i < num_prop_ptrs; i++)
  fprintf(pnf, "\t{\"%s\", %d}%s\n", prop_ptrs[i].name, prop_ptrs[i].val,
                                   i < num_prop_ptrs - 1 ? "," : "");
fputs("};\n", pnf);
fclose(pnf);
fprintf(gen_h, "/** Length of \\ref uni_propdesc_valueof */\n"
	       "#define uni_propdesc_valueof_len %d /* %d lookups max */\n"
	       "/** Sorted list of all names for generated properties for\n"
	       "  * approximate matching.  Names have been stripped using\n"
	       "  * \\ref uni_enum_name_strip, and are associated with the\n"
	       "  * \\ref uni_propdesc array index of that property's\n"
	       "  * descriptor.  The length of this array is also\n"
	       "  * \\ref uni_propdesc_valueof_len */\n"
               "extern const uni_valueof_t uni_propdesc_valueof_approx[];\n",
	       num_prop_ptrs, lg2(num_prop_ptrs + 1));
open_wf(paf, "uni_propdesc_valueof_approx.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_valueof_t uni_propdesc_valueof_approx[] = {\n", paf);
for(i = 0; i < num_prop_ptrs; i++)
  uni_enum_name_strip((char *)prop_ptrs[i].name);
qsort(prop_ptrs, num_prop_ptrs, sizeof(*prop_ptrs), uni_cmp_valueof);
for(i = num_prop_ptrs - 1; i > 0; i--) {
  if(!strcmp(prop_ptrs[i -1].name, prop_ptrs[i].name)) {
    if(prop_ptrs[i - 1].val != prop_ptrs[i].val) {
      fprintf(stderr, "Prop name loose conflict %s\n", prop_ptrs[i].name);
      exit(1);
    }
    num_prop_ptrs--;
    free((char *)prop_ptrs[i].name);
    movebuf(prop_ptrs + i, prop_ptrs + i + 1, num_prop_ptrs - i);
  }
}
for(i = 0; i < num_prop_ptrs; i++)
  fprintf(paf, "\t{\"%s\", %d}%s\n", prop_ptrs[i].name, prop_ptrs[i].val,
                                   i < num_prop_ptrs - 1 ? "," : "");
fputs("};\n", paf);
fclose(paf);
fprintf(gen_h, "/** Look up \\ref uni_propdesc array index for name \\p n\n"
	       "  * using loose matching.  See \\ref uni_x_valueof_approx\n"
	       "  * for details.  If lookup fails, ~0 is returned */\n"
	       "#define uni_propdesc_lookup(n) uni_x_valueof_approx(n, "
                      "uni_propdesc_valueof_approx, %d, ~0)\n",
	       num_prop_ptrs);
@

Now that the aliases have been taken care of, it's time to read in the
enumeration properties themselves.  As with the boolean types, a range
table will be built first, and then converted to a multi-level table
when finished.

<<Property parsed contents>>=
uni_chrrng_dat8_t *rng_dat8;
uint8_t def;
@

<<UCD parser local functions>>=
static void add_enum_rng(prop_t *p, uint32_t low, uint32_t high, uint8_t val)
{
  if(p->def == val)
    return;
  if(!p->max_len)
    inisize(p->rng_dat8, (p->max_len = 8));
  if(p->len && p->rng_dat8[p->len - 1].high == low - 1 &&
     p->rng_dat8[p->len - 1].dat == val)
    p->rng_dat8[p->len - 1].high = high;
  else {
    check_size(p->rng_dat8, p->max_len, p->len + 1);
    p->rng_dat8[p->len].low = low;
    p->rng_dat8[p->len].high = high;
    p->rng_dat8[p->len].dat = val;
    ++p->len;
  }
}
@

<<Set prop type for export>>=
if(parsed_props[i].rng_dat8)
  t = UNI_PROP_TYPE_ENUM;
@

<<Additional property structure members>>=
uint8_t def; /**< The default value for enumeration types */
const uni_alias_t *nameof; /**< The names and aliases for enumeration constants */
/** Name-to-value lookup tables for enumeration constants */
const uni_valueof_t *valueof, *valueof_approx;
uint32_t nameof_len, valueof_len;  /**< The length of the above three tables */
@

<<Print additional property structure members>>=
fprintf(pnf, ", %d", (int)parsed_props[i].def);
if(t == UNI_PROP_TYPE_ENUM && enum_vals[i]) {
  fprintf(pnf, ", uni_%s_nameof, uni_%s_valueof, uni_%s_valueof_approx,\n"
               "\t\tUNI_NUM_%s, uni_%s_valueof_len",
               name, name, name, name, name);
} else
  fputs(", NULL, NULL, NULL, 0, 0", pnf);
@

<<UCD parser local functions>>=
static uint32_t enum_val(int pno, const char *v)
{
  char *s = strdup(v), *t, *d;
  uni_valueof_t me, *vp;
  me.name = s;

  d = t = s;
  for(; *t; t++) {
    if(isupper(*t))
      *d++ = tolower(*t);
    else if(*t != '_' && *t != '-' && !isspace(*t))
      *d++ = *t;
  }
  *d = 0;
  vp = bsearch(&me, enum_vals[pno], enum_vals_len[pno], sizeof(me),
               uni_cmp_valueof);
  /* permit excess prefix of "is" */
  if(!vp && s[0] == 'i' && s[1] == 's' && s[2]) {
    me.name = s + 2;
    vp = bsearch(&me, enum_vals[pno], enum_vals_len[pno], sizeof(me),
                 uni_cmp_valueof);
  }
  free(s);
  if(!vp) {
    perror(v);
    exit(1);
  }
  return vp->val;
}
@

<<UCD parser local definitions>>=
#define decl_enum(n, d) \
  int prop_##n = -1; \
  const char *def_##n = d
#define add_enum(n, v) do { \
  if(prop_##n < 0) { \
    prop_##n = add_prop(#n); \
    parsed_props[prop_##n].def = def_##n ? enum_val(prop_##n, def_##n) : 0; \
  } \
  if(*v) \
    add_enum_rng(&parsed_props[prop_##n], low, high, enum_val(prop_##n, v)); \
} while(0)
#define add_int(n, val) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(isdigit(*val)) \
    add_enum_rng(&parsed_props[prop_##n], low, high, strtol(val, NULL, 0)); \
  else \
    add_enum(n, val); \
} while(0)
@

\subsection{Testing}

To test the table implementations, a different, but nearly identical,
set of routines is used compared to the ones one for booleans.

<<Functions to help test generated tables>>=
#define dat8(x, d) doit_dat8(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab, d)

static void doit_dat8(const char *name, const uni_chrrng_dat8_t *rng, uint32_t nent,
                      const uint32_t *mtab, uint8_t def)
{
    uint32_t i;

    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      uint8_t rv = uni_chrrng_dat8(i, rng, nent, def);
      uint8_t mv = <<range table data for [[i]]>>;
      if(rv != mv) {
        fprintf(stderr, "mismatch %s@%d %d %d\n", name, i, (int)rv, (int)mv);
	exit(1);
      }
    }
    /* check performance */
    int j;
    double tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = uni_chrrng_dat8(i, rng, nent, def);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        tres = <<range table data for [[i]]>>;
    tt = tend();
    printf("  r%.0f t%.0f %.2fx\n", tr, tt, tr / tt);
}
@

\subsection{Parsing the UCD}

First, [[UnicodeData.txt]] has a few fields.  Field 3 is gc.  Field 4
is ccc.  Field 5 provides bc, but its default value is complicated and
more easily obtained from [[extracted/DerivedBidiClass.txt]]%
\footnote{\label{fn:extracted}Technically, extracted files are
informative only, and are not guaranteed to be generated correctly.
However, I will trust that they are.  Generating the correct data from
scratch is trivial, but an extra complication to the code I'd rather
avoid.}%
. Field 6 provides
dt, albeit indirectly. Fields 7 through 9 provide nt, even more
indirectly%
\footnote{[[extracted/DerivedNumericType.txt]] might be a better
source for nt, as it includes Unihan data.}%
.

<<Initialize UCD files>>=
decl_enum(gc, "Cn");
decl_enum(ccc, 0);
decl_enum(bc, "L");
decl_enum(dt, "None");
decl_enum(nt, "None");
@

<<Process a line of [[UnicodeData.txt]]>>=
add_enum(gc, fields[2]);
add_int(ccc, fields[3]);
/* add_enum(bc, fields[4]); */
if(fields[5][0] == '<') {
  char *eval = fields[5] + 1;
  while(*eval && *eval != '>')
    eval++;
  if(!*eval) {
    perror("dt");
    exit(1);
  }
  *eval = 0;
  add_enum(dt, fields[5] + 1);
  *eval = '>';
} else if(fields[5][0])
  add_enum(dt, "can");
if(fields[6][0])
  add_enum(nt, "decimal");
else if(fields[7][0])
  add_enum(nt, "digit");
else if(fields[8][0])
  add_enum(nt, "numeric");
@

<<Parse UCD files>>=
open_f("extracted/DerivedBidiClass.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(bc, fields[1]);
}
fclose(f);
@

The other file that has mixed field types is
[[DerivedNormalizationProps.txt]].  The Boolean parser simply looked
at the number of fields, and added if the number of fields was too low
to be anything but a boolean.  This parser can't really do the same,
without first loading the expected default values.  There are only
four properties, so it's safer to just do them by hand.  In fact, two
of them can only have two values: Yes and No.  This makes them boolean
in my view, so that they will become.  There is one other caveat:  the
default value for those two is Yes, and the listed value is always No,
so they need to be inverted when finished.

<<Initialize UCD files>>=
decl_enum(NFC_QC, "Y");
decl_bool(NFD_QC);
decl_enum(NFKC_QC, "Y");
decl_bool(NFKD_QC);
@

<<Process a line of [[DerivedNormalizationProps.txt]]>>=
if(num_fields == 3) {
  if(!strcmp(fields[1], "NFC_QC"))
    add_enum(NFC_QC, fields[2]);
  else if(!strcmp(fields[1], "NFD_QC"))
    add_bool(NFD_QC);
  else if(!strcmp(fields[1], "NFKC_QC"))
    add_enum(NFKC_QC, fields[2]);
  else if(!strcmp(fields[1], "NFKD_QC"))
    add_bool(NFKD_QC);
}
@

<<Parse UCD files>>=
uni_chrrng_t *new;
uint32_t new_len;
uni_chrrng_setop(parsed_props[prop_NFD_QC].rng,
                 parsed_props[prop_NFD_QC].len, UNI_SOP_INV_A,
		 NULL, 0, &new, &new_len);
free(parsed_props[prop_NFD_QC].rng);
parsed_props[prop_NFD_QC].rng = new;
parsed_props[prop_NFD_QC].len = new_len;
uni_chrrng_setop(parsed_props[prop_NFKD_QC].rng,
                 parsed_props[prop_NFKD_QC].len, UNI_SOP_INV_A,
		 NULL, 0, &new, &new_len);
free(parsed_props[prop_NFKD_QC].rng);
parsed_props[prop_NFKD_QC].rng = new;
parsed_props[prop_NFKD_QC].len = new_len;
@

Then there are a number of files that describe just one enumerated
property.  Many of these follow here.

<<Initialize UCD files>>=
decl_enum(sc, "Zzzz");
decl_enum(blk, "No_Block");
decl_enum(hst, "NA");
decl_enum(lb, "XX");
decl_enum(GCB, "XX");
decl_enum(SB, "XX");
decl_enum(WB, "XX");
decl_enum(ea, "N");
decl_enum(age, "unassigned");
@

<<Parse UCD files>>=
open_f("Scripts.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  /* need to manually convert UNI_SC_Hrkt to UNI_SC_Hira+UNI_SC_Kana */
  add_enum(sc, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("Blocks.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(blk, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("HangulSyllableType.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(hst, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("LineBreak.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(lb, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("auxiliary/GraphemeBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
   add_enum(GCB, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("auxiliary/SentenceBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(SB, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("auxiliary/WordBreakProperty.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(WB, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("EastAsianWidth.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(ea, fields[1]);
}
fclose(f);
@

<<Parse UCD files>>=
open_f("DerivedAge.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(age, fields[1]);
}
fclose(f);
@

And there is one file that has two properties per line:
[[ArabicShaping.txt]]%
\footnote{Actually, [[ArabicShaping.txt]] has three values per line,
but field 2 (descriptive name) is informative and will not be read in
as a property unless I find a use.}%
.  However, like the bc property, the jt
property's default value is not a single value.  For that reason, it
is read from the derived file instead%
\footnote{see footnote \ref{fn:extracted} on page \pageref{fn:extracted}.}%
.

<<Initialize UCD files>>=
decl_enum(jt, "U");
decl_enum(jg, "No_Joining_Group");
@

<<Parse UCD files>>=
open_f("ArabicShaping.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  /* add_enum(jt, fields[2]); */
  add_enum(jg, fields[3]);
}
fclose(f);
open_f("extracted/DerivedJoiningType.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_enum(jt, fields[1]);
}
fclose(f);
@

\subsection{Generating the Static Data}

Once finished, the tables can be dumped.  Just as with boolean
properties, only range tables have been built, so it's time to
generate the multi-level table, assuming that the range data is
correct.  Since the way that the automatic enumerations are generated
does not guarantee that zero is the default value, and zero is stored
more efficiently than any other value, all values are incremented by
one and the default value is aliased to zero as well.  The default
value itself is never stored in the table; it is supplied in the
lookup function as well.  The actual multi-level table data is just
the adjusted enumeration value, one byte per entry.

<<Unicode property exports for generator>>=
/** Convert a range table with 8-bit data to a multi-level table.
  * The range table \p tab/\p len must be sorted as per \ref uni_cmprng_dat8,
  * with no duplicate ranges.  All values outside of the range, as well as
  * values matching the default (\p def) are converted to zeroes, and the
  * non-default values are incremented by one.  The resulting table is
  * returned, and its length is returned in \p ml_len if not NULL */
uint32_t *uni_rng_dat8_to_multi(const uni_chrrng_dat8_t *tab, uint32_t tab_len,
                                uint32_t *ml_len, uint8_t def);
@

<<Unicode property functions for generator>>=
uint32_t *uni_rng_dat8_to_multi(const uni_chrrng_dat8_t *tab, uint32_t tab_len,
                                uint32_t *ml_len, uint8_t def)
{
  uint32_t low, high, len, i;
  uint32_t *ml;
  uint8_t *bits;

  /* degenerate case:  always out-of-range */
  if(!tab_len) {
    uni_bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  for(i = tab_len; i > 0; i--)
    if(tab[i - 1].dat != def)
      break;
  high = tab[i - 1].high;
  for(i = 0; i < tab_len; i++)
    if(tab[i].dat != def)
      break;
  low = tab[i].low;
  len = high - low + 1;
  inisize(bits, len);
  /* Optimize(maybe): only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  for(; i < tab_len; i++)
    if(tab[i].dat != def)
      memset(bits + tab[i].low - low, tab[i].dat + 1, tab[i].high - tab[i].low + 1);
  uni_bits_to_multi(bits, len, low, high, 0, 0, &ml, ml_len);
  free(bits);
  return ml;
}
@

<<UCD parser local functions>>=
static void fixup_rng_dat8(prop_t *p)
{
  uint32_t i;
  qsort(p->rng_dat8, p->len, sizeof(uni_chrrng_dat8_t), uni_cmprng_dat8);
  /* starting at top means only optimized entries are memmove'd */
  for(i = p->len - 1; i > 0; i--) {
    uint32_t j = i;
    while(i > 0 && p->rng_dat8[i - 1].high == p->rng_dat8[i].low - 1 &&
          p->rng_dat8[i - 1].dat == p->rng_dat8[i].dat)
      i--;
    if(i == j)
      continue;
    p->rng_dat8[i].high = p->rng_dat8[j].high;
    if(j < p->len - 1)
        movebuf(p->rng_dat8 + i + 1, p->rng_dat8 + j + 1, p->len - (j + 1));
    p->len -= j - i;
    if(!i)
      break;
  }
}
@

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng_dat8) {
    fixup_rng_dat8(&parsed_props[i]);
    parsed_props[i].mt = uni_rng_dat8_to_multi(parsed_props[i].rng_dat8,
                                               parsed_props[i].len,
                                               &ml_len, parsed_props[i].def);
}
@

There are no 16-bit and 32-bit values to store yet, but here is a good
a place as any to write the multi-tab conversion functions.  Since
these are not for enumerations, no special handling is provided for
the default value:  it is always assumed to be zero.  Therefore the
multi-level table data is just the raw native-endian 16-bit or 32-bit
data.

<<Unicode property exports for generator>>=
/** Convert a range table with 16-bit data to a multi-level table.
  * The range table \p tab/\p tab_len must be sorted as per \ref uni_cmprng_dat16,
  * with no duplicate ranges.  The default is assumed to be zero.  The
  * resulting table is returned, and its length is returned in \p ml_len
  * if not NULL */
uint32_t *uni_rng_dat16_to_multi(const uni_chrrng_dat16_t *tab, uint32_t tab_len,
                                 uint32_t *ml_len);
@

<<Unicode property functions for generator>>=
uint32_t *uni_rng_dat16_to_multi(const uni_chrrng_dat16_t *tab, uint32_t tab_len,
                                 uint32_t *ml_len)
{
  uint32_t low, high, len, i, j;
  uint32_t *ml;
  uint16_t *bits;

  /* degenerate case:  always out-of-range */
  if(!tab_len) {
    uni_bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  for(i = tab_len; i > 0; i--)
    if(tab[i - 1].datl || tab[i - 1].dath)
      break;
  high = tab[i - 1].high;
  for(i = 0; i < tab_len; i++)
    if(tab[i].datl || tab[i].dath)
      break;
  low = tab[i].low;
  len = high - low + 1;
  inisize(bits, len);
  /* Optimize(maybe): only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  for(; i < tab_len; i++)
    if(tab[i].datl || tab[i].dath)
      for(j = tab[i].low; j <= tab[i].high; j++)
        bits[j - low] = (tab[i].dath << 8) + tab[i].datl;
  uni_bits_to_multi((uint8_t *)bits, len * 2, low * 2, high * 2, 0, 0, &ml, ml_len);
  free(bits);
  return ml;
}
@

<<Unicode property exports for generator>>=
/** Convert a range table with 32-bit data to a multi-level table.
  * The range table \p tab/\p tab_len must be sorted as per \ref uni_cmprng_dat32,
  * with no duplicate ranges.  The default is assumed to be zero.  The
  * resulting table is returned, and its length is returned in \p ml_len
  * if not NULL */
uint32_t *uni_rng_dat32_to_multi(const uni_chrrng_dat32_t *tab, uint32_t tab_len,
                                 uint32_t *ml_len);
@

<<Unicode property functions for generator>>=
uint32_t *uni_rng_dat32_to_multi(const uni_chrrng_dat32_t *tab, uint32_t tab_len,
                                 uint32_t *ml_len)
{
  uint32_t low, high, len, i, j;
  uint32_t *ml;
  uint32_t *bits;

  /* degenerate case:  always out-of-range */
  if(!tab_len) {
    uni_bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  for(i = tab_len; i > 0; i--)
    if(tab[i - 1].dat)
      break;
  high = tab[i - 1].low + tab[i - 1].len;
  for(i = 0; i < tab_len; i++)
    if(tab[i].dat)
      break;
  low = tab[i].low;
  len = high - low + 1;
  inisize(bits, len);
  /* Optimize(maybe): only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  for(; i < tab_len; i++)
    if(tab[i].dat)
      for(j = 0; j <= tab[i].len; j++)
        bits[tab[i].low - low + j] = tab[i].dat;
  uni_bits_to_multi((uint8_t *)bits, len * 4, low * 4, high * 4, 0, 0, &ml, ml_len);
  free(bits);
  return ml;
}
@

In addition to the property tables, a simple query function is
printed.  This calls a generic search function using the multi-level
table.  The test cases are printed as well, and added to the boolean
tests.

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng_dat8) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    const char *lname = i < num_prop_aliases ? prop_aliases[i].long_name :
                                               parsed_props[i].name;
    char nbuf[64];
    sprintf(nbuf, "uni_%s_rng.gen.c", name);
    open_wf(of, nbuf);
    fprintf(of, "#include \"uni_prop.h\"\n\n"
		"const uni_chrrng_dat8_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].len; j++) {
      uint32_t val = parsed_props[i].rng_dat8[j].dat;
      uni_alias_t *aliases = val_aliases[i];
      fprintf(of, "\t{ 0x%04X, 0x%04X, ",
                  parsed_props[i].rng_dat8[j].low,
	          parsed_props[i].rng_dat8[j].high);
      if(!aliases || (isdigit(aliases[0].short_name[0]) &&
                      !strchr(aliases[0].short_name, '.')))
        fprintf(of, "%d", (int)val);
      else {
        const uni_alias_t *alias = &aliases[val];
	const char *val_name = alias->short_name;
        if(isdigit(val_name[0]))
	  val_name = alias->long_name;
	if(strchr(val_name, '-')) {
	  const char *np;
	  /* slow, but necessary with e.g. ID_Restrict_Type */
	  fprintf(of, "UNI_%s_", name);
	  for(np = val_name; *np; np++)
	    putc(*np == '-' ? '_' : *np, of);
	} else
	  fprintf(of, "UNI_%s_%s", name, val_name);
      }
      fprintf(of, " }%s\n", j < parsed_props[i].len - 1 ? "," : "");
    }
    fputs("};\n", of);
    fclose(of);
    fprintf(gen_h, "/** Sorted range table for Unicode enumerated %s property */\n" 
                   "extern const uni_chrrng_dat8_t uni_%s_rng[];\n"
		   "/** Length of \\ref uni_%s_rng */\n"
		   "#define uni_%s_rng_len %d /* %d lookups max */\n"
		   "/** Retrieve value of enumerated %s property */\n",
		   lname, name, name, name, parsed_props[i].len,
		   lg2(parsed_props[i].len + 1), lname);
    if(enum_vals[i])
      fprintf(gen_h, "#define uni_%s_of(x) "
                                   "(uni_%s_t)uni_x_of(x, uni_%s_mtab, %d)\n",
		     name, name, name, parsed_props[i].def);
    else
      fprintf(gen_h, "#define uni_%s_of(x) uni_x_of(x, uni_%s_mtab, %d)\n",
		     name, name, parsed_props[i].def);
    print_mtab(name, lname, parsed_props[i].mt, gen_h);
    fprintf(tstf, "dat8(%s, %d);\n", name, parsed_props[i].def);
  }
}
@

<<Unicode property exports>>=
/** Look up an 8-bit value in a multi-level table.
  * The value associated with \p cp is looked up in \p tab, which is assumed
  * to follow the conventions of \ref uni_rng_dat8_to_multi.  That is, zeroes
  * (and lookup failures) are converted to \p def, and non-zero values
  * are decremented by one */ 
int uni_x_of(uint32_t cp, const uint32_t *tab, uint8_t def);
@

<<Unicode property functions>>=
int uni_x_of(uint32_t cp, const uint32_t *tab, uint8_t def)
{
  const uint8_t *mr;
  uint8_t mv = uni_multi_tab_lookup(tab, cp, &mr, 0);
  if(mr)
    mv = *mr;
  if(!mv)
    return def;
  else
    return mv - 1;
}
@

<<range table data for [[i]]>>=
uni_x_of(i, mtab, def)
@

\lstset{language=txt}
<<FIXME>>=
IndicSyllabicCategory.txt: field 2 == InSC (default = Other)
IndicMatraCategory.txt: field 2 == InMC (default = NA)
@

\subsection{Support}

Some of the above properties are meant to be used as part of larger
algorithms, rather than on their own.  A few of the default
implementations of those algorithms are implemented here, mostly as an
example.

The first algorithm is for grapheme cluster boundary determination
(UAX~\#29).  A grapheme cluster is essentially a logical character,
where code points are the physical characters. They usually map to a
single displayed glyph.  While the standard allows for locale-specific
tailoring of this algorithm, the CLDR provides no such tailoring at
present, so no support is provided.

UAX~\#18 (Regular Expressions), RL3.2 does provide tailoring, by
calling all multi-character collation elements grapheme clusters.
However, I prefer calling them multi-character collation elements,
since some common examples (like ll and ch) appear as multiple
distinct glyphs.  On the other hand, it might be because some
multi-character clusters have accents applied to the cluster as a
whole rather than individual characters.  If that is a problem, the
application can deal with breaking differently.  As stated above, this
routine serves mostly as an example.

The first two rules merely state that grapheme clusters can't escape
the boundaries of the text.  There is no point in implementing them.
The rest deal only with pairs of characters, so the boundary
determination function does a switch on the first character, for those
rules that have a first character, and then on any failed rules, the
second character is checked.  The rules are not repeated here; look
them up in the standard.  Instead, the implementation of each rule is
labeled with the rule number (for UAX~\#29, revision 23).  While the
CLDR root locale has this in a machine-readable format, it expects
regular expressions to be used for checks.  It is better to just
update the routine if the standard ever changes.

In order to avoid looking up GCB for all middle characters twice, the
next character's GCB property is returned, and expected to be passed
in the next time around.  Speaking of the GCB property, its use pulls
in a property table, so the function is in its own object file.

<<Library [[uni]] Members>>=
gcbrk.o
@

\lstset{language=C}
<<Unicode property exports>>=
/** Find boundary between grapheme clusters.
  * Grapheme clusters always start at the beginning of text and end at
  * the end; this function is for finding breaks in the middle.  Pass in
  * the two code points surrounding the check point (\p prev, \p next), for
  * every pair of code points, in order, from the start of text.  Pass in
  * zero for the first \p prevret, and the return code from the previous
  * call otherwise.  The return value is greater than zero if and only if
  * \p prev and \p next are in different grapheme clusters. Unicode
  * defines two types of grapheme clusters:  legacy and extended.  To
  * select the legacy form, set \p legacy to true.  Passing values other
  * that zero to \p prevret is optional and purely for performance: this
  * function will return accurate results at random input locations by
  * passing zero for \p prevret */
int uni_gc_brk(uint32_t prev, uint32_t next, int prevret, int legacy);
@

<<gcbrk.c>>=
<<Common C Header>>
#include "uni_prop.h"

int uni_gc_brk(uint32_t prev, uint32_t next, int prevret, int legacy)
{
  uni_GCB_t pgcb = !prevret ? uni_GCB_of(prev) : prevret < 0 ? -prevret - 1 :
                                                               prevret - 1,
            ngcb = uni_GCB_of(next);
  switch(pgcb) {
    case UNI_GCB_CR:
      if(ngcb == UNI_GCB_LF) /* GB3 */
        return -(ngcb + 1);
      /* fall through */
    case UNI_GCB_LF:
    case UNI_GCB_CN:
      return ngcb + 1; /* GB4 */
    case UNI_GCB_L:
      if(ngcb == UNI_GCB_L || ngcb == UNI_GCB_V || ngcb == UNI_GCB_LV ||
         ngcb == UNI_GCB_LVT) /* GB6 */
	return -(ngcb + 1);
      break;
    case UNI_GCB_LV:
    case UNI_GCB_V:
      if(ngcb == UNI_GCB_V || ngcb == UNI_GCB_T) /* GB7 */
        return -(ngcb + 1);
      break;
    case UNI_GCB_T:
    case UNI_GCB_LVT:
      if(ngcb == UNI_GCB_T) /* GB8 */
        return -(ngcb + 1);
      break;
    case UNI_GCB_RI:
      if(ngcb == UNI_GCB_RI)
        return -(ngcb + 1);
      break;
    case UNI_GCB_PP:
      if(!legacy)
        return -(ngcb + 1); /* GB9b */
      break;
    default:
      break;
  }
  if(ngcb == UNI_GCB_CR || ngcb == UNI_GCB_LF || ngcb == UNI_GCB_CN)
    return ngcb + 1; /* GB5 */
  if(ngcb == UNI_GCB_EX || /* GB9 */
     (ngcb == UNI_GCB_SM && !legacy)) /* GB9b */
    return -(ngcb + 1);
  return ngcb + 1; /* GB10 */
}
@

The next larger element of text that is useful to extract is a word.
One way to do this is to simply assign a boolean property to each
character which makes it part of or not part of a word, and assign
word boundaries after any contiguous group.  UAX~\#18, RL1.4 defines
such a class.  The only complication is that non-spacing marks inherit
their class from their preceding character.  This is reflected in a
special return value.  Once again, a property table or two is pulled
in, so the function is in its own object file.

<<Unicode property exports>>=
/** Perform a simple Unicode word boundary check.
  * If \p cp is definitely not part of a word, zero is returned.  If
  * it is definitely part of a word, a value greater than zero is returned.
  * Otherwise, a value less than zero is returned, and \p cp has the
  * same status as the previous character in the string, or, if is the
  * first character, it is definitely part of a word */
int uni_is_simple_word(uint32_t cp);
@

<<Library [[uni]] Members>>=
wb_simp.o
@

<<wb_simp.c>>=
<<Common C Header>>
#include "uni_prop.h"

int uni_is_simple_word(uint32_t cp)
{
  if(cp == 0x200C || cp == 0x200D)
    return 1;
  uni_gc_t gc = uni_gc_of(cp);
  if(gc != UNI_gc_Mc && (uni_gc_trans[UNI_gc_M] & (1 << gc)))
    return -1;
  return !!((uni_gc_trans[UNI_gc_L] & (1 << gc)) ||
            (uni_gc_trans[UNI_gc_N] & (1 << gc)));
}
@

However, a slightly more sophisticated (but still useless in most
eastern locales) algorithm is also specified by UAX~\#29.  Word
boundaries may also be altered using locale-specific overrides. The
CLDR provides a few, but its entire specification of word boundary
rules expects a regular expression storage format.  This library does
not use or implement pure regular expressions, though, so reading the
data files will not work. Instead, the locale-specific alterations are
provided in a different form: alternate property lookup tables and
special lookup codes to enable each of the current overrides.  This
has the huge disadvantage that future revisions of the CLDR will
likely require additional hand-coded overrides, which must be detected
by human perusal of the CLDR files. The regular expressions will not
be provided as a property, either, so an application which needs to
use the regular expression form will either have to use ICU or read
the CLDR manually.

Once again the first two rules are ignored, as they simply express
that words cannot extend past text boundaries.  The rest of the rules
are implemented in a switch and post-switch check, just like the
grapheme cluster check.  In addition to using rules from UAX~\#29,
revision 23, the CLDR overrides from CLDR version 22.1 are
implemented.  Note that the CLDR does not implement full Unicode 6.3
rules as of version 24, but the rule changes for 6.3 are not
incompatible with the CLDR overrides.

Rules WB6, WB7, WB11, and WB12 are difficult, though:  they require
three characters rather than just two to make their decision.  When
combined with WB4, essentially unlimited lookahead is required.  To
deal with this, a special token is returned between the first
and second character clusters.  When the next non-token is returned,
the first token becomes the same, and all remaining tokens become
negative (i.e., there are no breaks between tokens, but the first token
may become a break).

As with the other functions, a different property table is pulled in,
so the function gets its own object file.

<<Unicode property exports>>=
/** Perform full Unicode word boundary detection.
  * The beginning and end of a string are always word boundaries. Otherwise,
  * they happen between code points; that is what this function detects.
  * Unlike \ref uni_is_simple_word, there are no distinct non-word code
  * points, just boundaries between words.  The code points to check
  * between (\p prev, \p next) must be accompanied by the previous return
  * value (zero initially).  If the return value is less than -1, no
  * break occurs.  If it is greater than 1, a break occurs.  If it is -1
  * or 1, the first one in a group matches the next one outside of the
  * -1 to 1 range, and all others in the group are no break.  That is,
  * -1 -1 -1 -2 means no break occurs.  -1 -1 -1 2 means the first -1 breaks,
  * but the middle two do not.  If the string ends before resolution, there
  * is no break at the first -1/1.  Due to the need for backtracking and state
  * tracking, it is not generally possible to begin detecting words in
  * the middle of a string.  Instead, it must start at the latest at the
  * last known word boundary from a return value less than -1.
  * Localization can be accomplished by passing in an alternate lookup table
  * for the WB property in \p tab.  If NULL is passed in, \ref uni_WB_mtab
  * is used */
int uni_word_brk(uint32_t prev, uint32_t next, int prevret, const uint32_t *tab);
@

<<Library [[uni]] Members>>=
wbrk.o
@

<<Unicode property exports>>=
/** \addtogroup uni_prop_ext Enumeration Property Extensions
  * Extra enumeration literals for locale and compatibility */
/** @{ */
/* < 6.3 compat */
#if UNI_UCD_VER < 630
#define UNI_WB_SQ UNI_WB_MB /**< New symbol introduced in Unicode 6.3 */
#define UNI_WB_DQ UNI_WB_XX /**< New symbol introduced in Unicode 6.3 */
#define UNI_WB_HL UNI_WB_LE /**< New symbol introduced in Unicode 6.3 */
#endif

/* extra WB values for ja locale */
#define UNI_WB_HI UNI_NUM_WB /**< Hiragana characters (locale extension) */
#define UNI_WB_ID (UNI_WB_HI + 1) /**< CJK Ideographs (locale extension) */
/* extra WB value for 6.3 */
#define UNI_WB_SQ2 (UNI_WB_HI + 2) /**< Support for Unicode 6.3 algorithm */
/** @} */
@

<<wbrk.c>>=
<<Common C Header>>
#include "uni_prop.h"

int uni_word_brk(uint32_t prev, uint32_t next, int prevret, const uint32_t *tab)
{
  if(!tab)
    tab = uni_WB_mtab;
#define get_WB(x) uni_x_of(x, tab, UNI_WB_XX)
  uni_WB_t pwb, nwb = get_WB(next);
  if(prevret > 1)
    pwb = prevret - 2;
  else if(prevret < -1)
    pwb = -prevret - 2;
  else if(prevret)
    pwb = UNI_WB_LE; /* don't care */
  else
    pwb = get_WB(prev);
  switch(pwb) {
    case UNI_WB_CR:
      if(nwb == UNI_WB_LF) /* WB3 */
        return -(nwb + 2);
      /* fall through */
    case UNI_WB_LF:
    case UNI_WB_NL:
      return nwb + 2; /* WB3a */
    default:
      break;
  }
  if(nwb == UNI_WB_Extend || nwb == UNI_WB_FO) /* WB4 */
    return prevret && prevret <= 1 ? prevret : -(pwb + 2);
  if(prevret == 1
#if UNI_UCD_VER >= 630
     && get_WB(prev) != UNI_WB_DQ
#endif
     ) /* WB5, WB6 */
    return nwb != UNI_WB_LE && nwb != UNI_WB_HL ? nwb + 2 : -(nwb + 2);
  else if(prevret == -1) /* WB11, WB12 */
    return nwb != UNI_WB_NU ? nwb + 2 : -(nwb + 2);
#if UNI_UCD_VER >= 630
  else if(prevret == 1) /* WB7b, WB7c */
    return nwb != UNI_WB_HL ? nwb + 2 : -(nwb + 2);
#endif
  switch((int)pwb) { /* cast is to allow UNI_WB_ID w/o warning */
#if UNI_UCD_VER >= 630
    case UNI_WB_SQ2: /* WB7/WB7a */
      return nwb != UNI_WB_LE && nwb != UNI_WB_HL ? nwb + 2 : -(nwb + 2);
    case UNI_WB_HL:
      if(nwb == UNI_WB_DQ)
        return 1; /* WB7b/c */
      else if(nwb == UNI_WB_SQ)
        return -(UNI_WB_SQ2 + 2); /* WB7a */
      /* fall through */
#endif
    case UNI_WB_LE:
      switch(nwb) {
        case UNI_WB_LE: /* WB5 */
#if UNI_UCD_VER >= 630
	case UNI_WB_HL:
#endif
	case UNI_WB_NU: /* WB9 */
	  return -(nwb + 2);
	case UNI_WB_ML: /* WB6/WB7 */
	case UNI_WB_MB:
#if UNI_UCD_VER >= 630
	case UNI_WB_SQ:
#endif
	  return 1;
	case UNI_WB_EX: /* WB13a */
	  return -(nwb + 2);
	default:
	  break;
      }
      break;
    case UNI_WB_NU:
      switch(nwb) {
        case UNI_WB_NU: /* WB8 */
	case UNI_WB_LE: /* WB10 */
#if UNI_UCD_VER >= 630
	case UNI_WB_HL:
#endif
	  return -(nwb + 2);
	case UNI_WB_MN: /* WB11/WB12 */
	case UNI_WB_MB:
#if UNI_UCD_VER >= 630
	case UNI_WB_SQ:
#endif
	  return -1;
	case UNI_WB_EX: /* WB13a */
	  return -(nwb + 2);
	default:
	  break;
      }
      break;
    case UNI_WB_KA:
      if(nwb == UNI_WB_KA || /* WB13 */
         nwb == UNI_WB_EX) /* WB13a */
        return -(nwb + 2);
       break;
    case UNI_WB_HI:
      if(nwb == UNI_WB_HI) /* WB13d; ja only */
        return -(nwb + 2);
      break;
    case UNI_WB_ID:
      if(nwb == UNI_WB_ID) /* WB13e; ja only */
        return -(nwb + 2);
      break;
    case UNI_WB_EX:
      if(nwb == UNI_WB_EX || /* WB13a */
         nwb == UNI_WB_LE || nwb == UNI_WB_NU || nwb == UNI_WB_KA ||
	 nwb == UNI_WB_HL) /* WB13b */
        return -(nwb + 2);
      break;
    case UNI_WB_RI:
      if(nwb == UNI_WB_RI) /* WB13c */
        return -(nwb + 2);
      break;
    default:
      break;
  }
  if(nwb == UNI_WB_CR || nwb == UNI_WB_LF || nwb == UNI_WB_NL)
    return nwb + 2; /* WB3b */
  return nwb + 2; /* WB14 */
}
@

The next larger boundary is a sentence, also specfified by UAX~\#29. 
Again, CLDR overrides may be provided, but they are in an unusable
format.  Once again simple locale overrides are supported by replacing
the lookup table.  The version 24 CLDR introduces known abbreviations
(mostly titles) as non-sentence overrides.  This is present for seven
additional locales.  To support this, the routine does dead-simple
matching.  No case folding, normalization, word boundary detection, or
anything else is done.  The matching algorithm is also suboptimal, but
good enough given that sentence boundary detection is pretty awful.

<<Unicode property exports>>=
<<[[uni_sb_locale_t]]>>
@

<<[[uni_sb_locale_t]]>>=
/** Localization parameters for sentence break detection */
typedef struct {
  const uint32_t *tab; /**< lookup table for SB property if non-NULL */
  /** If non-NULL, a table of strings to suppress breaks after.
   ** This table is UTF-8 encoded and sorted by byte order */
  const uint8_t * const * suppressions;
  unsigned int num_sup; /**< length of suppressions table */
} uni_sb_locale_t;
@

<<Known Data Types>>=
uni_sb_locale_t,%
@

This break algorithm is the most complex so far.  There are patterns
other than simply the grapheme cluster pattern that require state
tracking, so instead of returning the previous SB property value, a
state identifier is returned.  The only rules which separate sentences
are SB4 and SB11.  SB4 is unconditional, so it is easy to implement.
However, it should be noted that SB4 assumes that explicit newlines
separate paragraphs.  SB10 and SB9 simply force all optional matches
in SB11 to match maximally.  SB6 is easy to implement with only one
pair.  SB8a is just a few extra terms to match with SB11. SB7 and SB8
are the difficult ones.  SB7 needs to remember if an ATerm was
preceded by an Upper.  SB8 requires backtracking in a similar manner
to the word boundary, and can be done by emitting a token where a
split might happen, and converting that to a split if the SB8 pattern
does not match.

<<Unicode property exports>>=
/** Perform full Unicode sentence boundary detection.
  * The beginning and end of a string are always sentence boundaries.
  * Otherwise, they happen between code points; that is what this function
  * detects.  The code points to check between (\p prev, \p next) must be
  * accompanied by the previous return value (zero initially).  If the
  * return value is less than -1, no break occurs.  If it is greater than 1,
  * a break occurs.  If it is -1 or 1, the first one in a group matches the
  * next one outside of the -1 to 1 range, and all others in the group are
  * no break, unless otherwise indicated by a sign change.  If the
  * sign changes (i.e., -1 to 1 or 1 to -1), a break occured between the
  * \a prior pair, but the first 1 or -1 in the group is not yet resolved.
  * If the string ends before resolution, the sign of the original 1 or -1
  * determines whether or not there is a break.
  * FIXME: correct above explanation and add example.
  * Due to the need for backtracking and state tracking, it is not
  * generally possible to begin detecting sentences in the middle of a
  * string.  Instead, it must start at the latest at the 
  * last known sentence boundary from a return value less than -1, and
  # perhaps even earlier.  Localization can be accomplished by passing in a
  * non-NULL \p locale.  If \p locale is non-NULL and \p locale->tab is
  * also non-NULL, it is used as the SB property lookup table (otherwise,
  * \ref uni_SB_mtab is used).  If \p locale is non-NULL and its suppression
  * table is present and non-empty, suppressions may be tracked.  These
  * are tracked separately from the main state, using \p *sup_state. This
  * must be initialized to zero at the start of matching.  If \p sup_state
  * is NULL, suppression tracking will not be performed, even if specified
  * by \p locale */
int uni_sentence_brk(uint32_t prev, uint32_t next, int prevret,
                     const uni_sb_locale_t *locale, uint32_t *sup_state);
@

<<Library [[uni]] Members>>=
sbrk.o
@

<<sbrk.c>>=
<<Common C Header>>
#include "uni_prop.h"

enum sb_state {
  sb_st_AT_CL_SP_X = 1, /* deferred */
  sb_st_none = 2,
  sb_st_UP, /* SB7 */
  sb_st_UP_AT, /* SB7-SB11 */
  sb_st_AT, /* SB6, SB8-SB11 */
  sb_st_AT_CL, sb_st_AT_CL_SP, /* SB8-SB11 */
  sb_st_ST, sb_st_ST_CL, sb_st_ST_CL_SP, /* SB8a-SB11 */
  sb_st_CR, sb_st_end /* SB9-SB11 */
};
int uni_sentence_brk(uint32_t prev, uint32_t next, int prevret,
                     const uni_sb_locale_t *locale, uint32_t *sup_state)
{
  const uint32_t *tab = locale && locale->tab ? locale->tab : uni_SB_mtab;
#define get_SB(x) uni_x_of(x, tab, UNI_SB_XX)
  uni_SB_t psb, nsb = get_SB(next);
  if(prevret == 1 || prevret == -1)  {
    <<Adjust [[prevret]] at end of [[sb_st_AT_CL_SP_X]]>>
  } else if(prevret)
    psb = UNI_SB_XX; /* don't care */
  else {
    uni_SB_t tsb = nsb;
    nsb = psb = get_SB(prev);
    <<Set default sentence state>>
    nsb = tsb;
  }
  <<Perform sentence break property locale adjustments>>
  switch(psb) {
    case UNI_SB_CR:
      if(nsb == UNI_SB_LF) /* SB3 */
        return -sb_st_none;
      /* fall through */
    case UNI_SB_LF:
    case UNI_SB_SE:
      <<Set default sentence state>>
      return prevret; /* SB4 */
    default:
      break;
  }
  if(prevret == -sb_st_CR && nsb == UNI_SB_LF)
    return -sb_st_end; /* SB3 */
  if(prevret == -sb_st_end || prevret == -sb_st_CR) {
    /* SB4 */
    <<Set default sentence state>>
    return prevret;
  }
  if(nsb == UNI_SB_EX || nsb == UNI_SB_FO) { /* SB5 */
    if(!prevret) {
      <<Set default sentence state>>
      return -prevret;
    }
    return prevret <= 1 ? prevret : -prevret;
  }
  switch(prevret < 0 ? -prevret : prevret) {
    case 0:
    case sb_st_none:
      break;
    case sb_st_UP:
      if(nsb == UNI_SB_AT)
        return -sb_st_UP_AT; /* SB7... */
      break;
    case sb_st_UP_AT:
      if(nsb == UNI_SB_UP)
        return -sb_st_UP; /* SB7 */
      /* fall through */
    case sb_st_AT:
      if(nsb == UNI_SB_NU)
        return -sb_st_none; /* SB6 */
      if(nsb == UNI_SB_CL)
        return -sb_st_AT_CL; /* SB8-SB11 */
      /* fall through */
    case sb_st_AT_CL:
      if(nsb == UNI_SB_SP)
        return -sb_st_AT_CL_SP; /* SB8-SB11 */
      if(nsb == UNI_SB_CL)
        return -sb_st_AT_CL; /* SB8-SB11 */
      /* fall through */
    case sb_st_AT_CL_SP:
      switch(nsb) {
        case UNI_SB_SP:
	  return -sb_st_AT_CL_SP; /* SB8-SB11 */
	case UNI_SB_LE:
	case UNI_SB_UP:
	case UNI_SB_SE:
	case UNI_SB_CR:
	case UNI_SB_LF:
	case UNI_SB_ST:
	case UNI_SB_AT:
	  break;
	case UNI_SB_LO:
	  return -sb_st_none; /* SB8 */
	case UNI_SB_SC:
	  return -sb_st_AT_CL_SP_X; /* SB8/SB8a */
	default:
	  return sb_st_AT_CL_SP_X; /* SB8 */
      }
      goto at_st_common;
    case sb_st_AT_CL_SP_X:
      switch(nsb) {
        case UNI_SB_UP:
          /* return -sb_st_UP; */
	  return -prevret; /* SB8a/SB11 */
        case UNI_SB_LE:
	case UNI_SB_SE:
	case UNI_SB_CR:
	case UNI_SB_LF:
          /* return -sb_st_none; */
	  return -prevret; /* SB8a/SB11 */
        case UNI_SB_ST:
          /* return -sb_st_ST; */
	  return -prevret; /* SB8a/SB11 */
        case UNI_SB_AT:
          /* return -sb_st_AT; */
	  return -prevret; /* SB8a/SB11 */
        case UNI_SB_LO:
          return -sb_st_none; /* SB8 */
        default:
	  return prevret; /* SB8 */
      }
      /* NOTREACHED */
    case sb_st_ST:
      if(nsb == UNI_SB_CL)
        return -sb_st_ST_CL; /* SB8a-SB11 */
      /* fall through */
    case sb_st_ST_CL:
      if(nsb == UNI_SB_SP)
        return -sb_st_ST_CL_SP; /* SB8a-SB11 */
      if(nsb == UNI_SB_CL)
        return -sb_st_ST_CL; /* SB8a-SB11 */
      /* fall through */
    case sb_st_ST_CL_SP:
    /* case sb_st_AT_CL_SP: */
    at_st_common:
      switch(nsb) {
        case UNI_SB_SP:
          return -sb_st_ST_CL_SP; /* SB8a-SB11 */
        case UNI_SB_SC:
          return -sb_st_none; /* SB8a */
        case UNI_SB_AT:
          return -sb_st_AT; /* SB8a */
        case UNI_SB_ST:
          return -sb_st_ST; /* SB8a */
        case UNI_SB_CR:
          return -sb_st_CR; /* SB10/SB3 */
        case UNI_SB_SE:
        case UNI_SB_LF:
          return -sb_st_end; /* SB10 */
        case UNI_SB_UP:
          return sb_st_UP; /* SB11/SB7 */
        default:
          return sb_st_none; /* SB11 */
      }
      /* NOTREACHED */
  }
  /* SB12 */
  <<Set default sentence state>>
  return -prevret;
}
@

<<Set default sentence state>>=
switch(nsb) {
  case UNI_SB_AT:
    prevret = sb_st_AT;
    break;
  case UNI_SB_ST:
    prevret = sb_st_ST;
    break;
  case UNI_SB_UP:
    prevret = sb_st_UP;
    break;
  default:
    prevret = sb_st_none;
    break;
}
@

<<Adjust [[prevret]] at end of [[sb_st_AT_CL_SP_X]]>>=
/* only needs to be done on sign change */
/* can't really detect sign change, though */
psb = get_SB(prev);
switch(psb) {
  case UNI_SB_UP:
    prevret = sb_st_UP;
    break;
  case UNI_SB_LE:
  case UNI_SB_SE:
  case UNI_SB_CR:
  case UNI_SB_LF:
  /* case UNI_SB_LO: */ /* LO already mapped to sb_st_none */
    prevret = sb_st_none;
    break;
  case UNI_SB_ST:
    prevret = sb_st_ST;
    break;
  case UNI_SB_AT:
    prevret = sb_st_AT;
    break;
  default: /* shut gcc up */
    break;
}
@

The suppression matcher always finds the lowest array entry which
matches the text so far.  Once a character mismatches, it can move up
in the array until a match is found or the prefix no longer matches.
This is inefficient if there are a lot of entries with the same
starting text.  Even more inefficient is the method of dealing with
match failure.  In that case, the text matched so far is advanced one
character at a time until a match is found, or all previously matched
text is exhausted.  Once a suppression is fully matched, a forced
non-break is returned, and matching is reset.  This means that
multi-part matches are not supported.  If this becomes a problem, the
[[sup_state]] reset upon match could be removed.

<<Perform sentence break property locale adjustments>>=
if(locale && sup_state && locale->suppressions) {
  const uint8_t * const * suppressions = locale->suppressions;
  const unsigned int num_sup = locale->num_sup;
  uint16_t match = *sup_state >> 16, pos = *sup_state & 0xffff;
  if(pos) {
    const uint8_t *sup = suppressions[match];
    unsigned int sclen;
    uint32_t sc = uni_int_utf8_decode(sup + pos, &sclen);
    if(sc == next) {
      pos += sclen;
      *sup_state = ((uint32_t)match << 16) + pos;
      goto matched1;
    } else if(!sc) {
      *sup_state = 0;
      return -sb_st_none; /* full match; no break & reset machine */
    } else {
      /* try to find another w/ same prefix */
      while(match < num_sup - 1 && !memcmp(sup, suppressions[match + 1], pos)) {
	sup = suppressions[++match];
	sc = uni_int_utf8_decode(sup + pos, &sclen);
	if(sc == next) {
	  pos += sclen;
	  *sup_state = ((uint32_t)match << 16) + pos;
	  goto matched1;
	} else if(!sc) {
	  *sup_state = 0;
	  return -sb_st_none; /* full match; no break & reset machine */
	} else if(sc > next)
	  break;
      }
      /* no luck; try to find first entry starting with already-matched chars */
      /* this is where an aho-corasik matcher would be faster, but oh well */
      unsigned int lpos, maxpos = pos;
      uni_int_utf8_decode(sup, &lpos);
      while(lpos < maxpos) {
        int l = 0, h = num_sup - 1;
	while(l <= h) {
          int m = l + (h - l) / 2, c;
	  c = memcmp(suppressions[m], sup + lpos, maxpos - lpos);
	  if(c >= 0)
	    h = m - 1;
	  else
	    l = m + 1;
	}
	while(l < num_sup && !memcmp(suppressions[l], sup + lpos, maxpos - lpos)) {
	  sc = uni_int_utf8_decode(suppressions[l] + maxpos - lpos, &sclen);
	  if(sc > next)
	    break;
	  if(sc == next) {
	    match = l;
	    pos = maxpos - lpos + sclen;
	    *sup_state = ((uint32_t)match << 16) + pos;
	    goto matched1;
	  }
	}
	uni_int_utf8_decode(sup + lpos, &sclen);
	lpos += sclen;
      }
    }
    *sup_state = 0;
  }
  /* still no luck; try to find first entry starting with current char */
  int l = 0, h = num_sup - 1;
  unsigned int sclen;
  uint32_t sc;
  while(l <= h) {
    int m = l + (h - l) / 2;
    sc = uni_int_utf8_decode(suppressions[m], &sclen);
    if(sc >= next)
      h = m - 1;
    else
      l = m + 1;
  }
  if(l < num_sup && uni_int_utf8_decode(suppressions[l], &sclen) == next) {
    match = l;
    pos = sclen;
    *sup_state = ((uint32_t)match << 16) + pos;
    goto matched1;
  }
  /* still no luck: no matches */
}
matched1:
@

The next larger boundary is a paragraph.  There is no implementation
here; it is generally up to the text file format to explicitly
indicate paragraph boundaries.  When line breaks are explicit, this is
often done using parapragh separators (gc = Zp) or multiple consective
line separators.  When they are implicit, these methods may also be
used, or simple line separators may function as paragraph separators
as well.  As mentioned above, the sentence breaking algorithm assumes
that single line separators are paragraph separators.

When paragraphs are known, a common operation is to narrow the view of
the paragraph to a limited width.  When doing so, the paragraph must
be split into multiple lines via word wrapping (or line breaking).
Unicode UAX~\#14 specifies a line breaking algorithm of limited use.  A
real line breaking algorithm is very context dependent.  The only
application that would benefit from the algorithm without the addition
of a hyphenation dictionary is a plain text formatter, such as the
automatic wrapping mode in a plain text editor.  Nonetheless, the
following function implements the algorithm specified in UAX~\#14,
revision 32.

Like the boundary determination algorithms, locale-specific tailoring
should be supported.  Once again, the CLDR does not provide tailorings
in a usable format.  Tailorings are hand-coded and enabled using
flags. Some of the standard-recommended tailorings involve the use of
hyphenation dictionaries, which will likely never be implemented here.
In fact, breaking using a hyphenation dictionary would require use of
the same dictionary by the user as well, as Unicode provides no
mechanism for \TeX{}-like three-parameter soft hyphens.  The CLDR
provides no real rule changes to date, although version 27 has made
chinese locales use the same adjustments as ja, but ``only as a
placeholder,'' whatever that means.

This algorithm is much more complex than the boundary determination
algorithms.  Like the sentence break detector, this function uses a
state machine internally, returning state identifiers with an
appropriate sign.  Since there are quite a few potential start
symbols, there is not a separate state for each start symbol; insted,
the symbol itself is the state in that case.

In addition, there are three different potential return codes:  may
break, may not break, and must break.  It is expected that mandatory
breaks will be rare within the text being processed, so zero is
returned to indicate that, which in turn always resets the state
machine and requires reading of the next character's lb property twice.

The number rule (LB25) that is implemented here is the pattern, not
the list of simple rules.  This is not too difficult to implement (it
requires backtracking, but so do some of the routines above), is
remarked as getting better results, and is required in any case for
the test to work.

<<Unicode property exports>>=
/** Perform full Unicode line break opportunity detection.
  * The beginning and end of a string are always line break opportunities.
  * Otherwise, they happen between code points; that is what this function
  * detects.  The code points to check between (\p prev, \p next) must be
  * accompanied by the previous return value (zero initially).  If the
  * return value is less than -1, no simple break is possible.  If it is
  * greater than 1, a simple break is possible.  If it is zero, a break is
  * mandatory.  If it is -1 or 1, the first one in a group matches the
  * next one outside of the -1 to 1 range, and all others in the group are
  * no break, unless otherwise indicated by a sign change.  If the
  * sign changes (i.e., -1 to 1 or 1 to -1), a break occured between the
  * \a prior pair, but the first 1 or -1 in the group is not yet resolved.
  * If the string ends before resolution, the sign of the original 1 or -1
  * determines whether or not there is a break.
  * FIXME: correct above explanation and add example.
  * Due to the need for backtracking and state tracking, it is not
  * generally possible to begin detecting line break opportunities in
  * the middle of a string.  Instead, it must start at the latest at the 
  * last known break opportunity from a return value less than -1, and
  * perhaps even earlier.  Localization can be accomplished by passing in a
  * non-NULL \p tab to specify the lb property.  If NULL is passed in,
  * \ref uni_lb_mtab is used */
int uni_line_brk(uint32_t prev, uint32_t next, int prevret, const uint32_t *tab);
@

<<Library [[uni]] Members>>=
lbrk.o
@

<<Unicode property exports>>=
/** \addtogroup uni_prop_ext */
/** @{ */
#define UNI_lb_HH UNI_NUM_lb /**< True hyphen (locale extension) */
/** @} */
@

<<lbrk.c>>=
<<Common C Header>>
#include "uni_prop.h"

typedef enum {
  lb_st_reset = 0,
  /* 2nd states */
  lb_st_PRO_OP, /* LB25 */ /* ×× NU(NU|SY|IS)*(CL|CP)?(PR|PO)? */
                             /* needs backtrack; break prev if next not NU */

  lb_st_OP_SP, /* LB14 */ /* SP* × */
  lb_st_QU_SP, /* LB15 */ /* SP* × OP */
  lb_st_CLP_SP, /* LB16 */ /* SP* × NS */
  lb_st_B2_SP, /* LB17 */ /* SP* × B2 */
  lb_st_NU_EXT_NU, lb_st_NU_EXT_SY, lb_st_NU_EXT_IS, /* LB25 */
                                          /* ×× (NU|SY|IS)*(CL|CP)?(PR|PO)? */
  lb_st_NU_EXT_CL, lb_st_NU_EXT_CP, /* LB25 */ /* × (PR|PO)? */
  lb_st_x, /* LB21a */ /* × */

  lb_num_st
} lb_state_t;

int uni_line_brk(uint32_t prev, uint32_t next, int prevret, const uint32_t *tab)
{
  if(!tab)
    tab = uni_lb_mtab;
#define get_lb(x) uni_x_of(x, tab, UNI_lb_XX)
  uni_lb_t plb, nlb = get_lb(next);
  if(prevret == -lb_st_PRO_OP) {
    /* ret was just negated; fix it to a real value */
    plb = get_lb(prev);
    if(plb == UNI_lb_SP)
      prevret = -lb_st_OP_SP; /* LB14/LB7 */
    else
      prevret = -(lb_num_st + plb); /* LB14 */
  } else if(prevret) {
    int p = prevret < 0 ? -prevret : prevret;
    if(p < lb_num_st)
      switch(p) {
        case lb_st_QU_SP:
	case lb_st_CLP_SP:
	case lb_st_B2_SP:
	case lb_st_OP_SP:
	  plb = UNI_lb_SP; /* some checks care */
	  break;
	default:
	  plb = UNI_lb_XX; /* but most don't */
      }
    else
      plb = p - lb_num_st;
  } else {
    plb = get_lb(prev);
    prevret = plb + lb_num_st;
  }
  if(nlb == UNI_lb_SA) {
    /* LB1 */
    /* note that if prev == SA, it doesn't matter (CM/AL are the same) */
    uni_gc_t gc = uni_gc_of(next);
    if(gc == UNI_gc_Mn || gc == UNI_gc_Mc)
      nlb = UNI_lb_CM;
    else
      nlb = UNI_lb_AL;
  }
  switch(plb) {
    case UNI_lb_CR:
      if(nlb == UNI_lb_LF) /* LB5 */
        return -(lb_num_st + UNI_lb_LF);
      /* fall through */
    case UNI_lb_BK: /* LB4 */
    case UNI_lb_LF: /* LB5 */
    case UNI_lb_NL: /* LB5 */
      return 0; /* LB4, LB5 */ /* this is the only place ! is returned */
    case UNI_lb_ZW:
      switch(nlb) {
	case UNI_lb_BK:
	case UNI_lb_CR:
	case UNI_lb_LF:
	case UNI_lb_NL:
	case UNI_lb_ZW:
	  return -(lb_num_st + nlb); /* LB6, LB7 */
        case UNI_lb_SP:
	  return -(lb_num_st + UNI_lb_ZW); /* LB8 */
	default:
	  return lb_num_st + nlb; /* LB8 */
      }
    default:
      if(nlb == UNI_lb_CM && plb != UNI_lb_SP) { /* LB9 */
        if(!prevret)
	  return -(lb_num_st + plb);
        return prevret <= 1 ? prevret : -prevret;
      }
      break;
  }
  switch(nlb) {
    case UNI_lb_BK:
    case UNI_lb_CR:
    case UNI_lb_LF:
    case UNI_lb_NL:
    case UNI_lb_ZW:
      return -(lb_num_st + nlb); /* LB6, LB7 */
    /* SP of LB7 is taken care of where SP allowed */
    default: /* shut gcc up */
      break;
  }
  /* LB18 (after SP) and LB31 (unspecified) and LB20 (before/after CB) break */
  switch(prevret < 0 ? -prevret : prevret) {
    case lb_num_st + UNI_lb_WJ: /* LB11 */
    case lb_num_st + UNI_lb_GL: /* LB12 */
      return -(lb_num_st + nlb);
#define LB20(x) do { \
  if((x) == UNI_lb_CB) \
    return lb_num_st + x; /* LB20 */ \
  else \
    return -(lb_num_st + x); \
} while(0)
    case lb_num_st + UNI_lb_BB:
      LB20(nlb); /* LB21 */
    case lb_num_st + UNI_lb_OP:
      if(nlb == UNI_lb_SP)
        return -lb_st_OP_SP; /* LB14/LB7 */
      if(nlb == UNI_lb_NU)
        return -lb_st_NU_EXT_NU; /* LB25 */
      return -(lb_num_st + nlb); /* LB14 */
    case lb_st_OP_SP:
      if(nlb == UNI_lb_SP)
        return -lb_st_QU_SP; /* LB14/LB7 */
      return -(lb_num_st + nlb); /* LB14 */
    case lb_num_st + UNI_lb_QU:
      if(nlb == UNI_lb_SP)
        return -lb_st_QU_SP; /* LB15/LB7 */
      return -(lb_num_st + nlb); /* LB19 */
    case lb_st_QU_SP:
      if(nlb == UNI_lb_SP)
        return -lb_st_QU_SP; /* LB15/LB7 */
      if(nlb == UNI_lb_OP)
        return -(lb_num_st + nlb); /* LB15 */
#define return_sp(x) do { \
  if((x) == UNI_lb_SP || /* LB7 */ \
     (x) == UNI_lb_WJ || /* LB11 */ \
     (x) == UNI_lb_CL || (x) == UNI_lb_CP || (x) == UNI_lb_EX || \
     (x) == UNI_lb_IS || (x) == UNI_lb_SY) /* LB13 */ \
     return -(lb_num_st + x); \
  return lb_num_st + x; /* LB18 */ \
} while(0)
      return_sp(nlb);
    case lb_num_st + UNI_lb_CL:
    finish_CL:
      if(nlb == UNI_lb_SP)
        return -lb_st_CLP_SP; /* LB16/LB7 */
      if(nlb == UNI_lb_NS || nlb == UNI_lb_CJ) /* LB1 */
        return -(lb_num_st + nlb); /* LB16 */
      break;
    case lb_st_CLP_SP:
      if(nlb == UNI_lb_SP)
        return -lb_st_CLP_SP; /* LB16/LB7 */
      if(nlb == UNI_lb_NS || nlb == UNI_lb_CJ) /* LB1 */
        return -(lb_num_st + nlb); /* LB16 */
      return_sp(nlb);
    case lb_num_st + UNI_lb_CP:
    finish_CP:
      if(nlb == UNI_lb_SP)
        return -lb_st_CLP_SP; /* LB16/LB7 */
      if(nlb == UNI_lb_NS || nlb == UNI_lb_CJ) /* LB1 */
        return -(lb_num_st + nlb); /* LB16 */
      if(nlb == UNI_lb_AL || nlb == UNI_lb_HL || nlb == UNI_lb_NU ||
         nlb == UNI_lb_CM || /* LB10 */
         nlb == UNI_lb_AI || nlb == UNI_lb_SG || nlb == UNI_lb_XX) /* LB1 */
	return -(lb_num_st + nlb); /* LB30 */
      break;
    case lb_num_st + UNI_lb_B2:
      if(nlb == UNI_lb_SP)
        return -lb_st_B2_SP; /* LB17/LB7 */
      if(nlb == UNI_lb_B2)
        return -(lb_num_st + nlb); /* LB17 */
      break;
    case lb_st_B2_SP:
      if(nlb == UNI_lb_SP)
        return -lb_st_B2_SP; /* LB17/LB7 */
      if(nlb == UNI_lb_B2)
        return -(lb_num_st + nlb); /* LB17 */
      return_sp(nlb);
    case lb_num_st + UNI_lb_SP:
      return_sp(nlb);
    case lb_num_st + UNI_lb_CB:
      if(nlb == UNI_lb_GL || /* LB12a */
         nlb == UNI_lb_QU) /* LB19 */
	return -(lb_num_st + nlb);
      return_sp(nlb); /* s/LB18/LB20/ */
    case lb_num_st + UNI_lb_HL:
      if(nlb == UNI_lb_HY || nlb == UNI_lb_BA)
        return -lb_st_x; /* LB21/LB21a */
      if(nlb == UNI_lb_IN || /* LB22 */
         nlb == UNI_lb_NU || /* LB23 */
	 nlb == UNI_lb_AL || nlb == UNI_lb_HL || /* LB28 */
	 nlb == UNI_lb_AI || nlb == UNI_lb_SG || nlb == UNI_lb_XX || /* LB1 */
	 nlb == UNI_lb_OP) /* LB30 */
        return -(lb_num_st + nlb);
      break;
    case lb_st_x:
      LB20(nlb); /* LB21a */
    case lb_num_st + UNI_lb_IN:
      if(nlb == UNI_lb_IN)
        return -(lb_num_st + nlb);
      break;
    case lb_num_st + UNI_lb_AL:
    case lb_num_st + UNI_lb_CM: /* LB10 */
    case lb_num_st + UNI_lb_AI: /* LB1 */
    case lb_num_st + UNI_lb_SG: /* LB1 */
    case lb_num_st + UNI_lb_XX: /* LB1 */
    case lb_num_st + UNI_lb_SA: /* LB1/LB10 */
      if(nlb == UNI_lb_IN || /* LB22 */
         nlb == UNI_lb_NU || /* LB23 */
	 nlb == UNI_lb_AL || nlb == UNI_lb_HL || /* LB28 */
	 nlb == UNI_lb_AI || nlb == UNI_lb_SG || nlb == UNI_lb_XX || /* LB1 */
	 nlb == UNI_lb_OP) /* LB30 */
	return -(lb_num_st + nlb);
      break;
    case lb_num_st + UNI_lb_ID:
      if(nlb == UNI_lb_IN || /* LB22 */
         nlb == UNI_lb_PO) /* LB23 */
	return -(lb_num_st + nlb);
      break;
    case lb_num_st + UNI_lb_NU:
    finish_NU:
      if(nlb == UNI_lb_IN || /* LB22 */
	 nlb == UNI_lb_AL || nlb == UNI_lb_HL || /* LB28 */
	 nlb == UNI_lb_AI || nlb == UNI_lb_SG || nlb == UNI_lb_XX || /* LB1 */
	 nlb == UNI_lb_OP) /* LB30 */
        return -(lb_num_st + nlb);
      if(prevret != -(lb_num_st + UNI_lb_NU) &&
         prevret != lb_num_st + UNI_lb_NU)
        break; /* finish_NU shouldn't try a number any more */
      /* fall through */
    case lb_st_NU_EXT_NU:
    case lb_st_NU_EXT_SY:
    case lb_st_NU_EXT_IS:
      if(nlb == UNI_lb_NU)
        return -lb_st_NU_EXT_NU; /* LB25 */
      if(nlb == UNI_lb_SY)
        return -lb_st_NU_EXT_SY; /* LB25 */
      if(nlb == UNI_lb_IS)
        return -lb_st_NU_EXT_IS; /* LB25 */
      if(nlb == UNI_lb_CL)
        return -lb_st_NU_EXT_CL; /* LB25 */
      if(nlb == UNI_lb_CP)
        return -lb_st_NU_EXT_CP; /* LB25 */
      /* fall through */
    case lb_st_NU_EXT_CL:
    case lb_st_NU_EXT_CP:
      if(nlb == UNI_lb_PR || nlb == UNI_lb_PO)
        return -(lb_num_st + nlb); /* LB25 */
      if(prevret == -lb_st_NU_EXT_CL)
        goto finish_CL;
      if(prevret == -lb_st_NU_EXT_CP)
        goto finish_CP;
      if(prevret == -lb_st_NU_EXT_NU)
        goto finish_NU;
      if(prevret == -lb_st_NU_EXT_IS)
        goto finish_IS;
      break; /* SY is always handled as default */
    case lb_num_st + UNI_lb_PR:
      if(nlb == UNI_lb_ID || nlb == UNI_lb_AL || nlb == UNI_lb_HL || /* LB24 */
	 nlb == UNI_lb_AI || nlb == UNI_lb_SG || nlb == UNI_lb_XX || /* LB1 */
	 nlb == UNI_lb_JL || nlb == UNI_lb_JV || nlb == UNI_lb_JT ||
	 nlb == UNI_lb_H2 || nlb == UNI_lb_H3) /* LB27 */
	return -(lb_num_st + nlb);
      if(nlb == UNI_lb_NU)
        return -lb_st_NU_EXT_NU; /* LB25 */
      if(nlb == UNI_lb_HY)
        return -(lb_num_st + nlb); /* LB21/LB25 */
      if(nlb == UNI_lb_OP)
        return lb_st_PRO_OP; /* LB25; backtracking */
      break;
    case lb_st_PRO_OP:
      if(nlb == UNI_lb_NU)
        return -lb_st_NU_EXT_NU; /* LB25 */
      if(nlb == UNI_lb_SP)
        return -prevret; /* LB14/LB7 */
       return -prevret; /* LB14 */
    case lb_num_st + UNI_lb_PO:
      if(nlb == UNI_lb_AL || nlb == UNI_lb_HL || /* LB24 */
	 nlb == UNI_lb_AI || nlb == UNI_lb_SG || nlb == UNI_lb_XX) /* LB1 */
	return -(lb_num_st + nlb);
      if(nlb == UNI_lb_NU)
        return -lb_st_NU_EXT_NU; /* LB25 */
      if(nlb == UNI_lb_HY)
        return -(lb_num_st + nlb); /* LB21/LB25 */
      if(nlb == UNI_lb_OP)
        return lb_st_PRO_OP; /* LB25; backtracking */
      break;
    case lb_num_st + UNI_lb_HY:
      if(nlb == UNI_lb_NU)
        return -lb_st_NU_EXT_NU; /* LB25 */
      if(nlb == UNI_lb_GL)
        return lb_num_st + nlb; /* LB12a/LB31 */
      break;
    case lb_num_st + UNI_lb_JL:
      if(nlb == UNI_lb_JL || nlb == UNI_lb_JV || nlb == UNI_lb_H2 ||
         nlb == UNI_lb_H3 || /* LB26 */
	 nlb == UNI_lb_IN || nlb == UNI_lb_PO) /* LB27 */
	return -(lb_num_st + nlb);
      break;
    case lb_num_st + UNI_lb_JV:
    case lb_num_st + UNI_lb_H2:
      if(nlb == UNI_lb_JV || nlb == UNI_lb_JT || /* LB26 */
	 nlb == UNI_lb_IN || nlb == UNI_lb_PO) /* LB27 */
	return -(lb_num_st + nlb);
      break;
    case lb_num_st + UNI_lb_JT:
    case lb_num_st + UNI_lb_H3:
      if(nlb == UNI_lb_JT || /* LB26 */
	 nlb == UNI_lb_IN || nlb == UNI_lb_PO) /* LB27 */
	return -(lb_num_st + nlb);
      break;
    case lb_num_st + UNI_lb_IS:
    finish_IS:
      if(nlb == UNI_lb_AL || nlb == UNI_lb_HL || /* LB29 */
	 nlb == UNI_lb_AI || nlb == UNI_lb_SG || nlb == UNI_lb_XX) /* LB1 */
	return -(lb_num_st + nlb);
      break;
    case lb_num_st + UNI_lb_RI:
      if(nlb == UNI_lb_RI) /* LB30a */
        return -(lb_num_st + nlb);
      break;
    default:
      break;
  }
  if(nlb == UNI_lb_SP || /* LB7 */
     nlb == UNI_lb_WJ || /* LB11 */
     nlb == UNI_lb_CL || nlb == UNI_lb_CP || nlb == UNI_lb_EX ||
     nlb == UNI_lb_IS || nlb == UNI_lb_SY || /* LB13 */
     nlb == UNI_lb_QU || /* LB19 */
     nlb == UNI_lb_BA || nlb == UNI_lb_HY || nlb == UNI_lb_NS ||
     nlb == UNI_lb_HH || /* LB21 */
     nlb == UNI_lb_CJ) /* LB1/LB21 */
    return prevret == lb_st_PRO_OP ? -prevret : -(lb_num_st + nlb);
  if(nlb == UNI_lb_GL && plb != UNI_lb_BA) /* SP and HY already taken care of */
    return -(lb_num_st + nlb); /* LB12a */
  return lb_num_st + nlb; /* LB21, LB31 */
}
@

\lstset{language=txt}
{\let\Tt\unimonox
<<FIXME>>=
Support u-lb locale extension.  Seems to be a reference to css3-text:
http://www.w3.org/TR/css3-text/#line-break-property
  values: auto strict normal loose
  The precise set of rules in effect for each level is up to the UA
  and should follow language conventions. However, this specification
  does require that:
    Following breaks be forbidden in ‘strict’ line breaking and
    allowed in ‘normal’ and ‘loose’: 
      breaks before Japanese small kana or the Katakana-Hiragana
      prolonged sound mark: i.e. characters with the Unicode Line Break
      property CJ. (See LineBreak.txt in [UNICODE].)  
    If the content language is Chinese or Japanese, then
    additionally allow (but otherwise forbid) for ‘normal’ and ‘loose’:
      breaks before hyphens:
        ‐ U+2010, – U+2013, 〜 U+301C, ゠ U+30A0 
    Following breaks be forbidden in ‘normal’ and ‘strict’ line
    breaking and allowed in ‘loose’:
       breaks before iteration marks:
         々 U+3005, 〻 U+303B, ゝ U+309D, ゞ U+309E, ヽ U+30FD, ヾ U+30FE
       breaks between inseparable characters such as ‥ U+2025, …
         U+2026 i.e. characters with the Unicode Line Break property IN. (See
        LineBreak.txt in [UNICODE].)  
    If the content language is Chinese or Japanese, then additionally
    allow (but otherwise forbid) for ‘loose’: 
      breaks before certain centered punctuation marks:
        : U+003A, ; U+003B, ・ U+30FB, ： U+FF1A, ； U+FF1B, ･ U+FF65,
        ! U+0021, ? U+003F, ‼ U+203C, ⁇ U+2047, ⁈ U+2048, ⁉ U+2049,
	！ U+FF01, ？ U+FF1F
      breaks before suffixes:
         % U+0025, ¢ U+00A2, ° U+00B0, ‰ U+2030, ′ U+2032, ″ U+2033,
	 ℃ U+2103, ％ U+FF05, ￠ U+FFE0
      breaks after prefixes:
         № U+2116 and all currency symbols (Unicode general category
         Sc) other than ¢ U+00A2 and ￠ U+FFE0
@
}

\subsection{Support Testing}

While the machine-readable breaking rules are unusable, the test files
are not.  A generic test driver can be used to test all of the
breaking functions, with special asides to deal with different calling
conventions.

\lstset{language=make}
<<C Test Support Executables>>=
tstbrk \
@

<<Additional Tests>>=
./tstbrk g <$(UCD_LOC)/auxiliary/GraphemeBreakTest.txt
./tstbrk w <$(UCD_LOC)/auxiliary/WordBreakTest.txt
./tstbrk s <$(UCD_LOC)/auxiliary/SentenceBreakTest.txt
./tstbrk l <$(UCD_LOC)/auxiliary/LineBreakTest.txt
@

Each line of a test file is either a comment or a test case followed
by a comment.  While the comments following test cases have a specific
format, all comments are ignored.  The line number is tracked even for
comments, though, so that error messages can show where the test
failed.  The tests run very quickly, so a dot is printed after every
10 tests to indicate that the program isn't simply exiting without
testing.

\lstset{language=C}
<<tstbrk.c>>=
<<Common C Header>>
#include "uni_prop.h"

/* longest line == 1788 chars */
char lbuf[4096];

int main(int argc, const char **argv)
{
  char tst = 'g';
  if(argc > 1)
    tst = **++argv;
  int lno;
  for(lno = 1; fgets(lbuf, sizeof(lbuf), stdin); lno++) {
    uint8_t *s = (uint8_t *)strchr(lbuf, '#');
    if(s)
      *s = 0;
    for(s = (uint8_t *)lbuf; isspace(*s); s++);
    if(!*s)
      continue;
    <<Process a break test line>>
    if(!((lno + 1) % 10)) {
      putchar('.');
      fflush(stdout);
    }
  }
  putchar('\n');
  return 0;
}
@

The test text is a string of code points, with marks for what the
breaking function should return: ÷ for 1, and × for 0.  Each string is
surrounded by ÷ to indicate the fact that breaks always occur at
start-of-text and end-of-text, which are not implemented in the braek
functions.

The main difference between the functions, other than of course the
specific function to call, is whether or not back tracking is
required.  For back tracking, the expected status at the backtrack
point is saved, and when it is resolved, it is checked.

<<Process a break test line>>=
/* ignore bot/eot marks */
unsigned int clen;
if(uni_utf8_decode(s, 4, &clen) != (tst == 'l' ? L'×' : L'÷')) {
  fprintf(stderr, "Invalid line %d: %ssot break\n", lno, tst == 'l' ? "" : "no ");
  return 1;
}
for(s += clen; isspace(*s); s++);
uint8_t *e;
for(e = s + strlen((char *)s); isspace(e[-1]); e--);
e += uni_utf8_prevc(e);
if(uni_utf8_decode(e, 4, NULL) != L'÷') {
  fprintf(stderr, "Invalid line %d: no eot break\n", lno);
  return 1;
}
while(isspace(e[-1]))
  e--;
uint32_t p, n, pp = 0, pn = 0; /* init to shut gcc up */
int ret = 0, pret = 0, pexp = 0;
n = strtoul((char *)s, (char **)&s, 16);
while(s < e) {
  p = n;
  while(isspace(*s))
    s++;
  int exp = uni_utf8_decode(s, 4, &clen) == L'÷';
  for(s += clen; isspace(*s); s++);
  n = strtoul((char *)s, (char **)&s, 16);
  switch(tst) {
    case 'g':
      ret = uni_gc_brk(p, n, ret, 0);
      break;
    case 'w':
      ret = uni_word_brk(p, n, ret, NULL);
      break;
    case 's':
      ret = uni_sentence_brk(p, n, ret, NULL, NULL);
      break;
    case 'l':
      ret = uni_line_brk(p, n, ret, NULL);
      break;
  }
  int match = 0;
  if(tst != 'g' && (ret == 1 || ret == -1)) {
    if(!pexp) {
      pexp = exp + 1;
      pp = p;
      pn = n;
      pret = ret;
      continue;
    }
    match = !exp;
    if((tst == 's' || tst == 'l') && pret != ret) {
      match = match && (pexp - 1);
      pexp = 0;
    }
  } else if(pexp) {
    match = (ret >= 0) == exp && exp == pexp - 1;
    pexp = 0;
  } else
    match = (ret >= 0) == exp;
  if(!match) {
    if((ret >= 0) == exp) {
      p = pp;
      n = pn;
    }
    fprintf(stderr, "Failed line %d %04X %s %04X\n",
                    lno, (int)p, exp ? "÷" : "×", (int)n);
    return 1;
  }
}
if(pexp && !(pexp - 1) && ((tst != 's' && tst != 'l') || pret >= 0)) {
  fprintf(stderr, "Failed line %d %04X × %04X\n", lno, (int)pp, (int)pn);
  return 1;
}
@

\section{Numeric Properties}

Numeric properties are a subset of enumerated properties.  The
property's primary values are integers or two dot-separated integers
(i.e., floating point values).  They may have non-numeric values and
aliases as well.  The main difference is that the sloppy matching
should also support matching the plain integer against any like-valued
integer (e.g., 01 matches 1) and the dotted value against any
like-valued floating point number (e.g., 01.10 matches 1.1).  In
addition, the age property is meant to match equal to or less than the
search value.  For example, 3.0 matches 1.1 as well.

\subsection{Storage Methods}

Numeric values are stored as rational numbers (fractions).  Since the
current standard has no numbers requiring denominators over 127, the
denominator is stored in a single byte.  The numerator needs no more
than 19 bits, so it is stored in the remaining 3 bytes.  The generic
32-bit data type developed in the last section can be used to store
these, by casting the 32-bit value back and forth between the native
fraction structure ([[uni_frac_t]]).

\lstset{language=C}
<<[[uni_frac_t]]>>=
/** 32-bit structure for storing rational numbers (fractions) */
typedef struct {
    int32_t num:24; /**< Numerator */
    uint32_t denom:8; /**< Denominator; could be zero if num is zero */
} uni_frac_t;
@

<<Unicode property exports for generator>>=
<<[[uni_frac_t]]>>
/** Look up rational number in a range table
  * The numerator (\p num) and denominator (\p denom) of a rational
  * number stored in \p tab/\p tab_len for \p cp is returned.  The
  * table must be sorted as per \ref uni_cmprng_dat32.  The 32-bit
  * data must be formatted as a \ref uni_frac_t structure */
void uni_chrrng_val(uint32_t cp, const uni_chrrng_dat32_t *tab,
                    uint32_t tab_len, int32_t *num, uint8_t *denom);
@

<<Known Data Types>>=
uni_frac_t,%
@

<<Unicode property functions for generator>>=
void uni_chrrng_val(uint32_t cp, const uni_chrrng_dat32_t *tab,
                    uint32_t tab_len, int32_t *num, uint8_t *denom)
{
  uint32_t res = uni_chrrng_dat32(cp, tab, tab_len);
  uni_frac_t *f = (uni_frac_t *)&res;
  if(!f->num)
    f->denom = 1;
  *num = f->num;
  *denom = f->denom;
}
@

<<Property parsed contents>>=
uni_chrrng_dat32_t *rng_dat32;
int rng_num; /* flag: rng_dat32's data is actually uni_frac_t */
@

<<UCD parser local functions>>=
static void add_dat32_rng(prop_t *p, uint32_t low, uint32_t high,
                          uint32_t dat)
{
  if(!p->max_len)
    inisize(p->rng_dat32, (p->max_len = 8));
  if(p->len) {
    uni_chrrng_dat32_t *last = &p->rng_dat32[p->len -1];
    if(last->len < 255 && last->low + last->len == low - 1 &&
       last->dat == dat) {
      if(high < last->low + 256) {
        last->len = high - last->low;
	return;
      }
      last->len = 255;
      low = last->low + 256;
    }
  }
  while(1) {
    check_size(p->rng_dat32, p->max_len, p->len + 1);
    p->rng_dat32[p->len].dat = dat;
    p->rng_dat32[p->len].low = low;
    ++p->len;
    if(high < low + 256) {
      p->rng_dat32[p->len - 1].len = high - low;
      return;
    }
    p->rng_dat32[p->len - 1].len = 255;
    low += 256;
  }
}

static void add_num_rng(prop_t *p, uint32_t low, uint32_t high,
                        int32_t num, uint8_t denom)
{
  uni_frac_t f = {num, denom};
  uint32_t *v = (uint32_t *)&f;
  if(!num) /* store 0 as 0/0 for consistency */
    f.denom = 0;
  p->rng_num = 1;
  add_dat32_rng(p, low, high, *v);
}
@

<<UCD parser local definitions>>=
#define decl_num(n) int prop_##n = -1
#define add_num(n, num, denom) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  add_num_rng(&parsed_props[prop_##n], low, high, num, denom); \
} while(0)
@

\subsection{Testing}

To test the table implementations, a different, but nearly identical,
set of routines is used compared to the ones one for booleans.

<<Functions to help test generated tables>>=
#define num(x) doit_num(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab)

static void doit_num(const char *name, const uni_chrrng_dat32_t *rng, uint32_t nent,
                     const uint32_t *mtab)
{
    uint32_t i;

    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      int32_t rn, mn;
      uint8_t rd, md;
      uni_chrrng_val(i, rng, nent, &rn, &rd);
      <<range table val for [[i]]>>;
      if(rn != mn || rd != md) {
        fprintf(stderr, "mismatch %s@%d %d/%d %d/%d\n", name, i,
	                (int)rn, (int)rd, (int)mn, (int)md);
	exit(1);
      }
    }
    /* check performance */
    int j;
    double tr, tt;
    tstart();
    int32_t mn;
    uint8_t md;
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        uni_chrrng_val(i, rng, nent, &mn, &md);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        <<range table val for [[i]]>>;
    tt = tend();
    printf("  r%.0f t%.0f %.2fx\n", tr, tt, tr / tt);
}
@

\subsection{Parsing the UCD}

Numeric properties from [[UnicodeData.txt]] are ccc (field 4) and nv
(field 9)%
\footnote{[[extracted/DerivedNumericValues.txt]] might be a better
source for nv, as it includes Unihan data.}%
.  Since ccc has aliases, it is already added as an enumeration.  The
nv field is already in normal rational form.

<<Initialize UCD files>>=
decl_num(nv);
@

<<Process a line of [[UnicodeData.txt]]>>=
if(*fields[8]) {
  int32_t num = strtol(fields[8], &s, 10);
  uint8_t denom = *s ? strtol(s + 1, NULL, 10) : 1;
  add_num(nv, num, denom);
}
@

An optional numeric property comes from
[[Unihan_RadicalStrokeCounts.txt]]: cjkRSUnicode%
\footnote{Listed in UniHan as Informative, but still needed for regex.}%
.  It is, like all unihan files, a tab-separated file, with the single
code point in U+ notation in field 1.  Note that the actual optional
property is CJK\_Radical, which I interpret as meaning the same as
cjkRSUnicode. However, I will not add a non-normative alias
(especially considering that the library currently only supports four
names, and there are already four names for this property).

Technically, this file also includes a number of other stroke count
properties (kRSAdobe\_Japan1\_6, kRSJapanese, kRSKangXi, kRSKanWa,
kRSKorean)%
\footnote{Listed in UniHan as Provisional}%
.  These are not required for regular expressions, so I will
not be provided as a property until I find a use for them.  Most of
them would be trivial to add, though, since they are the same format
as kRSUnicode.  However, kRSAdobe\_Japan1\_6 has additional
information that would turn it into a string property.

There is another, related file ([[CJKRadicals.txt]]) which provides
additional interpretation for the radical numbers returned by this
property.  Since this file's values have no property label, I will not
provide them as a property for now.

Version 7 has moved the cjkRSUnicode property to the
[[Unihan_IRGSources]] file.  For now, I will read from both files, and
whichever has the property will fill in the values.

<<UCD parser local functions>>=
static void split_line_tab(char *buf)
{
  if(!max_fields)
    inisize(fields, (max_fields = 16));
  num_fields = 0;
  while(*buf != '\t' && isspace(*buf)) buf++;
  if(!*buf || *buf == '#')
    return;
  while(1) {
    while(isspace(*buf) && *buf != '\t')
      buf++;
    char *f = buf, *nf, fc;
    for(nf = buf; *nf && *nf != '\t'; nf++);
    fc = *nf;
    buf = nf + 1;
    while(nf > f && isspace(nf[-1])) --nf;
    *nf = 0;
    check_size(fields, max_fields, num_fields + 1);
    fields[num_fields++] = f;
    if(!fc)
      return;
  }
}
@

<<Initialize Unihan files>>=
decl_num(cjkRSUnicode);
@

<<Parse Unihan files>>=
open_f("Unihan_RadicalStrokeCounts.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse Unihan cp>>
  <<Parse [[kRSUnicode]] property>>
}
fclose(f);
open_f("Unihan_IRGSources.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse Unihan cp>>
  <<Parse [[kRSUnicode]] property>>
  <<Further processing of [[IRGSources]]>>
}
fclose(f);
@

<<Parse [[kRSUnicode]] property>>=
if(!strcmp(fields[1], "kRSUnicode")) {
  /* due to complexity of this field, the num/denom is interpreted */
  /* differently: */
  /* denom is number after .; if after !., then it is negated */
  /* num is 3 byte fields; lowest field is left of . or !. */
  /* if there are two values, then the 2nd value is in the upper bytes */
  int32_t num;
  uint8_t denom;
  num = strtol(fields[2], &s, 10);
  if(*s == '!')
    denom = -strtol(s + 2, &s, 10);
  else
    denom = strtol(s + 1, &s, 10);
  if(*s) {
    num |= strtol(s, &s, 10) << 8;
    if(*s == '!')
      num |= -strtol(s + 2, &s, 10) << 16;
    else
      num |= strtol(s + 1, &s, 10) << 16;
  }
  add_num(cjkRSUnicode, num, denom);
}
@

<<Parse Unihan cp>>=
split_line_tab(lbuf);
if(num_fields < 2)
  continue;
low = high = strtol(lbuf + 2, &s, 16);
if(!low || *s) { /* should never happen, but if it does: ignore */
  fprintf(stderr, "bad col 1: %s\n", lbuf);
  continue;
}
@

<<Unicode property exports>>=
/** Extract first number pair for cjkRSUnicode property.
  * The first number pair in the cjkRSUnicode property is written as
  * l.r or l!.r.  This macro takes the "numerator" \p n and "denominator"
  * \p d values from its numeric property value, and sets lvalues \p l and \p r.
  * The presence of an exclamation point is indicated by a negative \p r, in
  * which case the actual \p r is its absolute value. */
#define uni_cjkRSUnicode_val1(n, d, l, r) do { \
  l = (uint8_t)(n); \
  r = (int8_t)d; \
} while(0)
/** Extract second number pair for cjkRSUnicode property.
  * The second number pair in the cjkRSUnicode property is written as
  * l.r or l!.r.  This macro takes the "numerator" \p n and "denominator"
  * \p d values from its numeric property value, and sets lvalues \p l and \p r.
  * The presence of an exclamation point is indicated by a negative \p r, in
  * which case the actual \p r is its absolute value. */
#define uni_cjkRSUnicode_val2(n, d, l, r) do { \
  l = (uint8_t)(n >> 8); \
  r = (int32_t)n >> 16; \
} while(0)
@

<<C Prototypes>>=
void uni_cjkRSUnicode_val1(int32_t num, uint8_t denom, uint8_t &l,
                           int8_t &r);
void uni_cjkRSUnicode_val2(int32_t num, uint8_t denom, uint8_t &l,
                           int8_t &r);
@

\lstset{language=txt}
<<FIXME>>=
CJKRadicals.txt:
  field 1 = radical # (index); may include ' (maps to !?)
  field 2 = CJK Radical character
  field 3 = CJK Unified Ideograph character
 2 & 3 are always 1 16-bit char, so could store as 32-bit value per index
@

The only other numeric property currently supported is age, which
comes from [[DerivedAge.txt]].  Since it has aliases, it is already
added as an enumeration.

\subsection{Generating the Static Data}

Once finished, the tables can be dumped.  Just as with other
properties, only range tables have been built, so it's time to
generate the multi-level table, assuming that the range data is correct.

\lstset{language=C}
<<UCD parser local functions>>=
static void fixup_rng_dat32(prop_t *p)
{
  uint32_t i;
  qsort(p->rng_dat32, p->len, sizeof(uni_chrrng_dat32_t), uni_cmprng_dat32);
  /* starting at top means only optimized entries are memmove'd */
  for(i = p->len - 1; i > 0; i--) {
    uint32_t j = i, len = p->rng_dat32[i].len, k;
    while(i > 0 &&
          p->rng_dat32[i - 1].low + p->rng_dat32[i - 1].len == p->rng_dat32[i].low - 1 &&
          p->rng_dat32[i - 1].dat == p->rng_dat32[i].dat) {
      i--;
      len += p->rng_dat32[i].len;
    }
    if(i == j)
      continue;
    k = i;
    while(len >= 256) {
      p->rng_dat32[k++].len = 255;
      p->rng_dat32[k].low = p->rng_dat32[k - 1].low + 256;
    }
    p->rng_dat32[k].len = len;
    if(k != j) {
      if(j < p->len - 1)
          movebuf(p->rng_dat32 + k + 1, p->rng_dat32 + j + 1, p->len - (j + 1));
      p->len -= j - k;
    }
    if(!i)
      break;
  }
}
@

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng_dat32) {
    fixup_rng_dat32(&parsed_props[i]);
    parsed_props[i].mt = uni_rng_dat32_to_multi(parsed_props[i].rng_dat32,
                                                parsed_props[i].len,
                                                &ml_len);
}
@

In addition to the property tables, a simple query function is
printed.  This calls a generic search function using the multi-level
table.

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng_dat32) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    const char *lname = i < num_prop_aliases ? prop_aliases[i].long_name :
                                               parsed_props[i].name;
    char nbuf[64];
    sprintf(nbuf, "uni_%s_rng.gen.c", name);
    open_wf(of, nbuf);
    fprintf(of, "#include \"uni_prop.h\"\n\n"
                "const uni_chrrng_dat32_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].len; j++)
      fprintf(of, "\t{ 0x%04X, %d, 0x%08x }%s\n",
                  parsed_props[i].rng_dat32[j].low,
	          parsed_props[i].rng_dat32[j].len,
	          parsed_props[i].rng_dat32[j].dat,
		  j < parsed_props[i].len - 1 ? "," : "");
    fputs("};\n", of);
    fclose(of);
    fprintf(gen_h, "/** Sorted range table for Unicode %s property */\n" 
                   "extern const uni_chrrng_dat32_t uni_%s_rng[];\n"
		   "/** Length of \\ref uni_%s_rng */\n"
		   "#define uni_%s_rng_len %d /* %d lookups max */\n",
		   lname, name, name, name, parsed_props[i].len,
		   lg2(parsed_props[i].len + 1));
    <<Print special 32-bit range table stuff>>
    print_mtab(name, lname, parsed_props[i].mt, gen_h);
  }
}
@

<<Print special 32-bit range table stuff>>=
if(parsed_props[i].rng_num) {
  fprintf(gen_h, "/** Retrieve value of numeric %s property \\p x.\n"
                 "  * \\p *n is set to the numerator, and \\p *d is set\n"
		 "  * to the denominator */", lname);
  fprintf(gen_h, "#define uni_%s_of(x, n, d) uni_x_val(x, uni_%s_mtab, n, d)\n",
	         name, name);
  fprintf(tstf, "num(%s);\n", name);
}
@

<<Unicode property exports>>=
/** Retrieve 32-bit value associated with \p cp in multi-level \p tab.
  * The default is assumed to be zero. */
uint32_t uni_x_dat32(uint32_t cp, const uint32_t *tab);
/** Look up rational number in a mutli-level table
  * The numerator (\p num) and denominator (\p denom) of a rational
  * number stored in \p tab for \p cp is returned.  The 32-bit
  * data must be formatted as a \ref uni_frac_t structure */
void uni_x_val(uint32_t cp, const uint32_t *tab, int32_t *num, uint8_t *denom);
@

<<Unicode property functions>>=
uint32_t uni_x_dat32(uint32_t cp, const uint32_t *tab)
{
  const uint8_t *mr;
  int r = uni_multi_tab_lookup(tab, cp * 4, &mr, 0);
  return mr ? *(uint32_t *)mr : r ? ~0 : 0;
}

void uni_x_val(uint32_t cp, const uint32_t *tab, int32_t *num, uint8_t *denom)
{
  const uint8_t *mr;
  uni_multi_tab_lookup(tab, cp * 4, &mr, 0);
  if(!mr)
    *num = 0;
  else {
    const uni_frac_t *v = (const uni_frac_t *)mr;
    *num = v->num;
    *denom = v->denom;
  }
  if(!*num)
    *denom = 1;
}
@

<<range table val for [[i]]>>=
uni_x_val(i, mtab, &mn, &md);
@

<<Additional property type names>>=
/** Numeric;  range table is \ref uni_chrrng_dat32_t, and multi-level table
 ** is 32 bits per code point.  32-bit value is actually \ref uni_frac_t */
UNI_PROP_TYPE_NUM,
@

<<Set prop type for export>>=
if(parsed_props[i].rng_num)
  t = UNI_PROP_TYPE_NUM;
@

\lstset{language=txt}
<<FIXME>>=
numeric (UAX44-LM1, which provides example and words but no algorithm):
  compare as floating-point number w/ limited precision
  e.g. 01.00 == 1, 0.3333 = 1/3 (but how limited precision?)
or, alternatively:
  - integer parts must match exactly (i.e. floor(A/B) == floor(C/D))
    a=rem(A/B), b=B, c=rem(C/D), d=D
  - fractional parts are multiplied by a fixed amount, say 1000:
    x=floor(a*1000/b); y=floor(c*1000/d)
    or, say, enough that multiplying by 10 more would make it > 1000
    thus 1/20 would become 500 rather than just 50.
  - if |x-y|<=10 (or some other slop factor), OK.
  1/3 = 333; .3 (33) bad .33 (3) OK .333 (0) best
  2/3 = 666; .7 (34) bad .67 (4) OK .667 (1) OK .6667 (0) best
  - note that if I used round instead of floor, .667 would be best
@

<<FIXME>>=
Unihan_NumericValues.txt: kAccountingNumeric, kOtherNumeric, kPrimaryNumeric
  (may need inclusion in nt/nv)
  (note: listed in unihan as "informative")
  (note: no character has more than one of these)
Unihan_DictionaryLikeData.txt:  kFourCornerCode, kFrequency, kGradeLevel,
                                kHKGlyph, kTotalStrokes
				kCHaiT: maybe; maybe needs alt. rep.
				kFenn, kPhonetic: could put last bit in denom
Unihan_DictionaryIndices.txt: lots.  may need alt. rep to fit values
Unihan_OtherMappings.txt: lots (may need string for some)
@

\section{String Properties}

String properties are those which have a value that cannot be
represented using the previously described methods.  Since the UCD is
text, this is generally either an ASCII or Unicode string.  Useful
operations include:

\begin{itemize}
\item Find out the string-valued value \emph{of} the property for a
character.  For variable-length strings, this should support two-part
retrieval: first obtain the length, and then obtain the string.  If
there is a maximum length, it should be listed so that the sizing step
can be avoided.
\end{itemize}

\subsection{Storage Methods}

The storage methods used for boolean properties can be used for
strings as well, with some adjustments.  First of all, there are very
few consecutive characters with the same value (other than the default
value, which is an empty string), so using a range list is wasteful.
Instead, a simple code point list with value is used.  One way to
store the string would be as a pointer to a zero-terminated string (no
strings use zero as a component value).  However, this requires
computing string length at every access, so a better way would be to
store a pointer and a length.  For 64-bit systems, this would require
at least 8 bytes for the pointer, and probably another 8 bytes to
align the structure, so 4 bytes for the code point and 4 bytes for the
length.  However, a more efficient storage method would be to use a
32-bit pointer into a single string containing all possible values,
reducing storage by 4 bytes.  Additional savings can be had if the
combined string size can be kept below 64K, in which case 16-bit
integers can represent the offset and length.  The length could be
stored in the string as well, eliminating the need for a separate
length, and generally reducing the space required for that to a byte.
Since fewer than 24 bits are used for the code point, the length could
be stored in the code point's lower byte as well; masking would be
required to test equality, though.  On the other hand, using UTF-16 to
encode strings will, on average, consume significantly less space than
UTF-32, but will not give space for an extra length byte.  Another
space-saving measure would be to store similar properties together.
For example, the main string properties are either case
folding-related or normalization-related, and some have the others as
their default values.  These could be combined when they have many
elements in common.  For now, though, plain tables are built using
32-bit pointers and lengths.  The merging into a single string table
can be done during post-processing. However, to assist with this,
multiple tables sharing the same string table can use the [[flags]]
field to distinguish themselves.

\lstset{language=C}
<<[[prop_t]] prerequisites>>=
typedef struct {
  uint32_t cp, off, flags: 8, len: 24;
} raw_cp_str_t;
@

<<Known Data Types>>=
raw_cp_str_t,%
@

<<Property parsed contents>>=
raw_cp_str_t *str_arr;
uint32_t *strs;
uint32_t strs_len, max_strs;
@

<<UCD parser local functions>>=
static void add_str_rng(prop_t *p, uint32_t low, uint32_t high, const uint32_t *val,
                        uint32_t len)
{
  uint32_t off = p->strs_len;
  if(!p->max_len) {
    inisize(p->str_arr, (p->max_len = 8));
    inisize(p->strs, (p->max_strs = 32));
  }
  check_size(p->strs, p->max_strs, off + len);
  cpybuf(p->strs + off, val, len);
  p->strs_len += len;
  for(; low <= high; low++) {
    check_size(p->str_arr, p->max_len, p->len + 1);
    p->str_arr[p->len].cp = low;
    p->str_arr[p->len].off = off;
    p->str_arr[p->len].len = len;
    p->str_arr[p->len].flags = 0;
    ++p->len;
  }
}
@

<<UCD parser local definitions>>=
#define decl_str(n) \
  int prop_##n = -1
#define add_str(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[64]; /* max known value == 30, so 64 should be enough */ \
    uint32_t len; \
    for(s = v, len = 0; *s; len++) \
      str[len] = strtol(s, &s, 16); \
    add_str_rng(&parsed_props[prop_##n], low, high, str, len); \
  } \
} while(0)
@

\subsection{Parsing the UCD -- Decomposition}

The first string field from [[UnicodeData.txt]] is the decomposition
mapping (dm) field.  For compatibility decompositions, the initial
part of the field is the decomposition type in angle brackets, which
has already been encoded as an enumeration (dt), so it is skipped,
along with the space which always follows it%
\footnote{The [[NormalizationCorrections.txt]] file provides previous
versions of this field, but since I see no use for it, and no property
name for it, it will not be provided.}%
.

<<Initialize UCD files>>=
decl_str(dm);
@

<<Process a line of [[UnicodeData.txt]]>>=
s = fields[5];
if(*s == '<')
  s = strchr(s, '>') + 2;
add_str(dm, s);
@

While the default values for dt and dm are normally blank, one range
actually has defined canonical values: the Hangul
Syllables\footnote{Note that I am not Korean, so I may refer to things
incorrectly or inappropriately.  Everything I know about Korean is
what I read about in the Unicode standard (and maybe a little watching
of subtitled Korean TV dramas).}, from AC00 through D7A3.  These are
only described in the standard, and can be generated with a simple
algorithm.  An entry in this range consists of three parts, called L,
V, and T.  There are normal, named code points for each L, V, and T
outside of this range, and every LV and LVT combination is contained
within this range.  There are defined decomposition mappings from LVT
to LV and T, and from LV to L and V. Rather than fill the dm string
table with easily generated values, the value is represented as an
empty result.  A function is then provided to convert that empty
string into the correct decomposition mapping.  The canonical
dm entry for LVT is LV T, so a flag is used to select the full L V T
decomposition directly.

<<Process a line of [[UnicodeData.txt]]>>=
if(low == 0xAC00) {
  /* hangul syllables */
  add_enum_rng(&parsed_props[prop_dt], low, high, enum_val(prop_dt, "can"));
  add_str_rng(&parsed_props[prop_dm], low, high, NULL, 0);
}
@

<<Unicode property exports>>=
/** Decompose Hangul syllable \p cp
  * Decomposes Hangule syllable LVT into LV and T (if \p full is false)
  * or L, V and T (if \p full is true), and decomposes LV into L and V.
  * If \p full is true, \p res should have space for three words.
  * Otherwise, or if \p cp is known to be LV, only two words are required.
  * The return value is negative if \p cp is not a decomposable Hangul syllable,
  * or the length of the return value otherwise */
int uni_hangul_syllable_decomp(uint32_t cp, uint32_t *res, int full);
@

<<Unicode property functions>>=
int uni_hangul_syllable_decomp(uint32_t cp, uint32_t *res, int full)
{
  int L, V, T;
  if(cp < 0xAC00 || cp > 0xD7A3)
    return -1;
  cp -= 0xac00;
  T = cp % 28;
  V = (cp / 28) % 21;
  L = cp / (28 * 21);
  if(!T) { /* LV -> L V */
    res[0] = 0x1100 + L;
    res[1] = 0x1161 + V;
    return 2;
  }
  if(!full) { /* LVT -> LV T */
    res[0] = 0xAC00 + 28 * (L * 21 + V);
    res[1] = 0x11A7 + T;
    return 2;
  }
  /* LVT -> L V T */
  res[0] = 0x1100 + L;
  res[1] = 0x1161 + V;
  res[2] = 0x11A7 + T;
  return 3;
}
@

The decomposition mappings are more useful if they are fully
decomposed.  Each character is given both a canonical and
compatibility full decomposition.  There is no real problem with
storing two entries with the same key in a sorted code point table, so
both are stored in the same table, with different flags.  Bit zero
indicates compatibility decomposition.

While I think the raw dm value is worthless, the Unicode standard
requires that it be provided for regular expressions.  It is stored in
the same string table as well, marked with bit one set if different
from the full decomposition.  Like the full decomposition, bit zero is
used to indicate compatibility decompositions, in order to avoid an
extra dt table lookup.

<<Initialize UCD files>>=
decl_str(dm_full);
@

<<Parse UCD files>>=
prop_dm_full = add_prop("dm_full");
prop_t *dmf_prop = &parsed_props[prop_dm_full];
prop_t *dm_prop = &parsed_props[prop_dm];
uint8_t can_val = enum_val(prop_dt, "can");
uint8_t none_val = enum_val(prop_dt, "None");
prop_t *dt_prop = &parsed_props[prop_dt];
for(i = 0; i < dm_prop->len; i++) {
  uint32_t cp = dm_prop->str_arr[i].cp;
  uint32_t str[64], len = dm_prop->str_arr[i].len;
  int is_can = uni_chrrng_dat8(cp, dt_prop->rng_dat8, dt_prop->len,
                               none_val) == can_val;
  int do_dec = !is_can;
  int did_repl = 0;
  raw_cp_str_t *unex;

  /* add unexpanded decomp */
  add_str_rng(dmf_prop, cp, cp, NULL, 0);
  if(!dmf_prop->strs_len) { /* deferred until first alloc */
    free(dmf_prop->strs);
    dmf_prop->strs = dm_prop->strs;
    dmf_prop->max_strs = dm_prop->max_strs;
    dmf_prop->strs_len = dm_prop->strs_len;
    dm_prop->strs = NULL;
    dm_prop->max_strs = dm_prop->strs_len = 0;
  }
  unex = &dmf_prop->str_arr[dmf_prop->len - 1];
  unex->off = dm_prop->str_arr[i].off;
  unex->len = len;
  cpybuf(str, dmf_prop->strs + unex->off, len);
  if(is_can)
    for(j = 0; j < len; j++) {
      uint8_t dt = uni_chrrng_dat8(str[j], dt_prop->rng_dat8, dt_prop->len,
                                   none_val);
      if(dt == can_val) {
        raw_cp_str_t *dec = bsearch(str + j, dm_prop->str_arr, dm_prop->len,
				    sizeof(raw_cp_str_t), uni_cmp_cp);
        if(dec->len > 1)
          movebuf(str + j + dec->len, str + j + 1, len - j - 1);
        cpybuf(str + j, dec->off + dmf_prop->strs, dec->len);
        len += dec->len - 1;
	did_repl = 1;
        j--; /* recheck 1st char of replacement */
      } else if(dt != none_val)
        do_dec = 1;
  }
  if(did_repl) {
    unex->flags = 2;
    add_str_rng(dmf_prop, cp, cp, str, len);
  }
  if(do_dec) {
    for(j = 0; j < len; j++) {
      uint8_t dt = uni_chrrng_dat8(str[j], dt_prop->rng_dat8, dt_prop->len,
                                   none_val);
      if(dt != none_val) {
        raw_cp_str_t *dec = bsearch(str + j, dm_prop->str_arr, dm_prop->len,
				    sizeof(raw_cp_str_t), uni_cmp_cp);
        if(dec->len > 1)
          movebuf(str + j + dec->len, str + j + 1, len - j - 1);
        cpybuf(str + j, dec->off + dmf_prop->strs, dec->len);
	did_repl = 1;
        len += dec->len - 1;
        j--; /* recheck 1st char of replacement */
      }
    }
    if(did_repl) {
      if(!is_can)
        unex->flags = 3;
      add_str_rng(dmf_prop, cp, cp, str, len);
      dmf_prop->str_arr[dmf_prop->len - 1].flags = 1;
    } else if(!is_can)
      unex->flags = 1;
  }
}
@

\subsection{Generating the Static Data}

This string table can now be printed.  Before doing so, the strings
are sorted, and any strings which are either the same or overlap from
the start share space.  This does not save as much as true common
substring elimination would, but it does save a little.  Additional
space could be saved by storing 24-bit (or even 21-bit) words, at the
expense of retrieval inefficiency.  Alternatively, the words could be
stored in UTF-16 format, which is somewhat natural and nearly always
takes up less space than UTF-32.  The same could be said of UTF-8, but
UTF-8 is not always smaller than UTF-16, and requires more complicated
encoding and decoding.  A property-wide parameter selects the format:
UTF-16 or UTF-32.

If 8-bit data is really wanted, it should probably be ASCII.  In that
case, the 32 bits per character are excessive, and should instead be
filled 4 characters per word.  This format is supported as well;
strings are assumed to be zero-terminated, or exactly four times the
length of the 32-bit string if no zeroes are encountered first.  The
terminating zero must always be in the last word.  Note that merging
strings works identically for 16-bit and 32-bit strings, but 8-bit
strings need a second pass to detect end-of-string overlaps.

<<Property parsed contents>>=
int strs_char_size; /* 8, 16, 32, or 0 (16) */
@

<<UCD parser local functions>>=
/* qsort has no user data, so this is a static */
const uint32_t *sort_strs;
static int cmp_strs(const void *a, const void *b)
{
   const raw_cp_str_t *A = a, *B = b;
   uint32_t len = A->len;
   int c;

   if(len > B->len)
     len = B->len;
   c = cmpbuf(sort_strs + A->off, sort_strs + B->off, len);
   if(c)
     return c;
   return (int32_t)A->len - (int32_t)B->len;
}
@

<<UCD parser local functions>>=
static void merge_strs(prop_t *p)
{
  uint32_t i;

  sort_strs = p->strs;
  qsort(p->str_arr, p->len, sizeof(*p->str_arr), cmp_strs);
  for(i = p->len - 1; i; i--)
    if(p->str_arr[i - 1].len <= p->str_arr[i].len &&
       !cmpbuf(p->strs + p->str_arr[i - 1].off,
               p->strs + p->str_arr[i].off,
	       p->str_arr[i - 1].len)) {
      p->str_arr[i - 1].off = p->str_arr[i].off;
  }
}
@

<<UCD parser local functions>>=
static void dump_strs(prop_t *p, const char *lname, FILE *gen_h)
{
  int i;
  unsigned int off, saved = 0;
  char nbuf[64];
  int char_size = p->strs_char_size;

  if(!char_size)
    char_size = 16;
  sprintf(nbuf, "uni_%s_strs.gen.c", p->name);
  open_wf(of, nbuf);
  fprintf(of, "#include <stdint.h>\n\n"
              "const uint%d_t uni_%s_strs[] = {\n\t", char_size, p->name);
  const char *prefix = "";
  merge_strs(p);
  for(i = p->len - 1, off = 0; i >= 0; i--) {
    int j, len = p->str_arr[i].len;
    unsigned int soff = p->str_arr[i].off;
    if(!len) {
      p->str_arr[i].off = 1;
      continue;
    }
    /* adjust 8-bit len */
    if(char_size == 8) {
      const char *s = (const char *)(p->strs + soff + len);
      len *= 4;
      while(len && !*--s)
        len--;
      p->str_arr[i].len = len;
    }

    p->str_arr[i].off = off;
    if(char_size > 8)
      for(j = 0; j < len; j++) {
        uint32_t c = p->strs[soff + j];
        if(char_size == 16 && c > 0xFFFF) {
          c -= 0x10000;
          fprintf(of, "%s0x%X,", prefix, 0xD800 + (c >> 10));
	  prefix = !((off + j + 1) % 8) ? "\n\t" : " ";
	  c = 0xDC00 + (c & 0x3ff);
	  p->str_arr[i].len++;
	  off++;
        }
        fprintf(of, "%s0x%04X", prefix, c);
	prefix = !((off + j + 1) % 8) ? ",\n\t" : ", ";
      }
    else {
      const char *s = (const char *)(p->strs + soff);
      for(j = 0; j < len; j++, s++) {
        if(*s == '\\' || *s == '\'')
	  fprintf(of, "%s'\\%c'", prefix, *s);
        else if(isprint(*s))
	  fprintf(of, "%s '%c'", prefix, (int)*s);
	else
	  fprintf(of, "%s0x%02X", prefix, (int)(uint8_t)*s);
	prefix = !((off + j + 1) % 8) ? ",\n\t" : ", ";
      }
    }
    off += len;
    /* skip predecessor if prefix of string already in table */
    /* but adjust length if necessary */
    while(i > 0 && p->str_arr[i - 1].off == soff) {
      if(char_size == 16) {
        for(j = len = 0; j < p->str_arr[i - 1].len; j++)
	  len += uni_utf16_enclen(p->strs[soff + j]);
	p->str_arr[i - 1].len = len;
	saved += len;
      } else if(char_size == 8) {
        len = p->str_arr[i - 1].len * 4;
	if(len > p->str_arr[i].len)
	  len = p->str_arr[i].len;
	p->str_arr[i - 1].len = len;
	saved += len;
      } else
        saved += p->str_arr[i - 1].len;
      p->str_arr[i - 1].off = p->str_arr[i].off;
      --i;
    }
    if(char_size == 8) {
      while(i > 0 && p->str_arr[i - 1].len &&
                     p->str_arr[i - 1].len * 4 <= p->str_arr[i].len) {
        len = p->str_arr[i - 1].len * 4;
        const char *s = (const char *)(p->strs + soff),
                   *t = (const char *)(p->strs + p->str_arr[i - 1].off),
		   *e = t + len;
        while(len && !*--e)
	  len--;
        if(!memcmp(s, t, len)) {
	  for(j = --i; j > 0; j--)
	    if(p->str_arr[j - 1].off != p->str_arr[i].off)
	      break;
	  for(; i >= j; i--) {
	    int plen = p->str_arr[i].len * 4;
	    if(plen > len)
	      plen = len;
	    p->str_arr[i].len = plen;
            p->str_arr[i].off = p->str_arr[i + 1].off;
	    saved += plen;
	  }
	  i++; /* last loop goes under by one */
        } else
	  break;
      }
      if(!i)
        break;
    }
  }
  fputs("\n};\n", of);
  fclose(of);
  fprintf(gen_h, "/** Bulk string data for Unicode %s property */\n"
                 "extern const uint%d_t uni_%s_strs[];\n"
		 "/** Length of \\ref uni_%s_strs */\n"
                 "#define uni_%s_strs_len %d /* (%d bytes) (%d words saved) */\n",
		 lname, char_size, p->name, p->name, p->name, off,
		 off * char_size / 8, saved);
}
@

<<Dump character information as C code>>=
/* ensure that each property has like-named string table */
s = (char *)dmf_prop->name;
dmf_prop->name = "dm";
dump_strs(dmf_prop, "Deomposition_Mapping", gen_h);
dmf_prop->name = s;
fputs("/** Bulk string data for full canonical decomposition property */\n"
      "#define uni_canon_decomp_strs uni_dm_strs\n"
      "/** Length of \\ref uni_canon_decomp_strs */\n"
      "#define uni_canon_decomp_strs_len uni_dm_strs_len\n"
      "/** Bulk string data for full compatibility decomposition property */\n"
      "#define uni_compat_decomp_strs uni_dm_strs\n"
      "/** Length of \\ref uni_compat_decomp_strs */\n"
      "#define uni_compat_decomp_strs_len uni_dm_strs_len\n", gen_h);
@

Now that the string offsets and lengths are correct, the plain table
is ready to be converted into a multi-level table, and then both can
be printed.  The plain table could simply be the same as the raw
table, using the flags to distinguish entries.  The multi-level table,
though, expects only one value per index, and each value must be the
same size.  Rather than figure out a way to squeeze all three values
into an efficient representation, three separate tables are generated.
While it would definitely be possible to encode the offset and length
into 24 bits, the current multi-level table implementation only
supports sizes which are powers of two.  Encoding into 16 bits might
be possible, but is likely too complex to be useful.  Other string
propreties may also benefit from more breathing room.  Instead, 32
bits are used: 16 for the offset and 16 for the length.  The raw dm
property also needs to distinquish between canonical and compatibility
results.  There is a binary property to help, but that requires an
extra lookup.  Instead, a flag is ored into the length field. The
compatibility decomposition table uses entries from the canonical
table where no compatibility decomposition exists.  Rather than make
the plain tables behave completely differently than the multi-level
tables (i.e., be a single merged table), three separate plain tables
will be generated as well.

<<UCD parser local functions>>=
/* sort by flag first, then cp */
static int cmp_cp_flg(const void *a, const void *b)
{
   const raw_cp_str_t *A = a, *B = b;
   int32_t c;

   c = (int32_t)A->cp - (int32_t)B->cp;
   if(c)
     return c;
   return (int32_t)A->flags - (int32_t)B->flags;
}
@

<<[[uni_str_arr_t]]>>=
/** A 64-bit structure for associating a code point with a string descriptor */
typedef struct {
  uint32_t cp; /**< The code point */
  uint16_t off, /**< Offset into bulk string */ len; /**< String length */
} uni_str_arr_t;
@

<<[[uni_str_ptr_t]]>>=
/** A 32-bit string descriptor */
typedef struct {
  uint16_t off, /**< Offset into bulk string */ len; /**< String length */
} uni_str_ptr_t;
@

<<Unicode property exports for generator>>=
<<[[uni_str_arr_t]]>>
<<[[uni_str_ptr_t]]>>
@

<<Known Data Types>>=
uni_str_arr_t,uni_str_ptr_t,%
@

<<Dump character information as C code>>=
qsort(dmf_prop->str_arr, dmf_prop->len, sizeof(*dmf_prop->str_arr), cmp_cp_flg);
uni_str_arr_t *dec;
inisize(dec, dmf_prop->len);
/* the easy table: canon full mapping is all w/ flags == 0 */
open_wf(cdf, "uni_canon_decomp_arr.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_arr_t uni_canon_decomp_arr[] = {\n\t", cdf);
uint32_t curent;
for(i = 0; dmf_prop->str_arr[i].flags; i++);
for(curent = 0; i < dmf_prop->len; i++, curent++) {
  dec[curent].cp = dmf_prop->str_arr[i].cp;
  dec[curent].off = dmf_prop->str_arr[i].off;
  dec[curent].len = dmf_prop->str_arr[i].len;
  while(i < dmf_prop->len - 1 && dmf_prop->str_arr[i + 1].flags)
    ++i;
  fprintf(cdf, "{ 0x%04X, %d, %d }%s", (int)dec[curent].cp, (int)dec[curent].off,
                                      (int)dec[curent].len,
				      i < dmf_prop->len - 1 ? ",\n\t" : "\n};\n");
}
fclose(cdf);
fprintf(gen_h, "/** Sorted string pointer array for full canonical decomposition */\n"
	       "extern const uni_str_arr_t uni_canon_decomp_arr[];\n"
	       "/** Length of \\ref uni_canon_decomp_arr */\n"
	       "#define uni_canon_decomp_arr_len %d /* %d lookups max */\n",
	       curent, lg2(curent + 1));
@

<<Clean up after parsing UCD files>>=
free(dec);
@

<<[[uni_cp_val_t]]>>=
/** A 64-bit structure for associating a single code point with a 32-bit value */
typedef struct {
  uint32_t cp; /**< The code point */
  uint32_t val; /**< The associated value */
} uni_cp_val_t;
@

<<Unicode property exports for generator>>=
<<[[uni_cp_val_t]]>>
/** Convert array of code points with 32-bit values to a multi-level table
  * The table \p tab/\p tab_len must sorted as per \ref uni_cmp_cp.  The
  * table is the return value; its length is returned in \p ml_len if
  * non-NULL */
uint32_t *uni_cp_val_to_multi(const uni_cp_val_t *tab, uint32_t tab_len,
                              uint32_t *ml_len);
@

<<Known Data Types>>=
uni_cp_val_t,%
@

<<Unicode property functions for generator>>=
uint32_t *uni_cp_val_to_multi(const uni_cp_val_t *tab, uint32_t tab_len,
                              uint32_t *ml_len)
{
  uint32_t low, high, len, i;
  uint32_t *ml;
  uint32_t *bits;

  /* degenerate case:  always out-of-range */
  if(!tab_len) {
    uni_bits_to_multi(NULL, 0, 1, 0, 0, 0, &ml, ml_len);
    return ml;
  }
  low = tab[0].cp;
  high = tab[tab_len - 1].cp;
  len = high - low + 1;
  inisize(bits, len);
  /* Optimize(maybe): only set def on unspecified ranges; may be faster */
  clearbuf(bits, len);
  for(i = 0; i < tab_len; i++)
    bits[tab[i].cp - low] = tab[i].val;
  uni_bits_to_multi((uint8_t *)bits, len * 4, low * 4, high * 4, 0, 4, &ml, ml_len);
  free(bits);
  return ml;
}
@

<<Dump character information as C code>>=
uint32_t *tmt = uni_cp_val_to_multi((uni_cp_val_t *)dec, curent, &ml_len);
print_mtab("canon_decomp", "full canonical decomposition", tmt, gen_h);
free(tmt);
@

<<Dump character information as C code>>=
/* the next harder table: flags == 1 -> full compat decomp */
/* needs to duplicate full canon decomp when empty */
open_wf(kf, "uni_compat_decomp_arr.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_arr_t uni_compat_decomp_arr[] = {\n\t", kf);
for(i = curent = 0; i < dmf_prop->len; i++, curent++) {
  raw_cp_str_t *cur = &dmf_prop->str_arr[i];
  /* could be canon followed by compat, or compat followed by dm */
  if(i < dmf_prop->len - 1 && cur[1].cp == cur->cp &&
     !(cur->flags & 1) && (cur[1].flags & 1)) {
    ++i;
    ++cur;
  }
  dec[curent].cp = cur->cp;
  dec[curent].off = cur->off;
  dec[curent].len = cur->len;
  if(cur->flags & 1)
    dec[curent].len |= 0x8000;
  /* could be canon/compat followed by dm */
  if(i < dmf_prop->len - 1 && cur->cp == cur[1].cp)
    ++i;
  fprintf(kf, "{ 0x%04X, %d, %d%s }%s",
              (int)cur->cp, (int)cur->off, (int)cur->len,
	      (cur->flags & 1) ? " | 0x8000" : "",
	      i < dmf_prop->len - 1 ? ",\n\t" : "\n};\n");
}
fclose(kf);
fprintf(gen_h, "/** Sorted string pointer array for full canonical decomposition */\n"
	       "extern const uni_str_arr_t uni_compat_decomp_arr[];\n"
	       "/** Length of \\ref uni_compat_decomp_arr */\n"
	       "#define uni_compat_decomp_arr_len %d /* %d lookups max */\n",
	       curent, lg2(curent + 1));
tmt = uni_cp_val_to_multi((uni_cp_val_t *)dec, curent, &ml_len);
print_mtab("compat_decomp", "full compatibility decomposition", tmt, gen_h);
free(tmt);
@

<<Dump character information as C code>>=
/* the hardest table: 2/3 -> dm */
/* needs to duplicate full canon decomp when empty */
/* needs to duplicate full compat decomp when empty & full canon empty*/
open_wf(dm, "uni_dm_arr.gen.c");
fputs("#include \"uni_prop.h\"\n\n"
      "const uni_str_arr_t uni_dm_arr[] = {\n\t", dm);
for(i = curent = 0; i < dmf_prop->len; i++) {
  raw_cp_str_t *cur = &dmf_prop->str_arr[i];

  if(i < dmf_prop->len - 1 && cur[1].cp == cur->cp) {
    /* possibilities: canon,compat,dm canon,dm canon,compat compat,dm */
    if(cur[1].flags & 2) {
      ++i; /* compat,dm or canon,dm: skip full */
      cur++;
    } else if(i < dmf_prop->len - 2 && cur->cp == cur[2].cp) {
      i += 2; /* canon,compat,dm: skip both full */
      cur += 2;
    } else /* canon,compat; skip compat when done */
      i++;
  }
  dec[curent].cp = cur->cp;
  dec[curent].off = cur->off;
  dec[curent].len = cur->len;
  if(cur->flags & 1)
    dec[curent].len |= 0x8000;
  fprintf(dm, "{ 0x%04X, %d, %d%s }%s", (int)cur->cp, (int)cur->off,
                                        (int)cur->len,
				        (cur->flags & 1) ? " | 0x8000" : "",
				        i < dmf_prop->len - 1 ? ",\n\t" : "\n};\n");
  curent++;
}
fclose(dm);
fprintf(gen_h, "/** Sorted string pointer array for Decomposition_Mapping property.\n"
               "  * Note that the len field has its upper bit set if this\n"
	       "  * is a compatibility decomposition */\n"
	       "extern const uni_str_arr_t uni_dm_arr[];\n"
	       "/** Length of \\ref uni_dm_arr */\n"
	       "#define uni_dm_arr_len %d /* %d lookups max */\n",
	       curent, lg2(curent + 1));
tmt = uni_cp_val_to_multi((uni_cp_val_t *)dec, curent, &ml_len);
print_mtab("dm", "Decomposition_Mapping", tmt, gen_h);
free(tmt);
@

The generic all-property table needs to have a pointer to the string
table for each property.

<<Property parsed contents>>=
const char *strs_name;
@

<<Parse UCD files>>=
#define do_dm_prop(p) do { \
  prop_t *_p = &parsed_props[add_prop(#p)]; \
  _p->strs_name = "dm"; \
  enable_str_mt(_p); \
} while(0)
do_dm_prop(canon_decomp);
do_dm_prop(compat_decomp);
do_dm_prop(dm);
@

<<Additional property type names>>=
/** String: "range" table is actually \ref uni_str_arr_t, and multi-level
 ** table is 32 bits per code point.  32-bit value is actually
 ** \ref uni_str_ptr_t */
UNI_PROP_TYPE_STR,
@

<<Set prop type for export>>=
if(parsed_props[i].strs_name || parsed_props[i].strs)
  t = UNI_PROP_TYPE_STR;
@

<<Additional property structure members>>=
const uint8_t *strs8;  /**< The string table for 8-bit string properties */
const uint16_t *strs16;  /**< The string table for 16-bit string properties */
const uint32_t *strs32;  /**< The string table for 32-bit string properties */
uint32_t strs_len;  /**< The number of words in the string table */
@

<<Print additional property structure members>>=
const char *strs_name = parsed_props[i].strs_name;
if(parsed_props[i].strs || strs_name) {
  if(!strs_name)
    strs_name = name;
  int char_size = parsed_props[i].strs_char_size;
  if(!char_size)
    char_size = 16;
  fprintf(pnf, "%s%s, uni_%s_strs%s%s, uni_%s_strs_len",
               char_size > 8 ? ", NULL" : "", char_size > 16 ? ", NULL" : "",
               strs_name,
               char_size < 32 ? ", NULL" : "", char_size < 16 ? ", NULL" : "",
	       strs_name);
} else
  fputs(", NULL, NULL, NULL, 0", pnf);
@

<<Plain table name>>=
t == UNI_PROP_TYPE_STR ? "arr" :
@

The lookup functions return offset and length.  An additional lookup
function fills in a string given the offset and length.  This is so
that the exact name of the special value for Hangul Syllables is
encoded in these functions, rather than everywhere lookups are used.

<<Unicode property exports>>=
/** Look up a decomposition property value for \p cp in \p tab.
  * The result is a string descriptor in \p off and \p len; both are zero
  * if lookup failed.  If \p compat is not NULL, it is set to a flag
  * which is non-zero if the lookup returns a compatibility decomposition.
  * If and only if the \p h flag is true, synthetic (Hangul syllables)
  * are considered valid as, well, and are indicated by a negative \p off */
void uni_x_dec(uint32_t cp, const uint32_t *tab, int16_t *off, uint8_t *len,
               uint8_t *compat, int h);
@

<<Unicode property exports>>=
/** Look up full canonical decomposition for \p cp.
  * Returns string descriptor in \p off and \p len.  Finish the lookup
  * using \ref uni_get_decomp.  This is a preprocessor macro */
void uni_find_canon_decomp(uint32_t cp, int16_t *off, uint8_t *len);
#define uni_find_canon_decomp(cp, off, len) \
  uni_x_dec(cp, uni_canon_decomp_mtab, off, len, NULL, 1)
/** Look up full compatibility decomposition for \p cp.
  * Returns string descriptor in \p off and \p len.  If non-NULL,
  * \p compat is set to a flag which is false if this is a purely canonical
  * decomposition.  Finish the lookup with \ref uni_get_decomp.  This is a
  * preprocessor macro */
void uni_find_compat_decomp(uint32_t cp, int16_t *off, uint8_t *len,
                            uint8_t *compat);
#define uni_find_compat_decomp(cp, off, len, compat) \
  uni_x_dec(cp, uni_compat_decomp_mtab, off, len, compat, 1)
/** Look up raw decomposition property for \p cp.
  * Returns string descriptor in \p off and \p len.  If non-NULL,
  * \p compat is set to a flag which is false if this is a canonical
  * decomposition.  Finish the lookup with \ref uni_get_decomp.  This is a
  * preprocessor macro */
void uni_find_dm(uint32_t cp, int16_t *off, uint8_t *len, uint8_t *compat);
#define uni_find_dm(cp, off, len, compat) \
  uni_x_dec(cp, uni_dm_mtab, off, len, compat, 0)
/** Finish decomposition property lookup
  * Write the results of decomposition from \ref uni_find_canon_decomp,
  * \ref uni_find_compat_decomp or \ref uni_find_dm into \p buf.  The
  * \p cp, \p off and \p len parameters are the same as for the first
  * part of the lookup (although \p off and \p len are not changed).  You
  * must ensure enough space in \p buf (currently at least 18 words) */
void uni_get_decomp(uint32_t cp, uint32_t *buf, int16_t *off, uint8_t *len);
#define uni_get_decomp(cp, buf, off, len) do { \
  if((off) == -1) \
    uni_hangul_syllable_decomp(cp, buf, (len) >= 3); \
  else \
    cpybuf(buf, uni_dm_strs + (off), len); \
} while(0)
@

<<Unicode property functions>>=
void uni_x_dec(uint32_t cp, const uint32_t *tab, int16_t *off, uint8_t *len,
               uint8_t *compat, int h)
{
  const uint8_t *mr;
  uni_multi_tab_lookup(tab, cp * 4, &mr, 0);
  if(!mr) {
    *off = 0;
    *len = 0;
    if(compat)
      *compat = 0;
    return;
  } else {
    uni_str_ptr_t *v = (void *)mr;
    if(h >= 0 && !v->len && v->off) {
      *off = -1;
      *len = h && (cp - 0xAC00) % 28 ? 3 : 2;
      if(compat)
        *compat = 0;
    } else {
      *off = v->off;
      *len = v->len & 0x7fff;
      if(compat)
        *compat = v->len >> 15;
    }
  }
}
@

\subsection{Testing}

To test the table implementations, a different, but nearly identical,
set of routines is used compared to the ones one for booleans.

<<Functions to help test generated tables>>=
#define str(x) doit_str(#x, uni_##x##_arr, uni_##x##_arr_len, uni_##x##_mtab)

static void doit_str(const char *name, const uni_str_arr_t *rng, uint32_t nent,
                     const uint32_t *mtab)
{
    uint32_t i;

    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    const uni_str_ptr_t *ms, *rs;
    static const uni_str_ptr_t z = {0};
    uni_str_arr_t *r;
    for(i = 0; i < 0x110000; i++) {
      r = bsearch(&i, rng, nent, sizeof(*rng), uni_cmp_cp);
      rs = r ? (uni_str_ptr_t *)&r->off : &z;
      ms = uni_x_str_of(i, mtab);
      if(cmpbuf(ms, rs, 1)) {
        fprintf(stderr, "mismatch %s@%d %d/%d/%d %d/%d/%d\n", name, i,
	                (int)rs->off, (int)rs->len & 0x7fff, (int)rs->len >> 15,
			(int)ms->off, (int)ms->len & 0x7fff, (int)ms->len >> 15);
	exit(1);
      }
    }
    /* check performance */
    int j;
    double tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        r = bsearch(&i, rng, nent, sizeof(*rng), uni_cmp_cp);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        uni_x_str_of(i, mtab);
    tt = tend();
    printf("  r%.0f t%.0f %.2fx\n", tr, tt, tr / tt);
}
@

<<Unicode property exports>>=
/** Look up string pointer associated with \p cp in multi-level table \p tab.
  * The default is zeroes, so an offset and length of zero generaly
  * indicate lookup failure */
const uni_str_ptr_t *uni_x_str_of(uint32_t cp, const uint32_t *tab);
@

<<Unicode property functions>>=
const uni_str_ptr_t *uni_x_str_of(uint32_t cp, const uint32_t *tab)
{
  const uni_str_ptr_t *ret;
  static const uni_str_ptr_t z = {0};
  uni_multi_tab_lookup(tab, cp * 4, (const uint8_t **)&ret, 0);
  return ret ? ret : &z;
}
@

<<Dump character information as C code>>=
fputs("str(canon_decomp);\n"
      "str(compat_decomp);\n"
      "str(dm);\n", tstf);
@

\subsection{Parsing the UCD -- DUCET Decompositions}

An alternate decomposition table is that provided by
\texttt{decomps.txt}.  This file is informative: no official algorithm
uses this, but it can be handy for examining the DUCET.  Some DUCET
mappings automatically decompose the character they are mapping.
These include canonical compositions, compatibility compositions, and
``other'' compositions defined by this text file.  The format of the
file is similar to \texttt{UnicodeData.txt}: semicolon-separated
fileds, the first of which is a single code point.  The second field
is similar to the dt portion of the decomposition mapping field, but
is separate from the dm-equivalent field.  The third field is the
dm-equivalent field, but without dt information.  A special
decomposition type, \texttt{sort}, is used to indicate mappings which
are only in this file.  Rather than create a new enumeration (or add
\texttt{sort} to the dt enumeration), this is encoded as compat.  The
way to distinguish real compat mappings from \texttt{sort} mappings is
to look it up in the regular dt table.

While the \texttt{sort} value is documented, decomps also adds
\texttt{circlekata} and \texttt{smallnarrow}, which are translated to
\texttt{circle} and \texttt{narrow}, repsectively, for now.  No great
effort will be put into solving this problem; I used this table when
analyzing the DUCET, and see little other value for it.

<<Initialize UCD files>>=
decl_str(DUCET_dm);
decl_enum(DUCET_dt, "None");
@

<<Parse UCD files>>=
prop_DUCET_dt = add_prop("DUCET_dt");
parsed_props[prop_DUCET_dt].def = parsed_props[prop_dt].def;
/* copy enum decls from dt for lookup */
enum_vals[prop_DUCET_dt] = enum_vals[prop_dt];
enum_vals_len[prop_DUCET_dt] = enum_vals_len[prop_dt];
open_f("decomps.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  char *dt_val;
  split_line(lbuf);
  if(num_fields != 3) /* e.g. comment lines */
   continue;
  low = high = strtol(fields[0], NULL, 16);
  dt_val = fields[1];
  if(*dt_val) {
    if(*dt_val++ != '<') {
      perror("decomps");
      exit(1);
    }
    for(s = dt_val; *s && *s != '>'; s++);
    if(*s != '>' || s[1]) {
      perror("decomps");
      exit(1);
    }
    *s = 0;
    if(!strcmp(dt_val, "sort"))
      dt_val = (char *)"compat";
    else if(!strcmp(dt_val, "circlekata"))
      dt_val[6] = 0;
    else if(!strcmp(dt_val, "smallnarrow"))
      dt_val += 5;
  }
  if(*dt_val)
    add_enum(DUCET_dt, dt_val);
  else
    add_enum(DUCET_dt, "can");
  add_str(DUCET_dm, fields[2]);
}
/* remove enum info so new enum not created */
enum_vals[prop_DUCET_dt] = NULL;
enum_vals_len[prop_DUCET_dt] = 0;
@

Unfortunately, some of the dm strings have multiple entries for one
index.  Luckily, the dt values do not have the same problem.  To fix
the dm strings, the result is sorted, and any duplicate indices are
removed by merging the two entries, separated by a zero.  Nothing ever
maps to zero, so there should be no problem distinguishing the
entries.  No attempt is made to sort the entries; there are at most
two anyway, so searching is no great effort.

<<Parse UCD files>>=
prop_t *prop_ddm = &parsed_props[prop_DUCET_dm];
qsort(prop_ddm->str_arr, prop_ddm->len, sizeof(raw_cp_str_t), uni_cmp_cp);
for(i = prop_ddm->len - 1; i > 0; i--) {
  for(j = i; j > 0; j--)
    if(prop_ddm->str_arr[j - 1].cp != prop_ddm->str_arr[i].cp)
      break;
  if(j == i)
    continue;
  low = j;
  int len = 0;
  for(j = low; j <= i; j++)
    len += prop_ddm->str_arr[j].len + 1;
  check_size(prop_ddm->strs, prop_ddm->max_strs, prop_ddm->strs_len + len);
  for(j = low, len = 0; j <= i; j++) {
    movebuf(prop_ddm->strs + prop_ddm->strs_len + len,
            prop_ddm->strs + prop_ddm->str_arr[j].off,
	    prop_ddm->str_arr[j].len);
    len += prop_ddm->str_arr[j].len + 1;
    prop_ddm->strs[prop_ddm->strs_len + len - 1] = 0;
  }
  len--;  /* strip final 0 */
  prop_ddm->str_arr[low].off = prop_ddm->strs_len;
  prop_ddm->str_arr[low].len = len;
  prop_ddm->strs_len += len;
  movebuf(prop_ddm->str_arr + low + 1, prop_ddm->str_arr + i + 1,
          prop_ddm->len - i - 1);
  prop_ddm->len -= i - low;
  if(!(i = low))
    break;
}
@

To dump the tables, the generic routines can be relied upon.  However,
there is not yet a generic routine to dump string tables.  In order to
avoid stomping on the work done for the decomposition tables, the
generic routine for strings needs to be called manually.

<<Dump character information as C code>>=
dump_str_tabs(prop_ddm, "DUCET Decomposition_Mapping", gen_h, tstf);
@

<<UCD parser local functions>>=
static void dump_str_arr(prop_t *prop, const char *lname, FILE *gen_h, FILE *tstf)
{
  uint32_t i;
  char nbuf[64];
  uni_str_arr_t *short_str = NULL;
  if(prop->mt)
    inisize(short_str, prop->len);
  sprintf(nbuf, "uni_%s_arr.gen.c", prop->name);
  open_wf(rt, nbuf);
  fprintf(rt, "#include \"uni_prop.h\"\n\n"
              "const uni_str_arr_t uni_%s_arr[] = {\n\t", prop->name);
  for(i = 0; i < prop->len; i++) {
    const raw_cp_str_t *ent = &prop->str_arr[i];
    if(prop->mt) {
      short_str[i].cp = ent->cp;
      short_str[i].off = ent->off;
      short_str[i].len = ent->len;
    }
    fprintf(rt, "{ 0x%04X, %d, %d}%s", (int)ent->cp, (int)ent->off, 
                                       (int)ent->len,
				       i < prop->len - 1 ? ",\n\t" : "\n};\n");
  }
  fclose(rt);
  fprintf(gen_h, "/** Sorted string pointer array for %s property */\n"
                 "extern const uni_str_arr_t uni_%s_arr[];\n"
		 "/** Length of \\ref uni_%s_arr */\n"
	         "#define uni_%s_arr_len %d /* %d lookups max */\n",
	         lname, prop->name, prop->name, prop->name, i, lg2(i + 1));
  if(prop->mt) {
    uint32_t ml_len, *tmt = uni_cp_val_to_multi((uni_cp_val_t *)short_str, i, &ml_len);
    free(short_str);
    print_mtab(prop->name, lname, tmt, gen_h);
    free(tmt);
    /* for direct lookup */
    fprintf(gen_h, "/** Returns string pointer associated with \\p cp for %s property */\n"
		   "#define uni_%s_of(cp) uni_x_str_of(cp, uni_%s_mtab)\n",
                   lname, prop->name, prop->name);
    /* test requires mtab as well */
    fprintf(tstf, "str(%s);\n", prop->name);
  }
}
@

<<UCD parser local functions>>=
static void dump_str_tabs(prop_t *prop, const char *lname, FILE *gen_h, FILE *tstf)
{
  dump_strs(prop, lname, gen_h);
  /* dump_strs "un-sorts" str_arr */
  qsort(prop->str_arr, prop->len, sizeof(*prop->str_arr), uni_cmp_cp);
  dump_str_arr(prop, lname, gen_h, tstf);
}
#define enable_str_mt(p) (p)->mt = (uint32_t *)~0
#define enable_str_mt_p(p) parsed_props[p].mt = (uint32_t *)~0
@

<<Parse UCD files>>=
enable_str_mt(prop_ddm);
@

\subsection{Parsing the UCD -- Composition}

As mentioned earlier, composition takes place on pairs.  The canonical
composition of A and B is C if the plain dm value for C is A B, and dt
for C is canonical, and C does not have the Comp\_Ex property.  Given
these conditions, B is removed and A is replaced by C.  After
replacement, C becomes A for a further pass at composition.  It is not
possible to make a ``fully composed'' table that, like the full
decomposition table, iterates thorugh all possible compositions. So,
the composition table is basically a table returning C (or nothing)
given A and B.  

There are a number of possible strategies for storing these values for
efficient retrieval. For sorted arrays, the pair of A and B could
simply be used as the key. When both A and B are known, this is the
most effective form of the sorted array.  However, real normalization
takes place by knowing A, and then scanning all potential candidates
for B.  For the sorted array, this is still not too terrible: the
first and last occurrence of A can be found relatively quickly, and
that limited subset can be searched for B.  The search for the last
and first occurence can be removed by instead having A return a table
(generally an address and a table length) indexed on B.  The B-indexed
table then returns C.  This arrangement can be used regardless of the
actual A-indexed table structure.

Rather than creating a new structure to store these B-indexed tables,
they are stored as if they were strings themselves: A is used to look
up a string, which is actually a code point/value table to look up C
given B.  As with the dm table, the Hangul Syllables are given special
treatment. The A-indexed entries for all LV and L characters point to
zero-length tables, indicating that the [[uni_hangul_syllable_comp]]
function should be used to compose instead.

After converting the string to UTF-16, the binary search no longer
works properly, because the size of each entry is not always exactly
two words.  In order to still support binary searching, all entries
are forced to become 32 bits by padding any non-32-bit entries with
trailing zeroes.  It is easy to detect this on lookup: if the second
word of the string is either a surrogate or a zero, 32-bit lookups are
required.

<<Initialize UCD files>>=
decl_str(cm);
@

<<Parse UCD files>>=
prop_cm = add_prop("canon_comp");
prop_t *fce_prop = &parsed_props[add_prop("Comp_Ex")];
prop_t *cm_prop = &parsed_props[prop_cm];
for(i = 0; i < dm_prop->len; i++) {
  uint32_t str[2];
  uint32_t cp = dm_prop->str_arr[i].cp; /* cp == C */
  if(dm_prop->str_arr[i].len != 2 ||
     uni_chrrng_dat8(cp, dt_prop->rng_dat8, dt_prop->len, none_val) !=
         can_val ||
     uni_is_cp_chrrng(cp, fce_prop->rng, fce_prop->len))
    continue; /* skip if not canonical composition */
  str[0] = dmf_prop->strs[dm_prop->str_arr[i].off + 1]; /* B */
  str[1] = cp;
  cp = dmf_prop->strs[dm_prop->str_arr[i].off]; /* A */
  /* just add; tables are merged later */
  add_str_rng(cm_prop, cp, cp, str, 2);
}
/* add entries for Hangul */
add_str_rng(cm_prop, 0x1100, 0x1112, NULL, 0); /* L */
for(i = 0xAC00; i <= 0xD7A3; i += 28) /* LV */
  add_str_rng(cm_prop, i, i, NULL, 0);
/* merge B->C tables, and add padding zeroes if needed */
qsort(cm_prop->str_arr, cm_prop->len, sizeof(*cm_prop->str_arr), uni_cmp_cp);
for(i = cm_prop->len - 1; i > 0; i--) {
  int off, k, is_32 = 0;;
  j = i;
  while(j > 0 && cm_prop->str_arr[i].cp == cm_prop->str_arr[j - 1].cp)
    j--;
  if(j == i && !cm_prop->str_arr[i].len)
    continue;
  uint32_t jb = cm_prop->strs[cm_prop->str_arr[j].off];
  uint32_t jc = cm_prop->strs[cm_prop->str_arr[j].off + 1];
  if(j == i && ((jb < 0x10000 && jc < 0x10000) ||
                (jb > 0xFFFF  && jc > 0xFFFF)))
    continue; /* note: in UCD 6.2, this path always taken if j == i */
  /* leave room for 2* (B+C) and possible padding zeroes (B+C again) */
  check_size(cm_prop->strs, cm_prop->max_strs,
             cm_prop->strs_len + 4 * (i - j + 1));
  uint32_t *dest = cm_prop->strs + cm_prop->strs_len;
  for(k = j, off = 0; k <= i; k++) {
    if((dest[off++] = cm_prop->strs[cm_prop->str_arr[k].off]) > 0xFFFF)
      is_32 = 1;
    if((dest[off++] = cm_prop->strs[cm_prop->str_arr[k].off + 1]) > 0xFFFF)
      is_32 = 1;
  }
  /* sort B->C table by B */
  qsort(dest, off / 2, 2 * sizeof(*cm_prop->strs), uni_cmp_cp);
  /* insert zeroes in string if 32-bit needed */
  /* NOTE: UCD 6.2 never touches this code (all are 1-entry, b & c 32-bit) */
  if(is_32) {
    for(k = off - 1; k >= 0; k--) {
      if(cm_prop->strs[cm_prop->strs_len + k] < 0x10000) {
        movebuf(dest + k + 2, dest + k + 1, off - k - 1);
	dest[k + 1] = 0;
	off++;
      }
    }
  }
  /* finish entry */
  cm_prop->str_arr[j].off = cm_prop->strs_len;
  cm_prop->str_arr[j].len = off;
  cm_prop->strs_len += off;
  /* remove excess entries */
  movebuf(cm_prop->str_arr + j + 1, cm_prop->str_arr + i + 1,
          cm_prop->len - i - 1);
  cm_prop->len -= i - j;
  if(!(i = j))
    break;
}
@

<<Unicode property exports>>=
/** Compose two characters into a Hangul syllable
  * Composes L and V into LV or LV and T into LVT.  Returns zero if
  * \p a and \p b are not one of those two combinations */
uint32_t uni_hangul_syllable_comp(uint32_t a, uint32_t b);
@

<<Unicode property functions>>=
uint32_t uni_hangul_syllable_comp(uint32_t a, uint32_t b)
{
  if(a >= 0x1100 && a <= 0x1112) /* L */
    if(b >= 0x1161 && b <= 0x1175) /* V */
      return 0xAC00 + 28 * ((a - 0x1100) * 21 + (b - 0x1161)); /* LV */
  if(b > 0x11A7 && b <= 0x11C2) /* T */
    if(a >= 0xAC00 && a <= 0xD7A3 && !((a - 0xAC00) % 28)) /* LV */
      return a + (b - 0x11A7); /* LVT */
  return 0;
}
@

There is probably little value in converting the B-indexed lookup
tables to multi-level tables, so the string table can be printed as is
now. The [[dump_strs]] function is the most convenient method to do
so. The fucntion will pointlessly try to compress the table, but
that's better than writing a dumper function just for this table.

<<Dump character information as C code>>=
int max_len = 0;
for(i = 0; i < cm_prop->len; i++)
  if(cm_prop->str_arr[i].len > max_len)
    max_len = cm_prop->str_arr[i].len;
max_len /= 2;
fprintf(gen_h, "/* %d max B->C len (%d lookups) */\n", max_len, lg2(max_len + 1));
dump_strs(cm_prop, "composition mapping", gen_h);
@

The A-indexed plain table can now be sorted, converted to a
multi-level table, and printed.

<<Dump character information as C code>>=
qsort(cm_prop->str_arr, cm_prop->len, sizeof(*cm_prop->str_arr), uni_cmp_cp);
dump_str_arr(cm_prop, "composition mapping", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt(cm_prop);
@

The same lookup function as was used for decomposition lookups mostly
works for the A-indexed level as well.  For the B-indexed level, a
binary search is performed.  This is hand-coded for speed and to
easily adjust for 32-bit entries.

<<Unicode property exports>>=
/** Find potential canonical compositions beginning with \p cp.
  * Returns string descriptor in \p off and \p len; finish lookup with
  * \ref uni_canon_comp.  This is a preprocessor macro */
void uni_find_canon_comp(uint32_t cp, int16_t *off, uint8_t *len);
#define uni_find_canon_comp(cp, off, len) \
  uni_x_dec(cp, uni_canon_comp_mtab, off, len, NULL, -1)
/** Find canonical composition of \p cpa with \p cpb.
  * The string descriptor \p off and \p len are from \ref uni_find_canon_comp.
  * The return value is zero if no composition is possible.  Otherwise, it
  * is the result of composition.  Note that multiple secondary lookups
  * can be performed after a single \ref uni_find_canon_comp lookup.
  * This is a preprocessor macro */
uint32_t uni_canon_comp(uint32_t cpa, uint32_t cpb, uint16_t off, uint8_t len);
#define uni_canon_comp(cpa, cpb, off, len) \
  ((len) > 0 ? uni_lookup_compent(cpb, uni_canon_comp_strs + (off), len) : \
               uni_hangul_syllable_comp(cpa, cpb))
@

<<Unicode property exports for generator>>=
/** Find composition with \p cp within potential compositions.
  * Given a string of canonical composition candidates (\p tab, \p len), find
  * one which matches \p cp and return it.  Zero is returned if there is no
  * such entry.  This does not take into account synthetic compositions;
  * use \ref uni_canon_comp instead */
uint32_t uni_lookup_compent(uint32_t cp, const uint16_t *tab, uint32_t len);
@

<<Unicode property functions for generator>>=
uint32_t uni_lookup_compent(uint32_t cp, const uint16_t *tab, uint32_t len)
{
  if(tab[1] && !uni_is_surrogate(tab[1])) {
    int32_t l = 0, h = len / 2 - 1;
    while(h >= l) {
      uint32_t m = (l + h) / 2;
      int32_t c = (int32_t)tab[m * 2] - (int32_t)cp;
      if(!c)
        return tab[m * 2 + 1];
      else if(c < 0)
        l = m + 1;
      else
        h = m - 1;
    }
  } else {
    int32_t l = 0, h = len / 4 - 1;
    while(h >= l) {
      uint32_t m = (l + h) / 2;
      uint32_t mc = uni_int_utf16_decode(tab + m * 4, NULL);
      int32_t c = (int32_t)mc - (int32_t)cp;
      if(!c) {
        mc = uni_int_utf16_decode(tab + m * 4 + 2, NULL);
	return mc;
      } else if(c < 0)
        l = m + 1;
      else
        h = m - 1;
    }
  }
  return 0;
}
@

\subsection{Parsing the UCD -- Case Conversion}

The next strings are the various case conversion tables.
[[UnicodeData.txt]] contains three of them: the simple case mappings
(lower, upper, and title case).  These are always just one character
long, so they are added as a simple value, instead.

It might make sense to make a sort of combined case conversion table,
but it's easier to just dump the three as separate tables.  The only
savings is to suppress redundant entries:  any which match the
character (which should never occur, but it doesn't hurt to check just
in case) or any title case entries which match the upper case entries
(which does occur).

<<Initialize UCD files>>=
decl_num(slc);
decl_num(suc);
decl_num(stc);
@

<<Process a line of [[UnicodeData.txt]]>>=
{
  const char *slcs = fields[13], *sucs = fields[12], *stcs = fields[14];
  uint32_t slc = strtol(slcs, NULL, 16);
  uint32_t suc = strtol(sucs, NULL, 16);
  uint32_t stc = strtol(stcs, NULL, 16);

  if(*slcs && slc != low)
    add_num(slc, slc, 1);
  if(*sucs && suc != low)
    add_num(suc, suc, 1);
  if(*stcs && stc != low && (!*sucs || stc != suc))
    add_num(stc, stc, 1);
}
@

[[SpecialCasing.txt]] lists the exceptions to the above case mappings.
The above values are always either zero or one character (technically
always one character, since the default value is the unmodified code
point); the ones from this file may be more than one character, or may
require more than one character as input, or may only apply under
certain conditions.  The mappings with conditions override the simple
mappings above when the condition is in effect.  It would be nice to
only require one lookup to do case conversion, but combining the above
tables with this one is not that simple.  Instead, this table needs to
always be consulted first, followed by the above tables.

The file is short enough that it can be encoded in a relatively
inefficient format, which simply directly encodes the contents of the
file.  Three tables are generated, corresponding to the desired case
conversion.  If the conversion is already in one of the above tables,
it is simply skipped.  Otherwise, it is encoded as a condition word,
followed by the result string.  Since the condition flags can be
encoded in fewer than 16 bits, the lower 5 bits also encode the length of
the conversion.  That way, multiple conversions with different
conditions can simply be appended to the string.  Unfortunately,
multiple entries with the same key are not always consecutive, so this
merging is done after they have all been gathered.

The string table generator converts 32-bit values to UTF-16, but
assumes that they are valid (i.e., not surrogates themselves).  This
assumption works to our advantage here, in that the flag/length field
is just a plain 16-bit value, and will not be mangled by the
conversion.  However, the length field needs to be adjusted for any
values that encode into two 16-bit values.

<<Special casing conditions>>=
/* all known conditions from SpecialCasing.txt, version 6.2 */
/* future revisions may need more flags/parsing */
/** \addtogroup uni_prop_sc Special Casing Conditions
  * Flags used by case-related functions and special casing properties
  * for casing context and locale.  In the descriptions, ccs stands
  * for combining character sequence (i.e., grapheme cluster).
  * @{ */
#define UNI_SC_FL_LT                (1<<5)  /**< lt locale */
#define UNI_SC_FL_AZ                (1<<6)  /**< az locale */
#define UNI_SC_FL_TR                (1<<7)  /**< tr locale */
#define UNI_SC_FL_LOCALE            (0x07 << 5)   /**< Mask: any locale flags */

#define UNI_SC_FL_NOT               (1<<8)  /**< Negates non-locale conditions */
#define UNI_SC_FL_AFTER_I           (1<<9)  /**< Capital I in same ccs */
#define UNI_SC_FL_AFTER_SOFT_DOTTED (1<<10) /**< Lower-case i in same ccs */
#define UNI_SC_FL_BEFORE_DOT        (1<<11) /**< Combining dot above in same ccs */
#define UNI_SC_FL_MORE_ABOVE        (1<<12) /**< Accent above glyph in same ccs */
#define UNI_SC_FL_FINAL_SIGMA       (1<<13) /**< No following letters in same word */
#define UNI_SC_FL_CONTEXT           (0x1f << 9) /**< Mask: any context flags */

#define UNI_SC_FL_LEN_MASK          (0x1f) /**< Mask: length in raw storage */
/** @} */
@

<<Unicode property exports for generator>>=
<<Special casing conditions>>
@

<<Initialize UCD files>>=
decl_str(lc);
decl_str(uc);
decl_str(tc);
@

<<Parse UCD files>>=
open_f("SpecialCasing.txt");
#define add_sc(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  uint32_t str[64]; /* max known value == 30, so 64 should be enough */ \
  uint32_t len, len16; \
  for(s = v, len = len16 = 0; *s; len++, len16++) { \
    str[len + 1] = strtol(s, &s, 16); \
    if(str[len + 1] > 0xFFFF) \
      len16++; \
  } \
  str[0] = flg | len16; \
  if(len == 1 && low == high) { \
    int32_t num; \
    uint8_t den; \
    prop_t *sc = &parsed_props[prop_s##n]; \
    uni_chrrng_val(low, sc->rng_dat32, sc->len, &num, &den); \
    if(den && num == str[1]) \
      break; \
  } \
  add_str_rng(&parsed_props[prop_##n], low, high, str, len + 1); \
} while(0)
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  uint16_t flg = 0;
  if(num_fields > 4) {
    s = fields[4];
    /* slow parse, but at least accurate */
    if(!strncasecmp(s, "lt", 2)) {
      flg |= UNI_SC_FL_LT;
      s += 2;
      if(*s)
        ++s;
    } else if(!strncasecmp(s, "az", 2)) {
      flg |= UNI_SC_FL_AZ;
      s += 2;
      if(*s)
        ++s;
    } else if(!strncasecmp(s, "tr", 2)) {
      flg |= UNI_SC_FL_TR;
      s += 2;
      if(*s)
        ++s;
    }
    if(!strncasecmp(s, "Not_", 4)) {
      flg |= UNI_SC_FL_NOT;
      s += 4;
    }
    if(!strcasecmp(s, "After_I"))
      flg |= UNI_SC_FL_AFTER_I;
    else if(!strcasecmp(s, "After_Soft_Dotted"))
      flg |= UNI_SC_FL_AFTER_SOFT_DOTTED;
    else if(!strcasecmp(s, "Before_Dot"))
      flg |= UNI_SC_FL_BEFORE_DOT;
    else if(!strcasecmp(s, "More_Above"))
      flg |= UNI_SC_FL_MORE_ABOVE;
    else if(!strcasecmp(s, "Final_Sigma"))
      flg |= UNI_SC_FL_FINAL_SIGMA;
    else if(*s) {
      fprintf(stderr, "Unknown case condition: %s\n", s);
      exit(1);
    }
  }
  add_sc(lc, fields[1]);
  add_sc(uc, fields[3]);
  /* note that this condition may be too simple */
  /* it does not account for excess ignorable whitespace */
  /* it does not check if it ends up changing stc */
  /* if(strcmp(fields[3], fields[2]) */
    add_sc(tc, fields[2]);
}
fclose(f);
@

<<Parse UCD files>>=
<<Postprocess simple casing for [[lc]]>>
<<Postprocess simple casing for [[uc]]>>
<<Postprocess simple casing for [[tc]]>>
@

<<Postprocess simple casing for (@c)>>=
{
  prop_t *prop = &parsed_props[prop_<<@c>>];
  qsort(prop->str_arr, prop->len, sizeof(*prop->str_arr), uni_cmp_cp);
  /* pass 1: merge */
  for(i = 1; i < prop->len; i++) {
    raw_cp_str_t *prev = &prop->str_arr[i - 1], *cur = &prop->str_arr[i];
    if(cur->cp == prev->cp) {
      uint32_t newstr[cur->len + prev->len];
      cpybuf(newstr, prop->strs + prev->off, prev->len);
      cpybuf(newstr + prev->len, prop->strs + cur->off, cur->len);
      prev->cp = 0;
      add_str_rng(prop, cur->cp, cur->cp, newstr, cur->len + prev->len);
      cur->off = prop->str_arr[prop->len - 1].off;
      cur->len += prev->len;
      prop->len--;
    }
  }
  /* pass 2: remove merged */
  /* doing it one at a time is inefficient, but simple and good enough */
  for(i = 0; i < prop->len; i++)
    if(!prop->str_arr[i].cp) {
      movebuf(&prop->str_arr[i], &prop->str_arr[i + 1], prop->len - i);
      prop->len--;
      i--;
    }
}
@

<<Additional parse-ucd C files>>=
btricks.h \
@

<<Additional parse-ucd includes>>=
#include "btricks.h"
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_lc], "Lower_Case", gen_h, tstf);
dump_str_tabs(&parsed_props[prop_uc], "Upper_Case", gen_h, tstf);
dump_str_tabs(&parsed_props[prop_tc], "Title_Case", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_lc);
enable_str_mt_p(prop_uc);
enable_str_mt_p(prop_tc);
@

While the properties were named after the final result, these tables
only provide a small part of it.  To get the final result, helper
functions are provided, along with macros to use those functions.  A
generic overall helper function returns a pointer to the raw results,
while the more specific functions convert the results into one of the
three supported output formats.

These helper functions are stored in their own file, as they are not
of general utility, and are used by the data-expensive string
converters below.

\lstset{language=txt}
<<Library [[uni]] Members>>=
case.o
@

\lstset{language=C}
<<case.c>>=
<<Common C Header>>
#include "uni_prop.h"
@

<<Unicode property exports>>=
<<[[uni_case_convert]][[32]] proto>>;
<<[[uni_case_convert]][[16]] proto>>;
<<[[uni_case_convert]][[8]] proto>>;
@

<<[[uni_case_convert]](@sz) proto>>=
/** Generic case conversion with context.
  * Convert \p cp to another case given context and locale \p cond and
  * case conversion tables \p simple and \p special/\p strs.  The result
  * is returned in \p buf, \p off and \p buf_len; see e.g.
  * \ref uni_return32_buf<<@sz>> for details on how these three parameters work.
  * This function is not meant to be called directly; see e.g. \ref uni_lc<<@sz>>. */
int uni_case_convert<<@sz>>(uint32_t cp, uint32_t cond,
                       <<Buffer return parameters for UTF-[[<<@sz>>]]>>,
                       const uint32_t *simple, const uint32_t *special,
		       const uint16_t *strs)
@

<<case.c>>=
/* return code is similar to above, except when ret == NULL, positive */
/*  return is return code point */
/* rather than filling a buffer, ret is the return string */
static int32_t uni_case_convert(uint32_t cp, uint32_t cond,
                                const uint16_t **ret, const uint32_t *simple,
				const uint32_t *special, const uint16_t *strs)
{
  const uint8_t *mr;
  uni_multi_tab_lookup(special, cp * 4, &mr, 0);
  if(!mr) {
    uni_multi_tab_lookup(simple, cp * 4, &mr, 0);
    *ret = NULL;
    return mr && ((uni_frac_t *)mr)->num ? ((uni_frac_t *)mr)->num : cp;
  }
  const uni_str_ptr_t *v = (const void *)mr;
  const uint16_t *str = strs + v->off;
  if(!(*str & ~UNI_SC_FL_LEN_MASK) && v->len == (*str & UNI_SC_FL_LEN_MASK) + 1) {
    /* no conditions */
    *ret = str + 1;
    return v->len - 1;
  }
  if(cond == (uint32_t)~0)
    return -1;
  uint32_t rem = v->len;
  const uint16_t *ncond = NULL;
  while(rem > 0) {
    if(!(*str & UNI_SC_FL_LOCALE) || (*str & UNI_SC_FL_LOCALE & cond)) {
      if(!(*str & UNI_SC_FL_CONTEXT)) {
        /* locale condition is stronger than no condition at all */
        if(!ncond || (*str & UNI_SC_FL_LOCALE))
	  ncond = str;
      } else if(!(*str & cond & UNI_SC_FL_CONTEXT) == !!(*str & UNI_SC_FL_NOT)) {
        /* condition true: return result (assumes no other cond could be true) */
	ncond = str;
	break;
      }
    }
    rem -= (*str & UNI_SC_FL_LEN_MASK) + 1;
    str += (*str & UNI_SC_FL_LEN_MASK) + 1;
  }
  if(!ncond) {
    uni_multi_tab_lookup(simple, cp * 4, &mr, 0);
    *ret = NULL;
    return mr && ((uni_frac_t *)mr)->num ? ((uni_frac_t *)mr)->num : cp;
  }
  *ret = ncond + 1;
  return *ncond & UNI_SC_FL_LEN_MASK;
}
@

<<case.c>>=
<<[[uni_case_convert]][[32]]>>
<<[[uni_case_convert]][[16]]>>
<<[[uni_case_convert]][[8]]>>
@

<<[[uni_case_convert]](@sz)>>=
<<[[uni_case_convert]][[<<@sz>>]] proto>>
{
  const uint16_t *rbuf;
  int32_t ret = uni_case_convert(cp, cond, &rbuf, simple, special, strs);
  if(ret < 0)
    return ret;
  if(!rbuf)
    return uni_return32_buf<<@sz>>((uint32_t *)&ret, 1, buf, off, buf_len);
  return uni_return16_buf<<@sz>>(rbuf, ret, buf, off, buf_len);
}
@

<<[[uni_case_convert]] support for (@type)>>=
<<[[uni_case_convert]][[32]] support>>
<<[[uni_case_convert]][[16]] support>>
<<[[uni_case_convert]][[8]] support>>
@

<<Doxydoc for [[uni_]](@type)>>=
/** Unicode conversion to [<<@type>>].
  * Convert \p cp to [<<@type>>] given context and locale flags \p cond.
  * If \p cond is ~0, return -1 if a condition may affect the results.
  * Otherwise, return the length and use the \p buf, \p off and \p buf_len
  * parameters as described for \ref uni_return32_buf<<@sz>>.  This is a
  * preprocessor macro */
@

\lstset{language=make}
<<makefile.vars>>=
uni_prop.h: NOTANGLE_POSTPROC+=|sed -e \
   's/\[lc\]/lower-case/;s/\[uc\]/upper-case/;s/\[tc\]/title-case/'
@

\lstset{language=C}
<<[[uni_case_convert]](@sz) support>>=
<<Doxydoc for [[uni_]][[<<@type>>]]>>
int uni_<<@type>><<@sz>>(uint32_t cp, uint32_t cond, <<Buffer return parameters for UTF-[[<<@sz>>]]>>);
#define uni_<<@type>><<@sz>>(cp, cond, buf, off, buf_len) \
  uni_case_convert<<@sz>>(cp, cond, buf, off, buf_len, \
                     uni_s<<@type>>_mtab, uni_<<@type>>_mtab, uni_<<@type>>_strs)
@

<<Doxydoc for [[uni_]][[tc]]>>=
/** Unicode conversion to [<<@type>>].
  * Convert \p cp to [<<@type>>] given context and locale flags \p cond.
  * If \p cond is ~0, return -1 if a condition may affect the results.
  * Otherwise, return the length and use the \p buf, \p off and \p buf_len
  * parameters as described for \ref uni_return32_buf<<@sz>>.  If the result
  * of this function call is just \p cp, convert using \ref uni_uc<<@sz>>
  * instead.  This is a preprocessor macro */
@

<<Unicode property exports>>=
<<[[uni_case_convert]] support for [[lc]]>>
<<[[uni_case_convert]] support for [[uc]]>>
<<[[uni_case_convert]] support for [[tc]]>>
@

These only convert; they don't check to see if a character \emph{is} a
certain case.  One way to do this is to check the gc property; in
fact, this I recommend this, since that returns \emph{which} case the
character is, rather than requiring three tests.  However, Unicode has
a different definition in mind: checking if the result of conversion
matches the input.

<<Unicode property exports>>=
/** Check if \p cp is cased
  * Checks if \p cp is cased given locale and context \p cond, and
  * case lookup tables \p simple and \p special/\p strs.  This
  * function is not meant to be called directly; see e.g. \ref uni_is_lc
  * instead */
int uni_case_check(uint32_t cp, uint32_t cond, const uint32_t *simple,
                   const uint32_t *special, const uint16_t *strs);
@

<<[[uni_case_convert]] support for (@type)>>=
/** Check if \p cp is [<<@type>>].
  * Convert \p cp to [<<@type>>] given context and locale flags \p cond.
  * If \p cond is ~0, return -1 if a condition may affect the results.
  * Otherwise, return 0 if the result does not match \p cp, and 1 if it
  * does */
int uni_is_<<@type>>(uint32_t cp, uint32_t cond);
<<[[uni_is_]][[<<@type>>]] macro>>
@

<<[[uni_is_]](@type) macro>>=
#define uni_is_<<@type>>(cp, cond) \
  uni_case_check(cp, cond, uni_s<<@type>>_mtab, uni_<<@type>>_mtab, \
                  uni_<<@type>>_strs)
@

<<[[uni_is_]][[tc]] macro>>=
#define uni_is_tc(cp, cond) \
  (uni_case_check(cp, cond, uni_stc_mtab, uni_tc_mtab, uni_tc_strs) || \
   uni_case_check(cp, cond, uni_suc_mtab, uni_uc_mtab, uni_uc_strs))
@

<<case.c>>=
int uni_case_check(uint32_t cp, uint32_t cond, const uint32_t *simple,
                   const uint32_t *special, const uint16_t *strs)
{
  const uint16_t *rbuf;
  int32_t ret = uni_case_convert(cp, cond, &rbuf, simple, special, strs);
  if(ret < 0)
    return ret;
  if(!rbuf)
    return ret == cp;
  if(ret == 1 || (ret == 2 && uni_is_surrogate(*rbuf)))
    return cp == uni_int_utf16_decode(rbuf, NULL);
  return 1;
}
@

Unicode also provides a generic ``cased'' condition; this means that
some case conversion may succeed.  Naturally, this pulls in all three
conversion tables.

<<Unicode property exports>>=
/** Check if \p cp is cased.
  * Convert \p cp to lc, uc, and tc given context and locale flags \p
  * cond.  If \p cond is ~0, return -1 if a condition may affect the
  * results.  Otherwise, return 1 if any result does not match \p cp,
  * and 0 if all results match \p cp.  This is a preprocessor macro */
int uni_is_cased(uint32_t cp, uint32_t cond);
#define uni_is_cased(cp, cond) \
  (!uni_case_check(cp, cond, uni_stc_mtab, uni_tc_mtab, uni_tc_strs) && \
   !uni_case_check(cp, cond, uni_suc_mtab, uni_uc_mtab, uni_uc_strs) && \
   !uni_case_check(cp, cond, uni_suc_mtab, uni_uc_mtab, uni_lc_strs))
@

These provide support for single-character conversion and detection. 
Normally, this is sufficient.  For users too lazy to loop over a
string themselves, here are some string case conversion and detection
functions. There are actually some additional functions which can be
performed when doing a full string conversion.  All of the non-locale
context flags can be resolved, for example.  Title case always
requires word boundary detection, as well.  In fact, regular context
requries word boundary detection as well.  Boundary detection pulls in
a table.  Title case and full case checking also pull in all tables.

While the lower-case and upper-case conversion functions are identical
except for the tables, the title-case conversion function is only
mostly identical.  Rather than duplicate the code, a single function
is used for both upper and lower case, whereas macro tricks are used
to get the title case converter.

Also, most of the logic of checking case is identical to the logic of
converting case, so the code is shared for that, as well.

<<Unicode property exports>>=
<<[[uni_str_case_convert]][[32]] proto>>;
<<[[uni_str_case_convert]][[16]] proto>>;
<<[[uni_str_case_convert]][[8]] proto>>;
<<[[uni_str_case_check]][[32]] proto>>;
<<[[uni_str_case_check]][[16]] proto>>;
<<[[uni_str_case_check]][[8]] proto>>;
@

<<[[uni_str_case_convert]](@sz) proto>>=
/** Convert case of entire UTF-<<@sz>> string \p s/\p slen.
  * If \p slen is less than zero, \p s is zero-terminated.
  * The return buffer parameters \p buf, \p off and \p buf_len are
  * described in \ref uni_return<<@sz>>_buf<<@sz>>.  The case conversion
  * is determined by which tables are used.  Set \p simple to the simple
  * case conversion lookup table and \p special to the special case lookup
  * table whose strings are in \p strs.  Note that word boundaries are not
  * taken into consideration, so this cannot be used for title casing.
  * Only the locale-related flags in \p cond are recognized */
int uni_str_case_convert<<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                            uint32_t cond, <<Buffer return parameters for UTF-[[<<@sz>>]]>>,
                            const uint32_t *simple, const uint32_t *special,
		            const uint16_t *strs)
@

<<[[uni_str_case_check]](@sz) proto>>=
/** Check case of entire UTF-<<@sz>> string \p s/\p slen.
  * If \p slen is less than zero, \p s is zero-terminated.
  * The routine converts the entire string's case and compares the results
  * with the original, returning true if they match.  The case conversion
  * is determined by which tables are used.  Set \p simple to the simple
  * case conversion lookup table and \p special to the special case lookup
  * table whose strings are in \p strs.  Note that word boundaries are not
  * taken into consideration, so this cannot be used for title casing.
  * Only the locale-related flags in \p cond are recognized */
int uni_str_case_check<<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                          uint32_t cond,
                          const uint32_t *simple, const uint32_t *special,
		          const uint16_t *strs)
@

<<[[uni_str_]](@type)(@sz) proto>>=
/** Convert entire UTF-<<@sz>> string \p s/\p slen to [<<@type>>].
  * If \p slen is less than zero, \p s is zero-terminated.
  * The return buffer parameters \p buf, \p off and \p buf_len are
  * described in \ref uni_return<<@sz>>_buf<<@sz>>.
  * Only the locale-related flags in \p cond are recognized */
int uni_str_<<@type>><<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                         uint32_t cond, <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<[[uni_str_is_]](@type)(@sz) proto>>=
/** Test if entire UTF-<<@sz>> string \p s/\p slen is [<<@type>>].
  * If \p slen is less than zero, \p s is zero-terminated.
  * Only the locale-related flags in \p cond are recognized.  */
int uni_str_is_<<@type>><<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                            uint32_t cond)
@

<<[[uni_case_convert]](@sz) support>>=
<<[[uni_str_case_convert_check]] support for [[<<@type>>]]>>
@

<<[[uni_str_case_convert_check]] support for (@type)>>=
<<[[uni_str_]][[<<@type>>]][[<<@sz>>]] proto>>;
<<[[uni_str_is_]][[<<@type>>]][[<<@sz>>]] proto>>;
<<Generic [[str_case_convert_check]] for [[<<@type>>]]>>
@

<<Unicode property exports>>=
<<[[uni_str_case_convert]][[32]] proto>>;
<<[[uni_str_case_convert]][[16]] proto>>;
<<[[uni_str_case_convert]][[8]] proto>>;
<<[[uni_str_is_]][[cased]][[32]] proto>>;
<<[[uni_str_is_]][[cased]][[16]] proto>>;
<<[[uni_str_is_]][[cased]][[8]] proto>>;
@

<<Generic [[str_case_convert_check]] for (@type)>>=
#define uni_str_<<@type>><<@sz>>(s, slen, cond, buf, off, buf_len) \
  uni_str_case_convert<<@sz>>(s, slen, cond, buf, off, buf_len, \
                     uni_s<<@type>>_mtab, uni_<<@type>>_mtab, uni_<<@type>>_strs)
#define uni_str_is_<<@type>><<@sz>>(s, slen, cond) \
  uni_str_case_check<<@sz>>(s, slen, cond, \
                   uni_s<<@type>>_mtab, uni_<<@type>>_mtab, uni_<<@type>>_strs)
@

<<Generic [[str_case_convert_check]] for [[tc]]>>=
@

<<case.c>>=
<<[[Generic]] [[uni_str_case_convert_check]][[32]]>>
<<[[Generic]] [[uni_str_case_convert_check]][[16]]>>
<<[[Generic]] [[uni_str_case_convert_check]][[8]]>>
<<[[Titlecase]] [[uni_str_case_convert_check]][[32]]>>
<<[[Titlecase]] [[uni_str_case_convert_check]][[16]]>>
<<[[Titlecase]] [[uni_str_case_convert_check]][[8]]>>
<<[[Cased]] [[uni_str_case_check]][[32]]>>
<<[[Cased]] [[uni_str_case_check]][[16]]>>
<<[[Cased]] [[uni_str_case_check]][[8]]>>
@

<<(@type) [[uni_str_case_convert]] proto>>=
<<[[uni_str_case_convert]][[<<@sz>>]] proto>>
@

<<[[Titlecase]] [[uni_str_case_convert]] proto>>=
<<[[uni_str_]][[tc]][[<<@sz>>]] proto>>
@

<<(@type) [[uni_str_case_convert_check]](@sz)>>=
<<[[<<@type>>]] [[uni_str_case_convert]] proto>>
{
  unsigned int rlen = 0;
  <<Initialize string case conversion>>
  
  while(slen) {
    <<Convert a character's case into output, updating [[rlen]]>>
  }
  <<Clean up after string case conversion>>
  return rlen;
}
@

<<(@type) [[uni_str_case_check]] proto>>=
<<[[uni_str_case_check]][[<<@sz>>]] proto>>
@

<<[[Titlecase]] [[uni_str_case_check]] proto>>=
<<[[uni_str_is_]][[tc]][[<<@sz>>]] proto>>
@

<<[[Cased]] [[uni_str_case_check]] proto>>=
<<[[uni_str_is_]][[cased]][[<<@sz>>]] proto>>
@

<<(@type) [[uni_str_case_convert_check]](@sz)>>=
<<[[<<@type>>]] [[uni_str_case_check]][[<<@sz>>]]>>
@

<<(@type) [[uni_str_case_check]](@sz)>>=
<<[[<<@type>>]] [[uni_str_case_check]] proto>>
{
  <<Initialize string case checking>>
  
  while(slen) {
    <<Convert a character's case, returning 1 if changed>>
  }
  <<Return default case check for [[<<@type>>]]>>
}
@

<<Return default case check for (@type)>>=
return 1;
@

<<Return default case check for [[Cased]]>>=
return 0;
@

In order to avoid having to declare separate noweb macros for every
case, the C preprocessor is used to select which code to include.

<<Initialize string case checking>>=
#undef USE_CHECK_CODE
#define USE_CHECK_CODE 2
<<Initialize string case conversion>>
@

<<Convert a character's case, returning 1 if changed>>=
<<Convert a character's case into output, updating [[rlen]]>>
@

<<Initialize string case conversion>>=
#if USE_CHECK_CODE == 2
#undef USE_CHECK_CODE
#define USE_CHECK_CODE 1
#else
#undef USE_CHECK_CODE
#endif
<<Use code for [[<<@type>>]] case conversion>>
@

<<Use code for (@type) case conversion>>=
#undef USE_TC_CODE
#undef USE_ALL_CODE
@

<<Use code for [[Titlecase]] case conversion>>=
#undef USE_TC_CODE
#define USE_TC_CODE 1
#undef USE_ALL_CODE
@

<<Use code for [[Cased]] case conversion>>=
<<Use code for [[Titlecase]] case conversion>>
#define USE_ALL_CODE 1
@

Empty input produces empty output (length zero).  For the case
checkers, empty strings always return false (zero as well).  This is
the only degenerate case.

<<Initialize string case conversion>>=
if(!slen)
  return 0;
@

No check is made to ensure that the input string does not overlap the
output buffer; the input string will simply be clobbered, possibly
corrupting the result.  However, one special case is permitted:  If
the input starts at the start of the output buffer, [[NULL]] can be
used for [[s]], meaning that care will be taken to not corrupt the
output if the input gets overwritten.

<<Initialize string case conversion>>=
#ifndef USE_CHECK_CODE
uint<<@sz>>_t *stmp = NULL;

if(!s) {
  /* FIXME: only do this if output will overwrite input */
  inisize(stmp, slen);
  cpybuf(stmp, *buf + (off > 0 ? off : 0), slen);
  s = stmp;
}
#endif
@

<<Clean up after string case conversion>>=
if(stmp)
  free(stmp);
@

While producing output, the output buffer parameters need to be
adjusted.  For dynamic buffers, this amounts to adding to the offset.
For static buffers, this requires managing a separate buffer pointer
and length.

<<Initialize string case conversion>>=
#ifndef USE_CHECK_CODE
uint<<@sz>>_t *sbuf = *buf;
unsigned int sblen = *buf_len;

if(off < 0) {
  buf = &sbuf;
  buf_len = &sblen;
}
#endif
@

Since many of the case conversion flags require knowledge of a
complete grapheme cluster, the loop operates one cluster at a time.
Since UTF conversion isn't too expensive, only the first character of
the next cluster is saved.  The grapheme cluster boundary is
determined by the character count to the next start.

<<Initialize string case conversion>>=
int gc_res = 0;
int next_gc = 0;
unsigned int clen, next_cp_len;
/* warning: _int_ may over-read buffer with invalid UTF input */
uint32_t next_cp = uni_int_utf<<@sz>>_decode(s, &clen);
next_cp_len = clen;
@

<<Convert a character's case into output, updating [[rlen]]>>=
/* warning: _int_ may over-read buffer with invalid UTF input */
uint32_t cp = next_gc ? uni_int_utf<<@sz>>_decode(s, &clen) : next_cp;
if(!next_gc)
  clen = next_cp_len;
if(clen > slen) /* should never happen with valid UTF input */
  slen = 0;
else {
  slen -= clen;
  s += clen;
}
if(!next_gc) {
  <<Find next grapheme cluster boundary for case conversion>>
} else
  next_gc -= clen;
@

While finding the next cluster boundary, context flags are adjusted as
well.  The final sigma flag is supposed to be set on the last letter
of a word.  However, the word boundary determination algorithm only
flags changes at the start of words, so a simplified ``anything but
alphabetic'' end-of-word boundary is determined, instead.

Note that the only user-supplied conditional flags respected are the
locale ones.  Unlike the single-character conversion functions, no
facility is provided to check if setting them would result in
different output.

<<Find next grapheme cluster boundary for case conversion>>=
cond &= UNI_SC_FL_LOCALE;
while(1) {
  <<Set casing condition flags for [[next_cp]]>>
  if(next_gc == slen) {
    cond |= UNI_SC_FL_FINAL_SIGMA;
    break;
  }
  /* warning: _int_ may over-read buffer with invalid UTF input */
  uint32_t ncp = uni_int_utf<<@sz>>_decode(s + next_gc, &clen);
  gc_res = uni_gc_brk(next_cp, ncp, gc_res, 0);
#if 0
  /* finding word break is pointless, since it happens too late */
  word_res = uni_word_brk(next_cp, ncp, word_res, NULL);
  if(word_res > 0)
    cond |= UNI_SC_FL_FINAL_SIGMA;
#else
  /* instead, eoword is defined as last before non-alpha or end */
  if(gc_res > 0) {
    uni_gc_t gc = uni_gc_of(ncp);
    if(!((1ULL << gc) & uni_gc_trans[UNI_gc_L]))
      cond |= UNI_SC_FL_FINAL_SIGMA;
  }
#endif
  next_cp = ncp;
  next_cp_len = clen;
  if(gc_res > 0)
    break;
  next_gc += clen;
  if(next_gc > slen)
    next_gc = slen; /* should never happen with valid UTF input */
}
@

<<Set casing condition flags for [[next_cp]]>>=
if(next_cp == 'I')
  cond |= UNI_SC_FL_AFTER_I;
else if(next_cp == 'i')
  cond |= UNI_SC_FL_AFTER_SOFT_DOTTED;
else if(next_cp == 0x0307) /* COMBINING DOT ABOVE */
  cond |= UNI_SC_FL_BEFORE_DOT;
/* note: this is supposed to be more generic */
/* "any combining mark above" */
/* but it's just for Lithuanian, and I'm using their list */
else if(next_cp == 0x0300 || /* COMBINING GRAVE ACCENT */
        next_cp == 0x0301 || /* COMBINING ACUTE ACCENT */
	next_cp == 0x0303 || /* COMBINING TILDE */
	/* unsure about these two */
	/* but I doubt a Lithuanian will ever complain */
	next_cp == 0x0328 || /* COMBINING OGONEK */
	next_cp == 0x1DCE) /* COMBINING OGONEK ABOVE */
  cond |= UNI_SC_FL_MORE_ABOVE;
@

However, title casing does need to know the start of a new word.  For
this, the official algorithm is used to set a flag.  This algorithm
requires locale knowledge, so the word break table needs to be passed
in, as well.  Since it technically applies to the next character, two
flags are used.  Also, since grapheme cluster extensions are generally
ignored, only the two cluster start characters are compared.

<<[[uni_str_]][[tc]](@sz) proto>>=
/** Convert entire UTF-<<@sz>> string \p s/\p slen to [<<@type>>].
  * If \p slen is less than zero, \p s is zero-terminated.
  * The return buffer parameters \p buf, \p off and \p buf_len are
  * described in \ref uni_return<<@sz>>_buf<<@sz>>.
  * Only the locale-related flags in \p cond are recognized.
  * Additional locale customization can be done by overriding the
  * WB property table (\p WB_tab) used for word boundary detection;
  * the default is to use \ref uni_WB_mtab */
int uni_str_tc<<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                         uint32_t cond, const uint32_t *WB_tab,
			 <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<[[uni_str_is_]][[tc]](@sz) proto>>=
/** Test if entire UTF-<<@sz>> string \p s/\p slen is [tc].
  * If \p slen is less than zero, \p s is zero-terminated.
  * Only the locale-related flags in \p cond are recognized.
  * Additional locale customization can be done by overriding the
  * WB property table (\p WB_tab) used for word boundary detection;
  * the default is to use \ref uni_WB_mtab */
int uni_str_is_tc<<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                            uint32_t cond, const uint32_t *WB_tab)
@

<<[[uni_str_is_]][[cased]](@sz) proto>>=
/** Test if entire UTF-<<@sz>> string \p s/\p slen is cased.
  * If \p slen is less than zero, \p s is zero-terminated.
  * The string is converted to lower-case, upper-case, and title-case,
  * and true is returned only if any of the three are not equal to \p s.
  * Only the locale-related flags in \p cond are recognized.
  * Additional locale customization can be done by overriding the
  * WB property table (\p WB_tab) used for word boundary detection;
  * the default is to use \ref uni_WB_mtab */
int uni_str_is_cased<<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                            uint32_t cond, const uint32_t *WB_tab)
@

<<Initialize string case conversion>>=
#ifdef USE_TC_CODE
int prev_word = 1, next_word = 0;
int word_res = 0;
#endif
@

<<Find next grapheme cluster boundary for case conversion>>=
#ifdef USE_TC_CODE
next_word = prev_word;
if(next_gc != slen) {
  word_res = uni_word_brk(cp, next_cp, word_res, WB_tab);
  prev_word = word_res > 0;
}
#endif
@

Once the flags have been determined, all characters can be converted
into the output buffer.  For regular casing, fallback is to the
original character.   Title casing chooses title or lower case
conversion depending on whether or not the character is at the start
of a word.  Fallback for title casing is upper casing, and then the
original character.

<<Convert a character's case into output, updating [[rlen]]>>=
int32_t rlen1;
const uint16_t *rbuf;
#ifndef USE_TC_CODE /* also always defined when USE_ALL_CODE */
rlen1 = uni_case_convert(cp, cond, &rbuf, simple, special, strs);
#else
#ifdef USE_ALL_CODE
int did_uc = 0, did_lc = 0;
/* doing tc below, even if tc not explicitly used */
#endif
if(next_word) {
  next_word = 0;
  rlen1 = uni_case_convert(cp, cond, &rbuf, uni_stc_mtab, uni_tc_mtab,
                            uni_tc_strs);
  if(!rbuf && rlen1 == cp) {
#ifdef USE_ALL_CODE
    did_uc = 1;
#endif
    rlen1 = uni_case_convert(cp, cond, &rbuf, uni_suc_mtab, uni_uc_mtab,
                              uni_uc_strs);
  }
} else {
#ifdef USE_ALL_CODE
  did_lc = 1;
#endif
  rlen1 = uni_case_convert(cp, cond, &rbuf, uni_slc_mtab, uni_lc_mtab,
                            uni_lc_strs);
}
#endif
@

<<Convert a character's case into output, updating [[rlen]]>>=
#ifndef USE_CHECK_CODE
if(rbuf)
  rlen1 = uni_return16_buf<<@sz>>(rbuf, rlen1, buf, off, buf_len);
else
  rlen1 = uni_return32_buf<<@sz>>((uint32_t *)&rlen1, 1, buf, off, buf_len);
if(off < 0) {
  if(sblen > rlen1) {
    sblen -= rlen1;
    sbuf += rlen1;
  } else
    sblen = 0;
  } else {
    if(!*buf) {
      /* continue gathering lengths, but final return will be NULL */
      sblen = 0;
      buf_len = &sblen;
      off = -1;
    }
  off += rlen1;
}
rlen += rlen1;
#endif
@

<<Convert a character's case into output, updating [[rlen]]>>=
#ifdef USE_CHECK_CODE
<<Check [[rbuf]]/[[rlen1]] for case conversion>>
#ifdef USE_ALL_CODE
if(!did_lc) {
  rlen1 = uni_case_convert(cp, cond, &rbuf, uni_slc_mtab, uni_lc_mtab,
                            uni_lc_strs);
  <<Check [[rbuf]]/[[rlen1]] for case conversion>>
}
if(!did_uc) {
  rlen1 = uni_case_convert(cp, cond, &rbuf, uni_suc_mtab, uni_uc_mtab,
                            uni_uc_strs);
  <<Check [[rbuf]]/[[rlen1]] for case conversion>>
}
#endif
#endif
@

<<Check [[rbuf]]/[[rlen1]] for case conversion>>=
if((!rbuf && rlen1 != cp) ||
   (rbuf && ((rlen1 != 1 && (rlen1 != 2 || !uni_is_surrogate(*rbuf))) ||
             cp != uni_int_utf16_decode(rbuf, NULL))))
#ifdef USE_ALL_CODE
  return 1;
#else
  return 0;
#endif
@

<<FIXME>>=
provide test data & driver
  current test is done externally, with simple driver to call all str
  functions (so char functions are completely untested).  Need test
  text to test conditional flags better and test locale flags
@

For more case transformation fun, there is [[CaseFolding.txt]].  It
provides four different case folding (lower-case) transformations:
common, simple, full, and Turkic special cases.  The common and simple
cases are guaranteed to always have length one.  There are few enough
full and Turkic cases that it makes sense to just encode them using
the same method as the full case conversion tables above.  Note that
the official cf property ignores the Turkic cases.

<<Initialize UCD files>>=
decl_num(scf);
decl_str(cf);
@

<<Parse UCD files>>=
open_f("CaseFolding.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  uint32_t flg;
  uint32_t scf = strtol(fields[2], NULL, 16);
  switch(fields[1][0]) {
    case 'C': /* default; value == cp */
      add_num(scf, scf, 1);
      break;
    case 'S': /* scf == C+S */
      add_num(scf, scf, 1);
      break;
    case 'F': /* cf == C+F */
      flg = 0;
      add_sc(cf, fields[2]);
      break;
    case 'T': /* tr, az lang only; not used by anything, technically */
      flg = UNI_SC_FL_AZ | UNI_SC_FL_TR;
      add_sc(cf, fields[2]);
      break;
  }
}
fclose(f);
<<Postprocess simple casing for [[cf]]>>
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_cf], "Case_Folding", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_cf);
@

Technically, any entry which is in the S class but not in the F class
should have no translation.  However, I believe that there is no such
case.  Like with the case conversions, a set of macros is provided for
cf property lookup.  Since there is no condition input, special macros
are provided for Turkic locales as well.

<<[[uni_case_convert]] cf support for (@type) using condition (@cond)>>=
<<[[uni_case_convert]][[32]] cf support>>
<<[[uni_case_convert]][[16]] cf support>>
<<[[uni_case_convert]][[8]] cf support>>
/** Check if \p cp is case-folded.
  * Returns true if case-folding \p cp does not change \p cp.
  * Special casing uses the condition <<@cond>>. */
int uni_is_<<@type>>(uint32_t cp);
#define uni_is_<<@type>>(cp) \
  uni_case_check(cp, <<@cond>>, uni_scf_mtab, uni_cf_mtab, uni_cf_strs)
@

<<[[uni_case_convert]](@sz) cf support>>=
/** Case-fold \p cp.
  * Return parameters are described with \ref uni_return<<@sz>>_buf<<@sz>>.
  * Special casing uses the condition <<@cond>>. */
int uni_<<@type>><<@sz>>(uint32_t cp, <<Buffer return parameters for UTF-[[<<@sz>>]]>>);
#define uni_<<@type>><<@sz>>(cp, buf, off, buf_len) \
  uni_case_convert<<@sz>>(cp, <<@cond>>, buf, off, buf_len, \
                     uni_return16_buf<<@sz>>, uni_return32_buf<<@sz>>, \
                     uni_scf_mtab, uni_cf_mtab, uni_cf_strs)
/** Case-fold UTF-<<@sz>> string \p s/\p slen.
  * If \p slen is negative, \p s is zero-terminated.
  * Return parameters are described with \ref uni_return<<@sz>>_buf<<@sz>>.
  * Special casing uses the condition <<@cond>>. */
int uni_str_<<@type>><<@sz>>(const uint<<@sz>>_t *s, unsigned int slen,
                           <<Buffer return parameters for UTF-[[<<@sz>>]]>>);
#define uni_str_<<@type>><<@sz>>(s, slen, buf, off, buf_len) \
  uni_str_case_convert<<@sz>>(s, slen, <<@cond>>, buf, off, buf_len, \
                            uni_scf_mtab, uni_cf_mtab, uni_cf_strs)
/** Check if string \p s/\p slen is case-folded.
  * If \p slen is negative, \p s is zero-terminated.
  * Returns true if case-folding \p s does not change \p s.
  * Special casing uses the condition <<@cond>>. */
int uni_str_is_<<@type>><<@sz>>(const uint<<@sz>>_t *s, unsigned int slen);
#define uni_str_is_<<@type>><<@sz>>(s, slen) \
  uni_str_case_check<<@sz>>(s, slen, <<@cond>>, uni_scf_mtab, uni_cf_mtab, \
                          uni_cf_strs)
@

<<Unicode property exports>>=
<<[[uni_case_convert]] cf support for [[cf]] using condition [[0]]>>
<<[[uni_case_convert]] cf support for [[tcf]] using condition [[UNI_SC_FL_TR]]>>
@

<<FIXME>>=
provide test data & driver (see other uni_case_convert stuff)
@

Once case folding is done, the obvious thing to do is compare the
results.   The following function combines the two steps in a way that
does not require intermediate storage.

<<[[uni_case_convert]](@sz) cf support>>=
/** Case-fold and compare two UTF-<<@sz>> strings without intermediate storage.
  * Case-folds \p a/\p alen and \p b/\p blen and compares the results by
  * code point, then length.  Returns -1 if the case-folded \p a is less
  * than the case-folded \p b, 1 if it is greater, and zero if they are equal.
  * The special casing uses the condition flag <<@cond>>. */
int uni_<<@type>>_strcmp<<@sz>>(const uint<<@sz>>_t *a, unsigned int alen,
                            const uint<<@sz>>_t *b, unsigned int blen);
#define uni_<<@type>>_strcmp<<@sz>>(a, alen, b, blen) \
  uni_either_cf_strcmp<<@sz>>(a, alen, b, blen, <<@cond>>)
@

<<Unicode property exports>>=
<<[[uni_either_cf_strcmp]][[32]] proto>>;
<<[[uni_either_cf_strcmp]][[16]] proto>>;
<<[[uni_either_cf_strcmp]][[8]] proto>>;
@

<<[[uni_either_cf_strcmp]](@sz) proto>>=
/** Case-fold and compare two UTF-<<@sz>> strings without intermediate storage.
  * Case-folds \p a/\p alen and \p b/\p blen and compares the results by
  * code point, then length.  Returns -1 if the case-folded \p a is less
  * than the case-folded \p b, 1 if it is greater, and zero if they are equal.
  * The special casing uses the condition flag \p cond, which is expected
  * to either be zero or UNI_SC_FLAG_TR. */
int uni_either_cf_strcmp<<@sz>>(const uint<<@sz>>_t *a, unsigned int alen,
                              const uint<<@sz>>_t *b, unsigned int blen,
			      uint32_t cond)
@

<<case.c>>=
<<[[uni_either_cf_strcmp]][[32]]>>
<<[[uni_either_cf_strcmp]][[16]]>>
<<[[uni_either_cf_strcmp]][[8]]>>
@

<<[[uni_either_cf_strcmp]](@sz)>>=
<<[[uni_either_cf_strcmp]][[<<@sz>>]] proto>>
{
  const uint16_t *arbuf = NULL, *brbuf = NULL;
  uint32_t arlen = 0, brlen = 0;
  uint32_t ac, bc;
  while((alen || arlen) && (blen || brlen)) {
    <<Get next case folded character from [[a]] using [[uni_cf]]>>
    <<Get next case folded character from [[b]] using [[uni_cf]]>>
    if(ac > bc)
      return 1;
    if(ac < bc)
      return -1;
  }
  return blen ? -1 : alen > 0;
}
@

<<Get next case folded character from (@ab) using (@stab)>>=
if(!<<@ab>>rlen) {
  unsigned int clen;
  <<Get case conversion result for next character using [[<<@stab>>]]>>
  if(clen > <<@ab>>len)
    return a == <<@ab>> ? 2 : -2; /* invalid input always mismatches */
  <<@ab>>len -= clen;
  <<@ab>> += clen;
}
if(!<<@ab>>rbuf) {
  <<@ab>>c = <<@ab>>rlen;
  <<@ab>>rlen = 0;
} else {
  unsigned int clen;
  <<@ab>>c = uni_int_utf16_decode(<<@ab>>rbuf, &clen);
  <<@ab>>rbuf += clen;
  <<@ab>>rlen -= clen;
}
@

<<Get case conversion result for next character using (@stab)>>=
<<@ab>>rlen = uni_case_convert(uni_int_utf<<@sz>>_decode(<<@ab>>, &clen), cond,
                             &<<@ab>>rbuf, uni_scf_mtab, <<@stab>>_mtab,
			     <<@stab>>_strs);
@

If the inputs are not stored in a form for which the case folding
transformation is valid, normalization needs to be done first.  In
fact, the Unicode standard expects normalization to performed both
before \emph{and after} case folding.  The simpler of these two
requires canonical normalization before and after, and the more
complex requires canonical, followed by case folding, followed by
compatibility, followed by case folding, followed by compatibility
again.  In order to reduce the complexity of these ridiculous
transformations, a few observations are made:

\begin{itemize}
\item Case folding applies one character at a time, so canonical
ordering does not affect case folding.  Likewise, decomposition
applies one character at a time, so canonical ordering does not affect
decomposition.  Thus, rather than decomposing, ordering, folding,
decomposing, and then ordering again, the first ordering can be
removed.  Similarly, for compatibility comparison, both inner
decompositions' ordering step can be skipped.
\item Since the result for any particular character of the entire
sequence of decompositions and case folding is independent of context,
a table can be pre-built with the results.  A similar table has
already been provided by the UCD for NFKC\_CF transformation; this
would just provide tables for NFD\_CF and NFKD\_CF transformations.
Note that the UCD-provided NFKC\_CF table also removes default
ignorable code points.  If it were not for that, the NFKC\_CF table
would in fact provide exactly what's needed for NFKD\_CF (this table
is misnamed; it does the decomposition and case folding, but does not
perform the final ordering and composition steps needed for NFKC\_CF).
\end{itemize}

In other words, it is possible to create a pair of new string
properties which perform all steps of the transformation except for
canonical ordering.  The first of these to generate is the canonical
decomposition form.  This performs NFD, case folding, and NFD again.
For the first step, the NFD decomposition table is scanned.  Any
decomposition is then case folded one character at a time.  For those
characters which do not decompose, the case folding tables are scanned
at the same time, so those entries are added to the table as well.
Technically the latter could be skipped, and a failure to look up a
code point in the generated table would fall back to normal case
folding.

The format of the table is similar to the long-form case conversion
tables: length words with flags, followed by strings.  However, to
save a little space, if there is only one string, with no flags, of
length one, no length word is stored.  Even if that string ends up as
two 16-bit words, the surrogate initiators cannot be confused with
length words if explicitly checked for.

Note that decompositions which are not explicitly stored cannot be
case converted.  Currently, this is the set of Hangul syllable
characters.  Since none of the L, V, and T elements have case
translations, this should be fine.  However, the lookup routine needs
to be aware that zero means that Hangul syllable decomposition needs
to happen.

<<Initialize UCD files>>=
decl_str(NFD_CF);
@

<<Parse UCD files>>=
prop_NFD_CF = add_prop("NFD_CF");
prop_t *nfd_cf = &parsed_props[prop_NFD_CF];
prop_t *cf = &parsed_props[prop_cf];
prop_t *scf = &parsed_props[prop_scf];
nfd_cf->max_len = dmf_prop->max_len;
inisize(nfd_cf->str_arr, nfd_cf->max_len);
nfd_cf->max_strs = cf->max_strs;
inisize(nfd_cf->strs, nfd_cf->max_strs);
/* prepare for merging and searching; may already be in order */
qsort(dmf_prop->str_arr, dmf_prop->len, sizeof(*dmf_prop->str_arr), cmp_cp_flg);
{
  unsigned int nfdi, cfi, scfi;
  uint32_t prev_cp = 0;

  for(nfdi = 0; dmf_prop->str_arr[nfdi].flags; nfdi++);
  for(cfi = scfi = 0; nfdi < dmf_prop->len || cfi < cf->len || scfi < scf->len; ) {
    check_size(nfd_cf->str_arr, nfd_cf->max_len, nfd_cf->len + 1);
    raw_cp_str_t *nent = &nfd_cf->str_arr[nfd_cf->len++];
    nent->off = nfd_cf->strs_len;
    /* prefer nfd over all */
    if(nfdi < dmf_prop->len &&
       (cfi == cf->len ||
        dmf_prop->str_arr[nfdi].cp <= cf->str_arr[cfi].cp) &&
       (scfi == scf->len ||
        dmf_prop->str_arr[nfdi].cp <= scf->rng_dat32[scfi].low +
	                              scf->rng_dat32[scfi].len)) {
      <<Copy and case-fold decomposition [[nfdi]] into [[nent]]/[[nfd_cf]]>>
      if(cfi < cf->len && dmf_prop->str_arr[nfdi].cp == cf->str_arr[cfi].cp)
        cfi++;
      if(scfi < scf->len &&
         dmf_prop->str_arr[nfdi].cp == scf->rng_dat32[scfi].low +
                                       scf->rng_dat32[scfi].len)
        scfi++;
      while(++nfdi < dmf_prop->len && dmf_prop->str_arr[nfdi].flags);
    /* prefer cf over scf */
    } else if(cfi < cf->len &&
              (scfi == scf->len ||
 	       cf->str_arr[cfi].cp <= scf->rng_dat32[scfi].low +
	                              scf->rng_dat32[scfi].len)) {
      <<Copy full case fold entry into [[nent]]/[[nfd_cf]]>>
      if(scfi < scf->len && cf->str_arr[cfi].cp == scf->rng_dat32[scfi].low +
                                                   scf->rng_dat32[scfi].len)
        scfi++;
      cfi++;
    /* fall back to scf */
    } else {
      <<Copy simple case fold entry into [[nent]]/[[nfd_cf]]>>
      if(cp == scf->rng_dat32[scfi].low + scf->rng_dat32[scfi].len)
        scfi++;
    }
    prev_cp = nent->cp;
  }
}
@

<<Copy and case-fold decomposition (@deci) into [[nent]]/(@prop)>>=
/* + 1 to allow for len/flag word */
unsigned int out_len = dmf_prop->str_arr[<<@deci>>].len + 1;
/* * 2 to allow room for tcf */
out_len = out_len * 2 + <<@prop>>->strs_len;
check_size(<<@prop>>->strs, <<@prop>>->max_strs, out_len);
nent->cp = dmf_prop->str_arr[<<@deci>>].cp;
/* plain */
unsigned int off = <<@prop>>->strs_len++; /* where to put len */
int has_turk = 0;
const uint32_t *str = dmf_prop->strs + dmf_prop->str_arr[<<@deci>>].off;
for(i = 0; i < dmf_prop->str_arr[<<@deci>>].len; i++) {
  <<Case fold [[str + i]] into [[<<@prop>>]] for non-Turkic>>
}
unsigned int slen = <<@prop>>->strs_len - off - 1;
for(i = slen; i > 0; i--)
  if(<<@prop>>->strs[off + i] > 0xffff)
    slen++;
if(slen > UNI_SC_FL_LEN_MASK) {
  fprintf(stderr, "<<@prop>> overflow %04X %d\n", nent->cp, slen);
  exit(1);
}
<<@prop>>->strs[off] = slen;
/* turkic */
if(has_turk) {
  off = <<@prop>>->strs_len++; /* where to put len & flags */
  for(i = 0; i < dmf_prop->str_arr[<<@deci>>].len; i++) {
    <<Case fold [[str + i]] into [[<<@prop>>]] for Turkic>>
  }
  slen = <<@prop>>->strs_len - off - 1;
  for(i = slen; i > 0; i--)
    if(<<@prop>>->strs[off + i] > 0xffff)
      slen++;
  if(slen > UNI_SC_FL_LEN_MASK) {
    fprintf(stderr, "<<@prop>> overflow %04X %d\n", nent->cp, slen);
    exit(1);
  }
  <<@prop>>->strs[off] = slen | UNI_SC_FL_AZ | UNI_SC_FL_TR;
}
if(<<@prop>>->strs_len == nent->off + 2 &&
   !(<<@prop>>->strs[nent->off] & UNI_SC_FL_TR)) {
  <<@prop>>->strs_len--;
  <<@prop>>->strs[nent->off] = <<@prop>>->strs[nent->off + 1];
}
nent->len = <<@prop>>->strs_len - nent->off;
@

<<Case fold (@char pointer) into (@prop) for non-Turkic>>=
raw_cp_str_t *cfs = bsearch(<<@char pointer>>, cf->str_arr, cf->len,
	                    sizeof(*cf->str_arr), uni_cmp_cp);
if(cfs) {
  const uint32_t *cfstr = cf->strs + cfs->off;
  unsigned int cflen = cfs->len;
  if(*cfstr & UNI_SC_FL_TR) {
    cflen -= (*cfstr & UNI_SC_FL_LEN_MASK) + 1;
    cfstr += (*cfstr & UNI_SC_FL_LEN_MASK) + 1;
    has_turk = 1;
  }
  if(cflen) {
    if(cflen != (*cfstr & UNI_SC_FL_LEN_MASK) + 1)
      has_turk = 1;
    cflen = *cfstr & UNI_SC_FL_LEN_MASK;
    cfstr++;
    if(cflen > 1) {
      out_len += cflen - 1;
      check_size(<<@prop>>->strs, <<@prop>>->max_strs, out_len);
    }
    while(cflen) {
      <<@prop>>->strs[<<@prop>>->strs_len++] = *cfstr++;
      cflen--;
    }
    continue;
  }
  /* else drop through and try scf */
}
uni_chrrng_dat32_t *res, scp;
scp.low = *(<<@char pointer>>);
scp.len = 0;
res = bsearch(&scp, scf->rng_dat32, scf->len, sizeof(scp), uni_cmprng_dat32);
<<@prop>>->strs[<<@prop>>->strs_len++] =
  res ? num_of(res->dat) : *(<<@char pointer>>);
@

<<UCD parser local functions>>=
static uint32_t num_of(uint32_t d32)
{
  /* wtf, C?  You used to be so cool.. */
  /* this removes gcc's "type punned pointer" warning when casting directly */
  /* gcc doesn't even like (void *)&d32; it needs an intermediary like p */
  /* if this compiles to more than a single instruction, the compiler is stupid */
  void *p = &d32;

  return ((uni_frac_t *)p)->num;
}
@

<<Case fold (@char pointer) into (@prop) for Turkic>>=
raw_cp_str_t *cfs = bsearch(<<@char pointer>>, cf->str_arr, cf->len,
	                    sizeof(*cf->str_arr), uni_cmp_cp);
if(cfs) {
  const uint32_t *cfstr = cf->strs + cfs->off;
  unsigned int cflen = cfs->len;
  if(!(*cfstr & UNI_SC_FL_TR) &&
     cflen > (*cfstr & UNI_SC_FL_LEN_MASK) + 1) {
    cflen -= (*cfstr & UNI_SC_FL_LEN_MASK) + 1;
    cfstr += (*cfstr & UNI_SC_FL_LEN_MASK) + 1;
  }
  cflen = *cfstr & UNI_SC_FL_LEN_MASK;
  cfstr++;
  if(cflen > 1) {
    out_len += cflen - 1;
    check_size(<<@prop>>->strs, <<@prop>>->max_strs, out_len);
  }
  while(cflen) {
    <<@prop>>->strs[<<@prop>>->strs_len++] = *cfstr++;
    cflen--;
  }
} else {
  uni_chrrng_dat32_t *res, scp;
  scp.low = *(<<@char pointer>>);
  scp.len = 0;
  res = bsearch(&scp, scf->rng_dat32, scf->len, sizeof(scp), uni_cmprng_dat32);
  <<@prop>>->strs[<<@prop>>->strs_len++] =
    res ? num_of(res->dat) : *(<<@char pointer>>);
}
@

<<Copy full case fold entry into [[nent]]/[[nfd_cf]]>>=
nent->cp = cf->str_arr[cfi].cp;
check_size(nfd_cf->strs, nfd_cf->max_strs,
           cf->str_arr[cfi].len + nfd_cf->strs_len);
if(cf->str_arr[cfi].len == 2 &&
   !(cf->strs[cf->str_arr[cfi].off] & UNI_SC_FL_TR)) {
  nfd_cf->strs[nfd_cf->strs_len] = cf->strs[cf->str_arr[cfi].off + 1];
  nent->len = 1;
} else {
  cpybuf(nfd_cf->strs + nfd_cf->strs_len, cf->strs + cf->str_arr[cfi].off,
         cf->str_arr[cfi].len);
  nent->len = cf->str_arr[cfi].len;
}
nfd_cf->strs_len += nent->len;
/* if turkic only, add scf */
if(nfd_cf->strs[nent->off] == (UNI_SC_FL_TR | UNI_SC_FL_AZ | (nent->len - 1)) &&
   scfi < scf->len && cf->str_arr[cfi].cp >= scf->rng_dat32[scfi].low) {
  check_size(nfd_cf->strs, nfd_cf->max_strs, nfd_cf->strs_len + 2);
  nfd_cf->strs[nfd_cf->strs_len++] = 1;
  nfd_cf->strs[nfd_cf->strs_len++] = num_of(scf->rng_dat32[scfi].dat);
  nent->len += 2;
}
@

<<Copy simple case fold entry into [[nent]]/[[nfd_cf]]>>=
uint32_t cp = prev_cp + 1;
if(cp < scf->rng_dat32[scfi].low)
  cp = scf->rng_dat32[scfi].low;
check_size(nfd_cf->strs, nfd_cf->max_strs, nfd_cf->strs_len + 1);
nent->cp = cp;
nent->len = 1;
nfd_cf->strs[nfd_cf->strs_len++] = num_of(scf->rng_dat32[scfi].dat);
@

The final NFD pass consists of NFD decomposition followed by canonical
ordering.  The canonical ordering needs to be done externally.  The
decomposition can be skipped if the above table already produces
normalized output.  The following code verifies that this is, in fact,
the case.  If the UCD ever fails this test, all changed entries will
have to be reallocated and fixed.

<<Parse UCD files>>=
for(i = 0; i < nfd_cf->len; i++) {
  const uint32_t *str = nfd_cf->strs + nfd_cf->str_arr[i].off;
  unsigned int tlen = nfd_cf->str_arr[i].len - 1, slen;
  
  <<Check if [[str]] is canonically decomposed>>
  if(!tlen)
    continue;
  <<Check if [[str]] is canonically decomposed>>
}
@

<<Check if [[str]] is canonically decomposed>>=
slen = tlen ? *str++ & UNI_SC_FL_LEN_MASK : *str > 0xffff ? 2 : 1;
if(!tlen)
  tlen++;
for(; slen; slen--, tlen--, str++) {
  if(*str > 0xffff)
    slen--;
  raw_cp_str_t *dec = bsearch(str, dmf_prop->str_arr, dmf_prop->len,
                              sizeof(*dmf_prop->str_arr), uni_cmp_cp);
  if(dec) {
    while(dec > dmf_prop->str_arr && dec[-1].cp == dec->cp)
      dec--;
    if(!dec->flags) {
      fprintf(stderr, "FIXME: nfd_cf %04X %04X\n", nfd_cf->str_arr[i].cp, *str);
      exit(1);
    }
  }
}
@

The second of these to generate is the compatibility decomposition
form.  This performs NFD, followed by case folding, followed by NFKD,
followed by case folding again, followed by NFKD again.  The first two
steps have already been performed to generate the above table, so that
is mostly copied into the result while applying NFKD and case folding.
At the same time, the NFKD table is used to fill in any gaps.  Using
the tables generated above means that it is unnecessary to fill in
gaps using the case folding tables (although, again, technically this
could just fall back to the case folding tables and not store them
explicitly anyway).

<<Initialize UCD files>>=
decl_str(NFKD_CF);
@

<<Parse UCD files>>=
prop_NFKD_CF = add_prop("NFKD_CF");
prop_t *nfkd_cf = &parsed_props[prop_NFKD_CF];
nfkd_cf->max_len = nfd_cf->max_len;
inisize(nfkd_cf->str_arr, nfkd_cf->max_len);
nfkd_cf->max_strs = cf->max_strs;
inisize(nfkd_cf->strs, nfkd_cf->max_strs);
{
  unsigned int nfdi, nfkdi;

  for(nfkdi = 0; dmf_prop->str_arr[nfkdi].flags != 1; nfkdi++);
  for(nfdi = 0; nfkdi < dmf_prop->len || nfdi < nfd_cf->len; ) {
    check_size(nfkd_cf->str_arr, nfkd_cf->max_len, nfkd_cf->len + 1);
    raw_cp_str_t *nent = &nfkd_cf->str_arr[nfkd_cf->len++];
    /* FIXME: share string table with nfd_cf? */
    nent->off = nfkd_cf->strs_len;
    /* start with nfd_cf */
    if(nfdi < nfd_cf->len &&
       (nfkdi == dmf_prop->len ||
        nfd_cf->str_arr[nfdi].cp <= dmf_prop->str_arr[nfkdi].cp)) {
      <<Copy, compatibility decompose and case fold [[nfd_cf]] entry into [[nent]]/[[nfkd_cf]]>>
      if(nfkdi != dmf_prop->len &&
         dmf_prop->str_arr[nfkdi].cp == nfd_cf->str_arr[nfdi].cp)
        while(++nfkdi < dmf_prop->len && dmf_prop->str_arr[nfkdi].flags != 1);
      nfdi++;
    /* othewise, there's a plain nfkd entry waiting */
    } else {
      <<Copy and case-fold decomposition [[nfkdi]] into [[nent]]/[[nfkd_cf]]>>
      while(++nfkdi < dmf_prop->len && dmf_prop->str_arr[nfkdi].flags != 1);
    }
  }
}
@

<<Copy, compatibility decompose and case fold [[nfd_cf]] entry into [[nent]]/[[nfkd_cf]]>>=
unsigned int out_len = nfd_cf->str_arr[nfdi].len + nfkd_cf->strs_len + 1;
check_size(nfkd_cf->strs, nfkd_cf->max_strs, out_len);
nent->cp = nfd_cf->str_arr[nfdi].cp;
/* plain */
int has_turk = 0;
const uint32_t *str = nfd_cf->strs + nfd_cf->str_arr[nfdi].off;
unsigned int slen = nfd_cf->str_arr[nfdi].len;
if(slen != 1) {
  if(*str & UNI_SC_FL_TR) {
    has_turk = 1;
    slen -= (*str & UNI_SC_FL_LEN_MASK) + 1;
    str += (*str & UNI_SC_FL_LEN_MASK) + 1;
  }
  if(slen)
    slen = *str++ & UNI_SC_FL_LEN_MASK;
}
unsigned int off;
int no_plain = !slen; /* needs nfkc(X) entry if no plain */
if(no_plain)
  fprintf(stderr, "%04X\n", nent->cp);
if(!no_plain) {
#undef DO_TURKIC
  <<Compatibility decompose and case fold [[str]]>>
}
/* turkic */
if(has_turk) {
#define DO_TURKIC 1
  <<Compatibility decompose and case fold [[str]]>>
#undef DO_TURKIC
}
nent->len = nfkd_cf->strs_len - nent->off;
if(nent->len == 2 && !(nfkd_cf->strs[nent->off] & UNI_SC_FL_TR)) {
  nfkd_cf->strs_len--;
  nent->len--;
  nfkd_cf->strs[nent->off] = nfkd_cf->strs[nent->off + 1];
}
@

<<Compatibility decompose and case fold [[str]]>>=
off = nfkd_cf->strs_len++; /* where to put len */
for(i = 0; i < slen; i++) {
  raw_cp_str_t *dm = bsearch(str + i, dmf_prop->str_arr, dmf_prop->len,
	                     sizeof(*dmf_prop->str_arr), uni_cmp_cp);
  if(dm) {
    /* only look for compatibility decomps */
    /* there shouldn't be any canon decomps anyway */
    while(dm > dmf_prop->str_arr && dm[-1].cp == dm->cp)
      dm--;
    if(!dm->flags)
      dm++;
    if(dm->flags != 1)
      dm = NULL;
  }
  if(dm) {
    const uint32_t *dmstr = dmf_prop->strs + dm->off;
    if(dm->len > 1) {
      out_len += dm->len - 1;
      check_size(nfkd_cf->strs, nfkd_cf->max_strs, out_len);
    }
    for(j = 0; j < dm->len; j++) {
#ifdef DO_TURKIC
      <<Case fold [[dmstr + j]] into [[nfkd_cf]] for Turkic>>
#else
      <<Case fold [[dmstr + j]] into [[nfkd_cf]] for non-Turkic>>
#endif
    }
  } else
    nfkd_cf->strs[nfkd_cf->strs_len++] = str[i];
}
slen = nfkd_cf->strs_len - off - 1;
for(i = slen; i > 0; i--)
  if(nfkd_cf->strs[off + i] > 0xffff)
    slen++;
if(slen > UNI_SC_FL_LEN_MASK) {
  fprintf(stderr, "nfkd_cf overflow %04X %d\n", nent->cp, slen);
  exit(1);
}
#ifdef DO_TURKIC
slen |= UNI_SC_FL_TR | UNI_SC_FL_AZ;
#endif
nfkd_cf->strs[off] = slen;
@

Once again, the final NFKD decomposition step can be skipped if the
result is already fully decomposed.  The following code verifies this.

<<Parse UCD files>>=
for(i = 0; i < nfkd_cf->len; i++) {
  const uint32_t *str = nfkd_cf->strs + nfkd_cf->str_arr[i].off;
  unsigned int tlen = nfkd_cf->str_arr[i].len - 1, slen;
  
  <<Check if [[str]] is compatibility decomposed>>
  if(!tlen)
    continue;
  <<Check if [[str]] is compatibility decomposed>>
}
@

<<Check if [[str]] is compatibility decomposed>>=
slen = tlen ? *str++ & UNI_SC_FL_LEN_MASK : *str > 0xffff ? 2 : 1;
if(!tlen)
  tlen++;
for(; slen; slen--, tlen--, str++) {
  if(*str > 0xffff)
    slen--;
  raw_cp_str_t *dec = bsearch(str, dmf_prop->str_arr, dmf_prop->len,
                              sizeof(*dmf_prop->str_arr), uni_cmp_cp);
  if(dec) {
    while(dec > dmf_prop->str_arr && dec[-1].cp == dec->cp)
      dec--;
    if(!dec->flags || dec->flags == 1) {
      fprintf(stderr, "FIXME: nfkd_cf %04X %04X\n", nfkd_cf->str_arr[i].cp, *str);
      exit(1);
    }
  }
}
@

Once finished, they can be dumped.

<<Dump character information as C code>>=
dump_str_tabs(nfd_cf, "NFD_CF", gen_h, tstf);
dump_str_tabs(nfkd_cf, "NFKD_CF", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_NFD_CF);
enable_str_mt_p(prop_NFKD_CF);
@

<<FIXME>>=
share string tables with NFKD_CF or other _decs?
@

Given the above properties, string comparison is pretty simple.  The
main complication is that any character with non-zero canonical
combining class requires accumulation of any adjacent characters with
non-zero ccc, followed by the canonical ordering step.  The only time
this can be skipped is if the entire group matches exactly.

<<[[uni_case_convert]](@sz) cf support>>=
<<[[uni_case_convert]] cf support using [[NFD]]>>
<<[[uni_case_convert]] cf support using [[NFKD]]>>
@

<<[[uni_case_convert]] cf support using (@norm)>>=
/** Case-fold, normalize and compare two UTF-<<@sz>> strings.
  * Normalizes and case-folds \p a/\p alen and \p b/\p blen and compares
  * the results by code point, then length.  Returns -1 if the
  * normalized, case-folded \p a is less than the normalized, case-folded
  * \p b, 1 if it is greater, and zero if they are equal.
  * The strings are normalized to <<@norm>>, and the special casing uses the
  * condition flag <<@cond>>. */
int uni_<<@norm>>_<<@type>>_strcmp<<@sz>>(const uint<<@sz>>_t *a, unsigned int alen,
                                   const uint<<@sz>>_t *b, unsigned int blen);
#define uni_<<@norm>>_<<@type>>_strcmp<<@sz>>(a, alen, b, blen) \
  uni_norm_cf_strcmp<<@sz>>(a, alen, b, blen, <<@cond>>, \
                            uni_<<@norm>>_CF_mtab, uni_<<@norm>>_CF_strs)
@

<<Unicode property exports>>=
<<[[uni_norm_cf_strcmp]][[32]] proto>>;
<<[[uni_norm_cf_strcmp]][[16]] proto>>;
<<[[uni_norm_cf_strcmp]][[8]] proto>>;
@

<<[[uni_norm_cf_strcmp]](@sz) proto>>=
/** Case-fold, normalize and compare two UTF-<<@sz>> strings.
  * Normalizes and case-folds \p a/\p alen and \p b/\p blen and compares
  * the results by code point, then length.  Returns -1 if the
  * normalized, case-folded \p a is less than the normalized, case-folded
  * \p b, 1 if it is greater, and zero if they are equal.
  * The strings are normalized using the property values \p ncf_mtab
  * and \p ncf_strs, and the special casing uses the condition flag \p cond. */
int uni_norm_cf_strcmp<<@sz>>(const uint<<@sz>>_t *a, unsigned int alen,
                            const uint<<@sz>>_t *b, unsigned int blen,
			    uint32_t cond,
			    const uint32_t *ncf_mtab, const uint16_t *ncf_strs)
@

<<case.c>>=
<<[[uni_norm_cf_strcmp]][[32]]>>
<<[[uni_norm_cf_strcmp]][[16]]>>
<<[[uni_norm_cf_strcmp]][[8]]>>
@

<<[[uni_norm_cf_strcmp]](@sz)>>=
<<[[uni_norm_cf_strcmp]][[<<@sz>>]] proto>>
{
  const uint16_t *arbuf = NULL, *brbuf = NULL;
  uint32_t arlen = 0, brlen = 0;
  uint32_t ac, bc;
  uint32_t abuf[4], *abuf_more = NULL, bbuf[4], *bbuf_more = NULL;
  /* 8 bits for ccc, 24 bits for index into buf */
  uint32_t accc[4], *accc_more = NULL, bccc[4], *bccc_more = NULL;
  unsigned int abuf_len = 0, bbuf_len = 0, abuf_ptr = 0, bbuf_ptr = 0,
               abuf_max = 0, bbuf_max = 0;
  while((alen || arlen) && (blen || brlen)) {
    <<Get next normalized, case folded character from [[a]] using [[ncf]]>>
    <<Get next normalized, case folded character from [[b]] using [[ncf]]>>
    if(ac > bc) {
      <<Free normalization buffers, if allocated>>
      return 1;
    }
    if(ac < bc) {
      <<Free normalization buffers, if allocated>>
      return -1;
    }
  }
  <<Free normalization buffers, if allocated>>
  return blen ? -1 : alen > 0;
}
@

<<Free normalization buffers, if allocated>>=
if(abuf_more)
  free(abuf_more);
if(bbuf_more)
  free(bbuf_more);
if(accc_more)
  free(accc_more);
if(bccc_more)
  free(bccc_more);
@

<<Get next normalized, case folded character from (@ab) using (@stab)>>=
if(<<@ab>>buf_ptr == <<@ab>>buf_len) {
  <<Get next case folded character from [[<<@ab>>]] using [[<<@stab>>]]>>
  uni_ccc_t ccc = uni_ccc_of(<<@ab>>c);
  if(ccc) {
    <<@ab>>ccc[0] = ccc;
    <<@ab>>buf[0] = <<@ab>>c;
    <<@ab>>buf_len = 1;
    while(<<@ab>>len || <<@ab>>rlen) {
      if(<<@ab>>buf_len == 4) {
        <<@ab>>buf_more = malloc(8 * sizeof(*<<@ab>>buf_more));
	<<@ab>>ccc_more = malloc(8 * sizeof(*<<@ab>>ccc_more));
	<<@ab>>buf_max = 8;
	if(!<<@ab>>buf_more || !<<@ab>>ccc_more) {
	  /* no way to indicate mem errors; just return mismatch */
	  <<Free normalization buffers, if allocated>>
	  return a == <<@ab>> ? 3 : -3;
	}
	memcpy(<<@ab>>buf_more, <<@ab>>buf, sizeof(<<@ab>>buf));
	memcpy(<<@ab>>ccc_more, <<@ab>>ccc, sizeof(<<@ab>>ccc));
      } else if(<<@ab>>buf_len == <<@ab>>buf_max) {
        <<@ab>>buf_max *= 2;
	uint32_t *t = realloc(<<@ab>>buf_more, <<@ab>>buf_max * sizeof(*t));
	if(!t) {
	  <<Free normalization buffers, if allocated>>
	  return a == <<@ab>> ? 3 : -3;
	}
	<<@ab>>buf_more = t;
	t = realloc(<<@ab>>ccc_more, <<@ab>>buf_max * sizeof(*t));
	if(!t) {
	  <<Free normalization buffers, if allocated>>
	  return a == <<@ab>> ? 3 : -3;
	}
	<<@ab>>ccc_more = t;
      }
      <<Get next case folded character from [[<<@ab>>]] using [[<<@stab>>]]>>
      ccc = uni_ccc_of(<<@ab>>c);
      if(<<@ab>>buf_len < 4) {
        <<@ab>>buf[<<@ab>>buf_len] = <<@ab>>c;
	<<@ab>>ccc[<<@ab>>buf_len] = ccc;
      } else {
        <<@ab>>buf_more[<<@ab>>buf_len] = <<@ab>>c;
	<<@ab>>ccc_more[<<@ab>>buf_len] = ccc;
      }
      <<@ab>>buf_len++;
      if(!ccc)
        break;
    }
    uint32_t *cccp = <<@ab>>buf_len > 4 ? <<@ab>>ccc_more : <<@ab>>ccc;
    unsigned int i;
    /* set back pointer for after sort */
    /* also makes sort stable */
    for(i = 0; i < <<@ab>>buf_len; i++) {
      if(!cccp[i]) /* last one may be 0 */
        break;
      /* << 23 instead of 24 to allow signed compares */
      cccp[i] = (cccp[i] << 23) | i; /* sort by ccc first, then index */
    }
    qsort(cccp, i, sizeof(*cccp), uni_cmp_cp);
    <<@ab>>buf_ptr = 0;
  }
}
if(<<@ab>>buf_ptr != <<@ab>>buf_len) {
  uint32_t *cccp = <<@ab>>buf_len > 4 ? <<@ab>>ccc_more : <<@ab>>ccc;
  uint32_t *bp = <<@ab>>buf_len > 4 ? <<@ab>>buf_more : <<@ab>>buf;
  
  if(cccp[<<@ab>>buf_ptr])
    <<@ab>>c = bp[cccp[<<@ab>>buf_ptr] & ((1 << 23) - 1)];
  else
    <<@ab>>c = bp[<<@ab>>buf_ptr];
  <<@ab>>buf_ptr++;
}
@

[[DerivedNormalizationProps.txt]] contains the final case folding
method%
\footnote{The FC\_NFKC property is related to case folding, but is
deprecated and therefore not provided as a property.}%
.  The main difference between it and the other methods is that
it applies multiple transformations simultaneously.  In addition to
case folding, it also removes default ignorable code points and
performs compatibility decomposition on the results repeatedly until
the result is stable.

<<Initialize UCD files>>=
decl_str(NFKC_CF);
@

<<Process a line of [[DerivedNormalizationProps.txt]]>>=
if(num_fields == 3 && !strcmp(fields[1], "NFKC_CF"))
  add_str(NFKC_CF, fields[2]);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_NFKC_CF], "NFKC_CF", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_NFKC_CF);
@

\subsection{Additional Support for Normalization}

The raw property lookup functions are not meant to be used directly. 
Instead, here are some more high-level functions.  They are stored in
their own object file, even though the actual amount of code is tiny.
That way, the tables they reference will not be pulled in for all
properties.

<<Library [[uni]] Members>>=
uni_norm.o
@

\lstset{language=C}
<<Library [[uni]] headers>>=
#include "uni_norm.h"
@

<<uni_norm.h>>=
<<Common C Warning>>
#ifndef UNI_NORM_H
#define UNI_NORM_H
/** \file uni_norm.h High-level normalization support */

#include "uni_prop.h"
/** \addtogroup uni_norm High-level Normalization */
/** @{ */
<<Unicode normalization support exports>>
/** @} */
#endif /* UNI_NORM_H */
@

<<Headers to Install>>=
uni/uni_norm.h \
@

<<uni_norm.c>>=
<<Common C Header>>
#include "uni_all.h"

<<Unicode normalization support local definitions>>

<<Unicode normalization support functions>>
@

The first step in any normalization is to decompose.  The only
difference in the three function groups is what table they use, so
only one group of functions exists; each group just calls the helper
with the appropriate tables.  This is one case where Ada or C++
generics would be very handy; each group of functions is virtually
triplicated with minor changes due to character type.  noweb makes
things additionally complicated by disallowing chunk references inside
preprocessor macro definitions; otherwise, the definition of each
group could have been made using a single preprocessor call.

There are a number of ways a function which modifies a Unicode string
might operate.  For example, it might use separate parameters to
specify input and output strings, or it might always allocate space
for its return value from memory, or it might use the input buffer as
an output buffer as well.  The original versions of these functions
only supported the latter.  However, they assumed that enough room was
available for the output, and moved the tail end of the string around
every time a character's transformed length was not one.  In order to
support checking lengths and resizable output buffers, the input
buffer is provided separately.  The only case where the input buffer
and output buffer are safe to be the same is when the input
buffer length is one character (which may result in [[ilen]] greater
than one in UTF-16 or UTF-8).  No check will be made to ensure that
they do not overlap, though.  To convert an entire string, the user
should duplicate the input string, first.  As a special convenience,
if [[iptr]] is [[NULL]], this is done internally.

<<Unicode normalization support exports>>=
/* wrappers */

/* decomp functions can operate on any number of chars, including 1 */
/* decomp functions return final length */
<<[[uni_X_dec]] protos for [[NFD_dec]]>>
<<[[uni_X_dec]] protos for [[NFKD_dec]]>>
/* NFKC_Casefold assumes NFD has already been run */
<<[[uni_X_dec]] protos for [[NFKC_Casefold]]>>
<<[[do_any_dec]][[32]] proto>>;
<<[[do_any_dec]][[16]] proto>>;
<<[[do_any_dec]][[8]] proto>>;
@

<<[[uni_X_dec]] protos for (@type)>>=
<<[[uni_X_dec]][[32]] proto>>;
<<[[uni_X_dec]][[16]] proto>>;
<<[[uni_X_dec]][[8]] proto>>;
@

<<[[uni_X_dec]](@sz) proto>>=
/** Perform full <<@type>> decomposition on UTF-<<@sz>> string.
  * The string \p ibuf/\p ilen is decomposed to <<@type>> form, but without
  * canonical ordering.  If \p ibuf is NULL, the input starts at the same
  * place as the output (i.e., \p *buf or \p *buf + \p off).  The
  * return string parameters are described with \ref uni_return<<@sz>>_buf<<@sz>>. */
int uni_<<@type>><<@sz>>(const uint<<@sz>>_t *ibuf, unsigned int ilen,
                 <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<Unicode normalization support functions>>=
<<[[do_any_dec]][[32]]>>
<<[[do_any_dec]][[16]]>>
<<[[do_any_dec]][[8]]>>
@

<<[[do_any_dec]](@sz) proto>>=
/** Perform full generic decomposition on UTF-<<@sz>> string.
  * The string \p ibuf/\p ilen is decomposed using decomposition table
  * \p tab/\p str, but without canonical ordering.  If \p ibuf is NULL,
  * the input starts at the same place as the output (i.e., \p *buf or
  * \p *buf + \p off).  The return string parameters are described with
  * \ref uni_return<<@sz>>_buf<<@sz>>. */
int uni_do_any_dec<<@sz>>(const uint<<@sz>>_t *ibuf, unsigned int ilen,
                       <<Buffer return parameters for UTF-[[<<@sz>>]]>>,
		       const uint32_t *tab, const uint16_t *str)
@

<<[[do_any_dec]](@sz)>>=
<<[[do_any_dec]](@sz) proto>>
{
  int i;
  int rlen = 0;
  unsigned int blen = off > 0 ? ~0 : buf_len ? *buf_len : 0, clen = 0;
  uint<<@sz>>_t *bptr = off < 0 && buf ? *buf : NULL;
  if(!ilen)
    return 0;
  uint<<@sz>>_t *ibuftmp = NULL;
  if(!ibuf) {
    /* FIXME: ideally, only duplicate input when output pointer exceeds input */
    /* but for now, just do it always */
    inisize(ibuftmp, ilen);
    cpybuf(ibuftmp, *buf + (off > 0 ? off : 0), ilen);
    ibuf = ibuftmp;
  }
  for(i = 0; i < ilen; i += clen) {
    int16_t doff;
    uint8_t len;
    uint8_t compat;
    const uint32_t *rptr32 = NULL;
    const uint16_t *rptr16 = NULL;
    uint32_t hbuf[3], cp = uni_int_utf<<@sz>>_decode(ibuf + i, &clen);
    uni_x_dec(cp, tab, &doff, &len, &compat, 1);
    if(!len && !doff) {
      len = 1;
      rptr32 = &cp;
    } else if(doff < 0) {
      uni_hangul_syllable_decomp(cp, hbuf, 1);
      rptr32 = hbuf;
    } else
      rptr16 = str + doff;
    len = rptr16 ? uni_return16_buf<<@sz>>(rptr16, len, off < 0 ? &bptr : buf,
	                                   off, off < 0 ? &blen : buf_len)
                 : uni_return32_buf<<@sz>>(rptr32, len, off < 0 ? &bptr : buf,
	                                   off, off < 0 ? &blen : buf_len);
    rlen += len;
    blen -= blen > len ? len : blen;
    if(off >= 0)
      off += len;
    else
      bptr += len;
  }
  if(ibuftmp)
    free(ibuftmp);
  return rlen;
}
@

<<[[uni_dec_any]] for (@type) using (@tab) and (@str)>>=
<<[[uni_dec_any]][[32]]>>
<<[[uni_dec_any]][[16]]>>
<<[[uni_dec_any]][[8]]>>
@

<<[[uni_dec_any]](@sz)>>=
#define uni_<<@type>><<@sz>>(ibuf, ilen, buf, off, buf_len) \
  uni_do_any_dec<<@sz>>(ibuf, ilen, buf, off, buf_len, <<@tab>>, <<@str>>)
@

<<Unicode normalization support exports>>=
<<[[uni_dec_any]] for [[NFD_dec]] using [[uni_canon_decomp_mtab]] and [[uni_dm_strs]]>>
<<[[uni_dec_any]] for [[NFKD_dec]] using [[uni_compat_decomp_mtab]] and [[uni_dm_strs]]>>
<<[[uni_dec_any]] for [[NFKC_Casefold]] using [[uni_NFKC_CF_mtab]] and [[uni_NFKC_CF_strs]]>>
@

Canonical ordering is the second step in any normalization.  This
requires that any consecutive characters with non-zero canonical
combining class be ordered by their canonial combining class.  The
procedure is described as a bubble sort, so the sort must be stable.
These two requirements indicate the need for a side array for sorting,
containing the original position of each character along with its ccc
value.  The code point itself is stored as well, so that no effort is
needed to write out the sorted array without clobbering the old string.
If the last character passed in has a non-zero ccc, there may be more,
so a continuation is requested.  Any characters which have already
been processed, and could not possibly be reordered, are returned as
output.

Since this is just a reordering of characters, there is no need to
provide resizable output or separate input and output buffers.
Instead, only an in-place sorter is provided.

<<Unicode normalization support exports>>=
<<[[canon_order]][[32]] proto>>;
<<[[canon_order]][[16]] proto>>;
<<[[canon_order]][[8]] proto>>;
@

<<[[canon_order]](@sz) proto>>=
/** Perform canonical ordering step of normalization on a UTF-<<@sz>> string.
  * This performs in-place ordering of \p buf of length \p ilen.  It may
  * be used to order partial strings, in which case \p last should be set
  * to true when no more characters are available.  It returns the number
  * of code points processed; if it is less than \p ilen, then more
  * code points are needed before ordering can be performed on the remaining
  * text (which can only happen if \p last is false). */
int uni_Canon_Order<<@sz>>(uint<<@sz>>_t *buf, unsigned int ilen, int last)
@

<<Unicode normalization support functions>>=
<<[[canon_order]][[32]]>>
<<[[canon_order]][[16]]>>
<<[[canon_order]][[8]]>>
@

<<Unicode normalization support local definitions>>=
/* for stable sort */
struct ccs {
  uint32_t cp /* :24 */, ccc /* :8 */;
  unsigned int opos;
};

static int cmpcc(const void *a, const void *b)
{
  const struct ccs *p1 = (const struct ccs *)a;
  const struct ccs *p2 = (const struct ccs *)b;
  if(p1->ccc != p2->ccc)
    return (int32_t)p1->ccc - (int32_t)p2->ccc;
  return (int)p1->opos - (int)p2->opos;
}
@

<<[[canon_order]](@sz)>>=
<<[[canon_order]][[<<@sz>>]] proto>>
{
  int i;
  for(i = 0; i < ilen; i += uni_utf<<@sz>>_nextc(buf + i)) {
    uint32_t cp = uni_int_utf<<@sz>>_decode(buf + i, NULL);
    int cc = uni_ccc_of(cp);
    if(cc) {
      int ni = i + uni_utf<<@sz>>_nextc(buf + i);
      int j, l;
      for(l = 1, j = ni; j < ilen; j += uni_utf<<@sz>>_nextc(buf + j), l++) {
        uint32_t cp2 = uni_int_utf<<@sz>>_decode(buf + j, NULL);
	int cc2 = uni_ccc_of(cp2);
	if(!cc2)
	  break;
      }
      if(j == ilen && !last)
        return i;
      struct ccs ccbuf[l];
      ccbuf[0].ccc = cc;
      ccbuf[0].cp = cp;
      ccbuf[0].opos = i;
      int k;
      for(l = 1, k = ni; k < j; k += uni_utf<<@sz>>_nextc(buf + k), l++) {
        uint32_t cp2 = uni_int_utf<<@sz>>_decode(buf + k, NULL);
	ccbuf[l].ccc = uni_ccc_of(cp2);
	ccbuf[l].cp = cp2;
	ccbuf[l].opos = k;
      }
      qsort(ccbuf, l, sizeof(*ccbuf), cmpcc);
      for(k = 0, j = i; k < l; k++, j += uni_utf<<@sz>>_nextc(buf + j))
        if(ccbuf[k].opos != j)
	  (void)uni_int_utf<<@sz>>_encode(buf + j, ccbuf[k].cp);
    }
  }
  return ilen;
}
@

The last step in any composition normalization is canonical
composition.  Two characers may be combined if the first character has
a canonical combining class of zero and all intervening characters
have a non-zero canonical combining class less than the canonical
combining class of the second character.  This requires a continuation
flag, just like canonical ordering.  However, since this is once again
a function which can transform its input, more thought is required.
Unlike decomposition, it is impossible for a string to expand in
length.  This means that in-place conversion is possible.  In fact,
just like canonical ordering, only in-place transformation is supported.

<<Unicode normalization support exports>>=
<<[[uni_NFC_comp]][[32]] proto>>;
<<[[uni_NFC_comp]][[16]] proto>>;
<<[[uni_NFC_comp]][[8]] proto>>;
@

<<[[uni_NFC_comp]](@sz) proto>>=
/** Perform canonical composition step of normalization for UTF-<<@sz>> string.
  * Performs composition in-place on \p buf of length \p ilen.  The length
  * of the result is returned.  This can be performed on partial
  * strings; set \p nok to a non-NULL pointer to indicate that more data
  * may be available.  The number of code points in the result which are
  * fully processed is returned in \p *nok.  If this is less than the
  * function's return value, the remaining code points in the result
  * have not yet been processed, and must be passed in again along with
  * additional data. */
int uni_NFC_comp<<@sz>>(uint<<@sz>>_t *buf, unsigned int ilen, unsigned int *nok)
@

<<Unicode normalization support functions>>=
<<[[uni_NFC_comp]][[32]]>>
<<[[uni_NFC_comp]][[16]]>>
<<[[uni_NFC_comp]][[8]]>>
@

<<[[uni_NFC_comp]](@sz)>>=
<<[[uni_NFC_comp]][[<<@sz>>]] proto>>
{
  int i, last0 = -1;
  for(i = 0; i < ilen; i += uni_utf<<@sz>>_nextc(buf + i)) {
    uint32_t cp = uni_int_utf<<@sz>>_decode(buf + i, NULL);
    int ccc = uni_ccc_of(cp);
    /* first char must be ccc=0 */
    if(ccc)
      continue;
    int16_t off;
    uint8_t len;
    uni_find_canon_comp(cp, &off, &len);
    if(!len && !off)
      continue; /* no compositions for this char */
    last0 = i;
    int lastccc = 0;
    int j;
    for(j = i + uni_utf<<@sz>>_nextc(buf + i); j < ilen;
        j += uni_utf<<@sz>>_nextc(buf + j)) {
      uint32_t cp2 = uni_int_utf<<@sz>>_decode(buf + j, NULL);
      ccc = uni_ccc_of(cp2);
      /* second char must have no intervening equal ccc */
      /* this assumes canonical ordering has been done */
      if(ccc && lastccc && ccc == lastccc)
        continue;
      /* or greater ccc; this only happens if back down to 0 */
      if(!ccc && lastccc) {
        i = j - 1;
	break;
      }
      lastccc = ccc;
      int ccp = uni_canon_comp(cp, cp2, off, len);
      if(ccp > 0) {
        /* composition replaces char 1 and deletes char 2 */
	int clen = uni_utf<<@sz>>_enclen(ccp);
	int len1 = uni_utf<<@sz>>_nextc(buf + i), len2 = uni_utf<<@sz>>_nextc(buf + j);
	/* make room for ccp, if necessary */
	if(len1 != clen) {
	  /* WARNING: this may cause buffer overrun if */
	  /*  clen > len1 + len2 */
	  movebuf(buf + i + clen, buf + i + len1, j - i - len1);
	  len2 += len1 - clen;
	}
	(void)uni_int_utf<<@sz>>_encode(buf + i, ccp);
	if(len2 > 0 && ilen - j - len2)
	  movebuf(buf + j, buf + j + len2, ilen - j - len2);
        ilen -= len2;
	/* start over with new start char */
	cp = ccp;
	uni_find_canon_comp(cp, &off, &len);
	if(!len && !off)
	  break; /* no compositions for this char */
        j = i;
	lastccc = 0;
	continue;
      }
      /* if no match and hit a ccc=0, try next 1st char */
      if(!ccc) {
        i = j + uni_utf<<@sz>>_prevc(buf + j);
	break;
      }
    }
  }
  if(nok) {
    /* need more if we got a potential start char */
    /* but don't need to look at anything before that char */
    if(last0 < 0)
      *nok = ilen;
    else {
      int16_t off;
      uint8_t len;
      uni_find_canon_comp(uni_int_utf<<@sz>>_decode(buf + last0, NULL), &off, &len);
      *nok = len || off ? last0 : ilen;
    }
  }
  return ilen;
}
@

The above routine may cause buffer overruns if the composition causes
the string to grow.  This happens when the composition result's word
length exceeds the sum of the composed characters' word lengths.  This
can never happen with 32-bit words, because all lengths are one.  This
can never happen with 16-bit words, because the maximum composed
character length is two, and the minimum lengths of the components are
one.  However, with 8-bit words, the maximum result length is 4, and
the minimum component lengths are one.  This leaves plenty of room for
growth.  To ensure this never happens, a check is made while reading
the data.  The only data which does not need to be checked is Hangul
syllable composition: all three are always of length 3.  For the
versions against which this library was built, the check has
succeeded.  If it ever fails, the composition code will need to be
rewritten to support resizable buffers.

<<Parse UCD files>>=
for(i = 0; i < cm_prop->len; i++) {
  const uint32_t *bctab = cm_prop->strs + cm_prop->str_arr[i].off;
  int len1 = uni_utf8_enclen(cm_prop->str_arr[i].cp);
  for(j = 0; j < cm_prop->str_arr[i].len; j += 2) {
    int len2 = uni_utf8_enclen(bctab[j]);
    int clen = uni_utf8_enclen(bctab[j + 1]);
    if(clen > len1 + len2) {
      fprintf(stderr, "Compose UTF-8 overflow: %04X(%d) + %04X(%d) < %04X(%d)\n",
                      (int)cm_prop->str_arr[i].cp, len1, (int)bctab[j], len2,
		      (int)bctab[j + 1], clen);
      exit(1);
    }
  }
}
@

<<Additional parse-ucd includes>>=
#include "uni_io.h" /* uni_utf8_enclen() */
@

<<Additional parse-ucd C files>>=
uni_io.h \
@

To demonstrate proper normalization, here are some functions which do the
full procedure, either in-place or using a separate input buffer.

<<Unicode normalization support exports>>=
<<[[full_dec]] proto for [[NFD]]>>
<<[[full_dec]] proto for [[NFKD]]>>
@

<<[[full_dec]] proto for (@type)>>=
<<[[full_dec]][[32]] proto>>
<<[[full_dec]][[16]] proto>>
<<[[full_dec]][[8]] proto>>
@

<<[[full_dec]](@sz) proto>>=
<<[[uni_dec]] proto>>;
@

<<[[uni_dec]] proto>>=
/** Perform full <<@type>> normalization on a UTF-<<@sz>> string.
  * The string \p ibuf/\p ilen is normalized to <<@type>> form.
  * If \p ibuf is NULL, the input starts at the same place as the output
  * (i.e., \p *buf or \p *buf + \p off).  The return string parameters are
  * described with \ref uni_return<<@sz>>_buf<<@sz>>. */
int uni_<<@type>><<@sz>>(uint<<@sz>>_t *ibuf, unsigned int ilen,
                 <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<Unicode normalization support functions>>=
<<[[full_dec]] for [[NFKD]]>>
<<[[full_dec]] for [[NFD]]>>
@

<<[[full_dec]] for (@type)>>=
<<[[full_dec]][[32]]>>
<<[[full_dec]][[16]]>>
<<[[full_dec]][[8]]>>
@

<<[[full_dec]](@sz)>>=
<<[[uni_dec]] proto>>
{
  <<Decompose using [[<<@type>>]]>>
  return ret;
}
@

<<Decompose using (@type)>>=
if(!ilen)
  return 0;
int ret = uni_<<@type>>_dec<<@sz>>(ibuf, ilen, buf, off, buf_len);
if(!*buf || (off < 0 && ret > *buf_len))
  return ret;
ret = uni_Canon_Order<<@sz>>(*buf + (off > 0 ? off : 0), ret, 1);
if(ret <= 0)
  return ret;
@

<<Unicode normalization support exports>>=
<<[[full_dec]] proto for [[NFC]]>>
<<[[full_dec]] proto for [[NFKC]]>>
@

<<Unicode normalization support functions>>=
<<[[full_comp]] for [[NFKC]] using [[NFKD]]>>
<<[[full_comp]] for [[NFC]] using [[NFD]]>>
@

<<[[full_comp]] for (@type) using (@dec)>>=
<<[[full_comp]][[32]]>>
<<[[full_comp]][[16]]>>
<<[[full_comp]][[8]]>>
@

<<[[full_comp]](@sz)>>=
<<[[uni_dec]] proto>>
{
  <<Decompose using [[<<@dec>>]]>>
  return uni_NFC_comp<<@sz>>(*buf + (off > 0 ? off : 0), ret, NULL);
}
@

Note that [[NFKC_Casefold]] is not a simple in-place function, so the
intermediate result is always duplicated in the last step.  This
should probably be fixed to be less expensive.

<<Unicode normalization support exports>>=
<<[[full_dec]] proto for [[NFKC_CF]]>>
@

<<Unicode normalization support functions>>=
<<[[NFKC_CF]] normalizer using UTF-[[32]]>>
<<[[NFKC_CF]] normalizer using UTF-[[16]]>>
<<[[NFKC_CF]] normalizer using UTF-[[8]]>>
@

<<(@type) normalizer using UTF-(@sz)>>=
<<[[uni_dec]] proto>>
{
  <<Decompose using [[NFD]]>>
  return uni_NFKC_Casefold<<@sz>>(NULL, ret, buf, off, buf_len);
}
@

String comparison using this method should be supported as well.  While
this method technically requires canonical composition as its last
step and canonical decomposition as its first step, both of these can
be skipped when simply using the method for string comparison. Here
are the steps, in detail:

\begin{enumerate}
\item Decomposition using the canonical decomposition tables.
\item Canonical ordering.
\item Decomposition, default ignorable removal, and case folding using
the NFKC\_CF table
\item Decomposition using the canonical decomposition tables.
\item Canonical ordering.
\item Canonical composition.
\end{enumerate}

The second step does not affect the third step, since decomposition
applies to individual characters without context.  It can be dropped,
since the fifth step repeats the process.  The first step does not
affect the third step, because the third step performs the same
transformation, and more.  The fourth step does not have any effect,
because the third step produces fully decomposed text.  The last step
is losslessly reversible using decomposition, so for comparison, there
is no need to compose, either.  That leaves only two steps:  applying
the NFKC\_CF table, and applying canonical ordering.  That means that
the comparison function using NFKC\_CF is identical to the normalizing
case folding functions above, except in how to use the tables.

<<Unicode property exports>>=
<<[[uni_NFKC_CF_strcmp]][[32]] proto>>;
<<[[uni_NFKC_CF_strcmp]][[16]] proto>>;
<<[[uni_NFKC_CF_strcmp]][[8]] proto>>;
@

<<[[uni_NFKC_CF_strcmp]](@sz) proto>>=
/** Case-fold, normalize and compare two UTF-<<@sz>> strings.
  * Normalizes and case-folds \p a/\p alen and \p b/\p blen using NFKC_CF
  * and compares the results by code point, then length.  Returns -1 if the
  * normalized, case-folded \p a is less than the normalized, case-folded
  * \p b, 1 if it is greater, and zero if they are equal.  */
int uni_NFKC_CF_strcmp<<@sz>>(const uint<<@sz>>_t *a, unsigned int alen,
                             const uint<<@sz>>_t *b, unsigned int blen)
@

<<case.c>>=
<<[[uni_NFKC_CF_strcmp]][[32]]>>
<<[[uni_NFKC_CF_strcmp]][[16]]>>
<<[[uni_NFKC_CF_strcmp]][[8]]>>
@

<<[[uni_NFKC_CF_strcmp]](@sz)>>=
<<[[uni_NFKC_CF_strcmp]][[<<@sz>>]] proto>>
{
  const uint16_t *arbuf = NULL, *brbuf = NULL;
  uint32_t arlen = 0, brlen = 0;
  uint32_t ac, bc;
  uint32_t abuf[4], *abuf_more = NULL, bbuf[4], *bbuf_more = NULL;
  /* 8 bits for ccc, 24 bits for index into buf */
  uint32_t accc[4], *accc_more = NULL, bccc[4], *bccc_more = NULL;
  unsigned int abuf_len = 0, bbuf_len = 0, abuf_ptr = 0, bbuf_ptr = 0,
               abuf_max = 0, bbuf_max = 0;
  while((alen || arlen) && (blen || brlen)) {
    <<Get next normalized, case folded character from [[a]] using [[uni_NFKC_CF]]>>
    <<Get next normalized, case folded character from [[b]] using [[uni_NFKC_CF]]>>
    if(ac > bc) {
      <<Free normalization buffers, if allocated>>
      return 1;
    }
    if(ac < bc) {
      <<Free normalization buffers, if allocated>>
      return -1;
    }
  }
  <<Free normalization buffers, if allocated>>
  return blen ? -1 : alen > 0;
}
@

<<Get case conversion result for next character using [[uni_NFKC_CF]]>>=
uint32_t lucp = uni_int_utf<<@sz>>_decode(<<@ab>>, &clen);
const uni_str_ptr_t *p = uni_NFKC_CF_of(lucp);
if(!p->off && !p->len) {
  <<@ab>>rbuf = NULL;
  <<@ab>>rlen = lucp;
} else {
  <<@ab>>rbuf = uni_NFKC_CF_strs + p->off;
  <<@ab>>rlen = p->len;
}
@

<<FIXME>>=
add NFD_CF and NFKD_CF as generic normalization routines?
What about NFC_CF and NFKC_CF w/o ignorable removal?
@

\subsection{Generic Normalized Unicode Input}

To make things even easier, we can make a function which returns a
normalized character from an input stream.  Any normalization type we
have code for, we support.

<<Unicode normalization support exports>>=
/** Normalization types for \ref uni_fgetc_norm */
typedef enum {
  UNI_NORM_NONE, /**< No normalization */
  UNI_NORM_NFD, /**< Canonical Decomposition normalization */
  UNI_NORM_NFKD, /**< Compatibility Decomposition normalization */
  UNI_NORM_NFC, /**< Canonical Composition normalization */
  UNI_NORM_NFKC, /**< Compatibility Composition normalization */
  UNI_NORM_NFKC_CF /**< Compatibility Composition normalization
                     * with Case Folding and Ignorable removal */
} uni_normtype_t;
@

<<Known Data Types>>=
uni_normtype_t,%
@

<<Unicode normalization support exports>>=
#include "uni_io.h"

/** Read a code point from a file after normalization.
  * The contents of the underlying file are transparently normalized
  * according to \p nt, and the next code point is returned.  The entire
  * file must be read using this function in order for it to work
  * correctly.  Changing \p nt mid-stream is also not supported, since
  * any normalization in progress will not be affected.  Errors are
  * ignored; invalid Unicode in the file is converted to U+FFFD, REPLACEMENT
  * CHARACTER. */
int32_t uni_fgetc_norm(uni_file_t *uf, uni_normtype_t nt);
@

<<Unicode normalization support functions>>=
int32_t uni_fgetc_norm(uni_file_t *uf, uni_normtype_t nt)
{
  <<Get normalized character>>
}
@

<<FIXME>>=
add NFD_CF and NFKD_CF to generic normalization routines?
What about NFC_CF and NFKC_CF w/o ignorable removal?
@

A simple function which returns a normalized character requires that
the current decomposition and composition state be kept.  It would be
ideal to extend the [[uni_file_t]] structure with the extra information
rather than allocating yet another wrapper, but for now I would prefer
[[uni_file_t]] to stay isolated to unnormalized I/O.

In order to make this transparent, the extra support structure is
automatically created and placed on a linked list.  It is assumed that
the file will not be closed and reopened without an intervening EOF.
It is also assumed that the function will not be called any more after
it returns EOF.

<<Unicode normalization support local definitions>>=
typedef struct unifile_norm {
  struct unifile_norm *next;
  uni_file_t *uf;
  <<Normalized input buffer state>>
} unifile_norm_t;
static unifile_norm_t *norm_files = NULL;
@

<<Known Data Types>>=
unifile_norm_t,%
@

<<Get normalized character>>=
unifile_norm_t *ufn = norm_files;

while(ufn && ufn->uf != uf)
  ufn = ufn->next;
if(!ufn) {
  inisize(ufn, 1);
  clearbuf(ufn, 1);
  ufn->next = norm_files;
  norm_files = ufn;
}

<<Return normalized character if not EOF>>

unifile_norm_t **bufp;
for(bufp = &norm_files; *bufp != ufn; bufp = &(*bufp)->next);
*bufp = ufn->next;
free(ufn);

return -1;
@

For support, we'll need to store at least the maximal decomposition of
a character.

<<Normalized input buffer state>>=
uint32_t *buf;
unsigned int buf_len;
@

We'll also need to know how many characters are in the buffer, and how
many of those are good to go.  Since more than one may be good to go,
a pointer to the next character to go is kept as well.  Since the last
good-to-go character may be the last in the file, a flag is added to
detect this.

<<Normalized input buffer state>>=
int len, oklen, okptr, eof;
@

To return a single character, we'll need to normalize until at least
one character could not possibly be modified any more (i.e., [[oklen]]
is non-zero).

<<Return normalized character if not EOF>>=
unsigned int oklen = ufn->oklen;
while(!oklen && !ufn->eof) {
  /* read a char into end of buf, if needed & possible */
  <<Read and normalize a character>>
}
if(oklen) {
  /* now we have oklen chars in buf that are ready to return */
  int c = ufn->buf[ufn->okptr++];
  if(ufn->okptr == oklen) {
    ufn->okptr = ufn->oklen = 0;
    if(ufn->len > oklen)
      movebuf(ufn->buf, ufn->buf + oklen, ufn->len - oklen);
    ufn->len -= oklen;
  } else
   ufn->oklen = oklen;
  return c;
} else if(ufn->eof)
  return -1;
@

To add more characters, the decomposition of the next read character
is appended to the buffer.  The total buffer length after
decomposition is stored in [[bp]].  The Unicode standard states that
case folding requires an additional NFD step beforehand, but reading
of the data suggests that instead, it needs canonical ordering and
composition afterwards, just like the NFC and NFKC forms.  This makes
case folding just another decomposition method.

Switching forms mid-stream is not really supportable.  Any characters
in the buffer have already been processed, at least using the
decomposition method which was in effect before the switch.  The only
compromise is to switch as soon as possible, performing no additional
work on the buffered characters.  A custom routine should be used if
mid-stream switching is desired.

<<Read and normalize a character>>=
int32_t c;
int bp = ufn->len;
if(bp && nt == UNI_NORM_NONE) {
  oklen = 1;
  break;
}
c = uni_fgetc(uf);
if(c == -1)
  ufn->eof = 1;
if(c < -1)
  c = 0xFFFD; /* recommended REPLACEMENT CHARACTER */
if(nt == UNI_NORM_NONE)
  return c; /* don't bother allocating buf */
if(c != -1)
  switch(nt) {
    case UNI_NORM_NONE: /* here to remove warning */
    case UNI_NORM_NFD:
    case UNI_NORM_NFC:
      bp += uni_NFD_dec32((uint32_t *)&c, 1, &ufn->buf, bp, &ufn->buf_len);
      break;
    case UNI_NORM_NFKD:
    case UNI_NORM_NFKC:
      bp += uni_NFKD_dec32((uint32_t *)&c, 1, &ufn->buf, bp, &ufn->buf_len);
      break;
    case UNI_NORM_NFKC_CF:
      bp += uni_NFKC_Casefold32((uint32_t *)&c, 1, &ufn->buf, bp, &ufn->buf_len);
  }
@

Then, the entire set of characters already read in can be ordered.  The
number of ordered characters is saved in [[oblen]].  If no characters
can be ordered, try to read more right away.

<<Read and normalize a character>>=
int oblen = uni_Canon_Order32(ufn->buf, bp, ufn->eof);
if(!oblen) {
  ufn->len = bp;
  continue;
}
@

Then, all characters which have been ordered can be composed.  If
composition might need more characters, the next trip around the loop
will get them.

<<Read and normalize a character>>=
switch(nt) {
  case UNI_NORM_NONE: /* here to remove warning */
  case UNI_NORM_NFD:
  case UNI_NORM_NFKD:
    /* no composition */
    oklen = oblen;
    break;
  case UNI_NORM_NFC:
  case UNI_NORM_NFKC:
  case UNI_NORM_NFKC_CF: {
    int cblen = uni_NFC_comp32(ufn->buf, oblen, ufn->eof ? NULL : &oklen);
    if(ufn->eof)
      oklen = cblen;
    /* move unordered characters down if composition removed chars */
    if(cblen < oblen) {
      if(bp > oblen)
        movebuf(ufn->buf + cblen, ufn->buf + oblen, bp - oblen);
      bp -= oblen - cblen;
    }
    break;
  }
}
ufn->len = bp;
@

\subsection{Testing - Normalization}

To fully demonstrate that the normalization data is correct, the
official test suite is run.  This is provided by the UCD in
NormalizationTest.txt.  This time, rather than compiling the file's
location in, it is simply fed into standard input.

\lstset{language=make}
<<C Test Support Executables>>=
tstnorm \
@

<<Additional Tests>>=
./tstnorm <$(UCD_LOC)/NormalizationTest.txt
@

\lstset{language=C}
<<tstnorm.c>>=
<<Common C Header>>
#include "uni_all.h"

/* longest line == 587 chars */
char lbuf[1024];

/* longest string == 18 chars */
uint32_t ibuf[5][128], ilen[5];
uint32_t obuf[128];
uint32_t * /* const */ obptr = obuf;
/* const */ unsigned int oblen = 128;

int main(void)
{
  int ret = 0;
  <<Read and process NormalizationTest.txt>>
  return ret;
}
@

First, we need to track when we're in part 1, and mark every code
point encountered there as having been processed.  According to the
test, after exiting part 1, we can run all four standard
normalizations on any code point not encountered and get no effect.

In general, when entering a new section, print the section name.
Otherwise, print a dot after every 50 tests.

<<Read and process NormalizationTest.txt>>=
char *didnorm;
inisize(didnorm, 0x110000);
clearbuf(didnorm, 0x110000);
int inpart1 = 0;
int ntests = 0;
while(fgets(lbuf, sizeof(lbuf), stdin)) {
  if(*lbuf == '#')
    continue;
  if(*lbuf == '@') {
    if(ntests)
      putchar('\n');
    if(lbuf[5] == '1')
      inpart1 = 1;
    else if(inpart1) {
      inpart1 = 0;
      int i;
      for(i = 0; i < 0x110000; i++)
        if(!didnorm[i]) {
	  <<Test normalization does not affect [[i]]>>
	}
    }
    fputs(lbuf, stdout);
    continue;
  }
  if(inpart1) {
    int cp = strtol(lbuf, NULL, 16);
    didnorm[cp] = 1;
  }
  <<Run normalization test for [[lbuf]]>>
  if(!(++ntests % 50)) {
    putchar('.');
    fflush(stdout);
  }
}
putchar('\n');
@

First, let's parse a line into the input buffers.  Each line has five
semicolon-separated fields, terminated by a semicolon.  Each field has
space-separated hexadecimal numbers.

<<Run normalization test for [[lbuf]]>>=
int i;
char *s = lbuf;
for(i = 0; i < 5; i++) {
  int l = 0;
  while(1) {
    ibuf[i][l++] = strtol(s, &s, 16);
    while(isspace(*s))
      s++;
    if(*s == ';')
      break;
  }
  ilen[i] = l;
  if(*s == ';')
    s++;
  else {
    fprintf(stderr, "bad line %s\n", lbuf);
    exit(1);
  }
}
@

Then, run the conformance tests.

<<Run normalization (@type) on fields (@i0) through (@il) to produce field (@r)>>=
for(i = <<@i0>>; i < <<@il>>; i++) {
  cpybuf(obuf, ibuf[i], ilen[i]);
  int l = uni_<<@type>>32(NULL, ilen[i], &obptr, -1, &oblen);
  if(l != ilen[<<@r>>] || cmpbuf(obuf, ibuf[<<@r>>], l)) {
    ret = 1;
    fprintf(stderr, "Failed <<@type>> test %d/%d on %s\nGot: ",
                    <<@r>> + 1, i + 1, lbuf);
    int j;
    for(j = 0; j < l; j++)
      fprintf(stderr, " %04X", obuf[j]);
    fputs("\nExpected: ", stderr);
    for(j = 0; j < ilen[<<@r>>]; j++)
      fprintf(stderr, " %04X", ibuf[<<@r>>][j]);
    fputc('\n', stderr);
    continue;
  }
}
@

<<Run normalization test for [[lbuf]]>>=
<<Run normalization [[NFC]] on fields [[0]] through [[3]] to produce field [[1]]>>
<<Run normalization [[NFC]] on fields [[3]] through [[5]] to produce field [[3]]>>
@

<<Run normalization test for [[lbuf]]>>=
<<Run normalization [[NFD]] on fields [[0]] through [[3]] to produce field [[2]]>>
<<Run normalization [[NFD]] on fields [[3]] through [[5]] to produce field [[4]]>>
@

<<Run normalization test for [[lbuf]]>>=
<<Run normalization [[NFKC]] on fields [[0]] through [[5]] to produce field [[3]]>>
@

<<Run normalization test for [[lbuf]]>>=
<<Run normalization [[NFKD]] on fields [[0]] through [[5]] to produce field [[4]]>>
@

<<Test normalization does not affect [[i]]>>=
obuf[0] = i;
unsigned int l1 = 1;
if(uni_NFC32(obuf, 1, &obptr, -1, &l1) != 1 || obuf[0] != i ||
   uni_NFD32(obuf, 1, &obptr, -1, &l1) != 1 || obuf[0] != i ||
   uni_NFKC32(obuf, 1, &obptr, -1, &l1) != 1 || obuf[0] != i ||
   uni_NFKD32(obuf, 1, &obptr, -1, &l1) != 1 || obuf[0] != i) {
  fprintf(stderr, "Failed Part1 end check at %04x\n", i);
  ret = 1;
}
@

\subsection{Parsing the UCD -- Names}

An ``Other'' property which can be treated as a string for now is
the na property, the last property to be retrieved from
[[UnicodeData.txt]]%
\footnote{Fields 11 and 12 (isc and na1) are obsolete and informative,
so they will not be read in as a property unless I find a use for
them.  na1 was useful prior to 6.2, since it contained names for
control characters, but those are now officially aliases.}%
.  This includes the Name\_Alias property (from
[[NameAliases.txt]]%
\footnote{The third field of [[NameAliases.txt]] (Type) is
informative, and will not be read in as a property unless I find a use.}%
), as well.

Since these names are always pure ASCII, storing one character per
32-bit word is even more wasteful than above:  more than 25 bits are
always wasted on each character.  For this reason, the string table
will be dumped as 8-bit characters.  The prerequisite format is to
cast the 32-bit string pointer to an 8-bit string pointer, and pad
with zeroes to the next 32-bit boundary.  In fact, since they are
always upper-case letters, digits, spaces, and hyphens, they could be
represented as 6 bits per word, but that amount of compression could
just end up being too expensive to extract.

Some characters' names are generated by appending the hexadecimal code
point to a range name.  The Hangul syllable characters also have
specially generated names.  In both cases, UnicodeData.txt simply
indicates a range.  Since code point tables are not good at storing
ranges, these are stored in a separate property, na\_rng.  This
includes all of the like-named <control> characters.  In addition,
since the CJK COMPATIBILITY IDEOGRAPH characters (U+F900-U+FAD9,
U+2F800-U+2FA1D) are named as though they were numbered ranges, they
are stored in that property as well.  For now, the na\_rng property is
stored as a plain string property, with each end point in the range
table.  The even-numbered table entries are always the low end of the
range, and the next entry the high end.

The actual names used in UnicodeData.txt are not always relevant.  In
fact, only the CJK ideographs and Hangul syllables have real names;
the rest have ``labels.'' To distinguish real names from labels, the
label has a leading <, which is meant to be terminated by a closing >
after the code point is appended.  The extensions have group names (A,
B, C, D) that need to be stripped.  All surrogate types are actually
just named surrogate, and all other private use types are just named
private-use.

<<UCD parser local definitions>>=
#define add_str8(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[64]; \
    uint32_t len = strlen(v); \
    str[len / 4] = 0; \
    memcpy(str, v, len); \
    len = (len + 3) / 4; \
    add_str_rng(&parsed_props[prop_##n], low, high, str, len); \
    parsed_props[prop_##n].strs_char_size = 8; \
  } \
} while(0)
@

<<Initialize UCD files>>=
decl_str(na);
decl_str(Name_Alias);
decl_str(na_rng);
@

<<Process a line of [[UnicodeData.txt]]>>=
if(fields[1][0] != '<' && (high < 0xF900 || low > 0xFAD9) &&
   (high < 0x2F800 || low > 0x2FA1D))
  add_str8(na, fields[1]);
else {
  uint32_t h = high;
  high = low;
  if(fields[1][0] == '<' && (s = strchr(fields[1], ','))) {
    *s = 0;
    /* standard claims all names are upper-case, but ranges have lower-case */
    for(s = fields[1]; *s; s++)
      if(islower(*s))
        *s = toupper(*s);
    if(strstr(fields[1] + 1, "CJK IDEOGRAPH")) {
      if((s = strstr(fields[1] + 1, "EXTENSION")))
        s[9] = 0;
    } else if(strstr(fields[1] + 1, "SURROGATE"))
      strcpy(fields[1] + 1, "<surrogate");
    else if(strstr(fields[1] + 1, "PRIVATE USE"))
      strcpy(fields[1] + 1, "<private-use");
    add_str8(na_rng, fields[1] + 1);
    low = high = h;
    add_str8(na_rng, fields[1] + 1);
  } else if(low == 0xF900 || low == 0x2F800) {
    *strchr(fields[1], '-') = 0;
    add_str8(na_rng, fields[1]);
    high = low = low == 0xF900 ? 0xFAD9 : 0x2FA1D;
    add_str8(na_rng, fields[1]);
  } else if(!strcmp(fields[1], "<control>")) {
    prop_t *na_rng = prop_na_rng < 0 ? NULL : &parsed_props[prop_na_rng];
    if(na_rng && na_rng->len &&
       na_rng->str_arr[na_rng->len - 1].cp == h - 1)
      na_rng->str_arr[na_rng->len - 1].cp = h;
    else {
      low = high = h;
      fields[1][8] = 0;
      add_str8(na_rng, fields[1]);
      add_str8(na_rng, fields[1]);
    }
  } /* else CJK COMPATIBILITY other than low or some unknown crap */
}
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_na], "Name", gen_h, tstf);
dump_str_tabs(&parsed_props[prop_na_rng], "Name synthetic range", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_na);
@

In addition, the Hangul syllable names require the name of each part,
also known as the Jamo\_Short\_Name (JSN) property.  It is extracted
from [[Jamo.txt]].  It has one blank entry, requiring a bypass of the
standard string adder.

<<Initialize UCD files>>=
decl_str(JSN);
@

<<Parse UCD files>>=
open_f("Jamo.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_str8(JSN, fields[1]);
  if(!*fields[1])
    add_str_rng(&parsed_props[prop_JSN], low, high, NULL, 0);
}
fclose(f);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_JSN], "Jamo_Short_name", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_JSN);
@

Unfortunately, the Name\_Alias property may have multiple values per
code point.  To fix this, all aliases are combined into a single
string, prefixing each with its length.

<<Parse UCD files>>=
open_f("NameAliases.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  char nabuf[128];
  strcpy(nabuf + 1, fields[1]);
  nabuf[0] = strlen(fields[1]) - 1;
  add_str8(Name_Alias, nabuf);
}
fclose(f);
prop_t *na_alias = &parsed_props[prop_Name_Alias];
@

<<Post-process property data>>=
qsort(na_alias->str_arr, na_alias->len, sizeof(*na_alias->str_arr), uni_cmp_cp);
for(i = na_alias->len - 1; i > 0; i--) {
  unsigned int nlen = na_alias->str_arr[i].len;
  for(low = i; low > 0; low--) {
    if(na_alias->str_arr[low - 1].cp != na_alias->str_arr[i].cp)
      break;
    nlen += na_alias->str_arr[low - 1].len;
  }
  if(low == i)
    continue;
  check_size(na_alias->strs, na_alias->max_strs, na_alias->strs_len + nlen);
  uint8_t *strs8 = (uint8_t *)(na_alias->strs + na_alias->strs_len);
  unsigned int strs8_len = 0;
  for(j = low; j <= i; j++) {
    uint8_t *p8 = (uint8_t *)(na_alias->strs + na_alias->str_arr[j].off);
    unsigned int len8 = *p8 + 2;
    memcpy(strs8 + strs8_len, p8, len8);
    strs8_len += len8;
  }
  while(strs8_len % 4)
    strs8[strs8_len++] = 0;
  movebuf(na_alias->str_arr + low + 1, na_alias->str_arr + i + 1,
          na_alias->len - (i + 1));
  na_alias->str_arr[low].off = na_alias->strs_len;
  na_alias->str_arr[low].len = strs8_len / 4;
  na_alias->strs_len += strs8_len / 4;
  na_alias->len -= i - low;
  if(!(i = low))
    break;
}
@

<<Dump character information as C code>>=
dump_str_tabs(na_alias, "Name_Alias", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt(na_alias);
@

A related table which is not strictly a per-character property is the
Named Sequences table (from [[NamedSequences.txt]]%
\footnote{There is another file, [[NamedSequencesProv.txt]], which
contains provisional named sequence names.  These will not be
supported.  Insted, wait for them to become official.}%
).  This associates names with character sequences, rather than just
single characters. This is read in as well, and stored in the
pseudo-property na\_seq.  The names are indexed by the first character
of the sequence, and both the name and the additional index characters
are stored in the string value.

The index needs to be word-aligned in order to use the simple decoder.
If the index is at the end of the entry, the last entry might be
stripped (in 8-bit mode) if its last byte is zero.  If it is at the
beginning, finding its length may be hard.  Placing it one up in order
to insert a length byte wastes two bytes, so it is placed at the
beginning, but the length is placed at the end of the string.  The
name then occupies the middle.  Multiple concatenated entries are
word-aligned by padding with a zero.  In fact, the entire table needs
to be word-aligned.  One way to do that would be to use 16-bit mode,
but I prefer being able to sort of read the generated table, so it
uses 8-bit mode and always pads the name (rather than the whole
string) to force the full length to be word-aligned.

<<Initialize UCD files>>=
decl_str(na_seq);
@

<<UCD parser local definitions>>=
#define add_strseq(n, seq, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[64]; \
    uint16_t *p16 = (uint16_t *)str; \
    low = strtol(seq, &s, 16); \
    uint32_t seqlen = 0; \
    while(*s) { \
      high = strtol(s + 1, &s, 16); \
      int clen = uni_int_utf16_encode(p16, high); \
      p16 += clen; \
      seqlen += clen; \
    } \
    uint8_t *p8 = (uint8_t *)p16; \
    strcpy((char *)p8, v); \
    int len = strlen((char *)p8); \
    p8 += len; \
    if(!(len & 1)) \
      *p8++ = 0; \
    *p8++ = (len - 1) | ((seqlen - 1) << 6); \
    /* current UCD has max strlen 57 and max seqlen 4 */ \
    if(seqlen > 4 || len > 64) { \
      fprintf(stderr, "Unparsable sequence file %s (%d/%d)\n", lbuf, len, seqlen); \
      exit(1); \
    } \
    len = p8 - (uint8_t *)str; \
    while(len % 4) { \
      *p8++ = 0; \
      len++; \
    } \
    add_str_rng(&parsed_props[prop_##n], low, low, str, len / 4); \
    parsed_props[prop_##n].strs_char_size = 8; \
  } \
} while(0)
@

<<Parse UCD files>>=
open_f("NamedSequences.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  split_line(lbuf);
  if(num_fields < 2 || !isxdigit(fields[1][0]))
    continue;
  add_strseq(na_seq, fields[1], fields[0]);
}
@

<<Post-process property data>>=
prop_t *na_seq = &parsed_props[prop_na_seq];
qsort(na_seq->str_arr, na_seq->len, sizeof(*na_seq->str_arr), uni_cmp_cp);
for(i = na_seq->len - 1; i > 0; i--) {
  while(i > 0 && na_seq->str_arr[i].cp == na_seq->str_arr[i - 1].cp) {
    int jlen = na_seq->str_arr[i - 1].len, ilen = na_seq->str_arr[i].len;
    check_size(na_seq->strs, na_seq->max_strs, na_seq->strs_len + ilen + jlen);
    uint16_t *newseq = (uint16_t *)(na_seq->strs + na_seq->strs_len);
    jlen *= 2;
    cpybuf(newseq, na_seq->strs + na_seq->str_arr[i - 1].off, jlen);
    if(!newseq[jlen - 1])
      jlen--;
    ilen *= 2;
    cpybuf(newseq + jlen, na_seq->strs + na_seq->str_arr[i].off, ilen);
    if(!newseq[jlen + ilen - 1])
      ilen--;
    if((ilen + jlen) % 2)
      newseq[ilen++ + jlen] = 0;
    na_seq->str_arr[i - 1].len = (ilen + jlen) / 2;
    na_seq->str_arr[i - 1].off = na_seq->strs_len;
    na_seq->strs_len += (ilen + jlen) / 2;
    movebuf(na_seq->str_arr + i, na_seq->str_arr + i + 1, na_seq->len - (i + 1));
    na_seq->len--;
    i--;
  }
}
@

<<Dump character information as C code>>=
dump_str_tabs(na_seq, "named sequences", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_na_seq);
@

The string tables are much too large for 16-bit offsets (509,515 bytes
for the na property alone).  One possible fix would be to use 8-bit
lengths and 24-bit offsets, instead.  However, since a function needs
to be used to retrieve the name anyway due to the synthesized names, a
more complex format could be used.  One way to reduce the string table
is to remove common substrings.  However, common substring removal is
rather expensive, so instead words are split out.  Each word of more
than two characters is placed into its own array, and replaced by a
two-byte pointer into that array.  The first byte of the pointer has
its upper bit set to distinguish from other characters.

While the na property is the only one which really needs this
compression, the same will be applied to the na\_seq, na\_rng, and
Name\_Alias properties.  In order to only have to manage pointers into
one string array during the word building phase, all names from the
other properties are temporarily added to na's strings.

<<UCD parser local functions>>=
#define NT_NAME  0
#define NT_SEQ   1
#define NT_ALIAS 2
static void collect_words(prop_t *words, prop_t *na, uint32_t *strs,
                          unsigned int off, int na_type)
{
  unsigned int i;

  if(off)
    cpybuf(strs + off, na->strs, na->strs_len);
  for(i = 0; i < na->len; i++) {
    <<For each word [[ws]] .. [[we]]>>
      if((we - ws) > 2) {
        check_size(words->str_arr, words->max_len, words->len + 1);
        words->str_arr[words->len].cp = 0; /* avoid valgrind warning */
        words->str_arr[words->len].off = ws - (uint8_t *)strs;
        words->str_arr[words->len].len = we - ws;
        words->len++;
      }
    <<End for each word [[ws]] .. [[we]]>>
  }
}
@

<<For each word [[ws]] .. [[we]]>>=
uint8_t *ws, *s, *n, *ne;
s = (uint8_t *)(strs + na->str_arr[i].off + off);
ne = (uint8_t *)(strs + na->str_arr[i].off + off + na->str_arr[i].len);
while(!ne[-1])
  ne--;
n = ne;
while(n != s) {
  uint8_t *e;
  if(na_type == NT_SEQ) {
    uint8_t wlen = *--n;
    if(!n[-1])
      n--;
    e = n;
    n -= (wlen & 0x3f) + 1;
    ws = n;
    n -=  2 * ((wlen >> 6) + 1);
  } else if(na_type == NT_ALIAS) {
    for(e = s; e < ne; e += *e + 2)
      if(n == e + *e + 2)
        break;
    n = e;
    ws = e + 1;
    e += *e + 2;
  } else {
    e = n;
    ws = n = s;
  }
  while(ws < e) {
    uint8_t *we;
    for(we = ws + 1; we < e; we++)
      if(*we == ' ' || *we == '-')
        break;
@

<<End for each word [[ws]] .. [[we]]>>=
    ws = we + 1;
    while(ws < e && (*ws == '-' || *ws == ' '))
      ws++;
    ws--; /* include leading char (space/-) in word */
  }
}
@

<<Post-process property data>>=
prop_t *na = &parsed_props[prop_na], *na_rng = &parsed_props[prop_na_rng];
prop_t words;
clearbuf(&words, 1);
/* collect all words as pointers into na_strs */
check_size(na->strs, na->max_strs, na->strs_len + na_seq->strs_len +
                                   na_alias->strs_len + na_rng->strs_len);
inisize(words.str_arr, words.max_len = 100);
collect_words(&words, na, na->strs, 0, NT_NAME);
collect_words(&words, na_seq, na->strs, na->strs_len, NT_SEQ);
collect_words(&words, na_alias, na->strs, na->strs_len + na_seq->strs_len, NT_ALIAS);
collect_words(&words, na_rng, na->strs,
              na->strs_len + na_seq->strs_len + na_alias->strs_len, NT_NAME);
/* prepare for merge */
inisize(words.strs, words.max_strs = words.len);
sort_strs = na->strs;
qsort(words.str_arr, words.len, sizeof(*words.str_arr), cmp_words);
@

<<UCD parser local functions>>=
static int cmp_words(const void *_a, const void *_b)
{
  const raw_cp_str_t *a = _a, *b = _b;
  const char *s = (const char *)sort_strs;
  int len = a->len < b->len ? a->len : b->len;
  int c = memcmp(s + a->off, s + b->off, len);
  if(c)
    return c;
  if(len < a->len)
    return 1;
  if(len < b->len)
    return -1;
  return 0;
}
@

<<Post-process property data>>=
/* fix words array */
uint8_t *na_strs = (uint8_t *)na->strs;
for(i = 0; i < words.len; i++) {
  /* merge duplicates */
  for(j = i + 1; j < words.len; j++)
    if(words.str_arr[j].len != words.str_arr[i].len ||
       memcmp(na_strs + words.str_arr[i].off, na_strs + words.str_arr[j].off,
              words.str_arr[i].len))
      break;
  /* copy unique word into words_strs and adjust pointers */
  int wlen = (words.str_arr[i].len + 3) / 4;
  check_size(words.strs, words.max_strs, words.strs_len + wlen);
  words.strs[words.strs_len + wlen - 1] = 0;
  memcpy(words.strs + words.strs_len, na_strs + words.str_arr[i].off,
         words.str_arr[i].len);
 words.str_arr[i].cp = i;
 words.str_arr[i].off = words.strs_len;
  words.strs_len += wlen;
 /* remove dups */
 movebuf(words.str_arr + i + 1, words.str_arr + j, words.len - j);
 words.len -= j - (i + 1);
}
@

<<Post-process property data>>=
/* replace words in name strings by pointers */
set_words(&words, na, NT_NAME);
set_words(&words, na_seq, NT_SEQ);
set_words(&words, na_alias, NT_ALIAS);
set_words(&words, na_rng, NT_NAME);
@

<<UCD parser local functions>>=
static void set_words(prop_t *words, prop_t *na, int na_type)
{
  unsigned int i;

  const uint32_t *strs = na->strs;
  int off = 0;
  for(i = 0; i < na->len; i++) {
    <<For each word [[ws]] .. [[we]]>>
      if((we - ws) > 2) {
        int l = 0, h = words->len - 1, m;
        while(l <= h) {
          m = (l + h) / 2;
	  int len = words->str_arr[m].len;
	  if(len > we - ws)
	    len = we - ws;
	  int cmp = memcmp(words->strs + words->str_arr[m].off, ws, len);
	  if(!cmp)
	    cmp = (int)words->str_arr[m].len - (int)(we - ws);
	  if(cmp < 0)
	    l = m + 1;
	  else if(cmp > 0)
	    h = m - 1;
	  else
	    break;
        }
        if(l <= h) {
	  if(na_type != NT_SEQ || !((ne - we) % 2))
            memmove(ws + 2, we, ne - we);
	  else {
	    /* need to keep things word-aligned */
	    memmove(ws + 2, we, e - we);
	    if(*e) {
	      ws[2 + (e - we)] = 0;
	      memmove(ws + 2 + (e - we) + 1, e, ne - e);
	      ne++;
	    } else {
	      memmove(ws + 2 + (e - we), e + 1, ne - (e + 1));
	      ne--;
	    }
	  }
	  ws[0] = (words->str_arr[m].cp >> 8) | 0x80;
	  ws[1] = words->str_arr[m].cp & 0xff;
	  e -= we - (ws + 2);
	  ne -= we - (ws + 2);
	  /* adjust name length byte */
	  if(na_type == NT_SEQ) {
	    if(*e)
	      *e -= we - (ws + 2);
	    else
	      e[1] -= we - (ws + 2);
	  } else if(na_type == NT_ALIAS)
	    *n -= we - (ws + 2);
	  we -= we - (ws + 2);
        }
      }
    <<End for each word [[ws]] .. [[we]]>>
    int slen = ne - (uint8_t *)(na->strs + na->str_arr[i].off);
    while(slen % 4) {
      *ne++ = 0;
      slen++;
    }
    na->str_arr[i].len = slen / 4;
  }
}
@

<<Post-process property data>>=
/* adjust word lens for dump_strtab */
for(i = 0; i < words.len; i++)
  words.str_arr[i].len = (words.str_arr[i].len + 3) / 4;
@

<<Dump character information as C code>>=
/* compress & dump strings */
words.strs_char_size = 8;
words.name = "na_words";
dump_str_ptr_arr(&words, "name compression (words)", gen_h);
free(words.strs);
free(words.str_arr);
@

<<UCD parser local functions>>=
static void dump_str_ptr_arr(prop_t *p, const char *lname, FILE *gen_h)
{
  unsigned int i;

  dump_strs(p, lname, gen_h);
  /* sort by word # */
  qsort(p->str_arr, p->len, sizeof(*p->str_arr), uni_cmp_cp);
  /* dump table */
  char fn[64];
  sprintf(fn, "uni_%s_arr.gen.c", p->name);
  open_wf(af, fn);
  fprintf(af, "#include \"uni_prop.h\"\n\n"
              "const uni_str_ptr_t uni_%s_arr[] = {\n", p->name);
  for(i = 0; i < p->len; i++) {
    fprintf(af, "\t{ %5u, %5u }", p->str_arr[i].off, p->str_arr[i].len);
    if(i < p->len - 1)
      putc(',', af);
    putc('\n', af);
  }
  fputs("};\n", af);
  fclose(af);
  /* dump header code */
  fprintf(gen_h, "/** Sorted string pointer array for %s property */\n"
                 "extern const uni_str_ptr_t uni_%s_arr[];\n"
		 "/** Length of \\ref uni_%s_arr */\n"
                 "#define uni_%s_arr_len %d\n",
		 lname, p->name, p->name, p->name, p->len);
}
@

This reduces the name table by well over half, to 179,282 bytes.  That
is still too long, but one easy way around it is to continue to retain
the 32-bit encoding.  The number of 32-bit words is only 54,286, which
adds a lot of padding (37,862 bytes), but allows 16-bit offsets.

<<Post-process property data>>=
na->strs_char_size = 32;
@

Since the format of the name table is not very simple, and there are
algorithmically generated names, a function is provided for name
lookup.  This function should also support looking up sequence names,
so the code point is passed in as a buffer pointer and length.  The
return format is similar to the UTF-8 static/dynamic return buffer
mechanism used by other string functions, plus a parameter to indicate
if passing more code points might result in a different name.  Since
the name table is rather large, this function is placed in a separate
object file.

<<Library [[uni]] Members>>=
cp_to_name.o
@

<<cp_to_name.c>>=
<<Common C Header>>
#include "uni_prop.h"

@

<<Unicode property exports>>=
/** Look up the Unicode name of code point(s).
 * Looks up name of \p len code points starting at \p cp.  If non-NULL,
 * the number of code points consumed to produce the name is returned
 * in the absolute value of \p *seq_len.  If \p *seq_len is less than zero,
 * using a longer input may produce a different name.  If non-NULL,
 * \p *alias can be used to select an alternate name.  If zero is passed in,
 * the canonical name is returned, and \p *alias is updated to the number
 * of aliases.  If \p *alias is a valid alias index, that alias is returned,
 * and \p *alias is decremented.  If \p *alias is not a valid alias index
 * or zero, it is updated to -1 and no name is returned.  If \p alias is
 * NULL, only the canonical name is returned.  The returned string is
 * described in \ref uni_return8_buf8.   An empty return string indicates
 * an invalid code point or nonexistent alias.  Otherwise, if the code point
 * has no name, the name is a synthetic one enclosed in angle brackets
 * (\<\e range-\e hex_cp\>).  Note that some code points have no name, but
 * do have aliases */
int uni_cp_to_name(const uint32_t *cp, unsigned int len, int *seq_len,
                   int *alias, <<Buffer return parameters for UTF-[[8]]>>);
@

<<cp_to_name.c>>=
int uni_cp_to_name(const uint32_t *cp, unsigned int len, int *seq_len,
                   int *alias, <<Buffer return parameters for UTF-[[8]]>>)
{
  if(!len) {
    if(seq_len)
      *seq_len = 0;
    if(alias && *alias)
      *alias = -1;
    return 0;
  }
  if(alias && *alias < 0) {
    if(seq_len)
      *seq_len = 0;
    return 0;
  }
  <<Return name for [[*cp]]>>
}
@

The first thing to do is look up the code point in the [[na_seq]] table.
If present, the longest possible match is taken, and if there is a
longer match beyond the passed-in length, [[*seq_len]] is negated as
well.  Since there may not be an exact match at all, this code may
just set [[*seq_len]] to $-1$ and fall through to the single-character
case.  Even if an exact match is found, every possible sequence is
checked, since there might be a longer sequence that matches.  Note
that there are no sequence aliases, so any attempt to get an alias is
denied.

<<Return name for [[*cp]]>>=
if(seq_len)
  *seq_len = 1;
const uni_str_ptr_t *lu = len > 1 || seq_len ? uni_na_seq_of(*cp) : NULL;
if(lu && lu->len && (len == 1 || (alias && *alias))) {
  if(seq_len)
    *seq_len = -1;
} else if(lu && lu->len) {
  const uint8_t *found_na = NULL;
  unsigned int found_nalen = 0, found_seqlen = 0, seqlen, nalen;
  const uint8_t *ep = uni_na_seq_strs + lu->off, *p = ep + lu->len;
  const uint16_t *seqptr;
  const uint32_t *cp2;
  while(p > ep) {
    nalen = *--p;
    seqlen = (nalen >> 6) + 1;
    nalen = (nalen & 0x3f) + 1;
    if(!p[-1])
      p--;
    p -= nalen + seqlen * 2;
    seqptr = (const uint16_t *)p;
    int sp = 0;
    for(cp2 = cp + 1; cp2 - cp < len; cp2++) {
      unsigned int clen;
      if(*cp2 != uni_int_utf16_decode(seqptr + sp, &clen))
        break;
      sp += clen;
      if(sp == seqlen) {
        if(seqlen > found_seqlen) {
          found_na = p + seqlen * 2;
	  found_nalen = nalen;
	  found_seqlen = seqlen;
	}
	break;
      }
    }
    /* if at end of string, mark as possibly longer */
    if(seq_len && sp < seqlen && cp2 - cp == len)
      *seq_len = -1;
  }
  if(found_na) {
    if(seq_len)
      *seq_len *= found_seqlen + 1; /* 1 or -1 */
    return uni_na_expand_words(found_na, found_nalen, buf, off, buf_len);
  }
}
@

In order to support raw lookup by the user as well, a function is
provided (and used by the main lookup function) to interpret at least
the word-encoded portion.

<<Unicode property exports>>=
/** Expand word-compressed string table entry.
  * Expands word-compressed string table entry \p na_raw/\p na_raw_len
  * using the word lookup table \p words/\p words_strs.  The return
  * value is described with \ref uni_return8_buf8. */
int uni_naX_expand_words(const uint8_t *na_raw, unsigned int na_raw_len,
                         const uni_str_ptr_t *words, const uint8_t *words_strs,
			 <<Buffer return parameters for UTF-[[8]]>>);
/** Expand compressed na(me) or na(med)_seq(uence) string table entry.
  * Expands compressed name string table entry \p na_raw/\p na_raw_len.
  * The return value is described with \ref uni_return8_buf8. */
int uni_na_expand_words(const uint8_t *na_raw, unsigned int na_raw_len,
                        <<Buffer return parameters for UTF-[[8]]>>);
#define uni_na_expand_words(na_raw, na_raw_len, buf, off, buf_len) \
  uni_naX_expand_words(na_raw, na_raw_len, uni_na_words_arr, uni_na_words_strs, \
                       buf, off, buf_len)
@

<<Unicode property functions>>=
int uni_naX_expand_words(const uint8_t *na_raw, unsigned int na_raw_len,
                         const uni_str_ptr_t *words, const uint8_t *words_strs,
			 <<Buffer return parameters for UTF-[[8]]>>)
{
  unsigned int i;
  unsigned int na_len = na_raw_len;

  for(i = 0; i < na_raw_len; i++)
    if(na_raw[i] & 0x80) {
      unsigned int wno = (((unsigned int)na_raw[i] & 0x7f) << 8);
      if(i < na_raw_len - 1)
	wno += na_raw[++i];
      else
        na_len++;
      na_len += words[wno].len - 2;
    }
  /* shortcut for just computing length */
  if(off < 0 && (!buf_len || !*buf_len))
    return na_len;
  /* otherwise build name on stack & return the usual way */
  uint8_t na[na_len], *np = na;
  for(i = 0; i < na_raw_len; i++) {
    if(na_raw[i] & 0x80) {
      unsigned int wno = (((unsigned int)na_raw[i] & 0x7f) << 8);
      if(i < na_raw_len - 1)
	wno += na_raw[++i];
      memcpy(np, words_strs + words[wno].off, words[wno].len);
      np += words[wno].len;
    } else
      *np++ = na_raw[i];
  }
  return uni_return8_buf8(na, na_len, buf, off, buf_len);
}
@

For the single code point case, the easiest thing to look up first is
the alias, if an alias index is provided.  The return value is either
that alias or an empty string to indicate an invalid alias index.  If
no alias was requested, the total alias count is returned in [[*alias]].

<<Return name for [[*cp]]>>=
if(alias) {
  int alias_no = *alias;
  const uni_str_ptr_t *ap = uni_Name_Alias_of(*cp);
  int alen = ap->len;
  if(!alen && alias_no) {
    if(seq_len)
      *seq_len = 0;
    *alias = -1;
    return 0;
  }
  if(alen) {
    const uint8_t *np = uni_Name_Alias_strs + ap->off;
    while(alen) {
      unsigned int nlen = *np + 1;
      if(nlen > alen - 1) /* in case last word# was cut off */
        nlen = alen - 1;
      np++;
      if(!--alias_no) {
        --*alias;
	if(seq_len)
	  *seq_len = 1;
	return uni_na_expand_words(np, nlen, buf, off, buf_len);
      }
      alen -= nlen + 1;
      np += nlen;
    }
    if(alias_no > 0) {
      if(seq_len)
        *seq_len = 0;
      *alias = -1;
      return 0;
    }
    *alias = -alias_no;
  }
}
@

For the single code point non-alias case, the first step is once again
lookup in the name table.  A successful match can sort of be returned
immediately, but the word pointers need to be replaced with actual
words first.

<<Return name for [[*cp]]>>=
lu = uni_na_of(*cp);
if(lu->len) {
  const uint8_t *na_raw = (const uint8_t *)(uni_na_strs + lu->off);
  unsigned int na_raw_len = lu->len * 4;
  while(!(na_raw[na_raw_len - 1]))
    na_raw_len--;
  return uni_na_expand_words(na_raw, na_raw_len, buf, off, buf_len);
}
@

Failure to look up a name may be because it is an invalid code point.
In that case, return an empty string.

<<Return name for [[*cp]]>>=
if(*cp > UNI_MAX_CP)
  return 0;
@

Otherwise, it must be a generated name.  All generated names consist
of the na\_rng string for the code point's range followed by a generated
suffix, except for unnamed code points, which map to either
``<noncharacter-'' or ``<reserved-'' followed by a generated suffix.
Note that all generated names with a leading < actually map to a blank
na property value.

<<Return name for [[*cp]]>>=
const uni_str_arr_t *rng = uni_na_rng_of(*cp);
unsigned int nalen;
int is_label = 0;
if(!rng) {
#define uni_return_const(str, sz) \
  uni_return8_buf##sz((uint8_t *)str, sizeof(str) - 1, buf, off, buf_len)
  if(uni_is_NChar(*cp))
    nalen = uni_return_const("<noncharacter", 8);
  else
    nalen = uni_return_const("<reserved", 8);
  is_label = 1;
} else {
  /* always extract at least 1st char of range name to determine if < */
  uint8_t c, *p = &c;
  unsigned int l = 1;
  if(!buf_len && off < 0) {
    nalen = uni_na_expand_words(uni_na_rng_strs + rng->off, rng->len,
                                &p, off, &l);
    is_label = c == '<';
  } else {
    nalen = uni_na_expand_words(uni_na_rng_strs + rng->off, rng->len,
                                buf, off, buf_len);
    is_label = **buf == '<';
  }
}
unsigned int suflen;
<<Compute [[suflen]] for [[*cp]]'s suffix>>
suflen += is_label;
/* shortcut for just computing length or if suffix won't fit */
if(off < 0 && (!buf_len || !*buf_len || *buf_len <= nalen))
  return nalen + suflen;
/* otherwise, build suffix from stack */
/* name was already built into output buffer by expand_words() */
char suffix[suflen + 1];
<<Build [[suffix]] from [[*cp]]>>
if(is_label)
  suffix[suflen - 1] = '>';
if(off < 0) {
  unsigned int blen = *buf_len - nalen;
  uint8_t *bptr = *buf + nalen;
  uni_return8_buf8((uint8_t *)suffix, suflen, &bptr, off, &blen);
} else if(off >= 0)
  uni_return8_buf8((uint8_t *)suffix, suflen, buf, off + nalen, buf_len);
return nalen + suflen;
@

<<Unicode property exports>>=
/** Determine synthetic name range for \p cp.
  * Returns the name string pointer of the synthetic range \p cp
  * belongs to, or NULL if it doesn't belong to any.  The string
  * pointer is for \ref uni_na_rng_strs, which is word-compressed
  * like a standard name. */
const uni_str_arr_t *uni_na_rng_of(uint32_t cp);
@

<<cp_to_name.c>>=
const uni_str_arr_t *uni_na_rng_of(uint32_t cp)
{
  int l = 0, h = uni_na_rng_arr_len / 2 - 1, m;
  while(l <= h) {
    m = (l + h) / 2;
    if(uni_na_rng_arr[m * 2].cp > cp)
      h = m - 1;
    else if(uni_na_rng_arr[m * 2 + 1].cp < cp)
      l = m + 1;
    else
      return &uni_na_rng_arr[m * 2];
  }
  return NULL;
}
@

The suffix for most ranges is just a hyphen, followed by the code point
in hex.  For Hangul syllables, though, the suffix is a space, followed
by the broken down compoents' names.

<<Compute [[suflen]] for [[*cp]]'s suffix>>=
<<Prepare to compute [[suflen]] for Hangul syllable>>
if(rng && rng->cp == 0xAC00) {
  <<Compute [[suflen]] for Hangul syllable>>
} else {
  suflen = 1 /* - */ + (lg2(*cp + 1) + 3) / 4 /* XXXX */;
  if(suflen < 5)
    suflen = 5;
}
@

<<Build [[suffix]] from [[*cp]]>>=
if(rng && rng->cp == 0xAC00) {
  <<Compute [[suffix]] for Hangul syllable>>
} else
  sprintf(suffix, "-%04X", (int)*cp);
@

For Hangul syllables, the Jamo\_Short\_Name property contains the text
to append to the name.  First the syllable is broken down into L, V,
and T, and then the string corresponding to each is looked up.

<<Prepare to compute [[suflen]] for Hangul syllable>>=
const uni_str_ptr_t *ls = NULL, *vs = NULL, *ts = NULL; /* shut gcc up */
@

<<Compute [[suflen]] for Hangul syllable>>=
unsigned int jt = (*cp - 0xAC00) % 28;
unsigned int jv = (*cp - 0xAC00) / 28;
unsigned int jl = jv / 21;
jv %= 21;
ls = uni_JSN_of(0x1100 + jl);
vs = uni_JSN_of(0x1161 + jv);
ts = jt ? uni_JSN_of(0x11A7 + jt) : NULL;
suflen = 1 + ls->len + vs->len + (ts ? ts->len : 0);
@

The suffix itself is just a space, followed by the L, V, and T short
names.

<<Compute [[suffix]] for Hangul syllable>>=
suffix[0] = ' ';
memcpy(suffix + 1, uni_JSN_strs + ls->off, ls->len);
memcpy(suffix + 1 + ls->len, uni_JSN_strs + vs->off, vs->len);
if(ts)
  memcpy(suffix + 1 + ls->len + vs->len, uni_JSN_strs + ts->off, ts->len);
@

To test this, a simple program just generates every single name.  No
verification is done; this is meant to be verified manually.

<<C Test Support Executables>>=
tstcp_na \
@

<<tstcp_na.c>>=
<<Common C Header>>

#include "uni_prop.h"

int main(void)
{
  uint32_t cp;
  uint8_t *buf = NULL;
  unsigned int buf_len = 0, clen;
  int has_more, aliases;
  for(cp = 0; cp <= UNI_MAX_CP + 1; cp++) {
    aliases = 0;
    clen = uni_cp_to_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
    while(!clen && aliases > 0)
      clen = uni_cp_to_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
    if(!clen)
      continue;
    printf("%04X %d %.*s\n", cp, has_more, clen, buf);
    while(aliases > 0) {
      clen = uni_cp_to_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
      printf("%04X %d &%.*s\n", cp, has_more, clen, buf);
    }
  }
  /* all sequences */
  int i;
  uint32_t seq[10];
  for(i = 0; i < uni_na_seq_arr_len; i++) {
    const uint8_t *ep = uni_na_seq_strs + uni_na_seq_arr[i].off,
                  *p = ep + uni_na_seq_arr[i].len;
    unsigned int seqlen, nalen;
    while(p > ep) {
      uint32_t *sp = seq;
      *sp = uni_na_seq_arr[i].cp;
      printf("%04X", (int)*sp++);
      nalen = *--p;
      if(!p[-1])
        p--;
      seqlen = (nalen >> 6) + 1;
      nalen = (nalen & 0x3f) + 1;
      p -= nalen + seqlen * 2;
      const uint16_t *seqp = (const uint16_t *)p;
      while(seqlen > 0) {
        *sp = uni_int_utf16_decode(seqp, &clen);
	printf(":%04X", (int)*sp++);
	seqp += clen;
	seqlen -= clen;
      }
      clen = uni_cp_to_name(seq, (int)(sp - seq), &has_more, NULL, &buf, 0, &buf_len);
      printf(" %d %.*s\n", has_more, clen, buf);
    }
  }
  return 0;
}
@

The name tables are not really that useful as is:  few programs
display a Unicode code point's name.  However, there are many file
parsers which allow Unicode code points to be specified by name.  Thus
a reverse lookup table is required.  There is no perfect reverse
lookup method; it depends on the application.  It is expected that a
program using the name tables will dump a custom version of both
forward and reverse lookups as application-specific C code.

The obvious thing to provide is an explicit name lookup function, using
either a range list sorted by name instead of code point, or a hash
table, or something similar.  Matching loosely needs to be supported as
well, by stripping spaces and hyphens for the lookup.

For the hash table, either some random hash function could be
provided, or a perfect hash function could be generated using
[[gperf]] or a similar tool.  My initial implementation used
[[gperf]], but generating a perfect hash for the full Unicode set took
two hours on my machine, and an additional hour to compile.  In
comparison, a simple hash used by a master symbol table in my compiler
took less than a second to generate and compile.

The first thing to do is generate a name table that is preprocessed
for lookup.  The lookup algorithm is always loose.  UAX44-LM2 states
that lookups should ignore case, whitespace, underscore, and medial
hyphens except in U+1180.  There are 15 names with non-medial hyphens,
but only two are actually ambiguous.  UTS~\#18 (Regular Expressions),
section 2.5.1 ("Individually Named Characters") acknowledges this by
requiring hyphen checks only for U+1180, U+0F60, and U+0FB0.  While
the algorithm I use violates the original loose matching algorithm, I
will go ahead and only restrict those three.  The code actually
derives this list by checking for duplicates, so it is guaranteed to
be safe for the current UCD.

The lookup table contains all possible names:  the na property, the
Name\_Alias property, and the na\_seq property.  The na\_rng property
is stored separately, because it needs to be searched as a prefix
rather than the whole name.

Since all names refer to the word table, the easiest way to preprocess
most of the text is to use a different word table.  The old table is
copied while stripping spaces and hyphens and converting to
lower-case.

<<Post-process property data>>=
prop_t revwords;
clearbuf(&revwords,1);
inisize(revwords.strs, revwords.max_strs = revwords.strs_len = words.strs_len);
uint8_t *rwstr = (uint8_t *)revwords.strs;
unsigned int rwstr_len = 0;
inisize(revwords.str_arr, revwords.len = revwords.max_len = words.len);
for(i = 0; i < revwords.len; i++) {
  const uint8_t *ws = (uint8_t *)(words.strs + words.str_arr[i].off);
  unsigned int wl = words.str_arr[i].len * 4;
  while(!ws[wl -1])
    wl--;
  /* strip all spaces and hypens in this array */
  while(*ws == ' ' || *ws == '-') {
    ws++;
    wl--;
  }
  revwords.str_arr[i].off = rwstr_len / 4;
  revwords.str_arr[i].cp = words.str_arr[i].cp;
  revwords.str_arr[i].flags = 0;
  for(j = 0; j < wl; j++)
    rwstr[rwstr_len++] = isupper(ws[j]) ? tolower(ws[j]) : ws[j];
  while(rwstr_len % 4)
    rwstr[rwstr_len++] = 0;
  revwords.str_arr[i].len = rwstr_len / 4 - revwords.str_arr[i].off;
}
@

The main name table is then converted, ignoring word pointers and
otherwise stripping spaces and hyphens and converting to lower-case.
While this does introduce the duplicates mentioned earlier, they are
not processed until the whole table has been filled.

<<Post-process property data>>=
int prop_rev_na = add_prop("rev_na");
prop_t *rev_na = &parsed_props[prop_rev_na];
rev_na->max_strs = na->strs_len + na_seq->strs_len + na_alias->strs_len;
inisize(rev_na->strs, rev_na->max_strs);
uint8_t *dstp = (uint8_t *)rev_na->strs;
unsigned int dstlen = 0;
rev_na->max_len = na->len + na_seq->len * 2 + na_alias->len * 2;
inisize(rev_na->str_arr, rev_na->max_len);
for(i = 0; i < na->len; i++) {
  uint8_t *src = (uint8_t *)(na->strs + na->str_arr[i].off);
  unsigned int srclen = na->str_arr[i].len * 4;
  while(!src[srclen - 1])
    srclen--;
  rev_na->str_arr[i].off = dstlen / 4;
  rev_na->str_arr[i].cp = na->str_arr[i].cp;
  rev_na->str_arr[i].flags = 0;
  for(j = 0; j < srclen; j++) {
    <<Preprocess [[src[j]]]] into [[dstp[dstlen]]]>>
  }
  while(dstlen % 4)
    dstp[dstlen++] = 0;
  rev_na->str_arr[i].len = dstlen / 4 - rev_na->str_arr[i].off;
}
rev_na->len = na->len;
rev_na->strs_char_size = 32; /* almost as large as na, so reduce same way */
@

<<Preprocess [[src[j]]]] into [[dstp[dstlen]]]>>=
if(src[j] & 0x80) {
  dstp[dstlen++] = src[j++];
  if(j < srclen)
    dstp[dstlen++] = src[j];
} else if(isupper(src[j]))
  dstp[dstlen++] = tolower(src[j]);
else if(src[j] != ' ' && src[j] != '-')
  dstp[dstlen++] = src[j];
@

The aliases must be split up into individual names.  The table being
built is indexed on name, so multiple entries can safely have the same
code point.

<<Post-process property data>>=
for(i = 0; i < na_alias->len; i++) {
  uint8_t *src = (uint8_t *)(na_alias->strs + na_alias->str_arr[i].off);
  unsigned int srclen = na_alias->str_arr[i].len * 4;
  while(!src[srclen - 1])
    srclen--;
  for(low = 0; low < srclen; low += src[low] + 2) {
    check_size(rev_na->str_arr, rev_na->max_len, rev_na->len + 1);
    rev_na->str_arr[rev_na->len].off = dstlen / 4;
    rev_na->str_arr[rev_na->len].cp = na_alias->str_arr[i].cp;
    rev_na->str_arr[rev_na->len].flags = 0;
    unsigned int elen = src[low] + 1;
    for(j = low + 1; j < low + 1 + elen; j++) {
      <<Preprocess [[src[j]]]] into [[dstp[dstlen]]]>>
    }
    while(dstlen % 4)
      dstp[dstlen++] = 0;
    rev_na->str_arr[rev_na->len].len = dstlen / 4 - rev_na->str_arr[rev_na->len].off;
    rev_na->len++;
  }
}
@

The name sequences must be split up as well.  The returned value
should be the code point, but sequences require special handling.  In
this case, each sequence is assigned a number, and that number is
added to [[UNI_MAX_CP]] to obtain the code point.  A separate table
([[na_seq_id]]) maps this to an actual sequence.

<<Post-process property data>>=
int prop_na_seq_id = add_prop("na_seq_id");
prop_t *na_seq_id = &parsed_props[prop_na_seq_id];
inisize(na_seq_id->str_arr, na_seq_id->max_len = na_seq->len * 2);
inisize(na_seq_id->strs, na_seq_id->max_strs = na_seq->max_strs);
for(i = 0; i < na_seq->len; i++) {
  uint8_t *src = (uint8_t *)(na_seq->strs + na_seq->str_arr[i].off);
  unsigned int srclen = na_seq->str_arr[i].len * 4;
  while(!src[srclen - 1])
    srclen--;
  while(srclen > 0) {
    low = srclen - 1;
    unsigned int elen = src[low];
    unsigned int seqlen = (elen >> 6) + 1;
    elen = (elen & 0x3f) + 1;
    if(!src[low - 1])
      low--;
    low -= elen;
    check_size(rev_na->str_arr, rev_na->max_len, rev_na->len + 1);
    rev_na->str_arr[rev_na->len].off = dstlen / 4;
    rev_na->str_arr[rev_na->len].flags = 0;
    <<Add [[src[i]]] sequence from [[na_seq]] at [[low[seqlen]]] to [[na_seq_id]]>>
    rev_na->str_arr[rev_na->len].cp = na_seq_id->len + UNI_MAX_CP;
    for(j = low; j < low + elen; j++) {
      <<Preprocess [[src[j]]]] into [[dstp[dstlen]]]>>
    }
    while(dstlen % 4)
      dstp[dstlen++] = 0;
    rev_na->str_arr[rev_na->len].len = dstlen / 4 - rev_na->str_arr[rev_na->len].off;
    rev_na->len++;
    srclen = low - seqlen * 2;
  }
}
@

<<Add [[src[i]]] sequence from (@seq) at [[low[seqlen]]] to (@id)>>=
check_size(<<@id>>->str_arr, <<@id>>->max_len, <<@id>>->len + 1);
check_size(<<@id>>->strs, <<@id>>->max_strs, <<@id>>->strs_len + 1 + seqlen);
<<@id>>->str_arr[<<@id>>->len].cp = <<@id>>->len;
<<@id>>->str_arr[<<@id>>->len].off = <<@id>>->strs_len;
<<@id>>->str_arr[<<@id>>->len].flags = 0;
<<@id>>->strs[<<@id>>->strs_len++] = <<@seq>>->str_arr[i].cp;
for(j = 0; j < seqlen;) {
  unsigned int clen;
  <<@id>>->strs[<<@id>>->strs_len++] = 
            uni_int_utf16_decode((uint16_t *)(src + low) - seqlen + j, &clen);
  j += clen;
}
<<@id>>->str_arr[<<@id>>->len].len = <<@id>>->strs_len - <<@id>>->str_arr[<<@id>>->len].off;
<<@id>>->len++;
@

<<Dump character information as C code>>=
dump_str_ptr_arr(na_seq_id, "named sequence mapping", gen_h);
@

Now that all constituent name lists have been collected into
[[rev_na]], it's time to detect and remove duplicates.  This is done by
simply sorting the array by name, and comparing consecutive elements
for equality.  When a pair matches, the location of the first hyphen
in one but not the other is chosen to distinguish the code points.
Since the hyphens have already been stripped out, this comparison is
done using the original [[na]] property.  In the current UCD, this is
safe, but it may be possible that this needs rewriting in the future
to ignore medial hyphens in everything but U+1180.  It may also become
necessary to check the name aliases or sequence names.

<<Post-process property data>>=
sort_words = &revwords;
sort_strs = rev_na->strs;
qsort(rev_na->str_arr, rev_na->len, sizeof(*rev_na->str_arr), cmp_wordstr);
int prop_na_hyphen = add_prop("na_hyphen");
prop_t *na_hyphen = &parsed_props[prop_na_hyphen];
inisize(na_hyphen->rng_dat32, na_hyphen->max_len = 10);
for(i = rev_na->len - 1; i > 0; i--)
  if(!cmp_wordstr(&rev_na->str_arr[i], &rev_na->str_arr[i - 1])) {
    /* this isn't really what the standard says, but it works with current data */
    /* just find the first mismatched char; if it's a -, store - location */
    /* also, this only searches na, not na_alias or na_seq */
    const raw_cp_str_t *a = bsearch(&rev_na->str_arr[i].cp, na->str_arr,
                                    na->len, sizeof(*na->str_arr), uni_cmp_cp),
                       *b = bsearch(&rev_na->str_arr[i - 1].cp, na->str_arr,
                                    na->len, sizeof(*na->str_arr), uni_cmp_cp);
    const uint8_t *astr = (const uint8_t *)(na->strs + a->off),
                  *bstr = (const uint8_t *)(na->strs + b->off);
    unsigned int alen = a->len * 4, blen = b->len * 4, hloc = 0;
    while(!astr[alen - 1])
      alen--;
    while(!bstr[blen - 1])
      blen--;
    const uint8_t *aword = NULL, *bword = NULL;
    unsigned int aword_len = 0, bword_len = 0;
    uint8_t ac = 0, bc; /* init to shut gcc up */
    sort_words = &words;
    while((alen || aword_len) && (blen || bword_len)) {
      <<Get next char from word-encoded string [[a]] during parse>>
      <<Get next char from word-encoded string [[b]] during parse>>
      if(ac != bc)
        break;
      if(ac != ' ' && ac != '-')
        hloc++;
    }
    sort_words = &revwords;
    check_size(na_hyphen->rng_dat32, na_hyphen->max_len, na_hyphen->len + 1);
    na_hyphen->rng_dat32[na_hyphen->len].dat = hloc;
    na_hyphen->rng_dat32[na_hyphen->len].len = 0;
    if(ac == '-') {
      na_hyphen->rng_dat32[na_hyphen->len].low = rev_na->str_arr[i - 1].cp;
      na_hyphen->rng_dat32[na_hyphen->len].dat |= rev_na->str_arr[i].cp << 8;
    } else {
      na_hyphen->rng_dat32[na_hyphen->len].low = rev_na->str_arr[i].cp;
      na_hyphen->rng_dat32[na_hyphen->len].dat |= rev_na->str_arr[i - 1].cp << 8;
      rev_na->str_arr[i - 1].cp = rev_na->str_arr[i].cp;
    }
    na_hyphen->len++;
    movebuf(rev_na->str_arr + i, rev_na->str_arr + i + 1, rev_na->len - (i + 1));
    rev_na->len--;
    if(!--i)
      break;
  }
qsort(na_hyphen->rng_dat32, na_hyphen->len, sizeof(*na_hyphen->rng_dat32), uni_cmp_cp);
@

<<UCD parser local functions>>=
static const prop_t *sort_words;
static int cmp_wordstr(const void *_a, const void *_b)
{
  const raw_cp_str_t *a = _a, *b = _b;
  const uint8_t *astr = (const uint8_t *)(sort_strs + a->off),
                *bstr = (const uint8_t *)(sort_strs + b->off);
  unsigned int alen = a->len * 4, blen = b->len * 4;
  while(alen && !astr[alen - 1])
    alen--;
  while(blen && !bstr[blen - 1])
    blen--;
  const uint8_t *aword = NULL, *bword = NULL;
  unsigned int aword_len = 0, bword_len = 0;
  while((alen || aword_len) && (blen || bword_len)) {
    uint8_t ac, bc;
    <<Get next char from word-encoded string [[a]] during parse>>
    <<Get next char from word-encoded string [[b]] during parse>>
    if(ac != bc)
      return (int)ac - (int)bc;
  }
  if(alen || aword_len)
    return 1;
  else if(blen || bword_len)
    return -1;
  return 0;
}
@

<<Get next char from word-encoded string (@ab) during parse>>=
if(<<@ab>>word_len) {
  <<@ab>>c = *<<@ab>>word++;
  <<@ab>>word_len--;
} else if(*<<@ab>>str & 0x80) {
  unsigned int wno = ((unsigned int)*<<@ab>>str++ & 0x7f) << 8;
  <<@ab>>len--;
  if(<<@ab>>len) {
    wno += *<<@ab>>str++;
    <<@ab>>len--;
  }
  <<@ab>>word = (const uint8_t *)(sort_words->strs + sort_words->str_arr[wno].off);
  <<@ab>>word_len = sort_words->str_arr[wno].len * 4;
  while(!<<@ab>>word[<<@ab>>word_len - 1])
    <<@ab>>word_len--;
  <<@ab>>c = *<<@ab>>word++;
  <<@ab>>word_len--;
} else {
  <<@ab>>c = *<<@ab>>str++;
  <<@ab>>len--;
}
@

While the generated [[na_hyphen]] table is sufficient to check for
whether or not hyphens are needed, this check needs to be performed on
every single lookup.  To make that go a little faster, a macro is
generated to test against all known potentially duplicate name matches.

<<Dump character information as C code>>=
const char *nd_next = "/** True if hyphens must be retained for loose matching "
                      "for name of code point \\p x */\n"
		      "int uni_rev_na_needs_hyphen(uint32_t cp);\n"
		      "#define uni_rev_na_needs_hyphen(x) (";
for(i = 0; i < na_hyphen->len; i++) {
  fprintf(gen_h, "%s(x) == 0x%04X", nd_next, na_hyphen->rng_dat32[i].low);
  nd_next = " || ";
}
fputs(")\n", gen_h);
@

The final table to generate is for range lookups.  As mentioned above,
it needs to be separate because its usage is different.  The other
tables are for direct searching, whereas this one is for prefix
searching.  Technically, a binary search could permit both; a special
indicator at the end of the name could be used to detect that this is
a prefix.  In fact, some of my old code used to do just that.  For
now, I prefer keeping this separate, though.  Stripping is performed
the same way as with the main name tables.

<<Post-process property data>>=
int prop_rev_na_rng = add_prop("rev_na_rng");
prop_t *rev_na_rng = &parsed_props[prop_rev_na_rng];
inisize(rev_na_rng->str_arr, rev_na_rng->max_len = rev_na_rng->len = na_rng->len);
inisize(rev_na_rng->strs, rev_na_rng->max_strs = na_rng->strs_len);
dstp = (uint8_t *)rev_na_rng->strs;
dstlen = 0;
for(i = 0; i < na_rng->len; i++) {
  uint8_t *src = (uint8_t *)(na_rng->strs + na_rng->str_arr[i].off);
  unsigned int srclen = na_rng->str_arr[i].len * 4;
  while(!src[srclen - 1])
    srclen--;
  rev_na_rng->str_arr[i].off = dstlen / 4;
  rev_na_rng->str_arr[i].cp = na_rng->str_arr[i].cp;
  rev_na_rng->str_arr[i].flags = 0;
  for(j = 0; j < srclen; j++) {
    <<Preprocess [[src[j]]]] into [[dstp[dstlen]]]>>
  }
  while(dstlen % 4)
    dstp[dstlen++] = 0;
  rev_na_rng->str_arr[i].len = dstlen / 4 - rev_na_rng->str_arr[i].off;
}
rev_na_rng->strs_char_size = 8;
@

Now, all of the tables can be dumped.  Since the default dumper sorts
by code point, it is rewritten here to sort by name instead.  In order
to sort, the offsets and lengths need to be accurate for the current
in-memory representation.  The [[dump_strs]] function changes these to
match the generated representation, though.  For this reason, dumping
the word table must be delayed until sorting (which needs the words in
memmory) is complete.

<<Dump character information as C code>>=
sort_words = &revwords;
dump_wordord_strs(rev_na, "reverse Name lookup", gen_h);
dump_wordord_strs(rev_na_rng, "reverse synthetic Name lookup", gen_h);
revwords.strs_char_size = 8;
revwords.name = "rev_na_words";
dump_str_ptr_arr(&revwords, "reverse Name lookup compression support", gen_h);
free(revwords.strs);
free(revwords.str_arr);
@

In fact, the [[dump_strs]] function sorts its input as well, which
screws up the table order.  In order to compensate for this, a copy is
made of the string array before dumping, so that the original offsets
and lengths are retained, and then the copy is sorted instead.  The
code point field is used process to keep the arrays matched up.  While
generating strings, the original's [[cp]] points to the array entry in
the copy, and then the copy's [[cp]] is switched to point to the
original.  Then, after sorting the copy, the copy is dumped in order
by dumping the original array entry corresponding to the [[cp]].

<<UCD parser local functions>>=
static void dump_wordord_strs(prop_t *p, const char *lname, FILE *gen_h)
{
  unsigned int i, j;

  /* dump_strs mangles str_arr, so save it and restore it afterwards */
  raw_cp_str_t *ostr_arr;
  inisize(ostr_arr, p->len);
  cpybuf(ostr_arr, p->str_arr, p->len);
  for(i = 0; i < p->len; i++)
    p->str_arr[i].cp = i;
  dump_strs(p, lname, gen_h);
  for(i = 0; i < p->len; i++) {
    j = p->str_arr[i].cp;
    p->str_arr[i].cp = ostr_arr[j].cp;
    ostr_arr[j].cp = i;
  }
  sort_strs = p->strs;
  qsort(ostr_arr, p->len, sizeof(*ostr_arr), cmp_wordstr);
  char nbuf[64];
  sprintf(nbuf, "uni_%s_arr.gen.c", p->name);
  open_wf(rt, nbuf);
  fprintf(rt, "#include \"uni_prop.h\"\n\n"
              "const uni_str_arr_t uni_%s_arr[] = {\n\t", p->name);
  for(i = 0; i < p->len; i++) {
    const raw_cp_str_t *ent = &p->str_arr[ostr_arr[i].cp];
    fprintf(rt, "{ 0x%04X, %d, %d}%s", (int)ent->cp, (int)ent->off, 
                                       (int)ent->len,
				       i < p->len - 1 ? ",\n\t" : "\n};\n");
  }
  free(ostr_arr);
  fclose(rt);
  fprintf(gen_h, "/** Sorted string pointer array for %s property */\n"
		 "extern const uni_str_arr_t uni_%s_arr[];\n"
		 "/** Length of \\ref uni_%s_arr */\n"
	         "#define uni_%s_arr_len %d /* %d lookups max */\n",
	         lname, p->name, p->name, p->name, i, lg2(i + 1));
}
@

It is expected that the user will take the tables as is and dump them
in a more suitable format.  Since they are meant to be handled raw, a
function is provided to interpret the word pointers, at least.

<<Unicode property exports>>=
/** Expand compressed rev(erse)_na(me) string table entry.
  * Expands compressed string table entry \p na_raw/\p na_raw_len.
  * The return value is described with \ref uni_return8_buf8. */
int uni_rev_na_expand_words(const uint8_t *na_raw, unsigned int na_raw_len,
                           const uni_str_ptr_t *words, const uint8_t *words_strs);
#define uni_rev_na_expand_words(na_raw, na_raw_len, buf, off, buf_len) \
  uni_naX_expand_words(na_raw, na_raw_len, uni_rev_na_words_arr, uni_rev_na_words_strs, \
                       buf, off, buf_len)
@

For raw comparison, the compressed form can be retained, if a special
string comparison function is used.

<<Unicode property exports>>=
/** Compare two strings, either one of which may be word-compressed.
  * Compares \p stra/\p lena to \p strb/\p lenb by code point, then length.
  * Returns -1 if \p stra is less than \p strb, 1 if greater, and 0 if equal.
  * If either string is word-compressed, it will be uncompressed without
  * explicit storage during comparison using the \p words/\p words_strs
  * word table.  Also, if \p eqlen is non-NULL, it is updated to the
  * uncompressed length of initial matching text */
int uni_cmp_word_str(const uint8_t *stra, unsigned int lena,
                     const uint8_t *strb, unsigned int lenb,
		     unsigned int *eqlen,
		     const uni_str_ptr_t *words, const uint8_t *words_strs);
/** Compare two strings, either one of which may be compressed
  * rev(erse)_na(me) strings.
  * Compares \p stra/\p lena to \p strb/\p lenb by code point, then length.
  * Returns -1 if \p stra is less than \p strb, 1 if greater, and 0 if equal.
  * If either string is word-compressed, it will be uncompressed without
  * explicit storage during comparison like a rev_na table entry.  Also, if
  * \p eqlen is non-NULL, it is updated to the uncompressed length of
  * initial matching text */
int uni_cmp_rev_na_str(const uint8_t *stra, unsigned int lena,
                      const uint8_t *strb, unsigned int lenb,
		      unsigned int *eqlen);
#define uni_cmp_rev_na_str(stra, lena, strb, lenb, eqlen) \
  uni_cmp_word_str(stra, lena, strb, lenb, eqlen, uni_rev_na_words_arr, \
                   uni_rev_na_words_strs)
@

<<Unicode property functions>>=
int uni_cmp_word_str(const uint8_t *astr, unsigned int alen,
                     const uint8_t *bstr, unsigned int blen,
		     unsigned int *eqlen,
		     const uni_str_ptr_t *words, const uint8_t *words_strs)
{
  const uint8_t *aword = NULL, *bword = NULL;
  unsigned int aword_len = 0, bword_len = 0;
  int eq = 0;
  while((alen || aword_len) && (blen || bword_len)) {
    uint8_t ac, bc;
    <<Get next char from word-encoded string [[a]]>>
    <<Get next char from word-encoded string [[b]]>>
    if(ac != bc) {
      if(eqlen)
        *eqlen = eq;
      return (int)ac - (int)bc;
    }
    eq++;
  }
  if(eqlen)
    *eqlen = eq;
  if(alen || aword_len)
    return 1;
  else if(blen || bword_len)
    return -1;
  return 0;
}
@

Fetching a character is slightly different than during parsing because
the table formats have been simplified.

<<Get next char from word-encoded string (@ab)>>=
if(<<@ab>>word_len) {
  <<@ab>>c = *<<@ab>>word++;
  <<@ab>>word_len--;
} else if(*<<@ab>>str & 0x80) {
  unsigned int wno = ((unsigned int)*<<@ab>>str++ & 0x7f) << 8;
  <<@ab>>len--;
  if(<<@ab>>len) {
    wno += *<<@ab>>str++;
    <<@ab>>len--;
  }
  <<@ab>>word = words_strs + words[wno].off;
  <<@ab>>word_len = words[wno].len;
  <<@ab>>c = *<<@ab>>word++;
  <<@ab>>word_len--;
} else {
  <<@ab>>c = *<<@ab>>str++;
  <<@ab>>len--;
}
@

Knowning the match length does little good without also knowing the
length of each string.

<<Unicode property exports>>=
/** Find uncompressed length of a word-compressed string.
  * Computes uncompressed length of \p str/\p len compressed using
  * word table \p words/words_strs */
int uni_word_strlen(const uint8_t *str, unsigned int len,
		    const uni_str_ptr_t *words, const uint8_t *words_strs);
/** Find uncompressed length of a rev(erse)_na(me) table string.
  * Computes uncompressed length of \p str/\p len as if it were a
  * rev_na table string  */
int uni_rev_na_strlen(const uint8_t *str, unsigned int len);
#define uni_rev_na_strlen(str, len) \
  uni_word_strlen(str, len, uni_rev_na_words_arr, uni_rev_na_words_strs)
@

<<Unicode property functions>>=
int uni_word_strlen(const uint8_t *str, unsigned int len,
		    const uni_str_ptr_t *words, const uint8_t *words_strs)
{
  unsigned int rlen = 0;
  while(len) {
    if(*str & 0x80) {
      unsigned int wno = ((unsigned int)*str++ & 0x7f) << 8;
      len--;
      if(len) {
        wno += *str++;
	len--;
      }
      rlen += words[wno].len;
    } else {
      rlen++;
      len--;
    }
  }
  return rlen;
}
@

The range lookup function is not just a simple lookup, so it is
provided as a separate function.  It might be possible to use this as
a fallback while using another mechanism to look up unsynthesized
names.  However, its usage is rare enough and not associated with the
other name functions that it is stored in a separate object file to be
ignored if needed.

<<Library [[uni]] Members>>=
narev_rng_bin.o
@

<<Unicode property exports>>=
/** Look up synthetic Unicode code point name.
  * Look up \p name/\p len, assuming it maps to one of the synthetic
  * ranges.  If \p len is negative, \p name is zero-terminated.
  * \p name must already be normalized and case-folded.
  * This function can also interpret angle-bracket-enclosed labels
  * (i.e. \<\e range-\e hex_cp\>).  If the return value is negative,
  * \p name does not map to a synthetic range.  */
int32_t uni_name_rng_to_cp(const char *name, int len);
@

<<narev_rng_bin.c>>=
<<Common C Header>>
#include "uni_prop.h"

int32_t uni_name_rng_to_cp(const char *name, int len)
{
  <<Convert synthetic [[name]] to code point>>
}
@

The first thing to do is to strip the name.  By requring normalized,
case-folded input, most of the work is already done.  The only thing
left is to strip all spaces and hyphens.  The code assumes that no
range will ever have an ambiguous hyphen.  Since no synthesized name
in the current UCD is longer than 127 characters (actually 83
characters in the current UCD), an input that long is automatically
rejected.  That also keeps invalid input from overflowing the stack,
where the stripped name is being built.  While labels are not
technically names, they can be looked up with this function as well.
It is up to the caller to ensure that the name does not start with a
less-than if no labels are desired.
 
<<Convert synthetic [[name]] to code point>>=
const char *n = name;
if(len < 0)
  len = strlen(name);
unsigned int olen = len;
if(olen > 127)
  olen = 127;
char oname[olen + 1];
olen = 0;
for(; len > 0; n++, len--) {
  if(isspace(*n) || *n == '_' || *n == '-')
    continue;
  if(olen == 127)
    return -1;
  if(!isdigit(*n) && !islower(*n) && *n != '<' && *n != '>')
    return -1;
  oname[olen++] = *n;
}
oname[olen] = 0;
@

Next, we do a simple binary search.  An exact match is rejected:
synthesized names always have extra characters.  The search will end
with [[h]] being the correct entry, if there is one.

<<Convert synthetic [[name]] to code point>>=
int l = 0, h = uni_rev_na_rng_arr_len / 2 - 1, m;
unsigned int heq = 0, meq;
while(l <= h) {
  m = l + h;
  int c = uni_cmp_rev_na_str((uint8_t *)oname, olen,
                             uni_rev_na_rng_strs + uni_rev_na_rng_arr[m].off,
			     uni_rev_na_rng_arr[m].len, &meq);
  if(l == h)
    heq = meq;
  if(!c)
    return -1;
  if(c > 0)
    l = m / 2 + 1;
  else {
    h = m / 2 - 1;
    heq = 0;
  }
}
@

To test if [[h]] is a valid match, it must first be a valid array
index.  It must also be a prefix of [[oname]]: the match length must
be equal to its length.  If it is not a valid match, it may still be a
reserved or non-character.

<<Convert synthetic [[name]] to code point>>=
if(h >= 0) {
  h *= 2;
  if(!heq)
    uni_cmp_rev_na_str((uint8_t *)oname, olen,
                       uni_rev_na_rng_strs + uni_rev_na_rng_arr[h].off,
		       uni_rev_na_rng_arr[h].len, &heq);
  if(heq != uni_rev_na_strlen(uni_rev_na_rng_strs + uni_rev_na_rng_arr[h].off,
		              uni_rev_na_rng_arr[h].len))
    h = -1;
}
if(h < 0) {
  if(!strncmp(oname, "<reserved", sizeof("<reserved") - 1)) {
    h = -1;
    heq = sizeof("<reserved") - 1;
  } else if(!strncmp(oname, "<noncharacter", sizeof("<noncharacter") - 1)) {
    h = -2;
    heq = sizeof("<noncharacter") - 1;
  } else
    return -1;
}
@

The next thing to check is the extension itself.  For all but Hangul
syllables, this is a hexadecimal digit string of four or more
characters, with no excess leading zeroes, possibly followed by a
greater-than symbol.  There is at least one range with the same name
but two different numerical ranges, so a name match is checked for all
others with the exact same name as well.

<<Convert synthetic [[name]] to code point>>=
char *s = oname + heq;
if(h >= 0 && uni_rev_na_rng_arr[h].cp == 0xAC00) {
  <<Parse Hangul syllable extension>>
} else {
  uint32_t ret;
  if((*s == '0' && (olen - heq) > 4 && s[4] != '>') ||
     olen - heq < 4 || (olen - heq > 8 && s[8] != '>'))
    return -1;
  ret = strtoul(s, &s, 16);
  if(oname[0] == '<')
    if(*s++ != '>')
      return -1;
  if(*s)
    return -1;
  if(h >= 0) {
    if(ret >= uni_rev_na_rng_arr[h].cp && ret <= uni_rev_na_rng_arr[h + 1].cp)
      return ret;
    m = h;
    while(m > 0 && uni_rev_na_rng_arr[m - 1].off == uni_rev_na_rng_arr[m].off &&
          uni_rev_na_rng_arr[m - 1].len == uni_rev_na_rng_arr[m].len) {
      m -= 2;
      if(ret >= uni_rev_na_rng_arr[m].cp && ret <= uni_rev_na_rng_arr[m + 1].cp)
        return ret;
    }
    m = h;
    while(m < uni_rev_na_rng_arr_len - 2 &&
          uni_rev_na_rng_arr[m + 2].off == uni_rev_na_rng_arr[m].off &&
          uni_rev_na_rng_arr[m + 2].len == uni_rev_na_rng_arr[m].len) {
      m += 2;
      if(ret >= uni_rev_na_rng_arr[m].cp && ret <= uni_rev_na_rng_arr[m + 1].cp)
        return ret;
    }
  } else {
    if(h == -2 && uni_is_NChar(ret)) /* nonchar */
      return ret;
    /* Name lookup, but at least it doesn't pull in name strings */
    /* Probably ought to turn this into a boolean property */
    else if(h == -1 && !uni_na_of(ret)->len && !uni_na_rng_of(ret))
      return ret;
  }
  return -1;
}
@

Parsing the Hangul syllables is a bit harder, requiring
character-by-character parsing.  The suffix is a JSN of hst L,
followed by a JSN of hst V, optionally followed by a JSN of hst T.  It
might be possible to come up with a regular expression to do the
match, but that would not help with returning the correct code point.
Instead, the JSN property is modified in a manner similar to the
names, but split by hst, to assist in reverse lookups.  The simplest
way to keep things separated by hst is to prefix each name with its
hst.  No strings are more than 3 characters, so this does not affect
the string table length.  Rather than look up hst, the first 19 are
marked as Choseong (L), the next 21 as Jungseong (V), and the rest as
Jongseong (T).  In fact, the enumeration values are not used at all,
but instead just 0, 1, and 2, respectively.  The code points
themselves are somewhat useless, so they are changed to an offset from
the first entry in each group.

<<Post-process property data>>=
int prop_rev_JSN = add_prop("rev_JSN");
prop_t *rev_JSN = &parsed_props[prop_rev_JSN];
prop_t *JSN = &parsed_props[prop_JSN];
rev_JSN->max_strs = rev_JSN->strs_len = JSN->strs_len;
inisize(rev_JSN->strs, rev_JSN->max_strs);
dstp = (uint8_t *)rev_JSN->strs;
dstlen = 0;
rev_JSN->max_len = rev_JSN->len = JSN->len;
inisize(rev_JSN->str_arr, rev_JSN->max_len);
int lvtno = 0;
for(i = 0; i < JSN->len; i++) {
  uint8_t *src = (uint8_t *)(JSN->strs + JSN->str_arr[i].off);
  unsigned int srclen = JSN->str_arr[i].len * 4;
  while(srclen && !src[srclen - 1])
    srclen--;
  rev_JSN->str_arr[i].off = dstlen / 4;
  if(i == 19 || i == 19 + 21)
    lvtno = 0;
  rev_JSN->str_arr[i].cp = lvtno++;
  rev_JSN->str_arr[i].flags = 0;
  if(srclen)
    dstp[dstlen++] = i < 19 ? 0 /* L */ : i < 19 + 21 ? 1 /* V */ : 2 /* T */;
  for(j = 0; j < srclen; j++) {
    <<Preprocess [[src[j]]]] into [[dstp[dstlen]]]>>
  }
  while(dstlen % 4)
    dstp[dstlen++] = 0;
  rev_JSN->str_arr[i].len = dstlen / 4 - rev_JSN->str_arr[i].off;
}
rev_JSN->strs_char_size = 8;
@

<<Dump character information as C code>>=
dump_wordord_strs(rev_JSN, "Jamo_Short_Name reverse lookup", gen_h);
@

The L is always either a single character, no character, or a doubled
character.  It is always followed by a V, which is always non-blank
and never begins with the same character as an L.  This means that the
first character (or two if the next is the same) can be looked up for
L.  A lookup failure is interpreted as a blank L.  

<<Parse Hangul syllable extension>>=
int slen = *s == s[1] ? 2 : 1;
l = 1; /* skip 0, since it's blank */
h = 18;
<<Search reverse JSN for [[s]] of length [[slen]]>>
int L = uni_rev_JSN_arr[l <= h ? m : 0].cp;
if(l <= h)
  s += slen;
@

<<Search reverse JSN for [[s]] of length [[slen]]>>=
while(l <= h) {
  m = (l + h) / 2;
  len = uni_rev_JSN_arr[m].len - 1;
  if(len > slen)
    len = slen;
  int c = memcmp(s, uni_rev_JSN_arr[m].off + uni_rev_JSN_strs + 1, len);
  if(!c && slen == uni_rev_JSN_arr[m].len - 1)
    break;
  else if(c < 0 || (!c && slen == len))
    h = m - 1;
  else
    l = m + 1;
}
@

The V is always a single character, followed by optional vowels (A,
E, I, O, or U).  Since T never starts with a vowel, V can be extracted
and looked up.  A lookup failure is an invalid suffix.

<<Parse Hangul syllable extension>>=
slen = 1;
while(s[slen] == 'a' || s[slen] == 'e' || s[slen] == 'i' ||
      s[slen] == 'o' || s[slen] == 'u')
  slen++;
l = 19;
h = l + 21 - 1;
<<Search reverse JSN for [[s]] of length [[slen]]>>
if(l > h)
  return -1;
int V = uni_rev_JSN_arr[m].cp;
s += slen;
@

Finally, the T part is what's left over, if anything.  A blank is a
missing T, and otherwise a lookup failure is an invalid suffix.

<<Parse Hangul syllable extension>>=
slen = strlen(s);
int T;
if(!slen)
  T = 0;
else {
  l = 19 + 21;
  h = uni_rev_JSN_arr_len - 1;
  <<Search reverse JSN for [[s]] of length [[slen]]>>
  if(l > h)
    return -1;
  T = uni_rev_JSN_arr[m].cp + 1;
}
@

A successful lookup on all three means that the return value can be
computed and returned.

<<Parse Hangul syllable extension>>=
return 0xAC00 + T + 28 * (L * 21 + V);
@

In addition, a binary search function for the main name table is
provided, although it is not intended to be used for serious
applications.  Since the range lookup function pulls in a smaller
table, it is placed in a separate object file from the main lookup
function.

<<Unicode property exports>>=
/** Look up Unicode code point name.
  * Look up \p name/\p len using binary searching and loose matching.  If
  * \p len is negative, \p name is zero-terminated.
  * This function can also interpret angle-bracket-enclosed labels
  * (i.e. \<\e range-\e hex_cp\>).  If the return value is negative,
  * \p name does not loosely match any Unicode code point name, sequence
  * name, or label.  If the return value is greather than \ref UNI_MAX_CP,
  * it is the sequence \ref uni_na_seq_id_arr[\e N - \ref UNI_MAX_CP - 1],
  * where \e N is the return value from this function.  */
int32_t uni_name_to_cp(const char *name, int len);
@

<<Library [[uni]] Members>>=
narev_bin.o
@

<<narev_bin.c>>=
<<Common C Header>>
#include "uni_prop.h"

int32_t uni_name_to_cp(const char *name, int len)
{
  unsigned int nlen = len < 0 ? len : strlen(name);
  <<Strip name before lookup>>
  <<Look up name and return if present>>
  return uni_name_rng_to_cp(name, nlen);
}
@

Unlike with the name ranges, dash locations must be tracked.  This is
done using a flag in a side array.

<<Strip name before lookup>>=
const char *n = name;
unsigned int olen = nlen;
if(olen > 127)
  olen = 127;
char oname[olen + 1], hyphen[olen];
olen = 0;
int had_hyphen = 0;
for(; len > 0; n++, len--) {
  if(isspace(*n) || *n == '_')
    continue;
  if(*n == '-') {
    had_hyphen = 1;
    continue;
  }
  if(olen == 127)
    return -1;
  if(!isdigit(*n) && !islower(*n) && *n != '<' && *n != '>')
    return -1;
  hyphen[olen] = had_hyphen;
  oname[olen++] = *n;
  had_hyphen = 0;
}
oname[olen] = 0;
@

Like with ranges, the next step is a binary search.  Unlike ranges, an
exact match is the only thing that matters.

<<Look up name and return if present>>=
int l = 0, h = uni_rev_na_arr_len - 1, m;
while(l <= h) {
  m = (l + h) / 2;
  const uint8_t *rev_na = (const uint8_t *)(uni_rev_na_strs + uni_rev_na_arr[m].off);
  len = uni_rev_na_arr[m].len * 4;
  while(!rev_na[len - 1])
    len--;
  int c = uni_cmp_rev_na_str((uint8_t *)oname, olen, rev_na, len, NULL);
  if(!c)
    break;
  if(c > 0)
    l = m + 1;
  else
    h = m - 1;
}
@

An exact match does not include hyphens, so once that's checked, a
value can be returned.

<<Look up name and return if present>>=
if(l <= h) {
  uint32_t ret = uni_rev_na_arr[m].cp;
  <<Return code point [[ret]] unless a hyphen overrides it>>
}
@

<<Return code point [[ret]] unless a hyphen overrides it>>=
if(!uni_rev_na_needs_hyphen(ret))
  return ret;
for(l = 0; l < uni_na_hyphen_rng_len; l++)
  if(ret == uni_na_hyphen_rng[l].low) {
    if(hyphen[uni_na_hyphen_rng[l].dat & 0xff])
      return uni_na_hyphen_rng[l].dat >> 8;
    else
      return ret;
  }
@

Testing all of this can be difficult.  Instead of a thorough test, the
following program takes the output of [[tstcp_na]] above, and looks up
the name.  If the looked up code points do not match the advertised
ones, an error is returned.

\lstset{language=make}
<<Additional Tests>>=
./tstcp_na | ./tstna_cp >/dev/null
@

<<C Test Support Executables>>=
tstna_cp \
@

\lstset{language=C}
<<tstna_cp.c>>=
<<Common C Header>>

#include "uni_prop.h"

char buf[256];

int main(void)
{
  int lno;
  for(lno = 0; fgets(buf, 256, stdin); lno++) {
    <<Set [[e]] to start of name to check>>
    for(s = e; *s; s++)
      if(isupper(*s))
        *s = tolower(*s);
    uint32_t ret = uni_name_to_cp(e, (int)(s - e));
    <<Print and check [[name_to_cp]] results using [[na_seq_id]]>>
  }
  fprintf(stderr, "Successfully looked up %d entries\n", lno);
  return 0;
}
@

<<Set [[e]] to start of name to check>>=
char *s, *e;
if(!(e = strchr(buf, ' ')))
  continue;
*e++ = 0;
if(!(e = strchr(e, ' ')))
  continue;
e++;
if(*e == '&')
  e++;
s = strchr(e, '\n');
if(s)
  *s = 0;
@

<<Print and check [[name_to_cp]] results using (@seqid)>>=
if(!strchr(buf, ':')) {
  printf("%04X %s\n", ret, e);
  if(ret != strtoul(buf, &s, 16) || *s)
    exit(1);
} else if(ret <= UNI_MAX_CP)
  exit(1);
else {
  const uni_str_ptr_t *seq = &uni_<<@seqid>>_arr[ret - UNI_MAX_CP - 1];
  const uint16_t *retp = seq->off + uni_<<@seqid>>_strs;
  int len = seq->len;
  int eq = 1;
  for(s = buf; len > 0; ) {
    unsigned int clen;
    uint32_t next = uni_int_utf16_decode(retp, &clen);
    if(strtoul(s, &s, 16) != next)
      eq = 0;
    printf("%04X ", (int)next);
    len -= clen;
    retp += clen;
    if(*s)
      s++;
  }
  puts(e);
  if(!eq)
    exit(1);
}
@

A binary search is not very effective with such a large table.  At
least a sample hash table implementation should be provided as well.
As menioned above, a perfect hash generated by [[gperf]] is
impractical.  However, while trying to find a faster [[gperf]], I came
across a program called [[cmph]]%
\footnote{\url{http://sourceforge.net/projects/cmph}}%
.  This generates minimal perfect hash functions in linear time (sub
second time for the entire Unicode name set!).  However, it is not
very good for inclusion in a library, because it generates the hash
table in an external file or in memory (but not as a single savable
memory block) rather than as compilable C code.  This means that the
hash table would need to be stored somewhere and be parsed every time
the hash table is first used in a program.  I have made changes to
[[cmph]] to allow generation of C code instead%
\footnote{\url{http://sourceforge.net/tracker/?func=detail\&aid=3590339\&group_id=126608\&atid=706189}
and \url{http://sourceforge.net/u/darktjm/cmph}}% .  While these
changes were submitted upstream, they were rejected by the authors as
``not being general enough.''   Their recommended workaround of
encoding the file as a static array is too much trouble (because there
is no function to load such an array, you'd have to write a program
which loads the file, converts it to a packed blob, and use that
instead, which is slower to use at run-time as well).  The following
assumes you use my method instead.

The [[cmph]] support is optional, and must be enabled with a
configuration option.  Since using it requires linking to its library
([[-lcmph]]) as well, an extra step is taken:  the hash table and its
support functions are not just in a separate object file, they are in
an entirely different library.

\lstset{language=make}
<<Library [[uni_cmph]] Members>>=
cmph_rev_na.o
cmph_na_supt.o
@

<<makefile.config>>=
# Path to cmph binary; set to enable cmph support
#CMPH=cmph

# Generation options for cmph table
CMPH_GEN=-a bdz_ph -b 3

# Include directory for cmph headers
#CMPH_INCLUDE=/usr/include

# Library flags for cmph library
#CMPH_LDFLAGS=-lcmph

@

<<makefile.rules>>=
cmph_in_na.gen: cmph_in_na_gen
	./cmph_in_na_gen >$@

cmph_in_na.gen.mph: cmph_in_na.gen
	$(CMPH) -g $(CMPH_GEN) cmph_in_na.gen

ifneq ($(CMPH),)
cmph_rev_na.c: cmph_in_na.gen.mph
	$(CMPH) -C uni_rev_na_mph -o $@ cmph_in_na.gen
else
cmph_rev_na.c: uni_prop.gen.h
	echo >$@
endif

# needed for cproto.h, so may as well include everywhere
ifneq ($(CMPH),)
EXTRA_CFLAGS += -DUSE_CMPH
ifneq ($(CMPH_INCLUDE),)
EXTRA_CFLAGS += -I$(CMPH_INCLUDE)
endif
else
# don't offer CMPH if it's not supported in binary
NOTANGLE_POSTPROC+=|sed -e 's/ifdef USE_CMPH/if 0/'
endif
@

<<Clean temporary files>>=
rm -f cmph_in_na.gen{,.mph}
@

\lstset{language=C}
<<cmph_na_supt.c>>=
#ifdef USE_CMPH
<<Common C Header>>

#include "uni_prop.h"

<<[[cmph]] name lookup support>>
#endif
@

<<Unicode property exports>>=
#ifdef USE_CMPH
#include <cmph.h>
<<Unicode property exports using [[cmph]]>>
#endif
@

<<Unicode property exports using [[cmph]]>>=
/** Pre-generated cmph perfect hash table for looking up names */
extern const cmph_t uni_rev_na_mph;
@

The input to [[cmph]] is a list of names.  The output function returns
an index into that list, which must be verified by comparing directly
to that list element.  Since the name list must be expanded, the
original na array cannot be used.  So, the same rev\_na list built above
is still needed.

\lstset{language=make}
<<C Build Executables>>=
cmph_in_na_gen \
@

<<makefile.rules>>=
cmph_in_na_gen: libuni.a cmph_in_na_gen.o
	$(CC) -o $@ cmph_in_na_gen.o -L. -luni
@

\lstset{language=C}
<<cmph_in_na_gen.c>>=
<<Common C Header>>

#include "uni_prop.h"

int main(void)
{
  unsigned int i;
  uint8_t *buf = NULL;
  unsigned int buf_len = 0;
  
  for(i = 0; i < uni_rev_na_arr_len; i++) {
    const uint8_t *p = (const uint8_t *)(uni_rev_na_strs + uni_rev_na_arr[i].off);
    unsigned int l = uni_rev_na_arr[i].len * 4;
    while(!p[l - 1])
      l--;
    l = uni_rev_na_expand_words(p, l, &buf, 0, &buf_len);
    printf("%.*s\n", l, (char *)buf);
  }
  return 0;
}
@

The value returned by the lookup function is arbitrary.  The only way
to map this number back to a name is to look up the names, and
generate a translation table from return value to name array index.
This translation is unnecessary if the algorithm is CHM, so a separate
preprocessor define activates this table (but it is always generated,
anyway).  On all but two algorithms, the return value is within range
of the input table size, so a blind direct lookup is possible.
However, for a few types, the values are sparse, so the return value
must be checked against the larger table size rather than the name
table size.  Thus the table size is exported (always) as well.

\lstset{language=make}
<<makefile.config>>=
# Set to non-blank if perfect hash function is not order-preserving
# at time of writing, this means any algorithm other than chm (the default)
CMPH_XLATE=y

# Set to non-blank if the perfect hash function's results are not within
# the range 0 - <len>-1
# at time of writing, this means bdz_ph and chd_ph
CMPH_SPARSE=y

@

<<makefile.rules>>=
# needed for cproto.h, so may as well include everywhere
ifneq ($(CMPH),)
ifneq ($(CMPH_XLATE),)
EXTRA_CFLAGS += -DCMPH_XLATE $(if $(CMPH_SPARSE),-DCMPH_SPARSE)
endif
endif
@

\lstset{language=C}
<<Unicode property exports using [[cmph]]>>=
/** Translate return from cmph name lookup return to a \ref uni_rev_na_arr
  * array index.  If the return value is ~0, the lookup failed.  */
extern const uint32_t uni_rev_na_mph_xlate[];
/** The maximum valid value returned by a cmph name lookup. */
extern const uint32_t uni_rev_na_max_mph_res;
@

\lstset{language=make}
<<makefile.rules>>=
cmph_na_xlate.gen.c: cmph_in_na.gen.mph
	$(call gen_xlate,na) >$@
@

<<makefile.vars>>=
gen_xlate = ( \
  echo "\#include <stdint.h>"; \
  echo "const uint32_t uni_rev_$1_mph_xlate[] = {"; \
  $(CMPH) -v -m cmph_in_$1.gen.mph cmph_in_$1.gen | sed 's/.* -> //' | \
  ( i=0; while read n; do echo $$n $$i; i=$$((i+1)); done ) | sort -n | \
  ( i=0; while read x n; do \
           while [ $$i -lt $$x ]; do echo $$i '~0'; i=$$((i+1)); done; \
	   echo $$x $$n; i=$$((i+1)); \
	 done ) | \
  ( \
    i=0; tr='\t'; \
    while read x n; do \
      i=$$((i+1)); \
      printf "$$tr$$n"; \
      if [ $$i -lt 8 ]; then tr=', '; else i=0; tr=',\n\t'; fi; \
    done); \
  echo; \
  echo "};"; \
  echo $(if $(CMPH_SPARSE), \
        "const uint32_t uni_rev_$1_max_mph_res = " \
        "sizeof(uni_rev_$1_mph_xlate)/sizeof(uint32_t);") )
@

<<Library [[uni_cmph]] Members>>=
cmph_na_xlate.gen.o
@

<<Clean temporary files>>=
rm -f cmph_na_xlate.gen.c
@

The lookup function itself is identical to the one using binary
searching, except for the primary name lookup.  That means that binary
searching is still used for the range lookups; there is no easy way to
convert that to a hash lookup.  A prefix tree (trie) might be more
effective, but that is left as an exercise for the end user.

\lstset{language=C}
<<Unicode property exports using [[cmph]]>>=
/** Look up Unicode code point name.
  * Look up \p name/\p len using a cmph perfefct hash function and loose
  * matching.  If \p len is negative, \p name is zero-terminated.
  * This function can also interpret angle-bracket-enclosed labels
  * (i.e. \<\e range-\e hex_cp\>).  If the return value is negative,
  * \p name does not loosely match any Unicode code point name, sequence
  * name, or label.  If the return value is greather than \ref UNI_MAX_CP,
  * it is the sequence \ref uni_na_seq_id_arr[\e N - \ref UNI_MAX_CP - 1],
  * where \e N is the return value from this function.  */
int32_t uni_cmph_name_to_cp(const char *name, int len);
@

<<[[cmph]] name lookup support>>=
int32_t uni_cmph_name_to_cp(const char *name, int len)
{
  unsigned int nlen = len < 0 ? len : strlen(name);
  <<Strip name before lookup>>
  <<Look up name using [[cmph]] and return if present>>
  return uni_name_rng_to_cp(name, nlen);
}
@

The primary name lookup requires a hash lookup, followed by an
explicit comparison.  A successful comparison is followed by a hyphen
check, just like with the binary searching routine.

<<Look up name using [[cmph]] and return if present>>=
uint32_t ret = cmph_search((cmph_t *)&uni_rev_na_mph, oname, olen);
#if defined(CMPH_SPARSE) && defined(CMPH_XLATE)
if(ret < uni_rev_na_max_mph_res)
  ret = uni_rev_na_mph_xlate[ret];
#endif
if(ret < uni_rev_na_arr_len) {
#if defined(CMPH_XLATE) && !defined(CMPH_SPARSE)
  ret = uni_rev_na_mph_xlate[ret];
#endif
  const uint8_t *rev_na = (const uint8_t *)(uni_rev_na_strs + uni_rev_na_arr[ret].off);
  len = uni_rev_na_arr[ret].len * 4;
  while(!rev_na[len - 1])
    len--;
  if(!uni_cmp_rev_na_str((uint8_t *)oname, olen, rev_na, len, NULL)) {
    ret = uni_rev_na_arr[ret].cp;
    int l; /* for hyphen check */
    <<Return code point [[ret]] unless a hyphen overrides it>>
  }
}
@

Testing this is mostly identical to testing the binary search
routines, but timings are also taken to verify that the hash is
actually faster.

\lstset{language=make}
<<Additional Tests>>=
$(if $(CMPH),./tstcp_na | ./tstna_cp_cmph >/dev/null)
@

<<C Test Support Executables>>=
$(if $(CMPH),tstna_cp_cmph) \
@

<<makefile.rules>>=
tstna_cp_cmph: LDFLAGS += $(if $(CMPH_LDFLAGS),$(CMPH_LDFLAGS),-lcmph)
@

\lstset{language=C}
<<tstna_cp_cmph.c>>=
#ifdef USE_CMPH
<<Common C Header>>

#include "uni_prop.h"

<<POSIX timing support>>

char buf[256];

int main(void)
{
  double tb = 0, th = 0, tbr = 0, thr = 0, tr = 0;
  unsigned long nn = 0, nr = 0;
  while(fgets(buf, 256, stdin)) {
    <<Set [[e]] to start of name to check>>
    for(s = e; *s; s++)
      if(isupper(*s))
        *s = tolower(*s);
    int32_t ret = uni_cmph_name_to_cp(e, (int)(s - e));
    <<Print and check [[name_to_cp]] results using [[na_seq_id]]>>
    /* time lookup failures (synthetic names) and lookup successes separately */
    int isr = ret < 0 || (ret <= UNI_MAX_CP && !uni_na_of(ret)->len);
    if(isr)
      nr++;
    else
      nn++;
    int j;
    tstart();
    for(j = 0; j < 25; j++)
      ret = uni_name_to_cp(e, (int)(s - e));
    if(isr)
      tbr += tend();
    else
      tb += tend();
    tstart();
    for(j = 0; j < 25; j++)
      ret = uni_cmph_name_to_cp(e, (int)(s - e));
    if(isr)
      thr += tend();
    else
      th += tend();
    if(isr) {
      /* also time pure synthetic name speed w/o lookup failure overhead */
      tstart();
      for(j = 0; j < 25; j++)
        ret = uni_name_rng_to_cp(e, (int)(s - e));
      tr += tend();
    }
  }
  /* report as ops per second rather than total seconds */
  tb = nn * 25000000.0 / tb;
  th = nn * 25000000.0 / th;
  fprintf(stderr, "norm %lu b%.0f/s h%.0f/s %.2fx\n", nn, tb, th, th/tb);
  tr = nr * 25000000.0 / tr;
  tbr = nr * 25000000.0 / tbr;
  thr = nr * 25000000.0 / thr;
  fprintf(stderr, "syn %lu r%.0f/s b%.0f/s h%.0f/s %.2fx\n", nr, tr, tbr, thr, thr/tbr);
  return 0;
}
#endif
@

There may be better tuning options, but my experiments show that the
hash function is not, in fact, sufficiently faster to justify its use.
For nearly all inputs, the hash function is within a few percent of
the performance of the binary search, and in fact it is sometimes
slower.  The fastest result I found was bdz\_ph with the lowest
possible [[-b]] setting, which is about 16\%-20\% faster.

\lstset{language=txt}
<<FIXME>>=
Document lack of NamesList.txt (informative, harder to parse, mostly useless)
@

\subsection{Parsing the UCD -- DUCET}

The most complex optional string value is derived from the Default
Unicode Collation Element Table
(\url{http://unicode.org/Public/UCA/}).  It associates many strings
(collation elements, which are the index to the table) with explicit
sort keys (the data).  This includes several multi-character collation
elements (called contractions) and multi-entry keys (called
expansions).  Each sort key entry consists of a number for each
supported sort level, plus a flag.  Currently, the DUCET supports four
sort levels, although the standard states that more may be added in a
future version (although recent versions have actually reduced the
number of levels to three for most keys).  The flag indicates
so-called variable entries.  The flag does not actually need to be
stored, since it is always true for entries with a non-zero first
level up to a certain top.  This top is stored separately as a single
integer.

Rather than make this reader as generic as possible, given the
definitions, it is tailored to the actual data present in the UCA
versions available at the time of this writing.  That means that
recoding may be necessary at a future date.  The first constraint is
that exactly four levels are supported.  Even though versions 6.3 and
later actually only have three levels, the fourth is still filled in
with the number zero.

While reading, each entry is indexed on the first character of the
collation element.  The raw string value is a four-word sequence for
every sort key entry, followed by the remainder of the collation
element index.  Since no multi-character index has more than four
characters, there is no problem distinguishing the character groups in
the value: the number of key entries is the length divided by four,
and the number of extra characters in the index is the remainder.
Since several multi-character collation elements start with the same
character (and in fact the well-formedness rules require this),
multiple entries with the same [[cp]] are created, but the raw string
table storage method does not care.

The CLDR has custom modifications to the DUCET, but in the same file
format (\texttt{allkeys\_CLDR.txt}).  In versions of the UCA data
prior to Unicode 6.3, this file was included with the standard DUCET in
\texttt{CollationAuxiliary.zip}.  Now, this file is in
\texttt{common/uca} with the CLDR data instead.  It should be unzipped
under the same directory if included in the UCA, or in the CLDR
diretories otherwise.  All CLDR data is based off of this root locale,
and it can be explicitly requested using the locale extension
\texttt{u-co-standard}.  The CLDR still requires the normal DUCET; it
can be selected manually using the locale extension
\texttt{u-co-ducet}.  The CLDR also provides this data in other
formats, but as long as the DUCET-format file exists, it's the one
that's parsed.

\lstset{language=C}
<<Initialize UCD files>>=
decl_str(DUCET);
decl_str(DUCET_CLDR);
@

<<Parse UCD files>>=
static uint32_t ducet_vartop, cldr_vartop;
ducet_vartop = parse_ducet("allkeys.txt", (prop_DUCET = add_prop("DUCET")));
if(!ducet_vartop) {
  perror("allkeys.txt");
  exit(1);
}
prop_DUCET_CLDR = add_prop("DUCET_CLDR");
cldr_vartop = parse_ducet("CollationAuxiliary/allkeys_CLDR.txt",
                          prop_DUCET_CLDR);
if(!cldr_vartop) {
  force_chdir(cwd);
  force_chdir(CLDR_LOC);
  cldr_vartop = parse_ducet("common/uca/allkeys_CLDR.txt", prop_DUCET_CLDR);
  force_chdir(cwd);
  force_chdir(UCD_LOC);
  if(!cldr_vartop) {
    perror("allkeys_CLDR.txt");
    exit(1);
  }
}
parsed_props[prop_DUCET].strs_char_size = 32;
parsed_props[prop_DUCET_CLDR].strs_char_size = 32;
@

<<UCD parser local functions>>=
<<DUCET parser globals>>
static uint32_t parse_ducet(const char *fname, int propn)
{
  <<Parser common variables>>
  prop_t *prop = &parsed_props[propn];
  uint32_t vartop = 0;
  if(!(f = fopen(fname, "r")))
    return 0;
  while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    uint32_t str[20*4]; /* current max: 18 * 4 */
    uint32_t len = 0;
    int is_var;

    split_line(lbuf);
    if(!num_fields || !isxdigit(*lbuf))
      continue;
    low = high = strtol(lbuf, &s, 16);
    for(s = fields[1]; s; s = strchr(s, '[')) {
      is_var = s[1] == '*';
      str[len++] = strtol(s + 2, &s, 16);
      if(is_var && str[len - 1] > vartop)
        vartop = str[len - 1];
      str[len++] = strtol(s + 1, &s, 16);
      str[len++] = strtol(s + 1, &s, 16);
      str[len++] = strtol(s + 1, &s, 16);
    }
    /* this assumes that fields[0] will never have more than 4 chars */
    /* that way, len % 4 == extra chars in fields[0]; len / 4 = # of keys */
    if((s = strchr(fields[0], ' '))) {
      do {
        str[len++] = strtol(s, &s, 16);
	if(!(len % 4)) {
	  fputs("DUCET contraction too long\n", stderr);
	  exit(1);
        }
      } while(*s);
    }
    /* 6.2: 120007 words (6212 saved) */
    /* 6.2-CLDR: 120493 words (5724 saved) */
    /* 7.0: 117482 words (17796 saved) */
    /* 7.0-CLDR: 117534 words (17688 saved) */
    add_str_rng(prop, low, high, str, len);
  }
  fclose(f);
  if(lbuf)
    free(lbuf);
  <<Post-process DUCET>>
  return vartop;
}
@

The raw data in the Unicode 6.2 DUCET comes to 126,219 words, of which
6,212 can be removed due to redundancy.  This is too much data for the
16-bit offset used by [[uni_str_arr_t]] type, so some adjustment needs
to be made.

A little space can be saved while storing this by making some strings
identical when they would normally not be, thereby increasing the
removals due to redundancy.  Level 4 is usually equal to the code
point, and is never 1, so that case can be stored as 1 to indicate
that it is equal to the code point.  This saves some space in 6.2, but
in versions 6.3 and later the level 4 field is dropped almost
entirely, making this a less useful optimization.  In fact, the data
becomes slightly larger as a result (I'm not sure why, and don't care
enough to investigate).  However, this transformation becomes more
useful later, so I'll leave it in.

<<Post-process DUCET>>=
<<Prepare for DUCET post-processing>>
for(i = 0; i < prop->len; i++) {
  uint32_t len = prop->str_arr[i].len;
  uint32_t *str = prop->strs + prop->str_arr[i].off;
  uint32_t cp = prop->str_arr[i].cp;
  <<Reduce DUCET entry>>
}
@

<<Reduce DUCET entry>>=
for(j = 3; j < len; j += 4)
  if(str[j] == cp)
    str[j] = 1;
@

One common expansion is for the sort key to be the concatenation of
the sort keys for each character in the canonical decomposition of the
collation element.  Since the official algorithm requires canonical
decomposition, these entries are removed, at least for
single-character collation elements.

The removal of this data requires decomposition lookup during DUCET
lookup.  However, the UCA algorithm specifies NFD form for its input
strings; in that case, the decomposition will already be done, and no
extra work is needed in the lookup function.  However, a special case
(FCD; see below) allows for unnormalized data inputs, where this
lookup would be necessary.  That is probably why these redundant
entries were present in the first place, but even they were not
complete:  they did not include synthetic compositions (i.e., the
Hangul syllables).  In any case, limiting it to single-character
collation elements means that at least the sorting part of NFD can be
skippped.

Note that the standard specifies a much more liberal decomposition
policy for generating expansions.  It uses compatibility
decomposition instead of canonical decomposition, and replaces level 3
of the resulting keys with a translation of the dt property value. 
The amount of additional work at run-time required to support this
isn't that great, and it might reduce the table a lot, but it affects
many characters even after NFD decomposition, so for now, entries
which use this technique are not suppressed.

% tertiary weight table (note: can is impossible):
%  none     def 02
%  wide     def 03
%  compat   def 04
%  font     def 05
%  circle   def 06
%  can/none uc  08
%  wide     uc  09
%  compat   uc  0A
%  font     uc  0B
%  circle   uc  0C
%  small    sh  0D
%  none     nh  0E
%  small    sk  0F
%  narrow   snk 10
%  none     nk  11
%  narrow   nkh 12
%  circle   ck  13
%  super    def 14
%  sub      def 15
%  vertical def 16
%  initial  def 17
%  medial   def 18
%  final    def 19
%  isolated def 1A
%  noBreak  def 1B
%  square   def 1C
%  square/super/sub uc 1D
%  fraction def 1E
%  MAX      --  1F
%
% uc == Uppercase, sh == small hira, nh == non-small hira,
% sk == small kata, nk == non-small kata, snk == small narrow kata,
% nkh == narrow katakana/hangul, ck == circled kata
% -- == def for last char in decomp; also used for disambiguation

% l4 weight:
%   l3-ignorable and Cc, Cf, or "variation selector":  0
%   otherwise:  cp (but what about contractions & expansions?)
% however, l4=0 weight rule is violated on 12 entries:
%  0600 0601 0602 0603 0604 0605 06DD 2061 2062 2063 2064 110BD
%   all of these are Cc or Cf, but have l4 == cp
% 6.3 simply drops l4 from allkeys.txt.

<<Prepare for DUCET post-processing>>=
prop_t *dmf_prop = &parsed_props[add_prop("dm_full")];
/* sort ducet for component lookup */
qsort(prop->str_arr, prop->len, sizeof(*prop->str_arr), uni_cmp_cp);
@

<<Reduce DUCET entry>>=
/* if it's pre-decomposed, then strip it */
if(str[3] != 1 && !(len % 4)) {
  raw_cp_str_t *dec = bsearch(&cp, dmf_prop->str_arr,
                              dmf_prop->len,
			      sizeof(raw_cp_str_t), uni_cmp_cp);
  if(dec) {
    while(dec->flags && dec > dmf_prop->str_arr && dec->cp == cp)
      dec--; /* select canonical full decomp only */
    if(dec->flags || dec->cp != cp)
      dec = NULL;
  }
  if(dec && dec->len * 4 <= len) { /* may be longer if synthetics present */
    int k;
    for(j = k = 0; j < dec->len && k < len; j++, k += 4) {
      uint32_t dc = dmf_prop->strs[dec->off + j];
      <<Skip if synthetic key for [[dc]] present>>
      raw_cp_str_t *cd;
      uint32_t ocp = str[3 + k];
      if(ocp && ocp != dc)
        break;
      cd = bsearch(&dc, prop->str_arr, prop->len, sizeof(raw_cp_str_t),
		   uni_cmp_cp);
      if(!cd)
        break;
      /* scan for single-element entry */
      /* some may have dc as 1st char of multi-char entry */
      while(cd->len != 4 && cd > prop->str_arr && cd[-1].cp == dc)
        cd--;
      while(cd->len != 4 && cd < prop->str_arr + prop->len - 1 && 
            cd[1].cp == dc)
	cd++;
      if(cd->len != 4 ||
         /* need to compare against dc in case not converted to 1 yet */
	 (prop->strs[cd->off + 3] && prop->strs[cd->off + 3] != 1 &&
	 prop->strs[cd->off + 3] != dc) ||
	 memcmp(prop->strs + cd->off, str + 4 * j, 12))
        break;
    }
    if(j == dec->len && k == len) {
      movebuf(prop->str_arr + i, prop->str_arr + i + 1, prop->len - i - 1);
      --i;
      --prop->len;
      continue;
    }
  }
}
@

While sort keys present in the table can simply be looked up, some of
the characters may require synthetic keys.  If the lookup fails, a
key is synthesized.  Note that synthesis requires the UIdeo and blk
properties as specified, although this could be modified to use
explicit range checks instead.  The UIdeo property is complicated
enough and yet small enough (12 ranges/300 bytes) that conversion
would not save much, but the blk checks are just single ranges each
(and the blk table is much larger, at 220 ranges/8032 bytes).  This
assumes that the ranges will never change in the future, which is
actually a pretty safe bet.

The UCA standard used to give no hints on how to set the secondary
value; presumably any non-zero number will work, because the primaries
will always differ.  The most common value (0020) is chosen for this.
The tertiary value has a table, but since no decomposition is
involved, and all special cases are already covered in the DUCET, all
characters get the default (0002) assigned to them.   Note that in
recent standards, 0020 and 0002 are the recomended values.

For the fourth level, the UCA standard states that any non-zero value
should be used on the first key element, and zero on the second.
However, actual 6.2 DUCET values which duplicate synthesized values
(due to decomposition) show a non-zero level four on the second key
element as well, and in fact both use the code point.  For
consistency, that is what is generated.  In 6.3 and later, zero is
generated for both level 4 values in the tables.

<<Skip if synthetic key for [[dc]] present>>=
if(k < len - 4 && str[k] == uca_synth1(dc) && str[k + 1] == 0x20 &&
  str[k + 2] == 2 && (!str[k + 3] || str[k + 3] == 1 || str[k + 3] == dc) &&
  str[k + 4] == uca_synth2(dc) && !str[k + 5] &&
  !str[k + 6] && (!str[k + 7] || str[k + 7] == 1 || str[k + 7] == dc)) {
 k += 4;
 continue;
}
@

<<Synthetic UCA key support>>=
static uint32_t uca_synth1(uint32_t c)
{
  uint32_t ret = c >> 15;
  if(uni_is_UIdeo(c)) {
#if 0
    uni_blk_t blk = uni_blk_of(c);
    if(blk == UNI_blk_CJK || blk == UNI_blk_CJK_Compat_Ideographs)
#else
    if((c >= 0x4E00 && c <= 0x9FFF) || (c >= 0xF900 && c <= 0xFAFF))
#endif
      ret += 0xFB40;
    else
      ret += 0xFB80;
  } else
    ret += 0xFBC0;
  return ret;
}
#define uca_synth2(c)   (((c) & 0x7fff) | 0x8000)
@

<<DUCET parser globals>>=
static int uni_is_UIdeo(uint32_t c)
{
  static int UIdeo = -1;
  if(UIdeo < 0)
    UIdeo = add_prop("UIdeo");
  uni_chrrng_t rng = {c, c};
  return bsearch(&rng, parsed_props[UIdeo].rng, parsed_props[UIdeo].len,
                 sizeof(rng), uni_cmprng) != NULL;
}
<<Synthetic UCA key support>>
@

Similarly, some contractions are the canonical decomposition of a
single character, and the sort key is the sort key for that character.
That case could be encoded by a zero-length key, but retaining the
extra collation element characters as its string. However, not much is
saved (about 415 words in 6.2, although surprisingly many more for the
CLDR table), so this extra lookup complication is not imposed.  Unlike
the decomposition entries above, the composition entries are
legitimate collation elements, even after NFD transformation.  In
fact, if anything, the entry for the character they represent should be
removed, but that would save even less space.

One other redundancy I've noticed in some of the tables is the
presence of raw synthetic keys.  There is no need to store something
which can be generated by code at no real extra expense.

<<Reduce DUCET entry>>=
if(len == 8 && str[0] == uca_synth1(cp) && str[1] == 0x20 && str[2] == 2 &&
   (!str[3] || str[3] == 1) && str[4] == uca_synth2(cp) && !str[5] && !str[6] &&
   (!str[7] || str[7] == 1)) {
  movebuf(prop->str_arr + i, prop->str_arr + i + 1, prop->len - i - 1);
  --prop->len;
  --i;
  continue;
}
@

While there is a pattern to most of the rest of the DUCET, it is
difficult to encode, and encoding would make lookup much more
expensive than it probably already is.  The only other features of the
numbers which can be easily taken advantage of are the ranges.  The
first three levels are never more than four digits, or 16 bits.  In
fact, they are quite a bit smaller.  Rather than relying on a single
revision's ranges, the ranges are extracted in the first pass.  This
is mainly to provide a sanity check on the only characteristic that is
likely to change rapidly.

<<Prepare for DUCET post-processing>>=
uint32_t max_0 = 0, max_1 = 0, max_2 = 0;
@

<<Reduce DUCET entry>>=
for(j = 0; j + 3 < len; j += 4) {
  if(str[j] > max_0)
    max_0 = str[j];
  if(str[j + 1] > max_1)
    max_1 = str[j + 1];
  if(str[j + 2] > max_2)
    max_2 = str[j + 2];
}
@

In practice, the first level can be stored in 16 bits, the second in 9
bits, and the third in 5 bits.  In other words, all three levels fit
easily in a single 32-bit word.  While the second and third could be
stored in more bits to allow for more wiggle room, storing minimally
leaves two extra bits for other purposes.  No allowance is made for
larger required field sizes; recoding would be needed in several
places.

<<Post-process DUCET>>=
if(lg2(max_0) > 16 || lg2(max_1) > 9 || lg2(max_2) > 5) {
  fputs("FIXME: Can't reduce DUCET\n", stderr);
  exit(1);
}
@

The extra two bits can be used to encode certain common cases,
possibly eliminating the need to store a full 32-bit word for the
fourth level:

\begin{itemize}
\item 0: Level 4 is 0
\item 1: Level 4 is the index cp (encoded above as 1)
\item 2: The next word is level 4
\end{itemize}

When combining like this, each entry drops in length to 2 or fewer
words.  This means that if there are more than two characters in the
index string, it can no longer be detected by length alone.  Since
every character can be encoded in fewer than 30 bits, the extra index
characters are simply shifted left two, leaving the lower two bits for
special encoding as above:

\begin{itemize}
\item 3: This word is actually part of the collation element (index)
\end{itemize}

This also means that locating the extra index characters is more
difficult:  the entire string would need to be scanned rather than
just computing the length of the key and skipping it.  To prevent the
need for scanning, the extra index characters are moved to the front
of the string as well.

For consistency, the explicitly stored level 4 values are also shifted
left by two.  This way, the only characters in the string with both of
the lower two bits set are the index characters.

After collapsing the keys like this, over two thirds of the required
space is removed.

<<Post-process DUCET>>=
for(i = 0; i < prop->len; i++) {
  int k;
  uint32_t len = prop->str_arr[i].len;
  uint32_t extra[3], extra_len = len % 4;
  uint32_t *str = prop->strs + prop->str_arr[i].off;
  for(j = 0; j < extra_len; j++)
    extra[j] = str[len - extra_len + j];
  for(j = 0, k = extra_len; j + 3 < len; j += 4, k++) {
    str[k] = (str[j] << 16) + (str[j + 1] << 7) + (str[j + 2] << 2);
    if(str[j + 3] == 1)
      str[k] |= prop->str_arr[i].cp != 0; /* special case: 0 is 0 */
    else if(str[j + 3]) {
      str[k] |= 2;
      str[++k] = str[j + 3] << 2;
    }
  }
  prop->str_arr[i].len = k;
  for(k = 0; j < len; j++, k++)
    str[k] = (extra[k] << 2) | 3;
}
@

The final problem to deal with is the possibility of multiple
contractions with the same starting character, which results in
several array entries with the same index.  Because of this, the
string table cannot be converted to a multi-level table.  After the
last encoding, though, it is easy to simply concatenate all entries
with the same starting character (index) into a single string.
Searching for entries after the first is not too efficient, but at
least it only needs to be done for relatively few first index
characters.  The encoding allows limited binary searching, as well:
the nearest index extension can be located by checking the lower two
bits for 3.

Before merging, though, a copy is made of the raw data for further
processing without having to extract individual elements from the
merged data.  Only the entry pointers are copied:  the strings
cannot be merged in-place anyway (since they might overlap), so
merging always appends new strings and leaves the old ones alone.

<<Post-process DUCET>>=
raw_cp_str_t *raw_ents;
uint32_t raw_ents_size;
inisize(raw_ents, raw_ents_size = prop->len);
cpybuf(raw_ents, prop->str_arr, raw_ents_size);
@

<<Post-process DUCET>>=
ducet_strs = prop->strs; /* for sort & abbrev */
for(i = prop->len - 1; i > 0; i--) {
  uint32_t cp = prop->str_arr[i].cp;
  for(j = i; j > 0 && prop->str_arr[j - 1].cp == cp; j--);
  if(i == j)
    continue;
  /* sort by index extension */
  low = j;
  qsort(prop->str_arr + low, i - low + 1, sizeof(*prop->str_arr), cmp_ducet_idx);
  if(prop->str_arr[low].len && (ducet_strs[prop->str_arr[low].off] & 3) == 3) {
    fprintf(stderr, "Error: no DUCET entry for %04X\n", (int)cp);
    exit(1);
  }
  uint32_t len = 0;
  for(j = low; j <= i; j++)
    len += prop->str_arr[j].len;
  check_size(prop->strs, prop->max_strs, prop->strs_len + len);
  ducet_strs = prop->strs; /* for sort & abbrev */
  cpybuf(ducet_strs + prop->strs_len,
         ducet_strs + prop->str_arr[low].off,
	 prop->str_arr[low].len);
  prop->str_arr[low].off = prop->strs_len;
  prop->strs_len += prop->str_arr[low].len;
  prop->str_arr[low].len = len;
  for(j = low + 1; j <= i; j++) {
    uint32_t *str = ducet_strs + prop->str_arr[j].off;
    len = prop->str_arr[j].len;
    cpybuf(ducet_strs + prop->strs_len, str, len);
    prop->strs_len += len;
  }
  movebuf(prop->str_arr + low + 1, prop->str_arr + i + 1,
          prop->len - i - 1);
  prop->len -= i - low;
  if(!(i = low))
    break;
}
/* 6.2: 27009 words (2850 saved) */
/* 6.2-CLDR: 25858 words (2756 saved) */
/* 7.0: 26739 words (3408 saved) */
/* 7.0-CLDR: 26650 words (3421 saved) */
@

<<DUCET parser globals>>=
static uint32_t *ducet_strs;
static int cmp_ducet_idx(const void *a, const void *b)
{
  const raw_cp_str_t *_a = a, *_b = b;
  uint32_t *stra = ducet_strs + _a->off, *strb = ducet_strs + _b->off;
  uint32_t lena = _a->len, lenb = _b->len;

  /* assume all indices are unique, so equality will never be returned */
  while(lena > 0 && lenb > 0) {
    if((*stra & 3) != 3)
      return -1;
    if((*strb & 3) != 3)
      return 1;
    if(*stra != *strb)
      return *stra > *strb ? 1 : -1;
    lena--;
    lenb--;
    stra++;
    strb++;
  }
  return lena ? 1 : -1;
}
@

Unsurprisingly, combining entries reduces redundancy a bit (gaining 2
words).  Thus the original 126,219 words are reduced to 27,009 words.
This allows the use of 16-bit offsets, as supported by the
[[uni_str_arr_t]] type.  Note that this is just the string table; the
lookup tables add additional overhead (about 200kb for the code point
table and 100kb for the multi-level table).  Finally, the raw DUCET is
ready to be dumped.  The collected default variable top is dumped to
the header as well.

<<Dump character information as C code>>=
fprintf(gen_h, "/** Default variable top property for the DUCET */\n"
               "#define uni_DUCET_var_top %d\n", (int)ducet_vartop);
dump_str_tabs(&parsed_props[prop_DUCET],
              "Default Unicode Collation Element Table", gen_h, tstf);
fprintf(gen_h, "/** Default variable top property for the CLDR DUCET */\n"
               "#define uni_DUCET_CLDR_var_top %d\n", (int)cldr_vartop);
dump_str_tabs(&parsed_props[prop_DUCET_CLDR],
              "Default Unicode Collation Elment Table "
	      "(Common Locale Data Repository version)", gen_h, tstf);
@

<<Post-process DUCET>>=
enable_str_mt(prop);
@

In order to compensate for some of the compression above, a special
lookup function is provided.  It is intended for piece-wise lookup of
a full string's key sequence.  This is \emph{not} a key string as
defined by the UCA.  It is ordered by level first, and then key
element sequence, and includes zeroes at some levels.  The only thing
needed to convert this to a UCA key is to reorder the results by key
element sequence first, and then by level, and to remove zeroes.  The
format of the returned values is similar to the raw format stored in
they table.  Also, since the table is so large, the related functions
are stored in their own object file.

<<Library [[uni]] Members>>=
ducet_lookup.o
@

<<ducet_lookup.c>>=
<<Common C Header>>
#include "uni_prop.h"
// static_proto

<<DUCET lookup functions>>
@

<<DUCET lookup format defs>>=
#define UNI_DUCET_LEV1_MASK  0xffff0000  /**< Level 1 component */
#define UNI_DUCET_LEV1_SHIFT 16          /**< Location of level 1 component */
#define UNI_DUCET_LEV2_MASK  0x0000ff80  /**< Level 2 component */
#define UNI_DUCET_LEV2_SHIFT 7           /**< Location of level 2 component */
#define UNI_DUCET_LEV3_MASK  0x0000007c  /**< Level 3 component */
#define UNI_DUCET_LEV3_SHIFT 2           /**< Location of level 3 component */
@

<<Unicode property exports>>=
/** \addtogroup uni_prop_uca_lookup DUCET lookup return values.
  * The low two bits of the level123 value are always zero.  */
/** @{ */
<<[[uni_uca_lookup]] parameters>>
/** @} */
@

<<[[uni_uca_lookup]] parameters>>=
<<DUCET lookup format defs>>
@

In order to make it piece-wise, only one character is passed in at a
time, along with a running state.  Either a key element is returned,
or not.  Either the next character must be passed in next, or nothing.
It makes no sense for nothing to be returned, and no new character to
be required from the caller, so only three cases are consisdered:
returning a value, and requiring a new input next time ([[OK]]),
returning a value, and requiring no new input ([[AGAIN]]), and
returning no value, and requiring new input next time ([[NONE]]).  At
the end of input (flagged with a special character, [[END]]), the
final returned value is indicated by [[OK]] or [[NONE]].

<<[[uni_uca_lookup]] parameters>>=
/**  \ref uni_uca_lookup return value.
  *  \p lev123 and \p lev4 are valid; pass in the next code point (in \p c)
  *  next time. That is, unless input \p c was \ref UNI_UCA_LOOKUP_END,
  *  in which case \p lev123 and \p lev4 are the last ones to be returned. */
#define UNI_UCA_LOOKUP_OK    0
/**  \ref uni_uca_lookup return value.
  * \p lev123 and lev4 are valid; call again for more values without
  * passing in the next code point.  In other words, the next time,
  * the \p c parameter will be ignored. */
#define UNI_UCA_LOOKUP_AGAIN 1
/**  \ref uni_uca_lookup return value.
  * \p lev123 and lev4 were not updated; call again with the next code point.
  * That is, unless input was \ref UNI_UCA_LOOKUP_END, in which case
  * there will never be any more return values.  */
#define UNI_UCA_LOOKUP_NONE -1

/** \ref uni_uca_lookup input indicating end-of-input */
#define UNI_UCA_LOOKUP_END 0xffffffff

/** Opaque state tracker used by \ref uni_uca_lookup */
typedef struct uni_uca_lookup_state_t uni_uca_lookup_state_t;
@

<<Known Data Types>>=
uni_uca_lookup_state_t,%
@

<<Unicode property exports>>=
/** Look up DUCET values.
  * Pass in first character for \p c; see \ref uni_prop_uca_lookup for
  * details on return values.  Point \p state to a local pointer initialized
  * to NULL.  It will be automatically allocated when needed, and freed
  * at the same time a return indicates no more values will be returned */
int uni_uca_lookup(uint32_t c, uint32_t *lev123, uint32_t *lev4,
                   uni_uca_lookup_state_t **state);
@

<<DUCET lookup functions>>=
<<Private UCA lookup definitions>>
struct uni_uca_lookup_state_t {
  <<UCA lookup state members>>
};
<<Private UCA lookup globals>>
int uni_uca_lookup(uint32_t c, uint32_t *lev123, uint32_t *lev4,
                   uni_uca_lookup_state_t **state)
{
  if(!*state && c == UNI_UCA_LOOKUP_END)
    return UNI_UCA_LOOKUP_NONE;
  <<Initialize single DUCET element lookup>>
  <<Look up and return a single key element from DUCET>>
}
@

Rather than just look up raw values from the DUCET, some other
optional UCA-related processing may be done.  To select the processing
options, a separate structure and function are provided to set the
options.   The main reason for providing a separate function is to
ensure that options do not change mid-lookup.

<<[[uni_uca_opts_t]]>>=
/** Options for routines related to the Unicode Collation Algorithm */
typedef struct {
  <<Unicode UCA function options>>
} uni_uca_opts_t;
@

<<Known Data Types>>=
uni_uca_opts_t,%
@

<<Unicode property exports>>=
<<[[uni_uca_opts_t]]>>
/** Set options for \ref uni_uca_lookup.
  * Call this with a NULL \p *state to set options in \p *state from
  * \p opts.  Must be called at most once before the first call to
  * \ref uni_uca_lookup.  */
/* set options for UCA lookups; call before lookup() */
void uni_uca_lookup_opts(uni_uca_lookup_state_t **state,
                         const uni_uca_opts_t *opts);
@

<<DUCET lookup functions>>=
void uni_uca_lookup_opts(uni_uca_lookup_state_t **state,
                           const uni_uca_opts_t *opts)
{
  if(*state)
    return; /* error */
  inisize(*state, 1);
  clearbuf(*state, 1);
  uni_uca_lookup_state_t *st = *state;
  if(opts) {
    <<Copy UCA lookup options from [[opts]] to [[st]]>>
  }
  <<Set default UCA lookup options>>
}
@

<<Initialize single DUCET element lookup>>=
/* if(!state) exit(1); */
uni_uca_lookup_state_t *st = *state;
if(!st) {
  uni_uca_lookup_opts(state, NULL);
  st = *state;
}
@

First of all, there are two DUCET tables that could be used for
lookup.  One way to implement locales would be to load tables from
files, or to construct tables on the fly, so other tables may be used
as well.  The default table to use is the plain DUCET, but other
tables may be specified.

<<Unicode UCA function options>>=
const uint32_t *tab, /**< DUCET multi-level table (default: \ref uni_DUCET_mtab) */
               *strs; /**< DUCET strings (default: \ref uni_DUCET_strs) */
@

<<UCA lookup state members>>=
const uint32_t *tab, *strs;
@

<<Copy UCA lookup options from [[opts]] to [[st]]>>=
st->tab = opts->tab;
st->strs = opts->strs;
@

<<Set default UCA lookup options>>=
if(!st->tab) {
  st->tab = uni_DUCET_mtab;
  st->strs = uni_DUCET_strs;
}
@

One other easily implemented option is to select the comparison
strength, which is a level number above which other levels are zeroed
out.  This may reduce the number of returned elements as well, by
increasing the number of all-zero elements.  The first three levels
are most easily zeored using a mask, so that is stored in the state as
well.

<<Unicode UCA function options>>=
uint8_t max_level; /**< Comparison strength (default: 3) */
@

<<UCA lookup state members>>=
uint32_t mask123;
uint8_t max_level;
@

<<Copy UCA lookup options from [[opts]] to [[st]]>>=
st->max_level = opts->max_level;
@

<<Set default UCA lookup options>>=
if(!st->max_level)
  st->max_level = 3;
if(st->max_level == 1)
  st->mask123 = UNI_DUCET_LEV1_MASK;
else if(st->max_level == 2)
  st->mask123 = UNI_DUCET_LEV1_MASK | UNI_DUCET_LEV2_MASK;
else
  st->mask123 = ~3;
@

In order to avoid constantly referring to a pointed-to value, the
return value is built in [[l123]] and [[l4]] rather than [[*lev123]]
and [[*lev4]].

<<Initialize single DUCET element lookup>>=
uint32_t l123, l4;
@

<<Mangle DUCET return value>>=
l123 &= st->mask123;
if(st->max_level < 4)
  l4 = 0;
@

Another option which immediately modifies results is the variable key
element processing.  Doing this requires the table's variable top and
a processing mode setting.  While the UCA apparently uses shifted mode
by default, this routine sets the default to minimal processing, which
is done with the non-ignorable mode.  Coincidentallly, non-ignorable
is also the default mode for CLDR.

<<Unicode UCA function options>>=
uint8_t var_mode; /**< Mode for procssing variable key elements (default: */
                  /**< NON_IGNORABLE -- the UCA default of SHIFTED must be */
		  /**< selected manually) */
uint32_t var_top; /**< Last variable key element in DUCET (default: \ref uni_DUCET_var_top) */
@

<<UCA lookup state members>>=
uint8_t var_mode;
uint32_t var_top;
int in_var;
@

<<Copy UCA lookup options from [[opts]] to [[st]]>>=
st->var_mode = opts->var_mode;
/* to allow direct <= comparison w/ lev123 */
if(opts->var_top)
  st->var_top = (opts->var_top << 16) | 0xffff;
@

<<Set default UCA lookup options>>=
if(!st->var_top)
  /* to allow direct <= comparison w/ lev123 */
  st->var_top = (uni_DUCET_var_top << 16) | 0xffff;
@

<<Unicode property exports>>=
<<UCA variable modes>>
@

<<UCA variable modes>>=
#define UNI_UCA_VAR_MODE_NON_IGNORABLE 0 /**< Variable key processing mode */
#define UNI_UCA_VAR_MODE_BLANKED       1 /**< Variable key processing mode */
#define UNI_UCA_VAR_MODE_SHIFTED       2 /**< Variable key processing mode */
#define UNI_UCA_VAR_MODE_IGNORE_SP     3 /**< Variable key processing mode */
#define UNI_UCA_VAR_MODE_SHIFT_TRIMMED 4 /**< Variable key processing mode */
@

<<Initialize single DUCET element lookup>>=
uint8_t var_mode = st->var_mode;
int shifted = var_mode >= UNI_UCA_VAR_MODE_SHIFTED,
    blanked = var_mode == UNI_UCA_VAR_MODE_BLANKED;
uint32_t var_top = st->var_top;
@

<<Mangle DUCET return value>>=
/* UCA variable mode modifies variable entry & anything following */
/* plus some other random ignorables */
if(shifted) {
  if(!l123)
    l4 = 0;
  else if(!(l123 & UNI_DUCET_LEV1_MASK)) {
    if(st->in_var)
      l4 = l123 = 0;
    else
      l4 = 0xffff;
  } else {
    st->in_var = 0;
    l4 = 0xffff;
  }
} else if(blanked && st->in_var) {
  if(!(l123 & UNI_DUCET_LEV1_MASK))
    l123 = l4 = 0;
  else
    st->in_var = 0;
}
/* due to modification of var_top above: */
/*    same as lev1 <= var_top && lev1 > 0 */
if(var_mode > 0 && l123 <= var_top && l123 > 0xffff) {
  st->in_var = 1;
  switch(var_mode) {
    /* UNI_UCA_VAR_MODE_NON_IGNORABLE does nothing */
    case UNI_UCA_VAR_MODE_BLANKED:
      l123 = l4 = 0;
      break;
    case UNI_UCA_VAR_MODE_SHIFTED:
      l4 = l123 >> UNI_DUCET_LEV1_SHIFT;
      /* !!! Undocumented CLDR behavior (required to pass test): !!! */
      /* UCA says lev1 > 0 and lev1 <= var_top is variable */
      /*  but CLDR defines one entry with lev1 == 1, but it is not var */
      /*  yet in shifted mode, it sets lev4 == lev1 as above */
      /*  GRRRR.... */
      /*  at least UCA data files never use lev1 < 0x0200, so */
      /*  just testing for 1 is safe */
      /* who knows what IgnoreSP and Trimmed are supposed to work like */
      /* IgnoreSP ws removed in 6.3.0, anyway */
      if(l4 > 1)
        l123 = 0;
      else
        st->in_var = 0;
      break;
    case UNI_UCA_VAR_MODE_IGNORE_SP: /* obsolete; removed in 6.3.0 */
      if((uni_gc_trans[UNI_gc_P] & (1 << uni_gc_of(c))) ||
         uni_is_WSpace(c)) {
        l4 = l123 >> UNI_DUCET_LEV1_SHIFT;
	/* undocumented behavior copied from SHIFTED */
	if(l4 > 1)
	  l123 = 0;
	else
	  st->in_var = 0;
      } else
        st->in_var = 0;
      break;
    case UNI_UCA_VAR_MODE_SHIFT_TRIMMED:
      l4 = l123 >> UNI_DUCET_LEV1_SHIFT;
      /* undocumented behavior copied from SHIFTED */
      if(l4 > 1)
        l123 = 0;
      else
        st->in_var = 0;
      break;
  }
}
@

Stripping trailing level 4 [[0xFFFF]] values, as required for
shift-trimmed mode, requires delaying any [[0xFFFF]] output until
either a non-[[0xFFFF]] output is found, or the final value is being
returned.  That could be best accomplished by the caller, by simply
accumulating the entire result and trimming the trailing [[0xFFFF]]
values.  However, it may be difficult to document that this mode, and
only this mode, is only partially implemented.  So, levels 1--3 of any
output whose level 4 is [[0xFFFF]] are saved in an expandable buffer.
Once a non-[[0xFFFF]] level 4 is encountered, all accumulated entries
need to be output before the key elements with a non-[[0xFFFF]] level
4, followed by that key element.  Once the end is encountered, all
accumulated entries need to be output, but with a level 4 of zero.
For the former case, levels 1--3 of the non-[[0xFFFF]] element can be
saved in place of the first saved element to be output, but level 4
needs to be saved separately.

Note that the caller can still prevent this accumulation by using
shifted mode, and trimming manually.

<<UCA lookup state members>>=
uint32_t *l4_ffff;
unsigned int l4_ffff_len, l4_ffff_max, l4_ffff_ptr;
uint32_t non_ffff_l4;
@

<<Free UCA lookup state members>>=
if(st->l4_ffff)
  free(st->l4_ffff);
@

<<Mangle DUCET return value>>=
if(var_mode == UNI_UCA_VAR_MODE_SHIFT_TRIMMED) {
  if(l4 == 0xffff) {
    /* FIXME: return error instead of exiting */
    check_size(st->l4_ffff, st->l4_ffff_max, st->l4_ffff_len + 1);
    st->l4_ffff[st->l4_ffff_len++] = l123;
    <<Skip [[l4_ffff]] member>>
  } else if(st->l4_ffff_len) {
    uint32_t l123s = l123;
    l123 = st->l4_ffff[0];
    st->l4_ffff[0] = l123s;
    st->non_ffff_l4 = l4;
    l4 = 0xffff;
    st->l4_ffff_ptr = 1;
    <<Return [[l4_ffff]] members>>
  }
}
@

<<Check for [[l4_ffff]] members>>=
if(st->l4_ffff_ptr) {
  <<Maybe check for [[l4_ffff]] members at end>>
  if(st->l4_ffff_ptr < st->l4_ffff_len) {
    l123 = st->l4_ffff[st->l4_ffff_ptr++];
    l4 = 0xffff;
    <<Return [[l4_ffff]] members>>
  } else {
    l123 = st->l4_ffff[0];
    l4 = st->non_ffff_l4;
    st->l4_ffff_len = st->l4_ffff_ptr = 0;
    <<Return post-[[l4_ffff]] members>>
  }
}
@

<<Check for [[l4_ffff]] members at end>>=
while(st->l4_ffff_len) {
  l123 = st->l4_ffff[st->l4_ffff_ptr++];
  l4 = 0;
  if(st->l4_ffff_ptr == st->l4_ffff_len) {
    st->l4_ffff_ptr = st->l4_ffff_len = 0;
    <<Return last [[l4_ffff]] member>>
  } else if(l123) {
    <<Return [[l4_ffff]] members at end>>
  }
}
@

There is one other UCA parameter: reversed level 2.  That is, level 2
values are returned in reverse order.  To do this, the entire string
would have be be accumulated before generating the first output.  Since
some routines may be more efficient if results are returned one at a
time, this could be a problem.  Instead, it is the responsibility of
the caller to implement this option.

There are additional CLDR-specific parameters which could affect the
results of this function.  These may be implemented at a future date.
Adding shift mode processing does not affect complexity much, but
adding DUCET reordering, additional pseudo-levels, numeric
accumulation and parsing, and other CLDR options will make this
function very complex.

Some operations, such as looking up multi-character collation
elements, require at least one character of lookahead.  In fact, the
reordering requirement requires arbitrary lookahead.  To support this,
a buffer may be filled with lookahead characters.  Any time a NONE
return would happen, if there are buffer characters, the routine
should instead restart itself.  Any time an OK return would happen, an
AGAIN should be returned instead.  Any time an AGAIN return would
happen, the character consumed from the buffer is reinserted, so that
the next time the function is called, that character is consumed again
instead of the next character in the buffer.  If there are no buffered
characters, this is not necessary as no character will be consumed.

This is also the appropriate place to mask out the unneeded levels of
the return value, and to alter the return code as needed.  An OK
return is treated as a NONE return, and an AGAIN return loops without
returning a value.

\lstset{language=C}
<<UCA lookup state members>>=
uint32_t *bt;
int32_t btp, num_bt, max_bt;
@

<<Look up and return a single key element from DUCET>>=
if(c != UNI_UCA_LOOKUP_END && c > UNI_MAX_CP)
  /* FIXME: actually, this should replace any length of illegal chars */
  /* including surrogates as well, at least */
  c = 0xFFFD; /* recommended REPLACEMENT CHARACTER */
<<Check for [[l4_ffff]] members>>
restart:
  if(st->num_bt) {
    if(st->btp >= 0)
      c = st->bt[st->btp++];
    else
      st->btp++;
    if(st->btp == st->num_bt)
      st->btp = st->num_bt = 0;
  }
@

<<Free UCA lookup state members>>=
if(st->bt)
  free(st->bt);
@

<<Private UCA lookup globals>>=
static void add_bt(uint32_t c, uni_uca_lookup_state_t *st)
{
  /* FIXME: return error instead of exiting */
  check_size(st->bt, st->max_bt, st->num_bt + 1);
  st->bt[st->num_bt++] = c;
}

static void add_bt_buf(const uint32_t *buf, uint32_t len,
                       uni_uca_lookup_state_t *st)
{
  /* FIXME: return error instead of exiting */
  check_size(st->bt, st->max_bt, st->num_bt + len);
  cpybuf(st->bt + st->num_bt, buf, len);
  st->num_bt += len;
}
@

<<Skip [[l4_ffff]] member>>=
<<Return no DUCET values>>
@

<<Return post-[[l4_ffff]] members>>=
*lev123 = l123;
*lev4 = l4;
/* assume that non-default state needed AGAIN */
return st->state ? UNI_UCA_LOOKUP_AGAIN : UNI_UCA_LOOKUP_OK;
@

<<Return [[l4_ffff]] members>>=
*lev123 = l123;
*lev4 = l4;
return UNI_UCA_LOOKUP_AGAIN;
@

<<Return no DUCET values>>=
if(st->num_bt)
  goto restart;
else
  return UNI_UCA_LOOKUP_NONE;
@

<<Return one DUCET value>>=
<<Mangle DUCET return value>>
if(!l123 && !l4) {
  <<Return no DUCET values>>
}
*lev123 = l123;
*lev4 = l4;
return st->num_bt ? UNI_UCA_LOOKUP_AGAIN : UNI_UCA_LOOKUP_OK;
@

<<Return more than one DUCET value>>=
if(st->num_bt)
  st->btp--;
<<Mangle DUCET return value>>
if(!l123 && !l4)
  goto restart;
*lev123 = l123;
*lev4 = l4;
return UNI_UCA_LOOKUP_AGAIN;
@

A private enumeration type is used to represent the current lookup
progress.

<<Private UCA lookup definitions>>=
typedef enum {
  ST_uca_lookup_start
  <<UCA lookup states>>
} uca_lookup_state_t;
@

<<Known Data Types>>=
uca_lookup_state_t,%
@

<<UCA lookup state members>>=
uca_lookup_state_t state;
@

<<Look up and return a single key element from DUCET>>=
switch(st->state) {
  case ST_uca_lookup_start:
    <<Look up in DUCET given no state>>
  <<Look up in DUCET given state>>
}
@

The first thing to do is check for end-of-string.  At end-of string,
the state is freed, and nothing is returned, unless there are
accumulated [[l4_ffff]] members.

<<UCA lookup state members>>=
int eoi;
@

<<Look up in DUCET given no state>>=
if(st->eoi || c == UNI_UCA_LOOKUP_END) {
  <<Check for [[l4_ffff]] members at end>>
  <<Free UCA lookup state members>>
  free(st);
  *state = NULL;
  return UNI_UCA_LOOKUP_NONE; /* can't have any more bt */
}
@

<<Return last [[l4_ffff]] member>>=
if(l123 || l4) {
  *lev123 = l123;
  *lev4 = l4;
  <<Free UCA lookup state members>>
  free(st);
  *state = NULL;
  return UNI_UCA_LOOKUP_OK;
} else {
  <<Free UCA lookup state members>>
  free(st);
  *state = NULL;
  return UNI_UCA_LOOKUP_NONE;
}
@

<<Return [[l4_ffff]] members at end>>=
st->eoi = 1;
*lev123 = l123;
*lev4 = l4;
return UNI_UCA_LOOKUP_AGAIN;
@

<<Maybe check for [[l4_ffff]] members at end>>=
if(st->eoi) {
  <<Check for [[l4_ffff]] members at end>>
}
@

Otherwise, a lookup is performed in the table.  If it succeeds, the
results are returned appropriately.  Otherwise, it might be a
canonically decomposable character.  The automatic single-character
decomposition performed by the original DUCET was stripped above, so
it must be done here, by pushing the decomposition onto the
backtracking buffer and retrying.  If it is neither in the table nor a
canonical composition, a key is synthesized.

<<UCA lookup state members>>=
uni_str_ptr_t v;
uint32_t c;
@

<<Private UCA lookup definitions>>=
<<Synthetic UCA key support>>
@

<<Look up in DUCET given no state>>=
const uni_str_ptr_t *v;
uni_multi_tab_lookup(st->tab, c * 4, (const uint8_t **)&v, 0);
if(!v || !v->len) {
  <<Set [[v]] from canonical decomposition>>
}
if(v && v->len) {
  st->v = *v;
  <<Process DUCET initial lookup>>
} else {
  /* synthesized elements are 2-parters; prepare for 2nd part */
  st->c = c;
  st->state = ST_uca_lookup_synth;
  l123 = (uca_synth1(c) << 16) + (0x0020 << 7) + (0x0002 << 2);
  l4 = c; /* actual value does not matter, as long as it's non-0 */
  <<Return more than one DUCET value>>
}
@

<<UCA lookup states>>=
,ST_uca_lookup_synth
@

<<Look up in DUCET given state>>=
case ST_uca_lookup_synth:
  l123 = uca_synth2(st->c) << 16;
  l4 = st->c; /* should be 0, but sample data uses c, so I will as well */
  st->state = 0;
  <<Return one DUCET value>>
@

Two sources of single-character decomposition exist:  table entries
and Hangul syllable decomposition.  While it may be more efficient to
store the decomposition results as a pointer and length, since it's a
static buffer, it is instead pushed onto the regular backtracking
buffer in order to avoid having to duplicate processing code.  The
input to the function should be canonically decomposed anyway, so this
will not happen often.

<<UCA lookup state members>>=
const uint16_t *multi_ret16;
uint32_t multi_ret_len;
@

<<Set [[v]] from canonical decomposition>>=
int16_t coff;
uint8_t clen;
uni_find_canon_decomp(c, &coff, &clen);
if(coff >= 0 && clen > 0) {
  const uint16_t *str = uni_canon_decomp_strs + coff;
  unsigned int cl;
  c = uni_int_utf16_decode(str, &cl);
  uni_multi_tab_lookup(st->tab, c * 4, (const uint8_t **)&v, 0);
  while(clen -= cl) {
    str += cl;
    add_bt(uni_int_utf16_decode(str, &cl), st);
  }
} else if(coff < 0) {
  uint32_t hbuf[3];
  int hlen = uni_hangul_syllable_decomp(c, hbuf, 1);
  if(hlen > 0) {
    c = hbuf[0];
    uni_multi_tab_lookup(st->tab, c * 4, (const uint8_t **)&v, 0);
    add_bt(hbuf[1], st);
    if(hlen > 2)
      add_bt(hbuf[2], st);
  }
}
@

For a successful lookup, the response for a single-character index is
extracted.  If multi-character indices are available as well, more
characters are requested after saving the work done so far in the
state.  Otherwise, the single-character reponse is returned.

<<UCA lookup state members>>=
uint32_t curoff;
@

<<Process DUCET initial lookup>>=
const uint32_t *str = st->strs + st->v.off;
uint32_t len = st->v.len, len1;
for(len1 = 0; len1 < len; len1++)
  if((str[len1] & 3) == 3) {
    <<Accumulate and check more collation element characters>>
    <<Return no DUCET values>>
  }
<<Return DUCET entry for single-character index>>
@

<<Accumulate and check more collation element characters>>=
st->state = ST_uca_lookup_gotfirst;
st->curoff = len1;
st->c = c;
@

<<UCA lookup states>>=
,ST_uca_lookup_gotfirst
@

If the next character is end-of-string, the match so far is returned.
Otherwise, it is searched for in the subindex strings.   A match
increases the total [[matchlen]], which can be skipped.  Only exact
matches (i.e. only one character after [[matchlen]], and it must match
[[c]]) are considered successful.

<<Look up in DUCET given state>>=
case ST_uca_lookup_gotfirst:
  if(c == UNI_UCA_LOOKUP_END) {
    <<Return DUCET entry for single or multi-character index>>
  } else {
    int32_t l, h, m;
    const uint32_t *str = st->strs + st->v.off;
    m = ducet_lookup_subindex(c, &l, &h, st);
    <<Process index extension search results>>
  }
@

<<UCA lookup state members>>=
uint32_t matchlen;
@

<<Accumulate and check more collation element characters>>=
st->matchlen = 0;
@

<<Private UCA lookup globals>>=
static int ducet_lookup_subindex(uint32_t c, int32_t *_l, int32_t *_h,
                                 const uni_uca_lookup_state_t *st)
{
  uint32_t l = st->curoff, h = st->v.len, m;
  const uint32_t *str = st->strs + st->v.off;
  while(l <= h) {
    <<Find start of middle index extension>>
    m += st->matchlen;
    uint32_t mc = str[m] >> 2;
    if(mc < c) {
      l = m + 1;
      <<Skip to next index extension>>
    } else if(mc > c ||
              /* also greater if longer than one char */
	      (h > m && mc == c &&
	      (str[m + 1] & 3) == 3 && str[m + 1] != 3)) {
      h = m - st->matchlen - 1;
    } else {
      *_l = l;
      *_h = h;
      return m - st->matchlen;
    }
  }
  *_l = l;
  *_h = h;
  return 0;
}
@

<<Find start of middle index extension>>=
m = (l + h) / 2;
while(m > l && ((str[m] & 3) != 3 || str[m] == 3))
  m--;
while(m > l && ((str[m - 1] & 3) == 3 && str[m - 1] != 3))
  m--;
@

<<Skip to next index extension>>=
while(l <= h && (str[l] & 3) == 3 && str[l] != 3)
  l++;
if(l <= h && str[l] == 3)
  l++;
else
  while(l <= h && (str[l] & 3) != 3)
    l++;
@

A successful match is returned if there can be no further extension of
the index.  Otherwise, the search results are narrowed to the
possibilities collected so far, and more input is requested.

<<Process index extension search results>>=
if(l <= h) {
  /* advance search result to match */
  st->v.off += m;
  st->v.len -= m;
  str += m;
  /* find end of match entry */
  l = 1;
  h = st->v.len - 1;
  <<Skip to next index extension>>
  ducet_adjust_search(c, l, st);
  if(st->curoff == st->v.len) {
    /* there are no more possible matches, so just return */
    st->state = ST_uca_lookup_start;
    <<Return DUCET entry for multi-character index>>
  } else {
    st->matchlen++;
    <<Return no DUCET values>>
  }
}
@

<<Private UCA lookup globals>>=
static void ducet_adjust_search(uint32_t c, int32_t l,
                                uni_uca_lookup_state_t *st)
{
  int32_t h = st->v.len - 1, m;
  const uint32_t *str = st->strs + st->v.off;
  st->curoff = l;
  while(l <= h) {
    <<Find start of middle index extension>>
    if((str[m + st->matchlen] >> 2) != c)
      h = m - 1;
    else {
      l = m + 1;
      <<Skip to next index extension>>
    }
  }
  st->v.len = h + 1;
}
@

When a mismatch occurs, however, it is not guaranteed that this will
never match.  There are two cases where a future character might still
cause a match.  If a longer string is required for a match due to a
DUCET which does not have entries for all intervening-length strings
(the standard DUCET does not), [[h]] always points just below at least
one such string.  If more data is required because a reordering might
cause a match, the ccc of [[c]] and any future characters must be
non-zero and not match the ccc of any potential candidates for
reordering.%
\footnote{But see above:  maybe what should be checked is start-ccc
and end-ccc rather than ccc.}

<<Process index extension search results>>=
else {
  uint32_t ccc;
  /* if c in str, but not only member of str, ill-formed DUCET */
  /* h always points just below longer index */
  h += st->matchlen + 1;
  if(h < st->v.len && str[h] == (c << 2) + 3) {
    <<Enable possible longer lookup>>
  /* check if possible match due to reordering rule */
  } else if((ccc = uni_ccc_of(c))) {
    <<Enable reordering extension>>
  } else {
    /* no possible longer matches, so go ahead and return */
    add_bt(c, st);
    <<Return DUCET entry for single or multi-character index>>
  }
}
@

The first case is easy to detect and manage.  At most one intermediate
is allowed to be missing, so the next character determines if a match
is made.  if so, processing continues just as above.  Otherwise, the
two characters that were not processed are stored, and special states
are entered to feed these back, one at a time.  They will never
collide with each other, because the second character is the first one
which could cause the extension state to reappear, and by then, the
stored characters are done being processed.

Rather than modify the code to loop and retrieve characters from
buffers, a recursive call is used.  Again, this should happen rarely,
and does not affect performance much.

<<UCA lookup state members>>=
uint32_t long_off;
@

<<UCA lookup states>>=
,ST_uca_lookup_longer
@

<<Enable possible longer lookup>>=
st->state = ST_uca_lookup_longer;
st->long_off = st->curoff;
st->c = c;
ducet_adjust_search(c, l, st);
<<Return no DUCET values>>
@

<<Look up in DUCET given state>>=
case ST_uca_lookup_longer:
  {
    int32_t m, l, h;
    const uint32_t *str = st->strs + st->v.off;
    st->matchlen++;
    m = ducet_lookup_subindex(c, &l, &h, st);
    if(l > h) { /* assume no even longer matches possible.. */
      add_bt(st->c, st);
      add_bt(c, st);
      st->curoff = st->long_off;
      st->state = ST_uca_lookup_start;
      <<Return DUCET entry for single or multi-character index>>
    } else {
      st->v.off += m;
      st->v.len -= m;
      l = 1;
      h = st->v.len - 1;
      <<Skip to next index extension>>
      st->curoff = l;
      <<Return DUCET entry for multi-character index>>
    }
  }
@

\label{doc:uca-ccc-reorder}The second case requires checking the ccc
of all subsequent characters; if a zero is detected, no further
reordering is possible. Otherwise, as long as a character does not
match the ccc of one of the previously accumulated characters, it is
looked up and if it matches, a reordering can take place.%
\footnote{But see above:  maybe what should be checked is start-ccc
rather than ccc.}


By the time the decision is made, at least one character, and possibly
more, have been accumulated.  A buffer (separate from [[bt]]) needs to
be used to accumulate these and eventually transfer them into [[bt]]. 

<<UCA lookup state members>>=
uint32_t *buf, buflen, maxbuf;
uint32_t *ccc_buf;
@

<<UCA lookup states>>=
,ST_uca_lookup_reorder
@

<<Enable reordering extension>>=
if(!st->buf) {
  inisize(st->buf, (st->maxbuf = 10));
  inisize(st->ccc_buf, st->maxbuf);
}
st->buf[0] = c;
st->ccc_buf[0] = ccc;
st->buflen = 1;
st->state = ST_uca_lookup_reorder;
<<Return no DUCET values>>
@

<<Look up in DUCET given state>>=
case ST_uca_lookup_reorder:
  {
    uint32_t ccc = c == UNI_UCA_LOOKUP_END ? 0 : uni_ccc_of(c);
    uint32_t i;
    if(st->maxbuf == st->buflen) {
      /* FIXME: return error instead of exiting */
      check_size(st->buf, st->maxbuf, st->buflen + 1);
      resize(st->ccc_buf, st->maxbuf);
    }
    st->buf[st->buflen] = c;
    st->ccc_buf[st->buflen++] = ccc;
    if(!ccc) {
      st->state = ST_uca_lookup_start;
      add_bt_buf(st->buf, st->buflen, st);
      <<Return DUCET entry for single or multi-character index>>
    }
    for(i = 0; i < st->buflen - 1; i++)
      if(st->ccc_buf[i] == ccc) {
        <<Return no DUCET values>>
      }
    int32_t l, h, m;
    m = ducet_lookup_subindex(c, &l, &h, st);
    if(l <= h) {
      /* 6.3.0: must fail if not exact match */
      /* otherwise, 0FB2 0334 0F81 sorts incorrectly */
      const uint32_t *str = st->strs + st->v.off + m;
      if((str[st->matchlen + 1] & 3) == 3 && str[st->matchlen + 1] != 3) {
        fprintf(stderr, "no ext for %04X %04X\n", st->c, c);
        <<Return no DUCET values>>
      }
      /* advance search result to match */
      st->v.off += m;
      st->v.len -= m;
      /* copy rest of characters accumulated to bt */
      add_bt_buf(st->buf, st->buflen - 1, st);
      /* find end of match entry */
      l = 1;
      h = st->v.len - 1;
      <<Skip to next index extension>>
      if(l > h) {
        /* there are no more possible matches, so just return */
        st->curoff = l;
	st->state = ST_uca_lookup_start;
	<<Return DUCET entry for multi-character index>>
      } else {
        st->state = ST_uca_lookup_gotfirst;
	ducet_adjust_search(c, l, st);
	st->matchlen++;
	<<Return no DUCET values>>
      }
    } else {
      <<Return no DUCET values>>
    }
  }
@

<<Free UCA lookup state members>>=
if(st->buf)
  free(st->buf);
@

Rather than reproduce the code for returning the results everywhere,
it is placed after the switch.  Thus a [[break]] indicates that a
result should be returned.  The only difference between them is that
[[c]] needs to be retrieved from the state for multi-character indices.
Also, [[curoff]] is assumed to be the end of the entry to return, but
it is never assigned for the single-character index case.

<<Return DUCET entry for single or multi-character index>>=
c = st->c;
break;
@

<<Return DUCET entry for multi-character index>>=
c = st->c;
break;
@

<<Return DUCET entry for single-character index>>=
st->curoff = v->len;
break;
@

Due to advancing the pointers above, [[v]] points to the desired result
to return.  The result only requires work if there is more than one
key entry.  If so, it is returned later.

<<UCA lookup state members>>=
const uint32_t *multi_ret32;
@

<<Look up and return a single key element from DUCET>>=
int i;
const uint32_t *str = st->strs + st->v.off;
for(i = 0; i < st->curoff; i++)
  if((str[i] & 3) != 3)
    break;
str += i;
st->multi_ret32 = str;
st->curoff -= i;
<<Return DUCET value for [[str]]/[[c]]>>
str++;
st->multi_ret_len = st->curoff - (int)(str - st->multi_ret32);
if(!st->multi_ret_len) {
  st->state = ST_uca_lookup_start;
  <<Return one DUCET value>>
}
st->multi_ret32 = str;
st->state = ST_uca_lookup_multikey;
<<Return more than one DUCET value>>
@

<<UCA lookup states>>=
,ST_uca_lookup_multikey
@

<<Return DUCET value for [[str]]/[[c]]>>=
l123 = *str & ~3;
switch(*str & 3) {
  case 0:
    l4 = 0;
    break;
  case 1:
    l4 = c;
    break;
  default:
    l4 = *++str >> 2;
    break;
}
@

<<Look up in DUCET given state>>=
case ST_uca_lookup_multikey:
  {
    const uint32_t *str = st->multi_ret32;
    c = st->c;
    <<Return DUCET value for [[str]]/[[c]]>>
    str++;
    st->multi_ret_len -= str - st->multi_ret32;
    if(st->multi_ret_len) {
      st->multi_ret32 = str;
      <<Return more than one DUCET value>>
    } else {
      st->state = ST_uca_lookup_start;
      <<Return one DUCET value>>
    }
  }
@

Note that the multi-level comparison algorithm requires that more than
one pass be made over the keys, unless the strings being compared
happen to generate keys of the same ignorable level.  This can be done
by either calling [[uni_uca_lookup]] as many times as there are
levels for each character, or by storing the results of the lookup in
dynamic arrays, to be processed in later passes.  The only entries
which do not need to be stored are all-zero entries.  A simple lookup
function is provided to do the lookups, and return the results as well
as the number of entries in each level (if requested).  While it would
be possible to implement level 2 reversal and literal string appends
(the as-yet unimplemented UCA options) in this function, they are left
for an even higher level function.  Since this is only a partial UCA
implementation, I have named it [[uca_raw]].

<<Unicode property exports>>=
<<[[uni_str_uca_raw]][[32]] proto>>;
<<[[uni_str_uca_raw]][[16]] proto>>;
<<[[uni_str_uca_raw]][[8]] proto>>;
@

<<[[uni_str_uca_raw]](@sz) proto>>=
/** Obtain DUCET values for an entire UTF-<<@sz>> string.
  * Returns the DUCET lookups for all memebers of \p str/\p slen.  If
  * \p slen is negative, \p str is zero-terminated.  Look options may
  * be specified in \p opts.  This does \e not return the UCA sort key,
  * but a raw concatenation of DUCET lookups.  It is allocated from
  * memory if non-empty, and has length \p *rlen.  If \p llen is not
  * NULL, it is a four-integer array which is filled in with the
  * number of non-zero elements at each of the four levels, in order. */
uint32_t *uni_str_uca_raw<<@sz>>(const uint<<@sz>>_t *str, int slen,
                        const uni_uca_opts_t *opts, unsigned int *rlen,
			unsigned int *llen)
@

<<DUCET lookup functions>>=
<<[[uni_str_uca_raw]][[32]]>>
<<[[uni_str_uca_raw]][[16]]>>
<<[[uni_str_uca_raw]][[8]]>>
@

<<[[uni_str_uca_raw]](@sz)>>=
<<[[uni_str_uca_raw]][[<<@sz>>]] proto>>
{
  uint32_t *res, res_max, nres = 0;
  uni_uca_lookup_state_t *st = NULL;
  int use_lev4 = opts && opts->max_level > 3 ? 1 : 0;
  int ret = UNI_UCA_LOOKUP_OK;

  if(!str || !slen) {
    *rlen = 0;
    return NULL;
  }
  inisize(res, (res_max = 10));
  if(opts)
    uni_uca_lookup_opts(&st, opts);
  if(llen)
    llen[0] = llen[1] = llen[2] = llen[3] = 0;
  while(1) {
    uint32_t lev123, lev4;
    uint32_t cp;
    if(!slen)
      cp = UNI_UCA_LOOKUP_END;
    else if(ret == UNI_UCA_LOOKUP_AGAIN)
      cp = 0; /* any old random value */
    else {
      unsigned int clen;
      cp = uni_int_utf<<@sz>>_decode(str, &clen);
      str += clen;
      if(slen > 0)
        slen -= clen; /* underflow if slen was wrong, but I don't care */
      else if(!cp) {
        cp = UNI_UCA_LOOKUP_END;
	slen = 0;
      }
    }
    ret = uni_uca_lookup(cp, &lev123, &lev4, &st);
    if(ret == UNI_UCA_LOOKUP_NONE) {
      if(cp == UNI_UCA_LOOKUP_END)
        break;
      else
        continue;
    }
    if(lev123 || lev4) {
      /* FIXME: return error instead of exiting */
      check_size(res, res_max, nres + 2);
      res[nres++] = lev123;
      if(use_lev4)
        res[nres++] = lev4;
      if(llen) {
        if(lev123 & UNI_DUCET_LEV1_MASK)
          llen[0]++;
        if(lev123 & UNI_DUCET_LEV2_MASK)
          llen[1]++;
        if(lev123 & UNI_DUCET_LEV3_MASK)
          llen[2]++;
        if(lev4)
          llen[3]++;
      }
    }
    if(ret == UNI_UCA_LOOKUP_OK && cp == UNI_UCA_LOOKUP_END)
      break;
  }
  *rlen = nres;
  return res;
}
@

The above result could be considered the UCA key, albeit in a
completely different format than recommended by the UCA.  However, the
options need to be passed along with it, in order to determine if
level 4 is present or not, and also to implement any UCA options that
were deferred to a higher level.  A function is provided to perform
the key comparison, but it only adds one UCA option:  reversal of
level 2.  The final option, literal string comparison, is deferred to
an even higher level.

<<Unicode UCA function options>>=
/** Reverse order of level 2 key elements.
  * Ignored by \ref uni_uca_lookup, but honored by higher-level routines. */
uint8_t reverse_lev2;
@

<<Unicode property exports>>=
/** Compre raw DUCET lookup strings.
  * Compares two results from \ref uni_str_uca_raw32 and friends. Results
  * are indeterminate if \p opts is different than what was used to generate
  * the keys.  */
int uni_uca_raw_cmp(const uint32_t *key1, unsigned int len1,
                    const uint32_t *key2, unsigned int len2,
		    const uni_uca_opts_t *opts);
@

<<DUCET lookup functions>>=
static int uni_uca_raw_cmp123(uint32_t mask,
                              const uint32_t *key1, unsigned int len1,
			      const uint32_t *key2, unsigned int len2,
			      int l4, int rev)
{
  uint32_t k1 = 0, k2; /* stupid gcc complains about k1 uninitialized */

  while(1) {
    while(len1 > 0 && !(k1 = *key1 & mask)) {
      key1 += 1 + l4;
      len1 -= 1 + l4;
    }
    while(len2 > 0 && !(k2 = *key2 & mask)) {
      key2 += 1 + l4;
      len2 -= 1 + l4;
    }
    if(!len2)
      return len1 > 0;
    if(!len1)
      return -1;
    if(k1 < k2)
      return -1;
    if(k2 < k1)
      return 1;
    key1 += 1 + l4;
    key2 += 1 + l4;
    len1 -= 1 + l4;
    len2 -= 1 + l4;
  }
}

int uni_uca_raw_cmp(const uint32_t *key1, unsigned int len1,
                    const uint32_t *key2, unsigned int len2,
		    const uni_uca_opts_t *opts)
{
  int lev = opts && opts->max_level ? opts->max_level : 3;
  int l4 = lev > 3;
  int c;
  c = uni_uca_raw_cmp123(UNI_DUCET_LEV1_MASK, key1, len1, key2, len2, l4, 0);
  if(c || lev == 1)
    return c;
  c = uni_uca_raw_cmp123(UNI_DUCET_LEV2_MASK, key1, len1, key2, len2, l4,
                           opts && opts->reverse_lev2);
  if(c || lev == 2)
    return c;
  c = uni_uca_raw_cmp123(UNI_DUCET_LEV3_MASK, key1, len1, key2, len2, l4, 0);
  if(c || lev == 3)
    return c;
  c = uni_uca_raw_cmp123(~0, key1 + 1, len1, key2 + 1, len2, 1, 0);
  return c;
}
@

Implementing the final step when literal comparison is to be made is
supported by a simple [[strcmp]]-like function.  Conversion to valid NFD
form is expected to have been done before the call.  The string length
can either be negative, indicating zero termination, or positive,
indicating an absolute length.

<<Unicode I/O Exports>>=
/* "int"ernal strcmp; assumes a & b are valid UTF */
<<[[uni_int_utf-]][[32]][[_strcmp]] proto>>;
<<[[uni_int_utf-]][[16]][[_strcmp]] proto>>;
<<[[uni_int_utf-]][[8]][[_strcmp]] proto>>;
@

<<[[uni_int_utf-]](@sz)[[_strcmp]] proto>>=
/** Compare two seqences of Unicode code points.
  * If \p alen is less than zero, \p a is zero-terminated.  Likewise, if
  * \p blen is less than zero, \p b is zero-terminated.  Encoding is
  * native UTF-<<@sz>>.  Ordering is by code point, then length; -1 is
  * returned if \p a is less than \p b, 1 if it is greater, and 0 if it
  * is equal to \p b.  Results are undefined if either string is not valid
  * native UTF-<<@sz>>. */
int uni_int_utf<<@sz>>_strcmp(const uint<<@sz>>_t *a, int alen, const uint<<@sz>>_t *b, int blen)
@

<<Unicode I/O functions>>=
<<[[uni_int_utf-]][[32]][[_strcmp]]>>
<<[[uni_int_utf-]][[16]][[_strcmp]]>>
<<[[uni_int_utf-]][[8]][[_strcmp]]>>
@

<<[[uni_int_utf-]](@sz)[[_strcmp]]>>=
<<[[uni_int_utf-]][[<<@sz>>]][[_strcmp]] proto>>
{
  while(alen && blen) {
    unsigned int aclen, bclen;
    if(alen < 0 && !*a)
      return blen < 0 && !*b ? 0 : -1;
    if(blen < 0 && !*b)
      return 1;
    uint32_t ac = uni_int_utf<<@sz>>_decode(a, &aclen);
    uint32_t bc = uni_int_utf<<@sz>>_decode(b, &bclen);
    if(ac < bc)
      return -1;
    if(ac > bc)
      return 1;
    a += aclen;
    b += bclen;
    alen -= aclen; /* underflows if alen was wrong, but don't care */
    blen -= bclen; /* likewise with blen */
  }
  return alen > 0 || (alen < 0 && *a) ? 1 :
         blen > 0 || (blen < 0 && *b) ? -1 : 0;
}
@

Another option would be to support [[memcmp]], [[strcmp]] or something
similar, by building a key more like the UCA format.  This might make
storing keys for later use much easier.  However, other than storage
efficiency, I see no advantage, and will not implement this.

For direct UCA string comparison, the above functions could be
combined by generating both keys, comparing them, and freeing them.
However, direct string comparison can also be performed by only
storing levels 2-4, and aborting the comparison if a level 1 mismatch
occurs.  The original string pointers need to be retained as well, in
case a literal comparison needs to be made for the last step.  In
order to avoid passing the literal comparison flag separately, this is
added to the options structure.  While the standard indicates that
literal comparison is selected with [[max_level]] of 5 or higher, in
fact the test data suggests that literal comparison should be possible
with lower levels as well.  If [[max_level]] is 5 or higher, or the
explicit [[do_literal]] flag is set, literal comparison will be made.

<<Unicode UCA function options>>=
/** Do a literal comparison as a final step.   Ignored by
  * \ref uni_uca_lookup and \ref uni_uca_raw_cmp, but honored by
  * \ref uni_uca_strcmp32 and friends.  Set automatically if \ref max_level
  * is greater than four.  */
int8_t do_literal;
@

<<Unicode property exports>>=
<<[[uni_uca_strcmp]][[32]] proto>>;
<<[[uni_uca_strcmp]][[16]] proto>>;
<<[[uni_uca_strcmp]][[8]] proto>>;
@

<<[[uni_uca_strcmp]](@sz) proto>>=
/** Compare two strings using the Unicode Collation Algorithm.
  * Strings \p a/\p alen and \p b/\p blen are converted to UCA sort keys.
  * Negative \p alen or \p blen indicate zero-terminated \p a or \p b,
  * respectively.  If the sort keys differ, -1 is returned if \p a's key
  * is less than \p b's, and 1 is returned if \p b's key is less than \p a's.
  * If the keys are equal, and the \p do_literal option is not set, zero
  * is returned.  Otherwise, the result of a raw code point comparison is
  * returned. */
int uni_uca_strcmp<<@sz>>(const uint<<@sz>>_t *a, int alen,
                     const uint<<@sz>>_t *b, int blen,
                     const uni_uca_opts_t *opts)
@

<<DUCET lookup functions>>=
<<[[uni_uca_strcmp]][[32]]>>
<<[[uni_uca_strcmp]][[16]]>>
<<[[uni_uca_strcmp]][[8]]>>
@

<<[[uni_uca_strcmp]](@sz)>>=
<<[[uni_uca_strcmp]][[<<@sz>>]] proto>>
{
  const uint<<@sz>>_t *rawa = a, *rawb = b;
  int raw_alen = alen, raw_blen = blen;
  uint32_t *duca, *ducb;
  unsigned int max_duca = 0, max_ducb = 0, duc_lena = 0, duc_lenb = 0;
  int do_literal = opts && opts->do_literal;
  int lev = opts && opts->max_level ? opts->max_level : 3;
  uni_uca_lookup_state_t *sta = NULL, *stb = NULL;
  int has_l4 = lev > 3;

  if(lev > 4) {
    do_literal = 1;
    lev = 4;
  }
  if(opts) {
    uni_uca_lookup_opts(&sta, opts);
    uni_uca_lookup_opts(&stb, opts);
  }
  uint32_t lev123a, lev123b, lev4a, lev4b;
  int reta = UNI_UCA_LOOKUP_OK, retb = UNI_UCA_LOOKUP_OK;

  <<Get next key element for string [[a]]>>
  <<Get next key element for string [[b]]>>
  if(reta == UNI_UCA_LOOKUP_NONE && retb == UNI_UCA_LOOKUP_NONE)
    return do_literal ? uni_int_utf<<@sz>>_strcmp(rawa, raw_alen, rawb, raw_blen) : 0;
  inisize(duca, max_duca = 10);
  inisize(ducb, max_ducb = 10);
  while(1) {
    <<Scan to next non-zero level 1 in string [[a]]>>
    <<Scan to next non-zero level 1 in string [[b]]>>
    if(reta == UNI_UCA_LOOKUP_NONE && retb == UNI_UCA_LOOKUP_NONE) {
      int c = 0;
      if(lev > 1)
        c = uni_uca_raw_cmp123(UNI_DUCET_LEV2_MASK, duca, duc_lena,
                                                      ducb, duc_lenb, has_l4,
						      opts && opts->reverse_lev2);
      if(lev > 2 && !c)
        c = uni_uca_raw_cmp123(UNI_DUCET_LEV3_MASK, duca, duc_lena,
                                                      ducb, duc_lenb, has_l4, 0);
      if(lev > 3 && !c)
        c = uni_uca_raw_cmp123(~0, duca + 1, duc_lena,
                                     ducb + 1, duc_lenb, 1, 0);
      free(duca);
      free(ducb);
      return c ? c : do_literal ? uni_int_utf<<@sz>>_strcmp(rawa, raw_alen,
                                                          rawb, raw_blen) : 0;
    }
    if(reta == UNI_UCA_LOOKUP_NONE) {
      free(duca);
      free(ducb);
      return -1;
    }
    if(retb == UNI_UCA_LOOKUP_NONE) {
      free(duca);
      free(ducb);
      return 1;
    }
    uint32_t l1a = lev123a & UNI_DUCET_LEV1_MASK,
             l1b = lev123b & UNI_DUCET_LEV1_MASK;
    if(l1a != l1b) {
      free(duca);
      free(ducb);
      return l1a < l1b ? -1 : 1;
    }
    lev123a &= ~UNI_DUCET_LEV1_MASK;
    lev123b &= ~UNI_DUCET_LEV1_MASK;
  }
}
@

<<Scan to next non-zero level 1 in string (@ab)>>=
while(ret<<@ab>> != UNI_UCA_LOOKUP_NONE && !(lev123<<@ab>> & UNI_DUCET_LEV1_MASK)) {
  /* save l2+ keys obtained so far if non-zero */
  if(lev4<<@ab>> || lev123<<@ab>> & ~(UNI_DUCET_LEV1_MASK | 3)) {
    /* FIXME: return error instead of exiting */
    check_size(duc<<@ab>>, max_duc<<@ab>>, duc_len<<@ab>> + 2);
    duc<<@ab>>[duc_len<<@ab>>++] = lev123<<@ab>>;
    if(has_l4)
      duc<<@ab>>[duc_len<<@ab>>++] = lev4<<@ab>>;
  }
  <<Get next key element for string [[<<@ab>>]]>>
}
@

<<Get next key element for string (@ab)>>=
while(1) {
  uint32_t cp;
  if(!<<@ab>>len)
    cp = UNI_UCA_LOOKUP_END;
  else if(ret<<@ab>> == UNI_UCA_LOOKUP_AGAIN)
    cp = 0; /* any old random value */
  else {
    unsigned int clen;
    cp = uni_int_utf<<@sz>>_decode(<<@ab>>, &clen);
    <<@ab>> += clen;
    if(<<@ab>>len > 0)
      <<@ab>>len -= clen; /* underflow if <<@ab>>len wrong, but don't care */
    else if(!cp) {
      cp = UNI_UCA_LOOKUP_END;
      <<@ab>>len = 0;
    }
  }
  ret<<@ab>> = uni_uca_lookup(cp, &lev123<<@ab>>, &lev4<<@ab>>, &st<<@ab>>);
  /* only ever return NONE if this was the last */
  if(ret<<@ab>> != UNI_UCA_LOOKUP_NONE || cp == UNI_UCA_LOOKUP_END)
    break;
}
@

The above code expects input strings to be in NFD normalization form.
This requires a single decomposition lookup, followed by multuple ccc
lookups and a sort.  However, most strings are already close enough to
being normalized that the DUCET lookup would work anyway.  In
particular, the standard allows a so-called Fast C or D form (FCD)
test%
\footnote{<sigh> the CLDR DUCET in 6.2 at least has additional
problems with this.  Specifically, U+0F73 decomposes to U+0F71 U+0F72.
U+0F73 is a starter (ccc=0), and the other two are non-starters
(ccc>0).  This means that the latter two will be reordered to combine
with U+0FB3, but U+0F73 will not.  Having the string in FCD form makes
no difference; UCA simply disallows moving starters around.  I don't
know if this is a bug in the UCD (maybe U+0F73 shouldn't be a starter,
since it decomposes into two non-starters) or a bug in the claim that
the UCA can be applied as-is (maybe it needs changes to support FCD)
or a bug in the CLDR DUCET or what.}%
.  This requires only the ccc of the first and last character of
each full canonical decomposition.  Since this property is not
provided directly in any other way, it is generated here.

<<Property parsed contents>>=
uni_chrrng_dat16_t *rng_dat16;
@

<<Initialize UCD files>>=
decl_num(FCD);
@

<<Parse UCD files>>=
prop_t *ccc_prop = &parsed_props[prop_ccc];
qsort(ccc_prop->rng_dat8, ccc_prop->len, sizeof(*ccc_prop->rng_dat8), uni_cmp_cp);
prop_t *FCD_prop = &parsed_props[prop_FCD = add_prop("FCD")];
FCD_prop->len = FCD_prop->max_len = ccc_prop->len;
inisize(FCD_prop->rng_dat16, FCD_prop->len);
for(i = 0; i < FCD_prop->len; i++) {
  FCD_prop->rng_dat16[i].low = ccc_prop->rng_dat8[i].low;
  FCD_prop->rng_dat16[i].high = ccc_prop->rng_dat8[i].high;
  FCD_prop->rng_dat16[i].dath = FCD_prop->rng_dat16[i].datl = ccc_prop->rng_dat8[i].dat;
}
for(i = 0; i < dmf_prop->len; i++) {
  uint8_t sccc, eccc;
  int l, h, m;
  uint32_t cp = dmf_prop->str_arr[i].cp;
  if(dmf_prop->str_arr[i].flags)
    continue;
  sccc = uni_chrrng_dat8(dmf_prop->strs[dmf_prop->str_arr[i].off],
                         ccc_prop->rng_dat8, ccc_prop->len, ccc_prop->def);
  eccc = uni_chrrng_dat8(dmf_prop->strs[dmf_prop->str_arr[i].off + 
                                        dmf_prop->str_arr[i].len - 1],
		         ccc_prop->rng_dat8, ccc_prop->len, ccc_prop->def);
  for(l = 0, h = FCD_prop->len - 1; l <= h; ) {
    m = (l + h) / 2;
    if(FCD_prop->rng_dat16[m].high < cp)
      l = m + 1;
    else if(FCD_prop->rng_dat16[m].low > cp)
      h = m - 1;
    else
      break;
  }
  if(l <= h) {
    if(sccc == FCD_prop->rng_dat16[m].dath &&
       eccc == FCD_prop->rng_dat16[m].datl)
      continue;
    if(FCD_prop->rng_dat16[m].low == FCD_prop->rng_dat16[m].high) {
      FCD_prop->rng_dat16[m].dath = sccc;
      FCD_prop->rng_dat16[m].datl = eccc;
      continue;
    }
    /* always ensure room for 2, even if only 1 needed */
    check_size(FCD_prop->rng_dat16, FCD_prop->max_len, FCD_prop->len + 2);
    if(FCD_prop->rng_dat16[m].low == cp) {
      movebuf(FCD_prop->rng_dat16 + m + 1, FCD_prop->rng_dat16 + m,
              FCD_prop->len - m);
      FCD_prop->len++;
      FCD_prop->rng_dat16[m + 1].low++;
    } else if(FCD_prop->rng_dat16[m].high == cp) {
      movebuf(FCD_prop->rng_dat16 + m + 2, FCD_prop->rng_dat16 + m + 1,
              FCD_prop->len - m - 1);
      FCD_prop->len++;
      FCD_prop->rng_dat16[m++].high--;
    } else {
      movebuf(FCD_prop->rng_dat16 + m + 2, FCD_prop->rng_dat16 + m,
              FCD_prop->len - m);
      FCD_prop->len += 2;
      FCD_prop->rng_dat16[m++].high = cp - 1;
      FCD_prop->rng_dat16[m + 1].low = cp + 1;
    }
    FCD_prop->rng_dat16[m].low = FCD_prop->rng_dat16[m].high = cp;
    FCD_prop->rng_dat16[m].dath = sccc;
    FCD_prop->rng_dat16[m].datl = eccc;
  } else if(sccc || eccc) {
    check_size(FCD_prop->rng_dat16, FCD_prop->max_len, FCD_prop->len + 1);
    movebuf(FCD_prop->rng_dat16 + l + 1, FCD_prop->rng_dat16 + l,
            FCD_prop->len - l);
    FCD_prop->len++;
    FCD_prop->rng_dat16[l].low = FCD_prop->rng_dat16[l].high = cp;
    FCD_prop->rng_dat16[l].dath = sccc;
    FCD_prop->rng_dat16[l].datl = eccc;
  }
}
@

Like the 8-bit and 32-bit data, a post-processing step fixes up the
tables, generates multi-level tables, and prints everything.  The
table testing program can ensure that the two representations are
equal, as well.

<<UCD parser local functions>>=
static void fixup_rng_dat16(prop_t *p)
{
  uint32_t i;
  qsort(p->rng_dat16, p->len, sizeof(uni_chrrng_dat16_t), uni_cmprng_dat16);
  /* starting at top means only optimized entries are memmove'd */
  for(i = p->len - 1; i > 0; i--) {
    uint32_t j = i;
    while(i > 0 && p->rng_dat16[i - 1].high == p->rng_dat16[i].low - 1 &&
          p->rng_dat16[i - 1].dath == p->rng_dat16[i].dath &&
          p->rng_dat16[i - 1].datl == p->rng_dat16[i].datl)
      i--;
    if(i == j)
      continue;
    p->rng_dat16[i].high = p->rng_dat16[j].high;
    if(j < p->len - 1)
        movebuf(p->rng_dat16 + i + 1, p->rng_dat16 + j + 1, p->len - (j + 1));
    p->len -= j - i;
    if(!i)
      break;
  }
}
@

<<Post-process property data>>=
for(i = 0; i < nparsed; i++)
  if(parsed_props[i].rng_dat16) {
    fixup_rng_dat16(&parsed_props[i]);
    parsed_props[i].mt = uni_rng_dat16_to_multi(parsed_props[i].rng_dat16,
                                                parsed_props[i].len,
                                                &ml_len);
}
@

<<Dump character information as C code>>=
for(i = 0; i < nparsed; i++) {
  if(parsed_props[i].rng_dat16) {
    const char *name = i < num_prop_aliases ? prop_aliases[i].short_name :
                                              parsed_props[i].name;
    const char *lname = i < num_prop_aliases ? prop_aliases[i].long_name :
                                               parsed_props[i].name;
    char nbuf[64];
    sprintf(nbuf, "uni_%s_rng.gen.c", name);
    open_wf(of, nbuf);
    fprintf(of, "#include \"uni_prop.h\"\n\n"
                "const uni_chrrng_dat16_t uni_%s_rng[] = {\n", name);
    for(j = 0; j < parsed_props[i].len; j++)
      fprintf(of, "\t{ 0x%04X, 0x%02X, 0x%04X, 0x%02X }%s\n",
                  parsed_props[i].rng_dat16[j].low,
                  parsed_props[i].rng_dat16[j].datl,
	          parsed_props[i].rng_dat16[j].high,
	          parsed_props[i].rng_dat16[j].dath,
		  j < parsed_props[i].len - 1 ? "," : "");
    fputs("};\n", of);
    fclose(of);
    fprintf(gen_h, "/** Sorted range table for Unicode %s property */"
                   "extern const uni_chrrng_dat16_t uni_%s_rng[];\n"
		   "/** Length of \\ref uni_%s_rng */"
		   "#define uni_%s_rng_len %d /* %d lookups max */\n"
		   "/** 16-bit data associated with \\p x for %s property */\n"
		   "#define uni_%s_of(x) uni_x_dat16(x, uni_%s_mtab)\n",
		   lname, name, name, name, parsed_props[i].len,
		   lg2(parsed_props[i].len + 1), lname, name, name);
    print_mtab(name, lname, parsed_props[i].mt, gen_h);
    fprintf(tstf, "dat16(%s);\n", name);
  }
}
@

<<Unicode property exports>>=
/** Retrieve 16-bit value associated with \p cp in multi-level \p tab.
  * The default is assumed to be zero. */
uint16_t uni_x_dat16(uint32_t cp, const uint32_t *tab);
@

<<Unicode property functions>>=
uint16_t uni_x_dat16(uint32_t cp, const uint32_t *tab)
{
  const uint8_t *mr;
  uni_multi_tab_lookup(tab, cp * 2, &mr, 0);
  return mr ? *(uint16_t *)mr : 0;
}
@

<<Functions to help test generated tables>>=
#define dat16(x) doit_dat16(#x, uni_##x##_rng, uni_##x##_rng_len, uni_##x##_mtab)

static void doit_dat16(const char *name, const uni_chrrng_dat16_t *rng, uint32_t nent,
                       const uint32_t *mtab)
{
    uint32_t i;

    /* print stats */
    printf("%s:\n"
           "  rng: %d entries (%d bytes; %d lookups max)\n",
           name, nent, nent * 8, lg2(nent + 1));
    print_mtab_info(mtab, nent * 8);
    /* check integrity */
    for(i = 0; i < 0x110000; i++) {
      uint16_t r = uni_chrrng_dat16(i, rng, nent);
      uint16_t m = uni_x_dat16(i, mtab);
      if(r != m) {
        fprintf(stderr, "mismatch %s@%d %d %d\n", name, i, (int)r, (int)m);
	exit(1);
      }
    }
    /* check performance */
    int j;
    double tr, tt;
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        uni_chrrng_dat16(i, rng, nent);
    tr = tend();
    tstart();
    for(j = 0; j < 10; j++)
      for(i = 0; i < 0x110000; i++)
        uni_x_dat16(i, mtab);
    tt = tend();
    printf("  r%.0f t%.0f %.2fx\n", tr, tt, tr / tt);
}
@

<<Additional property type names>>=
/** 16-bit integer data: range table is \ref uni_chrrng_dat16, and
 ** multi-level table is 16 bits per code point */
UNI_PROP_TYPE_DAT16,
@

<<Set prop type for export>>=
if(parsed_props[i].rng_dat16)
  t = UNI_PROP_TYPE_DAT16;
@

It would probably be possible to skip normalization up to (but not
including) the last zero start or end ccc, and even to just do one
normalization step and continue the FCD test for the rest of the
string, but that is left as an exercise for the future.  For now, the
FCD algorithm is copied directly from the technical note, and is
therefore full-string only.

<<Unicode normalization support exports>>=
<<[[uni_is_FCD]][[32]] proto>>;
<<[[uni_is_FCD]][[16]] proto>>;
<<[[uni_is_FCD]][[8]] proto>>;
@

<<[[uni_is_FCD]](@sz) proto>>=
/** Check if UTF-<<@sz>> string is in Fast C or D form.
  * Returns true if \p str/\p len is in FCD form.  This may
  * mean that full canonical decomposition need not be performed for
  * algorithms which would other require it. */
int uni_is_FCD<<@sz>>(const uint<<@sz>>_t *str, unsigned int len)
@

<<Unicode normalization support functions>>=
<<[[uni_is_FCD]][[32]]>>
<<[[uni_is_FCD]][[16]]>>
<<[[uni_is_FCD]][[8]]>>
@

<<[[uni_is_FCD]](@sz)>>=
<<[[uni_is_FCD]][[<<@sz>>]] proto>>
{
  uint8_t prev = 0, curs;

  while(len) {
    unsigned int clen;
    uint32_t cp = uni_int_utf<<@sz>>_decode(str, &clen);
    if(len < 0 && !cp)
      break;
    uint16_t both = uni_FCD_of(cp);
    curs = both >> 8;
    if(curs && curs < prev)
      return 0;
    prev = both & 0xff;
    len -= clen; /* underflow if clen incorrect, but I don't care */
    str += clen;
  }
  return 1;
}
@

To test collation, the CollationTest files are used.  As with the
normalization tests, the files are simply fed into standard input.
There are four separate files, and each requires a different
configuration.  These are selected using command-line arguments.  The
options are DUCET vs. CLDR DUCET, and variable mode non-ignorable vs.
variable mode shifted.

\lstset{language=make}
<<C Test Support Executables>>=
tstuca \
@

<<makefile.vars>>=
# this would be nice, but GNU make doesn't support it
#ifge ($(UCD_VER),630)
# so use shell instead:
CLDR_UCA_TESTS=$(shell if [ $(UCD_VER) -ge 630 ]; then \
                          echo $(CLDR_LOC)/common/uca; \
		       else \
		          echo $(UCD_LOC)/CollationAuxiliary; \
		       fi)
@

<<Additional Tests>>=
./tstuca <$(UCD_LOC)/CollationTest/CollationTest_NON_IGNORABLE.txt
./tstuca -s <$(UCD_LOC)/CollationTest/CollationTest_SHIFTED.txt
./tstuca -c <$(CLDR_UCA_TESTS)/CollationTest_CLDR_NON_IGNORABLE.txt
./tstuca -cs <$(CLDR_UCA_TESTS)/CollationTest_CLDR_SHIFTED.txt
@

\lstset{language=C}
<<tstuca.c>>=
<<Common C Header>>
#include "uni_all.h"
#include "mfgets.h"
#include "mallocdef.h"

<<UCA test support>>

int main(int argc, const char **argv)
{
  int do_cldr = 0, do_shift = 0, pr_keys = 0;
  while(argc-- > 1) {
    if(**++argv == '-') {
      const char *s = *argv;
      while(*++s) {
        if(*s == 'c')
	  do_cldr = 1;
	else if(*s == 's')
	  do_shift = 1;
	else if(*s == 'v')
	  pr_keys = 1;
      }
    }
  }
  <<Read and process CollationTest.txt>>
  return 0;
}
@

<<Read and process CollationTest.txt>>=
unsigned int fcd_saved = 0, tot = 0;
uni_uca_opts_t opts;
clearbuf(&opts, 1);
if(do_cldr) {
  opts.tab = uni_DUCET_CLDR_mtab;
  opts.strs = uni_DUCET_CLDR_strs;
  opts.var_top = uni_DUCET_CLDR_var_top;
}
if(do_shift) {
  opts.var_mode = UNI_UCA_VAR_MODE_SHIFTED;
  opts.max_level = 4;
} else {
  opts.var_mode = UNI_UCA_VAR_MODE_NON_IGNORABLE;
  opts.max_level = 3; /* undocumented requirement, apparently */
}
opts.do_literal = 1;
char *lbuf = NULL, *s;
unsigned int lbuflen, llen;
uint32_t *buf, *key, *fcd;
unsigned int buf_len, max_buf, key_len, fcd_len;
uint32_t *prev_key = NULL, *prev_buf = NULL;
unsigned int prev_buf_len = 0, prev_key_len = 0;
unsigned int i;
inisize(buf, (max_buf = 10));
#if 1 /* ensure key in comment matches generated key */
char *kbuf;
unsigned int kbuflen;
inisize(kbuf, (kbuflen = 800)); /* just make it big enough rather than resizing */
#endif
while(mfgets(&lbuf, &lbuflen, &llen, 0, stdin)) {
  if(!isxdigit(lbuf[0]))
    continue;
  buf[0] = strtol(lbuf, &s, 16);
  buf_len = 1;
  while(1) {
    while(isspace(*s))
      s++;
    if(!isxdigit(*s))
      break;
    check_size(buf, max_buf, buf_len + 1);
    buf[buf_len++] = strtol(s, &s, 16);
  }
  tot++;
  if(uni_is_FCD32(buf, buf_len)) {
    fcd_saved++;
    fcd = uni_str_uca_raw32(buf, buf_len, &opts, &fcd_len, NULL);
  } else
    fcd = NULL;
  /* needs to be NFD for literal append */
  buf_len = uni_NFD32(NULL, buf_len, &buf, 0, &max_buf);
  key = uni_str_uca_raw32(buf, buf_len, &opts, &key_len, NULL);
  if(fcd && (fcd_len != key_len || cmpbuf(fcd, key, fcd_len))) {
    fprintf(stderr, "key/fcd mismatch\n%s\n", lbuf);
    s = build_kbuf1234(kbuf, fcd, fcd_len, opts.max_level > 3);
    strcpy(s, "]\n");
    fputs(kbuf, stderr);
    s = build_kbuf1234(kbuf, key, key_len, opts.max_level > 3);
    strcpy(s, "]\n");
    fputs(kbuf, stderr);
    /* CLDR data does not conform! */
    /* main data no longer conforms; has one screwy entry */
    /* has algorithm changed? */
    /* 0FB3 0334 0F81 appears OK, but is wrong multiple ways */
    /*   - FCD and non-FCD key mismatch */
#if 0
    if(!do_cldr)
      exit(1);
#endif
  } else
    free(fcd);
#if 1 /* build string to match text in comment */
  s = build_kbuf1234(kbuf, key, key_len, opts.max_level > 3);
  if(pr_keys)
    fputs(kbuf, stdout);
  /* literal */
  strcpy(s, "|]"); /* error in data file: assumes lit level is empty */
  if(pr_keys) {
    fputs("| ", stdout);
    for(i = 0; i < buf_len; i++)
      printf("%04X ", (int)buf[i]);
    puts("]");
  }
  /* comment in test file has UCA key at end of line, formatted as above */
  {
    s = strrchr(lbuf, '[');
    strrchr(s, ']')[1] = 0;
    if(strcmp(s, kbuf)) {
      fputs("comment mismatch\n", stderr);
      for(i = 0; i < key_len; i++)
        fprintf(stderr, "%08X ", key[i]);
      fprintf(stderr, "%s %s\n%s", kbuf, s, lbuf);
      exit(1);
    }
  }
#endif
  if(prev_key) {
    /* note: can't use memcmp except on big-endian machines */
    int c = uni_uca_raw_cmp(prev_key, prev_key_len, key, key_len, &opts);
    if(!c)
      c = uni_int_utf32_strcmp(prev_buf, prev_buf_len, buf, buf_len);
    if(c > 0) {
      fflush(stdout);
      fprintf(stderr, "keys are out of order at %s\n", lbuf);
      exit(1);
    }
    if(uni_uca_strcmp32(prev_buf, prev_buf_len, buf, buf_len, &opts) > 0) {
      fflush(stdout);
      fprintf(stderr, "strcmp error at %s\n", lbuf);
      exit(1);
    }
    free(prev_key);
    free(prev_buf);
  }
  prev_key = key;
  inisize(prev_buf, buf_len);
  prev_buf_len = buf_len;
  cpybuf(prev_buf, buf, buf_len);
}
if(prev_key) {
  free(prev_key);
  free(prev_buf);
}
printf("%u keys generated; could have skipped NFD on %u tests\n",
       tot, fcd_saved);
@

<<UCA test support>>=
static char *build_kbuf1234(char *s, const uint32_t *key, unsigned int key_len,
                            int has_l4)
{
  unsigned int i;

  *s++ = '[';
  /* lev 1 */
  for(i = 0; i < key_len; i++) {
    uint32_t l1 = (key[i] & UNI_DUCET_LEV1_MASK) >> UNI_DUCET_LEV1_SHIFT;
    if(l1)
      s += sprintf(s, "%04X ", (int)l1);
    if(has_l4)
      i++;
  }
  *s++ = '|';
  *s++ = ' ';
  /* lev 2 */
  for(i = 0; i < key_len; i++) {
    uint32_t l2 = (key[i] & UNI_DUCET_LEV2_MASK) >> UNI_DUCET_LEV2_SHIFT;
    if(l2)
      s += sprintf(s, "%04X ", (int)l2);
    if(has_l4)
      i++;
  }
  *s++ = '|';
  *s++ = ' ';
  /* lev 3 */
  for(i = 0; i < key_len; i++) {
    uint32_t l3 = (key[i] & UNI_DUCET_LEV3_MASK) >> UNI_DUCET_LEV3_SHIFT;
    if(l3)
      s += sprintf(s, "%04X ", (int)l3);
    if(has_l4)
      i++;
  }
  if(has_l4) {
    *s++ = '|';
    *s++ = ' ';
    /* lev 4 */
    for(i = 1; i < key_len; i += 2) {
      uint32_t l4 = key[i];
      if(l4)
        s += sprintf(s, "%04X ", (int)l4);
    }
  }
  return s;
}
@

Since the standard tests only test Shifted and Non-ignorable variable
weighting options, a sample program to exercise the other options is
provided.  It takes the numeric variable treatment type as its first
argument (zero if not present), and if a second argument is present,
the search keys are printed as well.  There is no standard test data;
it must be run and checked manually.  This is not the way to sort a
file.  However, it is sufficient for running a small bit of text
through its paces.

<<C Test Support Executables>>=
tstucavar \
@

<<tstucavar.c>>=
<<Common C Header>>
#include "uni_all.h"
#include "mfgets.h"
#include "mallocdef.h"

static uni_uca_opts_t opts;

struct ustr {
  uint8_t *str, len;
};

static int uca_cmp(const void *_a, const void *_b)
{
  const struct ustr *a = _a, *b = _b;
  return uni_uca_strcmp8(a->str, a->len, b->str, b->len, &opts);
}

int main(int argc, const char **argv)
{
  clearbuf(&opts, 1);
  opts.max_level = 5;
  if(argc > 1)
    opts.var_mode = atoi(argv[1]);
  int pr_keys = argc > 2;
  char *lbuf = NULL;
  unsigned int lbuflen, llen;
  struct ustr *strs;
  int nstrs, maxstrs;
  inisize(strs, (maxstrs = 10));
  nstrs = 0;
  while(mfgets(&lbuf, &lbuflen, &llen, 0, stdin)) {
    /* assume utf-8 input */
    if(!*lbuf || *lbuf == '\n')
      continue;
    unsigned int slen = strlen(lbuf);
    while(slen > 0 && isspace(lbuf[slen - 1]))
      slen--;
    if(!slen)
      continue;
    check_size(strs, maxstrs, nstrs + 1);
    strs[nstrs].str = malloc(slen);
    strs[nstrs].len = slen;
    memcpy(strs[nstrs].str, lbuf, slen);
    nstrs++;
  }
  int i;
  if(pr_keys)
    for(i = 0; i < nstrs; i++) {
      uint32_t *key;
      unsigned int key_len, j;
      fwrite(strs[i].str, strs[i].len, 1, stdout);
      key = uni_str_uca_raw8(strs[i].str, strs[i].len, &opts, &key_len, NULL);
      for(j = 0; j < key_len; j += 2)
        printf(" %04X", (key[j] & UNI_DUCET_LEV1_MASK) >> UNI_DUCET_LEV1_SHIFT);
      putchar('|');
      for(j = 0; j < key_len; j += 2)
        printf(" %04X", (key[j] & UNI_DUCET_LEV2_MASK) >> UNI_DUCET_LEV2_SHIFT);
      putchar('|');
      for(j = 0; j < key_len; j += 2)
        printf(" %04X", (key[j] & UNI_DUCET_LEV3_MASK) >> UNI_DUCET_LEV3_SHIFT);
      putchar('|');
      for(j = 0; j < key_len; j += 2)
        printf(" %04X", key[j + 1]);
      putchar('\n');
    }
  qsort(strs, nstrs, sizeof(*strs), uca_cmp);
  for(i = 0; i < nstrs; i++)
    printf("%.*s\n", (int)strs[i].len, strs[i].str);
  return 0;
}
@

There is a table titled ``Comparison of Variable Ordering'' in the
standard; it is a good start.  It looks very similar to this (headers
have been shortened for formatting):

\lstset{language=txt}
{\let\Tt\unimono
<<varordtab.txt>>=
non-Ign	Blanked	Shifted	IgnSP	Shft-Tr
de luge	death	death	death	death
de Luge	de luge	de luge	de luge	deluge
de-luge	de-luge	de-luge	de-luge	de luge
de-Luge	deluge	de‐luge	de‐luge	de-luge
de‐luge	de‐luge	deluge	deluge	de‐luge
de‐Luge	de Luge	de Luge	de Luge	deLuge
death	de-Luge	de-Luge	de-Luge	de Luge
deluge	deLuge	de‐Luge	de‐Luge	de-Luge
deLuge	de‐Luge	deLuge	deLuge	de‐Luge
demark	demark	demark	demark	demark
				
☠happy	☠happy	☠happy	☠happy	☠happy
☠sad	♡happy	♡happy	☠sad	♡happy
♡happy	☠sad	☠sad	♡happy	☠sad
♡sad	♡sad	♡sad	♡sad	♡sad
				
@
}

To make the above table appear correctly, this document needs to
switch to UTF-8 mode.  The PDF fonts look a little worse, but at least
such files can display.  In addition, the skull-and-crossbones and
heart characters are not in the default font (Latin Modern), so
something else needs to be used.  I use DejaVu Sans Mono; see the
this noweb file's header for details.

\lstset{language=make}
<<makefile.vars>>=
uni.pdf uni.html: NW_UTF8=1
@

The following script generates a table like the above, given one or
more files to sort.  When more than one file is sorted, a blank line
is printed between them.  [[<<varord1.txt>>]] and [[<<varord2.txt>>]]
correspond to the two lists being sorted in the above table.  In order
to extract the table properly, tabs need to be expanded for it.

<<Test Support Scripts>>=
tstucavartab \
@

<<Plain Build Files>>=
varord1.txt varord2.txt varordtab.txt \
@

<<makefile.vars>>=
varordtab.txt: NOTANGLE_OPTS=-t8
@

<<makefile.rules>>=
test: varord1.txt varord2.txt varordtab.txt
@

\lstset{language=sh}
<<tstucavartab>>=
#!/bin/sh

t="non-Ign Blanked Shifted IgnSP Shft-Tr"
i=0
p=

for tt in $t; do
  (
    echo $tt
    for x; do
      ./tstucavar $i < $x
      echo
    done
  ) > out$$$i
  p="$p out$$$i"
  i=$((i+1))
done
eval "paste $p"
eval "rm $p"
@

<<Additional Tests>>=
./tstucavartab varord[12].txt | diff - varordtab.txt
@

\lstset{language=txt}
<<varord1.txt>>=
de luge
de Luge
de-luge
de-Luge
de‐luge
de‐Luge
death
deluge
deLuge
demark
@

{\let\Tt\unimono
<<varord2.txt>>=
☠happy
☠sad
♡happy
♡sad
@
}

One aspect of the data that is lost with such a simple return value is
a means to answer the question, ``what is a collation element?''  My
original plan to address this was to return a character count every
time a new key starts for [[uni_uca_lookup]], but tracking this was
messy, and, in some cases, impossible.  Instead, I provide a more
primitive search function, which could, in fact, be used to implement
the code above in a different way without having to worry too much
about the data format.  Rather than using a state machine and
operating on one character at a time, this function operates on a
buffer, and returns the number of buffer characters in the returned
key.  The key is returned as a simple pointer to static data, or
[[NULL]] if canonical decomposition or a synthetic key is needed.  A
function to produce the full synthetic key is provided as well.

One type of long collation element handled by [[uni_uca_lookup]] which
[[uni_DUCET_lookup]] can't is reordering of grapheme cluster elements
(see \ref{doc:uca-ccc-reorder}).  I really ought to implement this.

<<Unicode property exports>>=
/** Return raw DUCET entry for a collation element.
  * Returns a pointer to the raw DUCENT entry for a collation element
  * in \p buf/\p buf_len.  The collation element starts at \p buf, and
  * spans \p *ce_len code points (if \p ce_len is non-NULL).  The
  * raw data is pointed to by \p key, and its length is \p key_len.
  * A non-zero return indicates that providing more code points in \p buf
  * may give a different result.  However, the UCA grapheme cluster
  * reordering done by \ref uni_uca_lookup is not done by this routine,
  * so results may not be entirely accurate.  It is looked up from the
  * table \p tab/\p strs, or \ref uni_DUCET_mtab/\ref uni_DUCET_strs if NULL.
  * The format of the raw data is documented in the main documentation. */
int uni_DUCET_lookup(const uint32_t *buf, unsigned int buf_len,
                     unsigned int *ce_len, const uint32_t **key,
                     unsigned int *key_len, const uint32_t *tab,
		     const uint32_t *strs);
/** Return synthetic UCA key data for code point \p c.
  * \p key is a 2-word array, filled in with the two values for levels 1-3
  * of a synthetic key.  Level 4 is either 0 or \p c depending on whom you
  * ask. */
void uni_uca_synth_key(uint32_t c, uint32_t *key);
@

<<DUCET lookup functions>>=
int uni_DUCET_lookup(const uint32_t *buf, unsigned int buf_len,
                     unsigned int *ce_len, const uint32_t **key,
                     unsigned int *key_len, const uint32_t *tab,
		     const uint32_t *strs)
{
  if(ce_len)
    *ce_len = 0;
  if(key)
    *key = NULL;
  if(key_len)
    *key_len = 0;
  if(!buf_len)
    return 1;
  if(!tab) {
    tab = uni_DUCET_mtab;
    strs = uni_DUCET_strs;
  }
  const uni_str_ptr_t *v;
  uint32_t c = buf[0];
  if(c > UNI_MAX_CP)
    c = 0xFFFD;
  uni_multi_tab_lookup(tab, c * 4, (const uint8_t **)&v, 0);
  if(!v || !v->len)
    return 0;
  const uint32_t *str = strs + v->off;
  int i;
  for(i = 0; i < v->len && (str[i] & 3) != 3; i++);
  if(i == v->len || buf_len == 1) {
    if(ce_len)
      *ce_len = 1;
    if(key)
      *key = str;
    if(key_len)
      *key_len = i;
    return i < v->len;
  }
  /* FIXME: support gc reordering */
  int l = i, h = v->len - 1, cmp = 1;
  while(l <= h) {
    int m;
    <<Find start of middle index extension>>
    cmp = 0;
    for(i = 0; i < buf_len - 1; i++) {
      uint32_t bc = buf[i + 1];
      if(bc > UNI_MAX_CP)
        bc = 0xFFFD;
      uint32_t mc = str[m + i];
      if((mc & 3) != 3) {
        if(ce_len)
          *ce_len = i;
        if(key)
	  *key = str + m + i;
	if(key_len) {
	  int j;
	  for(j = m + i; j < v->len; j++)
	    if((str[j] & 3) == 3)
	      break;
	  *key_len = j - (m + i);
	}
        cmp = 1;
        break;
      }
      if(bc > mc) {
        cmp = 1;
	break;
      } else if(bc < mc) {
        cmp = -1;
	break;
      }
    }
    if(!cmp && (str[m + i] & 3) == 3)
      cmp = -2; /* special flag: string too long */
    if(!cmp) {
      l = m;
      break;
    } else if(cmp > 0)
      h = m - 1;
    else {
      l = m + 1;
      <<Skip to next index extension>>
    }
  }
  if(!cmp) {
    if(ce_len)
      *ce_len = buf_len;
    l += buf_len;
    if(key)
      *key = str + l;
    for(i = l; i < v->len; i++)
      if((str[i] & 3) == 3)
        break;
    if(key_len)
      *key_len = i - l;
    if(i < v->len) {
      for(l = 0; l < buf_len; l++) {
        uint32_t bc = buf[l];
	if(bc > UNI_MAX_CP)
	  bc = 0xFFFD;
	if((str[i + l] & 3) != 3 || (str[i + l] >> 2) != bc)
	  return 0;
      }
      return 1;
    }
  }
  return cmp == -2;
}
@

<<DUCET lookup functions>>=
void uni_uca_synth_key(uint32_t c, uint32_t *key)
{
  if(c > UNI_MAX_CP)
    c = 0xFFFD;
  key[0] = (uca_synth1(c) << 16) + (0x0020 << 7) + (0x0002 << 2);
  key[1] = (uca_synth2(c) << 16);
}
@

While implementation of the UCA algorithm with the raw tables was not
difficult, there are a few desirable functions which cannot be
accomplished with this data format:%
\footnote{In fact, the first two are the very reason this library
exists.  All of the prior information can be obtained from other
libraries, and in fact the sort keys can be obtained with Single UNIX
Specification functions.  However, no library to my knowledge gives
collation classes or a list of collation elements.  The lack of
locale-specific character classes is less important, since they can at
least be simulated using regular expressions.}

\begin{itemize}
\item Given a collation element, list all collation elements which
have the same key (i.e., determine its collation equivalence class).
This is required for regular expressions.  Then again, one could just
store the DUCET value for the element, and look up the DUCET value for
every single character that needs to match.  The primary advantage of
a reverse table for this is in the case where the equivalence class 
contains very few elements; in that case, the literal elements can be
matched instead, much more efficiently.  One aspect that this library
already provides that others do not is the ability to detect
multi-character collation elements, given the first character of such
an element.  Without that, even the forward lookup approach would not
be possible.
\item Given two collation elements, list all collation elements which
lie between them.  This is required for regular expressions.  Then
again, one could just store the DUCET values for each end of the
range, and look up the DUCET value for every single character that
needs to match.  The only advantage of a reverse table for this is if
the range is small, or the range translates directly to a code point
range.  In either case, a literal match test is more efficient.
\item Reorder elements.  This is required for static CLDR support. Then
again, the reverse table generated for that purpose doesn't need to be
exported.
\item Reorder element blocks.  This is required for static and dynamic
CLDR support.  Then again, I really need to analyze the table better.
If possible, a simple translation table may suffice.
\end{itemize}

At the minimum, these functions all require a lookup table ordered by
sort key rather than collation element.  Similar to the UCA, the sort
compares each level separately, ignoring zeroes.  The first level to
mismatch determines the order.  The table modified for this purpose is
the one which was saved before combining elements whose collation
element has the same starting character.  All synthetic elements and
canonical decompositions have already been filtered out.

\lstset{language=C}
<<DUCET parser globals>>=
<<DUCET lookup format defs>>

static int cmp_ducet_lev123(const uint32_t *kx, uint32_t xlen,
                            const uint32_t *ky, uint32_t ylen,
			    const uint32_t mask)
{
  uint32_t i = 0, j = 0;
  while(i < xlen && j < ylen) {
    while(i < xlen && !(kx[i] & mask))
      i += 1 + ((kx[i] & 3) == 2);
    while(j < ylen && !(ky[j] & mask))
      j += 1 + ((ky[j] & 3) == 2);
    if(j == ylen)
      return i != xlen; /* 0 if both at end, 1 otherwise */
    if(i == xlen)
      return -1;
    if((kx[i] & mask) > (ky[j] & mask))
      return 1;
    if((kx[i] & mask) < (ky[j] & mask))
      return -1;
    i += 1 + ((kx[i] & 3) == 2);
    j += 1 + ((ky[j] & 3) == 2);
  }
  while(i < xlen && !(kx[i] & mask))
    i += 1 + ((kx[i] & 3) == 2);
  while(j < ylen && !(ky[j] & mask))
    j += 1 + ((ky[j] & 3) == 2);
  return j == ylen ? i != xlen : -1;
}
@

<<DUCET parser globals>>=
static int cmp_by_key(const void *a, const void *b)
{
  const raw_cp_str_t *x = a, *y = b;
  const uint32_t *sx = ducet_strs + x->off, *kx,
                 *sy = ducet_strs + y->off, *ky;
  uint32_t xlen = x->len, ylen = y->len;
  /* find end of index extension */
  for(kx = sx; (*kx & 3) == 3; kx++, xlen--);
  for(ky = sy; (*ky & 3) == 3; ky++, ylen--);
  /* always compare level 1 on all keys, then 2 on all keys, etc. */
  int c = cmp_ducet_lev123(kx, xlen, ky, ylen, UNI_DUCET_LEV1_MASK);
  if(c)
    return c;
  c = cmp_ducet_lev123(kx, xlen, ky, ylen, UNI_DUCET_LEV2_MASK);
  if(c)
    return c;
  c = cmp_ducet_lev123(kx, xlen, ky, ylen, UNI_DUCET_LEV3_MASK);
  if(c)
    return c;
  /* level 4 */
  while(xlen && ylen) {
    uint32_t x4, y4;
    switch(*kx & 3) {
      case 0:
        x4 = 0;
	break;
      case 1:
        x4 = x->cp;
	break;
      default: /* can never be 3 */
      /* case 2: */
        x4 = *++kx;
	xlen--;
	break;
    }
    switch(*ky & 3) {
      case 0:
        y4 = 0;
	break;
      case 1:
        y4 = x->cp;
	break;
      default: /* can never be 3 */
      /* case 2: */
        y4 = *++ky;
	ylen--;
	break;
    }
    if(x4 && y4) {
      if(x4 < y4)
        return -1;
      if(x4 > y4)
        return 1;
      x4 = y4 = 0; /* skip */
    }
    if(!x4) {
      kx++;
      xlen--;
    }
    if(!y4) {
      ky++;
      ylen--;
    }
  }
  while(xlen && !(*kx & 3)) {
    kx++;
    xlen--;
  }
  while(ylen && !(*ky & 3)) {
    ky++;
    ylen--;
  }
  if(xlen)
    return 1;
  if(ylen)
    return -1;
  return 0;
}
@

<<Post-process DUCET>>=
qsort(raw_ents, raw_ents_size, sizeof(*raw_ents), cmp_by_key);
@

After sorting, the index and value should be swapped.  Using the full
51-bit key (30 for levels 1-3, plus up to 21 for level 4) as an index
is impractical.  Instead, much like the forward table, there should be
one ``character'' for the index, followed by ``index extensions''
containing the rest of the key.  In this case, the first level is the
most important, so that is the primary index.  The format of the
string at that index is similar to the forward lookup table:  the
first two bits indicate if something is part of the index extension,
or part of the value for that index.  A 3 in the lower two bits
indicates part of the value this time.  Unlike the forward table, the
value must include the first character, and there is little value in
suppressing the first index ``character'' in the ``index extension,''
so the ``index extension'' is the full key.

<<Post-process DUCET>>=
uint32_t *rev_strs, max_rev_strs, rev_strs_len = 0;
inisize(rev_strs, max_rev_strs = 2 * raw_ents_size);
for(i = raw_ents_size - 1; /* i >= 0 */ ; i--) {
  const uint32_t *si = prop->strs + raw_ents[i].off, *kpi, *sj, *kpj;
  int len = raw_ents[i].len + 1; /* + 1 for cp */
  for(kpi = si; (*kpi & 3) == 3; kpi++);
  for(j = i; j > 0; j--) {
    sj = prop->strs + raw_ents[j - 1].off;
    for(kpj = sj; (*kpj & 3) == 3; kpj++);
    if((*kpj & UNI_DUCET_LEV1_MASK) != (*kpi & UNI_DUCET_LEV1_MASK))
      break;
    len += raw_ents[j - 1].len + 1; /* + 1 for cp */
  }
  /* alread sorted by full index, so just append them all */
  check_size(rev_strs, max_rev_strs, rev_strs_len + len);
  uint32_t *dp = rev_strs + rev_strs_len;
  for(low = j; j <= i; j++) {
    sj = prop->strs + raw_ents[j].off;
    /* place key (new index extension) first */
    for(kpj = sj; (*kpj & 3) == 3; kpj++);
    cpybuf(dp, kpj, raw_ents[j].len - (kpj - sj));
    dp += raw_ents[j].len - (kpj - sj);
    /* then append full old index (including cp) */
    *dp++ = (raw_ents[j].cp << 2) + 3;
    cpybuf(dp, sj, kpj - sj);
    dp += kpj - sj;
  }
  raw_ents[low].cp = *kpi >> UNI_DUCET_LEV1_SHIFT;
  raw_ents[low].off = rev_strs_len;
  raw_ents[low].len = len;
  rev_strs_len += len;
  if(low != i)
    movebuf(raw_ents + low + 1, raw_ents + i + 1, raw_ents_size - i - 1);
  raw_ents_size -= i - low;
  if(!(i = low))
    break;
}
@

This transformation alone is already enough to implement the regular
expression requirements, so a property is created using the results.
The string table is rather large (53,053 words in 6.2, with no
redundancy due to the initial cp differing for each entry), but still
barely small enough for 16-bit offsets.

<<Post-process DUCET>>=
char rname[strlen(prop->name) + 5];
memcpy(rname, "rev_", 4);
strcpy(rname + 4, prop->name);
prop_t *rprop = &parsed_props[add_prop(rname)];
rprop->str_arr = raw_ents;
rprop->len = raw_ents_size;
rprop->strs = rev_strs;
rprop->max_strs = max_rev_strs;
rprop->strs_len = rev_strs_len;
enable_str_mt(rprop);
rprop->strs_char_size = 32;
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[add_prop("rev_DUCET")], "Reverse DUCET lookup",
              gen_h, tstf);
dump_str_tabs(&parsed_props[add_prop("rev_DUCET_CLDR")],
              "Reverse DUCET_CLDR lookup", gen_h, tstf);
@

To compute the equivalence class of a collation element, its DUCET
value is looked up, and all elements from the reverse table are
returned.  If the collation element can't be found in the DUCET lookup
table, it is always unique (the level 1 values of synthesized weights
ensure this, and canonical decompostion must be done before calling). 
The main difficulty in returning a result for this is that collation
elements vary in size.  The easiest way to deal with is to place them
in order of length, with a count preceding each group.  Since the
length of the total return value is known, there is no need for a
special terminator.  Other than the length order, returned values are
in an arbitrary order.

<<Unicode property exports>>=
<<[[uni_uca_char_class]][[32]] proto>>;
<<[[uni_uca_char_class]][[16]] proto>>;
<<[[uni_uca_char_class]][[8]] proto>>;
@

<<DUCET lookup functions>>=
<<[[uni_uca_char_class]][[32]]>>
<<[[uni_uca_char_class]][[16]]>>
<<[[uni_uca_char_class]][[8]]>>
@

<<[[uni_uca_char_class]](@sz) proto>>=
/** Find the set of strings with the same UCA key.
  * Looks up the UCA key data for \p ch/\p chlen.  If this data only requires
  * a single lookup (i.e., \p ch/\p chlen is a single collation element),
  * the key data is looked up in \p rev_DUCET/\p rev_strs (or
  * \ref uni_rev_DUCET_mtab/\ref uni_rev_DUCET_strs if NULL), and all
  * matching strings are returned.  See \ref uni_return<<@sz>>_buf<<@sz>>
  * for how the return value works.  The returned array is a sequence of
  * lengths, followed by strings of those lengths (in array elements, not
  * code points).  The first such sequence is for results of length one,
  * the second of length two, etcetera.  The \p opts parameter can be
  * used to influence the initial lookup.  If the forward table,
  * specified by \p opts, and the reverse table, specified by
  * \p rev_DUCET/\p rev_strs, do not correspond to the same data,
  * the results are undefined.  */
int uni_uca_char_class<<@sz>>(const uint<<@sz>>_t *ch, int chlen,
                         const uni_uca_opts_t *opts,
                         const uint32_t *rev_DUCET, const uint32_t *rev_strs,
                         <<Buffer return parameters for UTF-[[<<@sz>>]]>>)
@

<<[[uni_uca_char_class]](@sz)>>=
<<[[uni_uca_char_class]][[<<@sz>>]] proto>>
{
  /* there are many that have key==0, but this is not the way to find them */
  if(chlen <= 0)
    return 0;
  uint8_t level = opts && opts->max_level ? opts->max_level : 3;
  /* mask out l123 if needed */
  uint32_t l123_mask = level == 1 ? UNI_DUCET_LEV1_MASK :
                                2 ? UNI_DUCET_LEV1_MASK | UNI_DUCET_LEV2_MASK :
				~3;
  /* technically, levels > 4 are not supported */
  /* could mean "use literal char as well", but then return is always self */
  /* as such, "do_literal" is ignored as well */
  uint32_t lev123, l4;
  uni_uca_lookup_state_t *lst = NULL;
  if(opts)
    uni_uca_lookup_opts(&lst, opts);
  uint32_t c;
  unsigned int clen;
  c = uni_int_utf<<@sz>>_decode(ch, &clen);
  ch += clen;
  chlen -= clen;
  int lret = uni_uca_lookup(c, &lev123, &l4, &lst);
  while(lret == UNI_UCA_LOOKUP_NONE) { /* at least one lookup must succeed */
    if(chlen > 0) {
      c = uni_int_utf<<@sz>>_decode(ch, &clen);
      chlen -= clen;
      ch += clen;
    } else
      c = UNI_UCA_LOOKUP_END;
    lret = uni_uca_lookup(c, &lev123, &l4, &lst);
  }
  if(chlen) {
    do {
      lret = uni_uca_lookup(UNI_UCA_LOOKUP_END, &lev123, &l4, &lst);
    } while(lret == UNI_UCA_LOOKUP_AGAIN);
    return 0; /* ch is definitely not a single collation element */
  }
  /* valid forward lookup's first element now in lev123/l4 */
  if(!rev_DUCET)
    rev_DUCET = uni_rev_DUCET_mtab;
  const uint32_t *strs;
  const uni_str_ptr_t *v;
  uint32_t l1 = lev123 >> 16;
  uni_multi_tab_lookup(rev_DUCET, l1 * 4, (const uint8_t **)&v, 0);
  /* fails if synthetic or not found */
  /* could mean any number of things, but definitely class is 0 or 1 chars */
  /* should never be "not found" unless the tables are mismatched */
  if(!v || !v->len) {
    <<Return character set with only [[ch]]/[[chlen]]>>
  }
  strs = (rev_strs ? rev_strs : uni_rev_DUCET_strs) + v->off;
  /* find 1st match within result (l), setting hh to more than last */
  int l = v->off, h = v->off + v->len - 1, hh = h, m, i;
  /* repeat until full DUCET entry matched */
  int koff = 0;
  while(1) {
    lev123 &= l123_mask;
    while(l <= h) {
      m = (l + h) / 2;
      while(m > 0 && (strs[m] & 3) == 3)
        m--;
      while(m > 0 && (strs[m - 1] & 3) != 3)
        m--;
      for(i = 0; i < koff; i++, m++) {
        if((strs[m] & 3) == 1)
	  m++;
	else if((strs[m] & 3) == 3)
	  break;
      }
      if(i == koff) { /* else too low; handle with others below */
        uint32_t ol123 = strs[m] & l123_mask,
	         ol4 = !(strs[m] & 3) ? 0 : strs[m + 1] >> 2;
        if(lev123 == ol123 && (level < 4 || l4 == ol4)) {
          h = m - 1;
	  continue;
	} else if(lev123 < ol123 || (level >= 4 && lev123 == ol123 && l4 < ol4)) {
          hh = h = m - 1;
	  continue;
	} /* else too low; handle below */
      }
      while(m < v->len && (strs[m] & 3) != 3)
        m++;
      while(m < v->len && (strs[m] & 3) == 3)
        m++;
      l = m;
    }
    if(l > hh) { /* no possibility for match */
      while(lret == UNI_UCA_LOOKUP_AGAIN) {
        lret = uni_uca_lookup(UNI_UCA_LOOKUP_END, &lev123, &l4, &lst);
      }
      return 0;
    }
    if(lret != UNI_UCA_LOOKUP_AGAIN)
      break;
    ++koff;
    lret = uni_uca_lookup(UNI_UCA_LOOKUP_END, &lev123, &l4, &lst);
    h = hh;
  }
  /* return any results that have exactly koff+1 elements matching one at l */
  /* first, count result length, setting hh to actual end of results */
  int maxlen = 0, len = 0;
  for(h = l; h < hh; ) {
    for(i = 0, m = l; i <= koff; i++) {
      if((strs[m] & l123_mask) != (strs[h] & l123_mask))
        break;
      if(level > 3) {
        if(((strs[m] & 3) == 0 ? 0 : strs[m + 1] & ~3) !=
	   ((strs[h] & 3) == 0 ? 0 : strs[h + 1] & ~3))
	  break;
      }
      if((strs[m] & 3) == 1)
        m++;
      if((strs[h] & 3) == 1)
        h++;
    }
    if(i <= koff || (strs[h] & 3) != 3)
      break;
    for(i = 0; h < hh && (strs[h] & 3) == 3; i++, h++)
      len += uni_utf<<@sz>>_enclen(strs[h] >> 2);
    if(i > maxlen)
      maxlen = i;
  }
  hh = h;
  uint<<@sz>>_t *ret0, *retp;
  int maxl;
  if(off >= 0) {
    check_size(*buf, *buf_len, off + len + maxlen);
    maxl = len + maxlen;
    retp = *buf + off;
  } else {
    maxl = buf && buf_len ? *buf_len : 0;
    retp = buf ? *buf : NULL;
  }
  for(i = 0; i < maxlen; i++) {
    if(!maxl)
      return len + maxlen;
    ret0 = retp;
    *retp++ = 0;
    if(!--maxl)
      return len + maxlen;
    for(m = l; m < hh; ) {
      while((strs[m] & 3) != 3)
        m++;
      for(h = 1; h + m < hh; h++)
        if((strs[h + m] & 3) != 3)
 	  break;
      if(h != i)
        continue;
      ++*ret0; /* FIXME: no overflow check (may be needed for <<@sz>> == 8) */
      for(h = 0; h < i; h++) {
        c = strs[m + h] >> 2;
	clen = uni_utf<<@sz>>_enclen(c);
        if(maxl < clen) {
	  maxl = 0;
	  break;
	}
	(void)uni_int_utf<<@sz>>_encode(retp, c);
	retp += clen;
	maxl -= clen;
      }
    }
  }
  return len + maxlen;
}
@

<<Return character set with only [[ch]]/[[chlen]]>>=
/* finish up forward lookup to clean up its state */
do {
  lret = uni_uca_lookup(UNI_UCA_LOOKUP_END, &lev123, &l4, &lst);
} while(lret == UNI_UCA_LOOKUP_AGAIN);
int i;
if(off >= 0) {
  uni_return<<@sz>>_buf<<@sz>>(ch, chlen, buf, off + chlen, buf_len);
  if(*buf) {
    for(i = 0; i < chlen - 1; i++)
      (*buf)[i] = 0;
    (*buf)[i] = 1;
  }
} else {
  int l = buf && buf_len ? *buf_len : 0;
  for(i = 0; i < chlen - 1 && i < l; i++)
    (*buf)[i] = 0;
  if(l >= chlen)
    (*buf)[i] = chlen;
  if(l > chlen) {
    uint<<@sz>>_t *b = *buf + chlen;
    unsigned int bl = l - chlen;
    uni_return<<@sz>>_buf<<@sz>>(ch, chlen, &b, off, &bl);
  }
}
return chlen * 2;
@

\subsection{Parsing the UCD -- Others}

[[BidiMirroring.txt]] contains the last officially supported string
property.  However, like the simple case conversion properties, it
always returns just a single code point, and is therefore encoded as a
numeric property.

\lstset{language=C}
<<Initialize UCD files>>=
decl_num(bmg);
@

<<Parse UCD files>>=
open_f("BidiMirroring.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_num(bmg, strtol(fields[1], NULL, 16), 1);
}
fclose(f);
@

[[BidiBrackets.txt]] contains several properties that were introduced
in Unicode 6.3, and therefore not covered above.  The first is a code
point, and therefore numeric, and the second is an enumeration, even
though technically it could be encoded as a boolean property, using
the existence of the first property to provide the third value.
Instead, it's treated as a plain enumeration.

<<Initialize UCD files>>=
#if UCD_VER >= 630
decl_num(bpb);
decl_enum(bpt, "n");
#endif
@

<<Parse UCD files>>=
#if UCD_VER >= 630
open_f("BidiBrackets.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse dotted cp>>
  add_num(bpb, strtol(fields[1], NULL, 16), 1);
  add_enum(bpt, fields[2]);
}
fclose(f);
#endif
@

While scx is officially an ``Other'' property, it can be treated just
like a string.  The difference is that the string elements are script
enumeration values, rather than code points.  The values fit in a
byte, so it's tempting to encode it as 8 bits, but there are zeroes in
the values, which makes the string table dumper sometimes strip them.
The table is small enough that the savings wouldn't be great, anyway.

<<Initialize UCD files>>=
decl_str(scx);
@

<<Parse UCD files>>=
open_f("ScriptExtensions.txt");
prop_scx = add_prop("scx");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  uint32_t str[32], len = 0;
  <<Parse dotted cp>>
  s = fields[1];
  while(1) {
    char *n = strchr(s, ' ');
    if(n)
      *n = 0;
    str[len++] = enum_val(prop_sc, s);
    if(!n)
      break;
    s = n + 1;
  }
  if(prop_scx < 0)
    prop_scx = add_prop("scx");
  add_str_rng(&parsed_props[prop_scx], low, high, str, len);
}
fclose(f);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_scx], "Script_Extensions", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_scx);
@

\lstset{language=txt}
<<FIXME>>=
Change 8-bit array format to require byte length instead of word
length, so scx can be dumped 8-bit (as well as any others with 0 in data)
@

<<FIXME>>=
EmojiSources.txt
  field 1 = index (may be multi-char)
  field 2 = 16-bit DoCoMo Shift-JIS code
  field 3 = 16-bit KDDI Shift-JIS code
  field 4 = 16-bit SoftBank Shift-JIS code
  [may add more fields in the future]
 encoding: all 3 fields as 16-bit numbers (0 if absent), followed by index ext
   # of fields is given as #define
   can't really provide vendor name to field # mapping w/o parsing comments
     '# <n - 1>: <vendor> Shift-JIS code'
 property name = ???
StandardizedVariants.txt
  Field 2: English descriptive text; probably not really useful for computers
  Field 3 could be enumerated, but is useless w/o field 2
USourceData.txt: [note: uses cr for line termination]
  Hard to say if it's useful outside of IRG.  Index is U-source ID,
  not Unicode code point (UTC-x or UCI-x).  See tr45.
@

Unofficially, [[Unihan_Variants]] provides some properties as well.
Their fields are formatted differently, with U+ prefixes on every code
point.  Of these properties, only cjkCompatibilityVariant has an
official property name entry.  Only Traditional, Simplified, and
Compatibility variants are needed for regular expressions.  The others
(Semantic, SpecializedSemantic, and Z) will not be read into a
property unless I find a use for them.  Part of the reason for this is
that these properties contain additional information, in the form of
providence annotations.  The annotations could simply be dropped and
those properties added without issue (after all, static linking
ensures their removal if unused), but for now, they are left out.

Version 7 has moved the cjkCompatibilityVariant property to the
[[Unihan_IRGSources]] file.  Again, I just read from both files, and
whichever has the property will fill in the values.

\lstset{language=C}
<<UCD parser local definitions>>=
#define add_hstr(n, v) do { \
  if(prop_##n < 0) \
    prop_##n = add_prop(#n); \
  if(*v) { \
    uint32_t str[10]; \
    uint32_t len; \
    for(s = v, len = 0; *s; len++) { \
      while(isspace(*s)) s++; \
      if(*s == 'U') \
        s += 2; \
      str[len] = strtol(s, &s, 16); \
    } \
    add_str_rng(&parsed_props[prop_##n], low, high, str, len); \
  } \
} while(0)
@

<<Initialize Unihan files>>=
decl_str(cjkTraditionalVariant);
decl_str(cjkSimplifiedVariant);
decl_str(cjkCompatibilityVariant);
@

<<Parse Unihan files>>=
open_f("Unihan_Variants.txt");
while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
  <<Parse Unihan cp>>
  if(!strcmp(fields[1], "kTraditionalVariant"))
    add_hstr(cjkTraditionalVariant, fields[2]);
  else if(!strcmp(fields[1], "kSimplifiedVariant"))
    add_hstr(cjkSimplifiedVariant, fields[2]);
  else if(!strcmp(fields[1], "kCompatibilityVariant"))
    add_hstr(cjkCompatibilityVariant, fields[2]);
}
@

<<Further processing of [[IRGSources]]>>=
if(!strcmp(fields[1], "kCompatibilityVariant"))
  add_hstr(cjkCompatibilityVariant, fields[2]);
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_cjkTraditionalVariant],
              "cjkTraditionalVariant", gen_h, tstf);
dump_str_tabs(&parsed_props[prop_cjkSimplifiedVariant],
              "cjkSimplifiedVariant", gen_h, tstf);
dump_str_tabs(&parsed_props[prop_cjkCompatibilityVariant],
              "cjkCompatibilityVariant", gen_h, tstf);
@

<<Parse Unihan files>>=
enable_str_mt_p(prop_cjkTraditionalVariant);
enable_str_mt_p(prop_cjkSimplifiedVariant);
enable_str_mt_p(prop_cjkCompatibilityVariant);
@

\lstset{language=txt}
<<FIXME>>=
Unihan_DictionaryLikeData.txt:  kCangjie, kCheungBaur, kHDZRadBreak
Unihan_Readings.txt: kCantonese, kDefinition, ...
@

Another pair of optional string values comes from the IDNA compatiblity
database%
\footnote{Actually, there is a third field (IDNA2008 status), but it
is informative, and will not be read in as a property unless I find a
use.}%
.  This is available from a separate location
(\url{http://www.unicode.org/Public/idna/}), but is only checked for
in the UCD directory and a few minor variants.  If not present, it is
ignored.  The IDNA\_Status property is technically an enumeration, but
since it has no value aliases, it is instead accumulated as a string
property.  It is then converted to an enumeration.

\lstset{language=C}
<<Initialize UCD files>>=
decl_str(IDNA_Status);
decl_str(IDNA_Mapping);
@

<<Parse UCD files>>=
if((f = fopen("IdnaMappingTable.txt", "r")) ||
   (f = fopen("idna/IdnaMappingTable.txt", "r")) ||
   (f = fopen("../idna/IdnaMappingTable.txt", "r"))) {
  while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    <<Parse dotted cp>>
    add_str8(IDNA_Status, fields[1]);
    if(num_fields > 2)
      add_str(IDNA_Mapping, fields[2]);
  }
  fclose(f);
  str_to_enum(&parsed_props[prop_IDNA_Status], "disallowed");
}
@

<<Dump character information as C code>>=
dump_str_tabs(&parsed_props[prop_IDNA_Mapping], "IDNA_Mapping", gen_h, tstf);
@

<<Parse UCD files>>=
enable_str_mt_p(prop_IDNA_Mapping);
@

<<UCD parser local functions>>=
static void str_to_enum(prop_t *p, const char *def)
{
  uint32_t i, poff = 0, plen = 0, pnum = (uint32_t)(p - parsed_props);
  int32_t seq = -1;

  merge_strs(p);
  p->def = def ? ~0 : 0;
  inisize(val_aliases[pnum], max_val_aliases[pnum] = 5);
  inisize(p->rng_dat8, p->len);
  for(i = 0; i < p->len; i++) {
    if(p->str_arr[i].off == poff && p->str_arr[i].len == plen) {
      p->rng_dat8[i].low = p->rng_dat8[i].high = p->str_arr[i].cp;
      p->rng_dat8[i].dat = seq;
      continue;
    }
    seq++;
    poff = p->str_arr[i].off;
    plen = p->str_arr[i].len;
    check_size(val_aliases[pnum], max_val_aliases[pnum], num_val_aliases[pnum] + 1);
    {
      uni_alias_t *va = &val_aliases[pnum][num_val_aliases[pnum]];
      char *n;
      inisize(n, plen * 4 + 1);
      memcpy(n, (char *)(p->strs + poff), plen * 4);
      n[plen * 4] = 0;
      if(def && !strcmp(n, def))
        p->def = seq;
      va->short_name = va->long_name = n;
      va->alt_name = va->alt_name2 = NULL;
      num_val_aliases[pnum]++;
    }
    i--; /* re-run for current item to actually add it */
  }
  if(p->def == (uint8_t)~0) {
    p->def = seq;
    check_size(val_aliases[pnum], max_val_aliases[pnum], num_val_aliases[pnum] + 1);
    {
      uni_alias_t *va = &val_aliases[pnum][num_val_aliases[pnum]];
      va->short_name = va->long_name = strdup(def);
      va->alt_name = va->alt_name2 = NULL;
      num_val_aliases[pnum]++;
    }
  }
  inisize(enum_vals[pnum], enum_vals_len[pnum] = num_val_aliases[pnum]);
  for(i = 0; i < enum_vals_len[pnum]; i++) {
    enum_vals[pnum][i].name = strdup(val_aliases[pnum][i].short_name);
    enum_vals[pnum][i].val = i;
  }
  /* while strs was sorted, it was sorted by long word, not byte */
  /* this sort is really only necessary on little-endian systems */
  qsort(enum_vals[pnum], enum_vals_len[pnum], sizeof(*enum_vals[pnum]),
        uni_cmp_valueof);
  free(p->strs);
  p->strs = NULL;
  free(p->str_arr);
  p->str_arr = NULL;
}
@

Similarly, the security information database, available separately at
\url{http://www.unicode.org/Public/security/}, contains two optional
enumeration properties without value aliases.

<<Initialize UCD files>>=
decl_str(ID_Restrict_Status);
decl_str(ID_Restrict_Type);
@

<<Parse UCD files>>=
if((f = fopen("xidmodifications.txt", "r")) ||
   (f = fopen("security/xidmodifications.txt", "r")) ||
   (f = fopen("../security/xidmodifications.txt", "r"))) {
  while(mfgets(&lbuf, &lbuflen, &llen, 0, f)) {
    <<Parse dotted cp>>
    add_str8(ID_Restrict_Status, fields[1]);
    if(num_fields > 2)
      add_str8(ID_Restrict_Type, fields[2]);
  }
  fclose(f);
  str_to_enum(&parsed_props[prop_ID_Restrict_Status], "restricted");
  str_to_enum(&parsed_props[prop_ID_Restrict_Type], "not-chars");
}
@

\lstset{language=txt}
<<FIXME>>=
security/intentional.txt: could be numeric (char-to-char mapping) (propname?)
security/confusables.txt: propname? 4 props or field 3 as part of value?
security/confusablesWholeScript.txt: propname?
  could be plain 32-bit value: <from><to><type>, but there are dup indices
@

<<FIXME>>=
Unihan_IRGSources.txt: kIRG_* (not sure how to encode)
@

Finally, it may be useful to know what version of the UCD and CLDR
were used to compile this library.  This is provided in two forms:  a
preprocessor symbol and a constant variable.  The latter is in case
the library is used as a shared object, and is later linked at
run-time to a version that is compiled with a different set of data.

<<Dump character information as C code>>=
fprintf(gen_h, "/** The version of the UCD data files this library was generated with */\n"
               "#define UNI_UCD_VER %d\n"
	       "/** The version of the CLDR data files this library was generated with */\n"
	       "#define UNI_CLDR_VER %d\n", UCD_VER, CLDR_VER);
@

<<uni_prop.c>>=
const unsigned int uni_ucd_version = UNI_UCD_VER;
const unsigned int uni_cldr_version = UNI_CLDR_VER;
@

<<Unicode property exports>>=
/** The version of the UCD data files this library was generated with */
extern const unsigned int uni_ucd_version,
/** The version of the CLDR data files this library was generated with */
                          uni_cldr_version;
@

<<FIXME>>=
Versions of all other files used.
  UAX versions (always advance in sync with UCD)
  Unihan (same as UCD version)
  idna (same as UCD version)
  UCA (same as UCD version)
  security (Version: 3.0-draft) - not really versioned separately any more
  XML entity database - no version information available
@

\chapter{XML Data Files}

A number of data files outside of the UCD are encoded in XML.  These
include in particular the CLDR files.  One way to deal with this would
be to turn the files into files more like the rest of the UCD using
XSLT.  In fact, an earlier version of this library did just that for
the single XML file it supported.  The CLDR has many more files,
though, and not all are in the same format, so something else needs to
be done.  Since everything else is being parsed in parse-ucd,
parse-ucd will read and parse the XML files directly, using libxml2.%
\footnote{\url{http://xmlsoft.org/}}
For maximum simplicity, each file is simply read in using the
document-at-once
parser\footnote{\url{http://xmlsoft.org/html/libxml-parser.html}}.
While the incremental (xmlreader) mode would probably be more
efficient for memory usage, this program is only run once, during
build time, so the memory usage should not matter much.  At least the
XML structure can be freed when finished.  Scanning the tree is done
using manual traversal.  While the associated libxslt could be used to
run the old code directly, the C code is actually much simpler.

\lstset{language=C}
<<Additional parse-ucd includes>>=
#include <libxml/parser.h>
#include <libxml/xmlerror.h>
@

\lstset{language=make}
<<makefile.vars>>=
XML_CFLAGS := $(shell xml2-config --cflags)
XML_LDFLAGS := $(shell xml2-config --libs)
# adding xml flags only to PARSER_CFLAGS would be nice, but hard
# to integrate w/ static_proto gen
EXTRA_CFLAGS += $(XML_CFLAGS)
PARSER_LDFLAGS += $(XML_LDFLAGS)
@

\lstset{language=C}
<<UCD parser local definitions>>=
<<XML Support Definitions>>
@

<<XML Support Definitions>>=
#define xml_opts XML_PARSE_NOENT | /* expand known entities */ \
                 XML_PARSE_NONET | /* forbid network access */ \
		 XML_PARSE_NOCDATA | /* merge CDATA as text */ \
                 XML_PARSE_COMPACT | /* compact small text nodes */ \
		 XML_PARSE_HUGE /* no hard-coded limits */
/* avoid type cast in every strcmp */
#define xml_isname(n, s) !strcmp((const char *)n->name, s)
@

<<Parse character data files>>=
xmlInitParser();  /* xmlCleanupParser() when done */
xmlDocPtr doc; /* the document under consideration */
xmlNodePtr n, c; /* for traversing the nodes */
@

\section{XML Entity Names}

As mentioned in the introduction, the XML standard's entity names are
generally much shorter than Unicode names, so a facility to use them
instead is provided.  This requires parsing the XML entity database.

\lstset{language=make}
<<makefile.vars>>=
PARSER_CFLAGS += -DXMLUNI=\"$(XMLUNI)\"
@

\lstset{language=C}
<<Parse character data files>>=
doc = xmlReadFile(XMLUNI, NULL, xml_opts);
if(!doc) {
  perror(XMLUNI);
  exit(1);
}
for(n = doc->children; n; n = n->next)
  if(n->children && xml_isname(n, "unicode"))
    break;
if(!n) {
  perror(XMLUNI);
  exit(1);
}
@


The name database has subsets called groups.  Using every single name
is certain to frequently collide with Unicode names, so a particular
group should be chosen.  The 2007 group is used by my own
applications, and seems like a reasonable default.

\lstset{language=make}
<<makefile.config>>=
# XML entity name group
XML_ENTITY_NAME_GROUP = 2007

@

<<makefile.vars>>=
PARSER_CFLAGS += -DXML_ENTITY_NAME_GROUP=\"$(XML_ENTITY_NAME_GROUP)\"
@

\lstset{language=C}
<<XML Support Definitions>>=
/* avoid multiple type casts in every attr search */
#define xml_prop(n, p) (char *)xmlGetProp(n, (const xmlChar *)p)
#define xmlprop_free(p) do { \
  if(p) { \
    xmlFree((xmlChar *)p); \
    p = NULL; \
  } \
} while(0)
@

<<UCD parser local definitions>>=
const char **grp_set;
int ngrp_set;
@

<<Parse character data files>>=
for(c = n->children; c; c = c->next) {
  if(!c->children || !xml_isname(c, "entitygroups"))
    continue;
  for(c = c->children; c; c = c->next) {
    if(!c->children || !xml_isname(c, "group"))
      continue;
    char *gn = xml_prop(c, "name");
    int is_gr = gn && !strcmp(gn, XML_ENTITY_NAME_GROUP);
    xmlprop_free(gn);
    if(is_gr)
      break;
  }
  break;
}
if(!c) {
  perror("group " XML_ENTITY_NAME_GROUP " not in " XMLUNI);
  exit(1);
}
{
  xmlNodePtr cc;
  for(ngrp_set = 0, cc = c->children; cc; cc = cc->next) {
    if(cc->type != XML_ELEMENT_NODE || !xml_isname(cc, "set"))
      continue;
    ngrp_set++;
  }
  inisize(grp_set, ngrp_set);
  for(ngrp_set = 0, cc = c->children; cc; cc = cc->next) {
    if(cc->type != XML_ELEMENT_NODE || !xml_isname(cc, "set"))
      continue;
    grp_set[ngrp_set++] = xml_prop(cc, "name");
  }
  qsort(grp_set, ngrp_set, sizeof(*grp_set), sort_strcmp);
}
@

<<UCD parser local functions>>=
static int sort_strcmp(const void *a, const void *b)
{
  return strcmp(*(const char **)a, *(const char **)b);
}
@

Once the group's set names are known, the charlist is scanned for
character tags with an entity descriptor belonging to one of those
sets.  The id of the entity is the entity name, and the id of the
character is the Unicode code point.  Some names are for
multi-code-point sequences, similar to the Unicode named sequences.
For now, sequences are read into a separate property.

<<Parse character data files>>=
decl_str(na_xml);
decl_str(na_xml_seq);
for(n = n->children; n; n = n->next)
  if(n->children && xml_isname(n, "charlist"))
    break;
if(!n) {
  perror("charlist not in " XMLUNI);
  exit(1);
}
for(n = n->children; n; n = n->next) {
  if(!n->children || !xml_isname(n, "character"))
    continue;
  for(c = n->children; c; c = c->next) {
    const char *set = NULL;
    const char *cid = NULL, *eid = NULL;
    if(c->type != XML_ELEMENT_NODE || !xml_isname(c, "entity") ||
       !(set = xml_prop(c, "set")) ||
       !bsearch(&set, grp_set, ngrp_set, sizeof(*grp_set), sort_strcmp) ||
       !(eid = xml_prop(c, "id")) || !(cid = xml_prop(n, "id"))) {
      xmlprop_free(set);
      xmlprop_free(eid);
      xmlprop_free(cid);
      continue;
    }
    if(!strchr(cid, '-')) {
      low = high = strtol(cid + 1, NULL, 16);
      /* note: duplicates will occur: */
      /*   both fully duplicated entries and multiple names for one cp */
      add_str8(na_xml, eid);
    } else
      add_strseq(na_xml_seq, cid + 1, eid);
    xmlprop_free(set);
    xmlprop_free(eid);
    xmlprop_free(cid);
  }
}
xmlFreeDoc(doc);
@

Some code points have multiple entity names.  This problem can be
solved the same way as with Name\_Alias:  prepend a length to each
string.  As an optimization, since the word encoding is not done, and
there is no name longer than 127 characters, the length byte has its
high bit set.  The last string does not have a length byte; it just
occupies the remainder of the space.

<<UCD parser local functions>>=
static int cmp_cp_name(const void *a, const void *b)
{
  int c = *(int32_t *)a - *(int32_t *)b;
  return c ? c : cmp_strs(a, b);
}
@

<<Post-process property data>>=
prop_t *xna = &parsed_props[prop_na_xml];
sort_strs = xna->strs;
qsort(xna->str_arr, xna->len, sizeof(*xna->str_arr), cmp_cp_name);
for(i = xna->len - 1; i > 0; i--) {
  unsigned int nlen = xna->str_arr[i].len;
  for(low = i; low > 0; low--) {
    if(xna->str_arr[low - 1].cp != xna->str_arr[i].cp)
      break;
    if(!cmp_strs(&xna->str_arr[low - 1], &xna->str_arr[low]))
      continue;
    nlen += xna->str_arr[low - 1].len + 1;
  }
  if(low == i)
    continue;
  check_size(xna->strs, xna->max_strs, xna->strs_len + nlen);
  uint8_t *strs8 = (uint8_t *)(xna->strs + xna->strs_len);
  unsigned int strs8_len = 0;
  for(j = low; j <= i; j++) {
    while(j < i && !cmp_strs(&xna->str_arr[j], &xna->str_arr[j + 1]))
      j++;
    uint8_t *p8 = (uint8_t *)(xna->strs + xna->str_arr[j].off);
    unsigned int len8 = xna->str_arr[j].len * 4;
    while(!p8[len8 - 1])
      len8--;
    if(j != i)
      strs8[strs8_len++] = 0x80 | (len8 - 1);
    memcpy(strs8 + strs8_len, p8, len8);
    strs8_len += len8;
  }
  while(strs8_len % 4)
    strs8[strs8_len++] = 0;
  movebuf(xna->str_arr + low + 1, xna->str_arr + i + 1,
          xna->len - (i + 1));
  xna->str_arr[low].off = xna->strs_len;
  xna->str_arr[low].len = strs8_len / 4;
  xna->strs_len += strs8_len / 4;
  xna->len -= i - low;
  if(!(i = low))
    break;
}
@

<<Dump character information as C code>>=
dump_str_tabs(xna, "XML entity names for code points", gen_h, tstf);
@

<<Parse character data files>>=
enable_str_mt_p(prop_na_xml);
@

Some sequences have multiple entries with the same starting character.
There is no guarantee that there are no multiple names for the same
sequence, either.  The encoding used for na\_seq above is good enough to
capture this.

<<Post-process property data>>=
prop_t *xna_seq = &parsed_props[prop_na_xml_seq];
sort_strs = xna_seq->strs;
qsort(xna_seq->str_arr, xna_seq->len, sizeof(*xna_seq->str_arr), cmp_cp_name);
for(i = xna_seq->len - 1; i > 0; i--) {
  for(j = i; j > 0 && xna_seq->str_arr[j].cp == xna_seq->str_arr[j - 1].cp &&
             !cmp_strs(&xna_seq->str_arr[j], &xna_seq->str_arr[j - 1]); j--);
  if(j != i) {
    movebuf(xna_seq->str_arr + j + 1, xna_seq->str_arr + i + 1, xna_seq->len - (i + 1));
    xna_seq->len -= i - j;
    if(!(i -= j))
      break;
  }
  while(i > 0 && xna_seq->str_arr[i].cp == xna_seq->str_arr[i - 1].cp) {
    int jlen = xna_seq->str_arr[i - 1].len, ilen = xna_seq->str_arr[i].len;
    check_size(xna_seq->strs, xna_seq->max_strs, xna_seq->strs_len + ilen + jlen);
    uint16_t *newseq = (uint16_t *)(xna_seq->strs + xna_seq->strs_len);
    jlen *= 2;
    cpybuf(newseq, xna_seq->strs + xna_seq->str_arr[i - 1].off, jlen);
    if(!newseq[jlen - 1])
      jlen--;
    ilen *= 2;
    cpybuf(newseq + jlen, xna_seq->strs + xna_seq->str_arr[i].off, ilen);
    if(!newseq[jlen + ilen - 1])
      ilen--;
    if((ilen + jlen) % 2)
      newseq[ilen++ + jlen] = 0;
    for(j = i - 1; j > 0; j--)
      if(cmp_strs(&xna_seq->str_arr[j], &xna_seq->str_arr[j - 1]))
        break;
    xna_seq->str_arr[j].len = (ilen + jlen) / 2;
    xna_seq->str_arr[j].off = xna_seq->strs_len;
    xna_seq->strs_len += (ilen + jlen) / 2;
    movebuf(xna_seq->str_arr + j + 1, xna_seq->str_arr + i + 1, xna_seq->len - (i + 1));
    xna_seq->len--;
    i = j;
  }
  if(!i)
    break;
}
@

<<Dump character information as C code>>=
dump_str_tabs(xna_seq, "XML entity names for code point sequences", gen_h, tstf);
@

<<Parse character data files>>=
enable_str_mt_p(prop_na_xml_seq);
@

Finally, although the encoding is fairly simple, a function analogous
to the plain Unicode name lookup function is provided.

<<Library [[uni]] Members>>=
xml_cp_to_name.o
@

<<xml_cp_to_name.c>>=
<<Common C Header>>
#include "uni_prop.h"

@

<<Unicode property exports>>=
/** Look up the XML entity name of code point(s).
 * Looks up name of \p len code points starting at \p cp.  If non-NULL,
 * the number of code points consumed to produce the name is returned
 * in the absolute value of \p *seq_len.  If \p *seq_len is less than zero,
 * using a longer input may produce a different name.  If non-NULL,
 * \p *alias can be used to select an alternate name.  If zero is passed in,
 * the canonical name is returned, and \p *alias is updated to the number
 * of aliases.  If \p *alias is a valid alias index, that alias is returned,
 * and \p *alias is decremented.  If \p *alias is not a valid alias index
 * or zero, it is updated to -1 and no name is returned.  If \p alias is
 * NULL, only the canonical name is returned.  The returned string is
 * described in \ref uni_return8_buf8.   An empty return string indicates
 * a code point without a name or an invalid alias */
int uni_cp_to_xml_name(const uint32_t *cp, unsigned int len, int *seq_len,
                       int *alias, <<Buffer return parameters for UTF-[[8]]>>);
@

<<xml_cp_to_name.c>>=
int uni_cp_to_xml_name(const uint32_t *cp, unsigned int len, int *seq_len,
                       int *alias, <<Buffer return parameters for UTF-[[8]]>>)
{
  if(!len) {
    if(seq_len)
      *seq_len = 0;
    if(alias && *alias)
      *alias = -1;
    return 0;
  }
  if(alias && *alias < 0) {
    if(seq_len)
      *seq_len = 0;
    return 0;
  }
  <<Return XML name for [[*cp]]>>
}
@

Just like with Unicode names, the first thing to do is to see if there
is a matching sequence name.

<<Return XML name for [[*cp]]>>=
if(seq_len)
  *seq_len = 1;
const uni_str_ptr_t *lu = len > 1 || seq_len ? uni_na_xml_seq_of(*cp) : NULL;
if(lu && lu->len && len == 1) {
  if(seq_len)
    *seq_len = -1;
} else if(lu && lu->len) {
  const uint8_t *found_na = NULL;
  unsigned int found_nalen = 0, found_seqlen = 0, found_alias = 0, seqlen, nalen;
  const uint8_t *ep = uni_na_xml_seq_strs + lu->off, *p = ep + lu->len;
  const uint16_t *seqptr;
  const uint32_t *cp2;
  while(p > ep) {
    nalen = *--p;
    seqlen = (nalen >> 6) + 1;
    nalen = (nalen & 0x3f) + 1;
    if(!p[-1])
      p--;
    p -= nalen + seqlen * 2;
    seqptr = (const uint16_t *)p;
    int sp = 0;
    for(cp2 = cp + 1; cp2 - cp < len; cp2++) {
      unsigned int clen;
      if(*cp2 != uni_int_utf16_decode(seqptr + sp, &clen))
        break;
      sp += clen;
      if(sp == seqlen) {
        if(seqlen > found_seqlen) {
          found_na = p + seqlen * 2;
	  found_nalen = nalen;
	  found_seqlen = seqlen;
	  found_alias = 0;
	} else if(seqlen == found_seqlen) {
	  found_alias++;
	  if(alias && *alias == found_alias) {
	    found_na = p + seqlen * 2;
	    found_nalen = nalen;
	  }
	}
	break;
      }
    }
    /* if at end of string, mark as possibly longer */
    if(seq_len && sp < seqlen && cp2 - cp == len)
      *seq_len = -1;
  }
  if(found_na) {
    if(alias && *alias > found_alias) {
      *alias = -1;
      *seq_len = 0;
      return 0;
    } else if(alias && !*alias)
      *alias = found_alias;
    else if(alias)
      --*alias;
    if(seq_len)
      *seq_len *= found_seqlen + 1; /* 1 or -1 */
    return uni_return8_buf8(found_na, found_nalen, buf, off, buf_len);
  }
}
@

For the single code point case, there is only one table to consult.
It behaves much like the alias table, except the last entry for each
code point does not have an explicit length, and the explicit lengths
for the others all have their high bit set.

<<Return XML name for [[*cp]]>>=
lu = uni_na_xml_of(*cp);
if(!lu->len)
  return 0;
const uint8_t *str = uni_na_xml_strs + lu->off;
if(!alias)
  return uni_return8_buf8(*str & 0x80 ? str + 1 : str,
                          *str & 0x80 ? (*str & 0x7f) + 1 : lu->len,
			  buf, off, buf_len);
int num_alias = 0;
int alen = lu->len, retlen = 0;
while(1) {
  if(num_alias == *alias) {
    retlen = uni_return8_buf8(*str & 0x80 ? str + 1 : str,
                              *str & 0x80 ? (*str & 0x7f) + 1 : alen,
			      buf, off, buf_len);
    if(num_alias) {
      --*alias;
      return retlen;
    }
  }
  if(!(*str & 0x80))
    break;
  alen -= (*str & 0x7f) + 2;
  str += (*str & 0x7f) + 2;
  num_alias++;
}
if(*alias) /* couldn't find requested alias */
  *alias = -1;
else /* set # of aliases */
  *alias = num_alias;
return retlen;
@

To test this, a simple program just generates every single name.  No
verification is done; this is meant to be verified manually.

<<C Test Support Executables>>=
tstcp_xml_na \
@

<<tstcp_xml_na.c>>=
<<Common C Header>>

#include "uni_prop.h"

int main(void)
{
  uint32_t cp;
  uint8_t *buf = NULL;
  unsigned int buf_len = 0, clen;
  int has_more, aliases;
  for(cp = 0; cp <= UNI_MAX_CP + 1; cp++) {
    aliases = 0;
    clen = uni_cp_to_xml_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
    if(!clen)
      continue;
    printf("%04X %d %.*s\n", cp, has_more, clen, buf);
    while(aliases > 0) {
      clen = uni_cp_to_xml_name(&cp, 1, &has_more, &aliases, &buf, 0, &buf_len);
      printf("%04X %d &%.*s\n", cp, has_more, clen, buf);
    }
  }
  /* all sequences */
  int i;
  uint32_t seq[10];
  for(i = 0; i < uni_na_xml_seq_arr_len; i++) {
    const uint8_t *ep = uni_na_xml_seq_strs + uni_na_xml_seq_arr[i].off,
                  *p = ep + uni_na_xml_seq_arr[i].len;
    unsigned int seqlen, nalen;
    while(p > ep) {
      uint32_t *sp = seq;
      *sp = uni_na_xml_seq_arr[i].cp;
      printf("%04X", (int)*sp++);
      nalen = *--p;
      if(!p[-1])
        p--;
      seqlen = (nalen >> 6) + 1;
      nalen = (nalen & 0x3f) + 1;
      p -= nalen + seqlen * 2;
      const uint16_t *seqp = (const uint16_t *)p;
      while(seqlen > 0) {
        *sp = uni_int_utf16_decode(seqp, &clen);
	printf(":%04X", (int)*sp++);
	seqp += clen;
	seqlen -= clen;
      }
      clen = uni_cp_to_xml_name(seq, (int)(sp - seq), &has_more, &aliases, &buf, 0, &buf_len);
      printf(" %d %.*s\n", has_more, clen, buf);
      while(aliases) {
        clen = uni_cp_to_xml_name(seq, (int)(sp - seq), &has_more, &aliases, &buf, 0, &buf_len);
        uint32_t *sp2 = seq;
	printf("%04X", (int)*sp2++);
	while(sp2 < sp)
	  printf(":%04X", (int)*sp2++);
        printf(" %d &%.*s\n", has_more, clen, buf);
	nalen = *--p;
	if(!p[-1])
	  p--;
	seqlen = (nalen >> 6) + 1;
	nalen = (nalen & 0x3f) + 1;
	p -= nalen + seqlen * 2;
      }
    }
  }
  return 0;
}
@

As with the Unicode names, the primary expected usage of these names
is for looking up the code point(s) from the name, rather than the
other way around.  Again, it is expected that a program using the name
tables will dump a custom version of both forward and reverse lookups
as application-specific C code.

Converting the forward tables into reverse tables is much simpler than
with the Unicode names: the names are case-sensitive and only support
exact matching.  There are no synthesized names to worry about.  They
are also short enough that no special word encoding was done.  That
means the only work that needs to be done is to split multi-name code
points into separate entries.  In fact, the string tables can be
retained as-is; only the pointer arrays need adjusting.

The actual offsets and lengths are not available until the string
table has been dumped.  Instead, the offsets are set to the index in
the forward array shifted left 8 bits ored with the offset into that
entry's string.  The lengths are also the pure byte count rather than
the longword count as it currently stands in the forward array.

<<Post-process property data>>=
int prop_rev_na_xml = add_prop("rev_na_xml");
prop_t *rev_xna = &parsed_props[prop_rev_na_xml];
rev_xna->strs_name = "na_xml";
qsort(xna->str_arr, xna->len, sizeof(*xna->str_arr), uni_cmp_cp);
rev_xna->max_len = xna->len + xna_seq->len * 2;
inisize(rev_xna->str_arr, rev_xna->max_len);
for(i = 0; i < xna->len; i++) {
  const uint8_t *src = (const uint8_t *)(xna->strs + xna->str_arr[i].off);
  unsigned int srclen = xna->str_arr[i].len * 4;
  while(!src[srclen - 1])
    srclen--;
  unsigned int elen;
  for(low = 0; src[low] & 0x80; low += elen) {
    elen = (src[low++] & 0x7f) + 1;
    <<Add [[src[i]]] at [[low[elen]]] to [[rev_xna]]>>
  }
  elen = srclen - low;
  <<Add [[src[i]]] at [[low[elen]]] to [[rev_xna]]>>
}
@

<<Add [[src[i]]] at [[low[elen]]] to [[rev_xna]]>>=
check_size(rev_xna->str_arr, rev_xna->max_len, rev_xna->len + 1);
rev_xna->str_arr[rev_xna->len].cp = xna->str_arr[i].cp;
rev_xna->str_arr[rev_xna->len].off = (i << 8) + low;
rev_xna->str_arr[rev_xna->len].len = elen;
rev_xna->str_arr[rev_xna->len].flags = 0;
rev_xna->len++;
@

The name sequences must be split up as well.  As with the Unicode
names, high code points are used to indicate them, along with an
auxiliary table to convert those code points to the real sequences.
Since the high code point distinguishes these from regular names, the
offsets and lengths can be into the sequence property's string table
without conflict.

<<Post-process property data>>=
qsort(xna_seq->str_arr, xna_seq->len, sizeof(*xna_seq->str_arr), uni_cmp_cp);
int prop_na_xml_seq_id = add_prop("na_xml_seq_id");
prop_t *xna_seq_id = &parsed_props[prop_na_xml_seq_id];
inisize(xna_seq_id->str_arr, xna_seq_id->max_len = xna_seq->len * 2);
inisize(xna_seq_id->strs, xna_seq_id->max_strs = xna_seq->max_strs);
for(i = 0; i < xna_seq->len; i++) {
  uint8_t *src = (uint8_t *)(xna_seq->strs + xna_seq->str_arr[i].off);
  unsigned int srclen = xna_seq->str_arr[i].len * 4;
  while(!src[srclen - 1])
    srclen--;
  while(srclen > 0) {
    low = srclen - 1;
    unsigned int elen = src[low];
    unsigned int seqlen = (elen >> 6) + 1;
    elen = (elen & 0x3f) + 1;
    if(!src[low - 1])
      low--;
    low -= elen;
    <<Add [[src[i]]] at [[low[elen]]] to [[rev_xna]]>>
    <<Add [[src[i]]] sequence from [[xna_seq]] at [[low[seqlen]]] to [[xna_seq_id]]>>
    rev_xna->str_arr[rev_xna->len - 1].cp = xna_seq_id->len + UNI_MAX_CP;
    srclen = low - seqlen * 2;
  }
}
@

<<Dump character information as C code>>=
dump_str_ptr_arr(xna_seq_id, "XML entity code point sequence mapping", gen_h);
@

The only things left to do for this table are to sort by name and
correct the offsets.  The former needs to be done before the name
tables are mangled by dumping, and the latter needs to be done
afterwards.

<<Post-process property data>>=
sort_na = xna;
sort_na_seq = xna_seq;
qsort(rev_xna->str_arr, rev_xna->len, sizeof(*rev_xna->str_arr), 
      cmp_rev_xna);
@

<<UCD parser local functions>>=
static const prop_t *sort_na, *sort_na_seq;
static int cmp_rev_xna(const void *_a, const void *_b)
{
  const raw_cp_str_t *a = _a, *b = _b;
  const prop_t *a_na = a->cp > UNI_MAX_CP ? sort_na_seq : sort_na,
               *b_na = b->cp > UNI_MAX_CP ? sort_na_seq : sort_na;
  const uint8_t *astr = (const uint8_t *)(a_na->strs +
                                          a_na->str_arr[a->off >> 8].off) +
		        (a->off & 0xff),
                *bstr = (const uint8_t *)(b_na->strs +
                                          b_na->str_arr[b->off >> 8].off) +
		        (b->off & 0xff);
  unsigned int len = a->len > b->len ? b->len : a->len;
  int c = memcmp(astr, bstr, len);
  if(c)
    return c;
  if(a->len > b->len)
    return 1;
  else if(b->len > a->len)
    return -1;
  return 0;
}
@

<<Dump character information as C code>>=
for(i = 0; i < rev_xna->len; i++) {
  const prop_t *srcna = rev_xna->str_arr[i].cp > UNI_MAX_CP ? xna_seq : xna;
  uint32_t off = rev_xna->str_arr[i].off;
  rev_xna->str_arr[i].off = srcna->str_arr[off >> 8].off + (off & 0xff);
}
@


Now, the table can be dumped.

<<Dump character information as C code>>=
dump_str_arr(rev_xna, "Reverse lookup table for XML entity names", gen_h, NULL);
@

As with the Unicode names, a binary search function is provided,
mainly for illustrative purposes.  Since it is not actually meant to
be used, it is placed in a separate object file.

<<Unicode property exports>>=
/** Look up Unicode code point(s) for an XML entity name.
  * Look up \p name/\p len using binary, case-sensitive searching.  If
  * \p len is negative, \p name is zero-terminated.  If the return value
  * is negative, \p name does not match any known XML entity name.  If
  * the return value is greather than \ref UNI_MAX_CP,
  * it is the sequence \ref uni_na_xml_seq_id_arr[\e N - \ref UNI_MAX_CP - 1],
  * where \e N is the return value from this function.  */
int32_t uni_xml_name_to_cp(const char *name, int len);
@

<<Library [[uni]] Members>>=
xnarev_bin.o
@

<<xnarev_bin.c>>=
<<Common C Header>>
#include "uni_prop.h"

int32_t uni_xml_name_to_cp(const char *name, int len)
{
  unsigned int nlen = len < 0 ? strlen(name) : len;
  <<Look up XML name and return if present>>
  return -1;
}
@

Like with ranges, the next step is a binary search.  Unlike ranges, an
exact match is the only thing that matters.

<<Look up XML name and return if present>>=
int l = 0, h = uni_rev_na_xml_arr_len - 1, m;
while(l <= h) {
  m = (l + h) / 2;
  const uni_str_arr_t *retp = &uni_rev_na_xml_arr[m];
  const uint8_t *rev_na = retp->cp > UNI_MAX_CP ?
                                 uni_na_xml_seq_strs : uni_na_xml_strs;
  rev_na += retp->off;
  unsigned int rlen = retp->len;
  if(rlen > nlen)
    rlen = nlen;
  int c = memcmp(name, rev_na, rlen);
  if(!c)
    c = nlen > rlen ? 1 : retp->len > rlen ? -1 : 0;
  if(!c)
    return retp->cp;
  if(c > 0)
    l = m + 1;
  else
    h = m - 1;
}
@

Testing all of this can be difficult.  Instead of a thorough test, the
following program takes the output of [[tstcp_xml_na]] above, and
looks up the name.  If the looked up code points do not match the
advertised ones, an error is returned.

\lstset{language=make}
<<Additional Tests>>=
./tstcp_xml_na | ./tstxml_na_cp >/dev/null
@

<<C Test Support Executables>>=
tstxml_na_cp \
@

\lstset{language=C}
<<tstxml_na_cp.c>>=
<<Common C Header>>

#include "uni_prop.h"

char buf[256];

int main(void)
{
  int lno;
  for(lno = 0; fgets(buf, 256, stdin); lno++) {
    <<Set [[e]] to start of name to check>>
    uint32_t ret = uni_xml_name_to_cp(e, -1);
    <<Print and check [[name_to_cp]] results using [[na_xml_seq_id]]>>
  }
  fprintf(stderr, "Successfully looked up %d entries\n", lno);
  return 0;
}
@

While the number of strings is much lower than with the Unicode names,
a hash implementation using [[cmph]] is provided as well.

\lstset{language=make}
<<Library [[uni_cmph]] Members>>=
cmph_rev_xna.o
cmph_na_xml_supt.o
cmph_na_xml_xlate.gen.o
@

<<Libraries to Install>>=
$(if $(CMPH),uni_cmph) \
@

<<makefile.rules>>=
ifneq ($(CMPH),)
cmph_in_na_xml.gen: cmph_in_na_xml_gen
	./cmph_in_na_xml_gen >$@

cmph_in_na_xml.gen.mph: cmph_in_na_xml.gen
	$(CMPH) -g $(CMPH_GEN) cmph_in_na_xml.gen

cmph_rev_xna.c: cmph_in_na_xml.gen.mph
	$(CMPH) -C uni_rev_na_xml_mph -o $@ cmph_in_na_xml.gen
cmph_na_xml_xlate.gen.c: cmph_in_na_xml.gen.mph
	$(call gen_xlate,na_xml) >$@
else
cmph_rev_xna.c: uni_prop.gen.h
	echo >$@
endif
@

<<Clean temporary files>>=
rm -f cmph_in_na_xml.gen{,.mph} cmph_na_xml_xlate.gen.c
@

\lstset{language=C}
<<Unicode property exports using [[cmph]]>>=
/** Pre-generated cmph perfect hash table for looking up XML entity names */
extern const cmph_t uni_rev_na_xml_mph;
/** Translate return from cmph XML entity name lookup return to a
  * \ref uni_rev_na_xml_arr array index.  If the return value is ~0, the
  * lookup failed.  */
extern const uint32_t uni_rev_na_xml_mph_xlate[];
/** The maximum valid value returned by a cmph XML entity name lookup. */
extern const uint32_t uni_rev_na_xml_max_mph_res;
@

\lstset{language=C}
<<cmph_na_xml_supt.c>>=
#ifdef USE_CMPH
<<Common C Header>>

#include "uni_prop.h"

<<[[cmph]] XML name lookup support>>
#endif
@

Once again, the reverse name list used for binary searching is also
used to generate names for [[cmph]].

<<C Build Executables>>=
cmph_in_na_xml_gen \
@

<<makefile.rules>>=
cmph_in_na_xml_gen: libuni.a cmph_in_na_xml_gen.o
	$(CC) -o $@ cmph_in_na_xml_gen.o -L. -luni
@

<<cmph_in_na_xml_gen.c>>=
<<Common C Header>>

#include "uni_prop.h"

int main(void)
{
  unsigned int i;
  
  for(i = 0; i < uni_rev_na_xml_arr_len; i++) {
    const uint8_t *p = uni_rev_na_xml_arr[i].cp > UNI_MAX_CP ?
                          uni_na_xml_seq_strs : uni_na_xml_strs;
    p += uni_rev_na_xml_arr[i].off;
    printf("%.*s\n", (unsigned int)uni_rev_na_xml_arr[i].len, (char *)p);
  }
  return 0;
}
@

The lookup function only does primary lookup, so it is completely
different from the binary search version.

<<Unicode property exports using [[cmph]]>>=
/** Look up Unicode code point(s) for an XML entity name.
  * Look up \p name/\p len using case-sensitive search with a cmph perfect
  * hash table.  If \p len is negative, \p name is zero-terminated.  If
  * the return value is negative, \p name does not match any known XML
  * entity name.  If the return value is greather than \ref UNI_MAX_CP,
  * it is the sequence \ref uni_na_xml_seq_id_arr[\e N - \ref UNI_MAX_CP - 1],
  * where \e N is the return value from this function.  */
int32_t uni_cmph_xml_name_to_cp(const char *name, int len);
@

<<[[cmph]] XML name lookup support>>=
int32_t uni_cmph_xml_name_to_cp(const char *name, int len)
{
  unsigned int nlen = len < 0 ? strlen(name) : len;
  <<Look up XML name using [[cmph]] and return if present>>
  return -1;
}
@

The primary name lookup requires a hash lookup, followed by an
explicit comparison.

<<Look up XML name using [[cmph]] and return if present>>=
uint32_t ret = cmph_search((cmph_t *)&uni_rev_na_xml_mph, name, nlen);
#if defined(CMPH_SPARSE) && defined(CMPH_XLATE)
if(ret < uni_rev_na_xml_max_mph_res)
  ret = uni_rev_na_xml_mph_xlate[ret];
#endif
if(ret < uni_rev_na_xml_arr_len) {
#if defined(CMPH_XLATE) && !defined(CMPH_SPARSE)
  ret = uni_rev_na_xml_mph_xlate[ret];
#endif
  const uni_str_arr_t *retp = &uni_rev_na_xml_arr[ret];
  if(nlen == retp->len) {
    const uint8_t *rev_na = retp->cp > UNI_MAX_CP ?
                                     uni_na_xml_seq_strs : uni_na_xml_strs;
    rev_na += retp->off;
    if(!memcmp(name, rev_na, nlen))
      return retp->cp;
  }
}
@

Testing this is mostly identical to testing the binary search
routines, but timings are also taken to verify that the hash is
actually faster.

\lstset{language=make}
<<Additional Tests>>=
$(if $(CMPH),./tstcp_xml_na | ./tstxml_na_cp_cmph >/dev/null)
@

<<C Test Support Executables>>=
$(if $(CMPH),tstxml_na_cp_cmph) \
@

<<makefile.rules>>=
tstxml_na_cp_cmph: LDFLAGS += $(if $(CMPH_LDFLAGS),$(CMPH_LDFLAGS),-lcmph)
@

\lstset{language=C}
<<tstxml_na_cp_cmph.c>>=
#ifdef USE_CMPH
<<Common C Header>>

#include "uni_prop.h"

<<POSIX timing support>>

char buf[256];

int main(void)
{
  double tb = 0, th = 0;
  unsigned long nn = 0;
  while(fgets(buf, 256, stdin)) {
    <<Set [[e]] to start of name to check>>
    uint32_t ret = uni_cmph_xml_name_to_cp(e, -1);
    <<Print and check [[name_to_cp]] results using [[na_xml_seq_id]]>>
    nn++;
    int j;
    tstart();
    for(j = 0; j < 25; j++)
      ret = uni_xml_name_to_cp(e, -1);
    tb += tend();
    tstart();
    for(j = 0; j < 25; j++)
      ret = uni_cmph_xml_name_to_cp(e, -1);
    th += tend();
  }
  /* report as ops per second rather than total seconds */
  tb = nn * 25000000.0 / tb;
  th = nn * 25000000.0 / th;
  fprintf(stderr, "b%.0f/s h%.0f/s %.2fx\n", tb, th, th/tb);
  return 0;
}
#endif
@

There may be better tuning options, but my experiments show that the
hash function is not, in fact, faster at all for any algorithms but
one: bdz\_ph.  The best performance I got, again with bdz\_ph with the
lowest possible [[-b]] setting, was a whole 6\% faster.  This just
goes to show that the computation of the hash function is far too
expensive.

\section{The CLDR}

The CLDR and the ICU library go hand-in-hand; the CLDR is basically
the ICU data files somewhat sloppily extracted from ICU.  It may be
best to not support the CLDR data at all, but instead use ICU to query
it.  A few support functions could be provided to use it along with
the UCD (but then again, ICU has all of the UCD as well, so why bother
with libuni at all?).  However, like the POSIX locale functions, the
ICU functions do not provide a way to query the information needed for
a compliant regular expression engine.  It is also a fairly large
library, and I am unsure it does a good job of allowing minimal
linkage.

The CLDR also uses XML rather than simple tables.  Doing hand-coded
XSLT for hundreds of data files seemed excessive, which is the main
reason why I abandoned XSLT.  Note that unlike for the XML entity
data, there is a local, valid DTD for every CLDR file.

\lstset{language=make}
<<makefile.vars>>=
PARSER_CFLAGS += -DCLDR_LOC=\"$(CLDR_LOC)\"
@

\lstset{language=C}
<<XML Support Definitions>>=
#define xml_opts_dtd xml_opts | \
		     XML_PARSE_DTDLOAD | XML_PARSE_DTDATTR | \
		     XML_PARSE_DTDVALID /* use DTD if on local disk */
@

<<Parse character data files>>=
force_chdir(cwd);
force_chdir(CLDR_LOC);
<<Parse CLDR files>>
@

The CLDR is, like most XML data, insanely complex and at times poorly
documented%
\footnote{XML was promoted as a self-documenting data format.  The DTD
documents the format not quite as well as a BNF grammar documents a
programming language:  that is, not even well enough to get the syntax
completely right (complex conditions cannot be expressed in BNF or
DTD, so there are often exceptions written in the supplementary text).
Like all data formats, XML needs supplementary documentation.  The
CLDR has such documentation, but it is of medium-to-poor quality.}%
.  UTS~\#35 is the primary documentation, although some clarification
can be obtained by reading the data itself.  Much of its functionality
also has dubious value for general projects.  As such, the only CLDR
data provided is that which supports localization of UCD data (such as
breaking and collation rules that are supposed to be localized), plus
one property that is required for regular expressions (the exemplar
character set).

The first thing necessary for localization is to determine what locale
is being used.  The locale name consists of a language identifier,
followed by optional extensions.  The language identifier consists of
a language subtag, followed by an optional script subtag, followed by
an optional territory subtag, followed by optional variant subtags,
followed by optional extensions.  These are all case-insensitive, and
are separated by either hyphens or underscores.  The supported
language identifier tags are listed in the supplemental metadata.

<<Parse CLDR files>>=
doc = xmlReadFile("common/supplemental/supplementalMetadata.xml",
                  NULL, xml_opts_dtd);
if(!doc) {
  perror("supplementalMetadata.xml");
  exit(1);
}
int lang = -1, scr = -1, ter = -1, var = -1;
for(n = doc->children; n; n = n->next) {
  if(n->children && xml_isname(n, "supplementalData")) {
    for(n = n->children; n; n = n->next) {
      /* ignore version, generation, cldrVersion */
      if(!n->children || !xml_isname(n, "metadata"))
        continue;
      for(n = n->children; n; n = n->next) {
        if(!n->children)
	  continue;
        <<Parse CLDR supplemental metadata>>
      }
      break;
    }
    break;
  }
}
xmlFreeDoc(doc);
@

The data we're looking for is under the validity tag.  Each set is
listed as a ``variable'' whose value is the whitespace-separated list
of names.  Technically, their definition in [[attributeValues]] is
used, but for those which have large lists, the value is just an
expansion of one of these variables.  In fact, one could make an
argument that all of the variables of type choice are useful to add as
enumerations.  The main argument against this is that some of the
lists are rather long, and the enumerations' use of binary searching
(rather than, say, hash tables) may be slow.  It also violates my
mandate of only including UCD localization information.

Since matching is case-insensitive, and the capitalized name is not
used anywhere (and can be capitalized according to canonical
capitialization if desired), all names are converted to lower-case
while adding.

<<Parse CLDR supplemental metadata>>=
else if(xml_isname(n, "validity")) {
  for(c = n->children; c; c = c->next) {
    if(c->type != XML_ELEMENT_NODE)
      continue;
    <<Parse CLDR validity entries>>
  }
}
@

<<Parse CLDR validity entries>>=
else if(xml_isname(c, "variable") || xml_isname(c, "attributeValues")) {
  char *t = xml_prop(c, "type");
  if(t && !strcmp(t, "choice")) {
    char *v = xml_prop(c, "id");
    if(!v)
      v = xml_prop(c, "elements");
    if(!c->children || !c->children->content || !v || /* should never happpen */
       /* I guess linear search is good enough here */
       (strcmp(v, "$language") &&
        strcmp(v, "$territory") &&
        strcmp(v, "$script") &&
        strcmp(v, "$variant"))) {
      xmlprop_free(v);
      xmlprop_free(t);
      continue;
    }
    char *name;
    inisize(name, strlen(v) + 5 + 1);
    memcpy(name, "CLDR_", 5);
    strcpy(name + 5, v + 1);
    xmlprop_free(v);
    int pno = add_prop(name);
    free(name);
    uint32_t str[64];
    for(s = (char *)c->children->content; isspace(*s); s++);
    for(i = 0; *s; i++) {
      char *p = (char *)str;
      while(!isspace(*s) && *s)
        *p++ = tolower(*s++);
      while((p - (char *)str) % 4)
        *p++ = 0;
      add_str_rng(&parsed_props[pno], i, i, str, (p - (char *)str) / 4);
      while(isspace(*s))
        s++;
    }
    str_to_enum(&parsed_props[pno], NULL);
    free(parsed_props[pno].rng_dat8);
    parsed_props[pno].rng_dat8 = NULL;
  }
  xmlprop_free(t);
}
@

<<Ignore unimplemented enums>>=
/* ignore auto-generated enums for obsolete properties */
if(!parsed_props[i].rng_dat8 && i < num_prop_aliases)
  continue;
@

In addition, some ISO language codes are treated differently within
Unicode and the CLDR.  These translations are under the alias tag, in
languageAlias elements. Similarly, script, territory, and variants may
have aliases as well.  In order to process the aliases, the lists
generated from the validity tag need to already be finished.  The DTD
places the alias tag after the validity tag, so it is probably safe to
assume this is true.  A check is made just in case it isn't, as that
will require rewriting this code into a separate loop.

<<Parse CLDR supplemental metadata>>=
else if(xml_isname(n, "alias")) {
  lang = add_prop("CLDR_language");
  scr = add_prop("CLDR_script");
  ter = add_prop("CLDR_territory");
  var = add_prop("CLDR_variant");
  if(!num_val_aliases[lang] || !num_val_aliases[scr] ||
     !num_val_aliases[ter] || !num_val_aliases[var]) {
    fputs("variable defs do not precede aliases!\n", stderr);
    exit(1);
  }
  for(c = n->children; c; c = c->next) {
    if(c->type != XML_ELEMENT_NODE)
      continue;
    <<Parse CLDR bcp47 alias entries>>
  }
}
@

All properties are mostly handled the same way, so a function is used
for this.  The main difference is that languages may include scripts
or regions, and territories have special behavior with multiple
replacements.

<<Parse CLDR bcp47 alias entries>>=
else if(xml_isname(c, "languageAlias"))
  do_attr_alias(c, lang, scr, ter);
else if(xml_isname(c, "scriptAlias"))
  do_attr_alias(c, scr, 0, 0);
else if(xml_isname(c, "territoryAlias"))
  do_attr_alias(c, ter, 0, 0);
else if(xml_isname(c, "variantAlias"))
  do_attr_alias(c, var, 0, 0);
@

The easy way to do this would be to look up the value to be aliased
([[from]]) and the target ([[to]]) and merge them.  However, this
assumes simple aliases, which is not always true.  Some names are used
by ISO locale names only, and are translated to Unicode locale names;
these require adding [[from]] when it does not exist in the table.
Some language [[from]] values also include region or variant codes.
Presumably this is meant to only apply when those regions or variants
are explicitly selected; this is not really documented in UTS~\#35.
In order to support this, the [[from]] name is added with the
additional information.  This means that the approximate matching
table is worthless; all lookups must be made using the (already
case-folded) plain [[valueof]] table.

Some territory [[to]] values are lists; these must be handled based on
other available information, as documented in UTS~\#35.  Some language
[[to]] targets include script and/or territory tags; these tags are
added to the result if not explicitly overridden (again, as documented
in UTS~\#35).  It would probably be easiest to just add them as raw
names, which would require an additional check after lookup to ensure
that there are no spaces or underscores in the final name (there are
never dashes, at least in the current CLDR).  Since they are never to
be looked up, they could also be encoded in a way that makes it easier
to use.  The first case can be encoded as a utf8-encoded number of
list elements, followed by the looked-up code for each element (plus
one to ensure no zeroes result).  As long as there are fewer than 48
elements, there will never be a successful lookup.  The second case
can be encoded by a byte indicating the presence of script, territory,
or both, followed by the utf8-encoded language code, the utf8-encoded
script code (if present), and the utf8-encoded territory code (if
present).  Both of these encode indices into the enumeration array,
but enumeration array elements are dropped when they are alias
sources, so the actual translation is done in a separate pass, after
those indicies are stable.

A few variants have issues.  AALAND translates to AX, but that is a
territory, not a variant.  POSIX translates to [[u-va-posix]], which
is a locale extension, not a variant.  Both of these issues need to be
handled manually in the locale parser.

Some targets have unexplained issues, though.  Some simple targets do
not exist (e.g. ami for the i-ami to ami alias); I'll probably just
treat them like any other unknown locale.  In earlier CLDR revisions,
some had a blank target; I assume this was meant to delete them (later
revisions did, in fact, delete them).  All of these should probably
just be treated like any other unknown value, except that some of them
are used in the [[from]] text of aliases.

One additional problem presented by these aliases is that there are
sometimes more than three aliases for a name.  The alias structure
developed above only supports three aliases.  Aliases are meant to be
strict translations without retaining the original value, so there is
no point in even keeping the aliases.  They just clutter up the
namespace due to enumeration constants being created for each one.
Instead, the strings are just added directly to the [[val_aliases]]
array.

<<UCD parser local functions>>=
static uni_valueof_t *add_enum_lit(int prop, const char *s)
{
  int l, h, m, c;
  for(l = 0, h = enum_vals_len[prop] - 1; l <= h; ) {
    m = (l + h) / 2;
    c = strcmp(s, enum_vals[prop][m].name);
    if(c > 0)
      l = m + 1;
    else if(c < 0)
      h = m - 1;
    else
      return &enum_vals[prop][m]; /* happens when adding aliases */
  }
  /* growing enum_vals 1 value at a time is slow, inefficient */
  /* so don't manually add to enum array too often */
  if(enum_vals_len[prop]) {
    resize(enum_vals[prop], ++enum_vals_len[prop]);
    if(enum_vals_len[prop] - 1 != l)
      movebuf(enum_vals[prop] + l + 1, enum_vals[prop] + l, enum_vals_len[prop] - 1 - l);
  } else
    inisize(enum_vals[prop], enum_vals_len[prop] = 1);
  uni_valueof_t *ret = &enum_vals[prop][l];
  ret->name = strdup(s);
  ret->val = num_val_aliases[prop];
  check_size(val_aliases[prop], max_val_aliases[prop], num_val_aliases[prop] + 1);
  num_val_aliases[prop]++;
  clearbuf(&val_aliases[prop][ret->val], 1);
  val_aliases[prop][ret->val].short_name = 
    val_aliases[prop][ret->val].long_name = ret->name;
  return ret;
}
@

<<UCD parser local functions>>=
static void do_attr_alias(xmlNodePtr alias, int prop, int scr, int ter)
{
  char *from = xml_prop(alias, "type"), *to = xml_prop(alias, "replacement"), *s;
  uni_valueof_t me, *fp, *tp;
  int tval;
  /* don't care about reason */
  /* some aliases have no to.  just ignore. */
  /* deprecated names have moved to deprecated/deprecatedItems in CLDR 27 */
  if(!to) {
    /* this doesn't happen in CLDR 27 */
    fprintf(stderr, "Invalid %s alias %s (no target)\n",
                    parsed_props[prop].name, from);
    xmlprop_free(from);
    return;
  }
  for(s = to; *s; s++) {
    if(*s == '-')
      *s = '_';
    else
      *s = tolower(*s);
  }
  me.name = to;
  tp = bsearch(&me, enum_vals[prop], enum_vals_len[prop], sizeof(me),
               uni_cmp_valueof);
  if(!tp) {
    /* this has been observed in following conditions: */
    /*  - target language ID has non-language tags (e.g. sr_Latn) */
    /*    observed values are always scr/ter, never var */
    if((scr || ter) && (s = strchr(to, '_'))) {
      uint32_t sc = ~0, tc = ~0;
      /* if lookup fails, ignore, but spit out message */
      *s = 0;
      tp = bsearch(&me, enum_vals[prop], enum_vals_len[prop], sizeof(me),
                   uni_cmp_valueof);
      if(!tp) {
        /* this doesn't happen in CLDR 27 */
        fprintf(stderr, "Unknown target for %s alias %s->*%s*_%s\n",
	                parsed_props[prop].name, from, to, s + 1);
        xmlprop_free(from);
	xmlprop_free(to);
	return;
      }
      *s++ = '_';
      char *n = strchr(s, '_');
      if(n)
        *n = 0;
      me.name = s;
      uni_valueof_t *sub;
      sub = bsearch(&me, enum_vals[scr], enum_vals_len[scr], sizeof(me),
	            uni_cmp_valueof);
      if(sub) {
        sc = sub->val;
        if(n) {
	  *n++ = '_';
	  me.name = n;
	  sub = bsearch(&me, enum_vals[ter], enum_vals_len[ter], sizeof(me),
	                uni_cmp_valueof);
          if(sub)
	    tc = sub->val;
        }
      } else if(!n) {
	sub = bsearch(&me, enum_vals[ter], enum_vals_len[ter], sizeof(me),
	              uni_cmp_valueof);
        if(sub)
	  tc = sub->val;
      } else
        *n = '_';
      if(!sub) {
        /* this doesn't happen in CLDR 27 */
        fprintf(stderr, "Unknown target subtag for %s alias %s->%s\n",
	                parsed_props[prop].name, from, to);
        xmlprop_free(from);
	xmlprop_free(to);
	return;
      }
      uint8_t buf[1 + strlen(to) + 1];
      /* setting buf[0] forces sorting to bottom, at least */
      buf[0] = sc == (uint32_t)~0 ? 2 : tc == (uint32_t)~0 ? 1 : 3;
      strcpy((char *)buf + 1, to);
      tp = add_enum_lit(prop, (char *)buf);
    /*  - target territory is a space-separated list */
    } else if((s = strchr(to, ' '))) {
      uint8_t buf[strlen(to) + 2];
      buf[0] = 0;
      char *n = (char *)buf + 1;
      strcpy(n, to);
      s = (s - to) + n;
      *s = 0;
      while(1) {
	uni_valueof_t *sub;
	me.name = n;
	sub = bsearch(&me, enum_vals[prop], enum_vals_len[prop], sizeof(me),
	              uni_cmp_valueof);
        if(!sub)
	/* if lookup fails, spit out message, and drop failed lookup */
	  /* this doesn't happen in CLDR 27 */
	  fprintf(stderr, "Unknown list element %s for %s alias %s->%s %s\n",
	                  n, parsed_props[prop].name, from, to, s ? s + 1 : "");
        ++buf[0]; /* actually, don't drop until 2nd lookup below */
        if(s) {
	  *s++ = ' ';
	  n = s;
	  s = strchr(s, ' ');
	  if(s)
	    *s = 0;
        } else
	  break;
      }
      /* again, setting buf[0] above forces sorting to bottom, at least */
      tp = add_enum_lit(prop, (char *)buf);
    /*  - target language or whatever does not exist (e.g. ami) */
    /* in CLDR 27, these are: 1 variant, 12 languages, 76 territories */
    } else {
      /* ignore these, but spit out a message */
      fprintf(stderr, "Unknown target for %s alias %s->%s\n",
	              parsed_props[prop].name, from, to);
      xmlprop_free(from);
      xmlprop_free(to);
      return;
    }
  }
  tval = tp->val;
  for(s = from; *s; s++) {
    if(*s == '-')
      *s = '_';
    else
      *s = tolower(*s);
  }
  me.name = from;
  fp = bsearch(&me, enum_vals[prop], enum_vals_len[prop], sizeof(me),
               uni_cmp_valueof);
  if(!fp) {
    /* some aliases are not listed in any original data */
    /* so auto-add them */
    fp = add_enum_lit(prop, from);
    /* val_aliases[prop][fp->val] will just be deleted below */
  }
  /* note:  this isn't entirely safe if data is corrupt */
  uni_alias_t *fa = &val_aliases[prop][fp->val];
  if(fp->val == tval)
    fprintf(stderr, "duplicate alias %s -> %s\n", from, to);
  else {
    num_val_aliases[prop]--;
    /* at this point, could add fa to ta, but then too many aliases */
    /* so fa will not appear in nameof[], but will still be in valueof[] */
    if(fp->name != fa->short_name && !strcmp(fa->short_name, fp->name)) {
      if(fp->name != fa->short_name)
        free((char *)fa->short_name);
      if(fa->long_name != fa->short_name)
        free((char *)fa->long_name);
    }
    if(num_val_aliases[prop] > fp->val)
      movebuf(fa, fa + 1, num_val_aliases[prop] - fp->val);
    int i, oval = fp->val;
    if(tval > oval)
      --tval;
    for(i = 0; i < enum_vals_len[prop]; i++) {
      if(enum_vals[prop][i].val > oval)
        enum_vals[prop][i].val--;
      else if(enum_vals[prop][i].val == oval)
        enum_vals[prop][i].val = tval;
    }
  }
  xmlprop_free(from);
  xmlprop_free(to);
}
@

<<parse-ucd.c>>=
<<Encode code point as UTF-8>>
@

<<Parse CLDR files>>=
for(i = 0; i < enum_vals_len[lang]; i++) {
  if(enum_vals[lang][i].name[0] <= 3) {
    <<Adjust multi-element language alias target [[i]]>>
  } else {
    qsort(enum_vals[lang], i, sizeof(uni_valueof_t), uni_cmp_valueof);
    break;
  }
}
for(i = 0; i < enum_vals_len[ter]; i++) {
  if(enum_vals[ter][i].name[0] < '0') {
    <<Adjust multi-item territory alias target [[i]]>>
  } else {
    qsort(enum_vals[ter], i, sizeof(uni_valueof_t), uni_cmp_valueof);
    break;
  }
}
@

<<Adjust multi-element language alias target [[i]]>>=
char *to = (char *)enum_vals[lang][i].name + 1;
s = strchr(to, '_');
*s = 0;
uint32_t lc, sc = ~0, tc = ~0;
uni_valueof_t me, *tp;
me.name = to;
tp = bsearch(&me, enum_vals[lang], enum_vals_len[lang], sizeof(me),
             uni_cmp_valueof);
lc = tp->val;
char *nxt = strchr(++s, '_');
if(nxt)
  *nxt++ = 0;
me.name = s;
if(to[-1] & 1) {
  tp = bsearch(&me, enum_vals[scr], enum_vals_len[scr], sizeof(me),
	       uni_cmp_valueof);
  sc = tp->val;
  me.name = nxt;
}
if(to[-1] & 2) {
  tp = bsearch(&me, enum_vals[ter], enum_vals_len[ter], sizeof(me),
	       uni_cmp_valueof);
  tc = tp->val;
}
uint8_t buf[1 + uni_utf8_enclen(lc + 1) + uni_utf8_enclen(sc + 1) +
            uni_utf8_enclen(tc + 1) + 1], *bp = buf;
*bp++ = to[-1];
bp += uni_utf8_encode(bp, lc + 1);
if(sc != (uint32_t)~0)
  bp += uni_utf8_encode(bp, sc + 1);
if(tc != (uint32_t)~0)
  bp += uni_utf8_encode(bp, tc + 1);
*bp = 0;
free(to - 1);
enum_vals[lang][i].name = strdup((char *)buf);
val_aliases[lang][enum_vals[lang][i].val].short_name =
  val_aliases[lang][enum_vals[lang][i].val].long_name = enum_vals[lang][i].name;
@

<<Adjust multi-item territory alias target [[i]]>>=
/* adjust multi-territory targets */
/* territory tags are 2 bytes or more, but territory codes are */
/* never more than 2 bytes, so buffer can be same len as before */
/* in other words, rather than build after in a buffer, this is built in place */
uint8_t *buf = (uint8_t *)enum_vals[ter][i].name, *bp = buf + 1;
buf[0] = 0;
char *nxt = (char *)buf + 1;
s = strchr(nxt, ' ');
*s = 0;
while(1) {
  uni_valueof_t me, *tp;
  me.name = nxt;
  tp = bsearch(&me, enum_vals[ter], enum_vals_len[ter], sizeof(me),
	        uni_cmp_valueof);
  if(tp) {
    ++buf[0];
    bp += uni_utf8_encode(bp, tp->val + 1);
  }
  if(s) {
    nxt = ++s;
    s = strchr(s, ' ');
    if(s)
      *s = 0;
  } else
    break;
}
*bp = 0;
@

\lstset{language=txt}
<<FIXME>>=
Following translations ae not covered above, but do they really matter?
translated private use territory codes:
  AA -> 958 -> AAA
  QM..QZ -> 959..972 -> QMM..QZZ
  XA..XZ -> 973..998 -> XAA..XZZ
translated private use script codes:
  Qaaa..Qabx -> 900..949
@

In order to deal with territory lists, the ``most likely'' territory
must be selected.  It appears that the ``most likely'' designator can
be obtained from the likelySubtag supplemental data.  This is stored
as a simple mapping from 32-bit values to 32-bit values.  The values
are a merged triplet of language, territory, and script.  Since there
are 606 languages in CLDR-27, language codes occupy 10 bits.
Similarly, 286 territories occupy 9 bits, and 213 scripts occupy 8
bits.  The 70 variants need to go into a different word, but that
doesn't matter for this table.  If any of the above is actually not
present, it is replaced by an invalid code.  It would be easiest to
just use the maximum count, but that sorts things oddly.  It is
actually better to store the enumeration plus one and use zero to mean
it is absent.

\lstset{language=C}
<<Locale triplet support macros>>=
#define uni_locale_triplet(l, s, t) \
  (((l) >= UNI_NUM_CLDR_language ? 0 : ((l) + 1) << 17) + \
   ((s) >= UNI_NUM_CLDR_script ? 0 : ((s) + 1) << 9) + \
   ((t) >= UNI_NUM_CLDR_territory ? 0 : (t) + 1))
#define uni_locale_check_tripletid_(x, max) ((x) ? (x) - 1 : (max))
#define uni_locale_extract_language(x) \
   uni_locale_check_tripletid_(((x) >> 17) & 0x3ff, UNI_NUM_CLDR_language)
#define uni_locale_extract_script(x) \
   uni_locale_check_tripletid_(((x) >> 9) & 0xff, UNI_NUM_CLDR_script)
#define uni_locale_extract_territory(x) \
   uni_locale_check_tripletid_((x) & 0x1ff, UNI_NUM_CLDR_territory)
#define uni_locale_set_language(t, l) \
   uni_locale_triplet(l, uni_locale_extract_script(t), \
                      uni_locale_extract_territory(t))
#define uni_locale_set_script(t, s) \
   uni_locale_triplet(uni_locale_extract_language(t), s, \
                      uni_locale_extract_territory(t))
#define uni_locale_set_territory(t, r) \
   uni_locale_triplet(uni_locale_extract_language(t), \
                      uni_locale_extract_script(t), r)
@

<<Library [[uni]] headers>>=
#include "uni_locale.h"
@

<<uni_locale.h>>=
<<Common C Warning>>
#ifndef UNI_LOCALE_H
#define UNI_LOCALE_H
/** \file uni_locale.h Unicode Locale Support */

#include "uni_prop.h"
/** \addtogroup uni_locale Unicode Locale Utilities
    @{ */
<<Unicode locale exports>>
/** @} */
#endif /* UNI_LOCALE_H */
@

<<Headers to Install>>=
uni/uni_locale.h \
@

<<Unicode locale exports>>=
/** Convert separate language, script and territory to triplet word.
  * Generates triplet word from \p l, \p s and \p t into locale triplet.
  * This is a preprocessor macro.  */
uint32_t uni_locale_triplet(uni_CLDR_language_t l, uni_CLDR_script_t s,
                            uni_CLDR_territory_t t);
/** Extract language from locale triplet word.
  * This is a preprocessor macro.  */
uni_CLDR_language_t uni_locale_extract_language(uint32_t x);
/** Extract script from locale triplet word.
  * This is a preprocessor macro.  */
uni_CLDR_script_t uni_locale_extract_script(uint32_t x);
/** Extract territory from locale triplet word.
  * This is a preprocessor macro.  */
uni_CLDR_territory_t uni_locale_extract_territory(uint32_t x);
/** Replace language component of locale triplet word.
  * This is a preprocessor macro.  */
uint32_t uni_locale_set_language(uint32_t t, uni_CLDR_language_t l);
/** Replace script component of locale triplet word.
  * This is a preprocessor macro.  */
uint32_t uni_locale_set_script(uint32_t t, uni_CLDR_script_t s);
/** Replace territory component of locale triplet word.
  * This is a preprocessor macro.  */
uint32_t uni_locale_set_territory(uint32_t t, uni_CLDR_territory_t r);
<<Locale triplet support macros>>
@

<<UCD parser local functions>>=
/* fake numbers to make macros work */
#define UNI_NUM_CLDR_language ~0
#define UNI_NUM_CLDR_script ~0
#define UNI_NUM_CLDR_territory ~0
<<Locale triplet support macros>>
@

<<UCD parser local functions>>=
static int parse_locale_triplet(const char *txt)
{
  static int lang = -1, scr = -1, ter = -1;
  if(lang < 0) {
    lang = add_prop("CLDR_language");
    scr = add_prop("CLDR_script");
    ter = add_prop("CLDR_territory");
    if(lang < 0 || scr < 0 || ter < 0)
      exit(1);
  }
  int l = 0, sc = 0, t = 0;
  char *s = strdup(txt), *p;
  for(p = s; *p; p++) {
    if(*p == '-')
      *p = '_';
    else
      *p = tolower(*p);
  }
  p = strchr(s, '_');
  if(p)
    *p = 0;
  uni_valueof_t *v, me;
  me.name = s;
  v = bsearch(&me, enum_vals[lang], enum_vals_len[lang], sizeof(me),
              uni_cmp_valueof);
  if(!v) {
#if 0 /* too many of 'em, so just ignore */
    fprintf(stderr, "Unknown language %s\n", me.name);
#endif
#if 1 /* this is no time to be introducing new names */
    free(s);
    return 0;
#endif
  } else
    l = v->val + 1;
  if(p) {
    me.name = ++p;
    p = strchr(p, '_');
    if(p)
      *p = 0;
    v = bsearch(&me, enum_vals[scr], enum_vals_len[scr], sizeof(me),
                uni_cmp_valueof);
    if(v) {
      sc = v->val + 1;
      if(p)
	me.name = ++p;
    } else if(p) {
      fprintf(stderr, "Unknown script %s\n", me.name);
#if 1 /* this is no time to be introducing new names */
      free(s);
      return 0;
#else
      me.name = ++p;
#endif
    } else
      p = s; /* non-NULL; re-use name used for script check */
  }
  if(p) {
    v = bsearch(&me, enum_vals[ter], enum_vals_len[ter], sizeof(me),
	        uni_cmp_valueof);
    if(!v) {
      fprintf(stderr, "Unknown territory %s\n", me.name);
#if 1 /* this is no time to be introducing new names */
      free(s);
      return 0;
#endif
    } else
      t = v->val + 1;
  }
  free(s);
  return uni_locale_triplet(l, sc, t);
}
@

<<Parse CLDR files>>=
uni_cp_val_t *likely = NULL;
int num_likely = 0;
doc = xmlReadFile("common/supplemental/likelySubtags.xml", NULL, xml_opts_dtd);
if(!doc) {
  perror("likelySubtags.xml");
  exit(1);
}
for(n = doc->children; n; n = n->next) {
  if(n->children && xml_isname(n, "supplementalData")) {
    for(n = n->children; n; n = n->next) {
      /* ignore version, generation, cldrVersion */
      if(!n->children || !xml_isname(n, "likelySubtags"))
        continue;
      for(n = n->children; n; n = n->next) {
        if(!xml_isname(n, "likelySubtag"))
	  continue;
        char *from = xml_prop(n, "from");
	char *to = xml_prop(n, "to");
	int ft = parse_locale_triplet(from), tt = parse_locale_triplet(to);
	if(ft && tt) { /* this is no time to be introducing new names */
	  grow_size(likely, num_likely, num_likely + 1);
	  likely[num_likely - 1].cp = ft;
	  likely[num_likely - 1].val = tt;
        }
	xmlprop_free(from);
	xmlprop_free(to);
      }
      break;
    }
    break;
  }
}
xmlFreeDoc(doc);
@

<<Dump character information as C code>>=
{
  qsort(likely, num_likely, sizeof(*likely), uni_cmp_cp);
  open_wf(of, "uni_CLDR_likely_subtags.gen.c");
  fputs("#include \"uni_locale.h\"\n\n"
        "const uni_cp_val_t uni_CLDR_likely_subtags[] = {\n", of);
  for(i = 0; i < num_likely; i++) {
    fprintf(of, "\t{ 0x%08X, 0x%08X }", likely[i].cp, likely[i].val);
    if(i < num_likely - 1)
      putc(',', of);
    putc('\n', of);
  }
  fputs("};\n", of);
  fclose(of);
  /* FIXME: move to a locale-specific header */
  fprintf(gen_h, "/** Length of \\ref uni_CLDR_likely_subtags */"
		 "#define uni_CLDR_likely_subtags_len %d\n", num_likely);
}
@

<<Unicode locale exports>>=
/** An array mapping partial locale triplets to more complete ones.
  * This array uses the \p cp field as the partial triplet, and the
  * \p val field as the complete one.  It is sorted by the \p cp
  * field.  */
extern const uni_cp_val_t uni_CLDR_likely_subtags[];
@

<<Library [[uni]] Members>>=
uni_CLDR_likely_subtags.gen.o
@

The extensions supported by Unicode are listed in the common/bcp47
data files.  For each extension, an enumeration literal is added.  The
descriptor for the extension is indexed on this literal.  Most
attribute extensions have an enumeration of possible values; for
these, the descriptor is the property index of the enumeration while
parsing.  For others, special negative values are used.

<<Parse CLDR files>>=
int CLDR_locale_ext = add_prop("CLDR_locale_ext");
int *CLDR_locale_ext_desc = NULL;
int CLDR_locale_ext_desc_len = 0;
struct dirent *de;
DIR *d = opendir("common/bcp47");
if(!d) {
  perror("CLDR bcp47");
  exit(1);
}
force_chdir("common/bcp47");
while((de = readdir(d))) {
  if(de->d_name[0] == '.')
    continue;
  doc = xmlReadFile(de->d_name, NULL, xml_opts_dtd);
  if(!doc) {
    perror(de->d_name);
    exit(1);
  }
  xmlNodePtr p; /* need to scan multiple ldmlBCP47 entries for some reason */
  for(p = doc->children; p; p = p->next)
    if(p->children && xml_isname(p, "ldmlBCP47")) {
      for(n = p->children; n; n = n->next) {
        /* ignore version, generic, cldrVersion */
	/* keywords are for kw=val exts, and attributes are just flags */
	if(xml_isname(n, "attribute")) {
          /* ignore description deprecated since preferred */
	  /* I guess the extension is always "u" */
	  const char *ext = "u";
	  int elit;
	  <<Add enum lit for locale extension>>
	  elit = elit; /* shut gcc up */
	  grow_size(CLDR_locale_ext_desc, CLDR_locale_ext_desc_len,
	            CLDR_locale_ext_desc_len + 1);
          CLDR_locale_ext_desc[CLDR_locale_ext_desc_len - 1] = LEXT_ATTR;
	} else if(n->children && xml_isname(n, "keyword")) {
          for(n = n->children; n; n = n->next) {
	    if(xml_isname(n, "key")) {
              /* ignore description deprecated since preferred */
	      char *xext = xml_prop(n, "extension");
	      const char *ext = xext;
	      if(!ext)
	        ext = "u";
	      int elit;
	      <<Add enum lit for locale extension>>
	      xmlprop_free(xext);
	      /* note: alias may be multiple space-separated aliases */
	      /* never observed in wild, though */
	      <<Add enum lit aliases for [[CLDR_locale_ext]] node [[n]]>>
	      uni_alias_t *a = &val_aliases[CLDR_locale_ext][elit];
	      inisize(s, strlen(a->short_name) + sizeof("CLDR_locale_"));
	      strcpy(s, "CLDR_locale_");
	      strcpy(s + sizeof("CLDR_locale_") - 1, a->short_name);
	      int parm_prop = add_prop(s);
	      free(s);
	      grow_size(CLDR_locale_ext_desc, CLDR_locale_ext_desc_len,
	                CLDR_locale_ext_desc_len + 1);
	      for(c = n->children; c; c = c->next) {
	        if(xml_isname(c, "type")) {
		  /* ignore description deprecated since preferred */
		  char *name = xml_prop(c, "name");
		  elit = add_enum_lit(parm_prop, name)->val;
		  /* note: alias may be multiple space-separated aliases */
		  /* 1st alias is "preferred" (over name?) */
		  <<Add enum lit aliases for [[parm_prop]] node [[c]]>>
		  xmlprop_free(name);
	        }
	      }
	      /* special names */
	      int special = 0;
	      if(num_val_aliases[parm_prop] == 1 &&
	         !strcmp(enum_vals[parm_prop][0].name, "REORDER_CODE"))
		/* reordering block name(s) */
		special = LEXT_REORDER;
              else if(num_val_aliases[parm_prop] == 1 &&
	                !strcmp(enum_vals[parm_prop][0].name, "CODEPOINTS"))
	        /* one or more 4-6 hex-digit code points */
		special = LEXT_CP;
	      else if(num_val_aliases[parm_prop] == 2 &&
	                /* FIXME: use bsearch, in case aliases different */
	                !strcmp(enum_vals[parm_prop][0].name, "false") &&
			!strcmp(enum_vals[parm_prop][2].name, "true"))
                /* booleans don't need enums really */
		/* always true=yes false=no */
		special = LEXT_BOOL;
	      else if(!num_val_aliases[parm_prop])
	        /* x0 defines no valid values: anything is valid */
		special = LEXT_GENERIC;
	      if(special) {
	        for(i = 0; i < enum_vals_len[parm_prop]; i++)
		  free((char *)enum_vals[parm_prop][i].name);
		free(enum_vals[parm_prop]);
		enum_vals[parm_prop] = NULL;
		free(val_aliases[parm_prop]);
		val_aliases[parm_prop] = NULL;
		enum_vals_len[parm_prop] = num_val_aliases[parm_prop] =
		    max_val_aliases[parm_prop] = 0;
		CLDR_locale_ext_desc[CLDR_locale_ext_desc_len - 1] = special;
              } else
		CLDR_locale_ext_desc[CLDR_locale_ext_desc_len - 1] = parm_prop;
	    }
	  }
	  break; /* never more than one keyword element */
	}
      }
    }
  xmlFreeDoc(doc);
}
closedir(d);
force_chdir(cwd);
force_chdir(CLDR_LOC);
@

<<Add enum lit for locale extension>>=
{
  char *name = xml_prop(n, "name");
  char *en, *ename;
  inisize(ename, strlen(name) + 3);
  en = ename;
  *en++ = tolower(*ext);
  *en++ = '_';
  s = name;
  while(*s)
    *en++ = tolower(*s++);
  *en = 0;
  xmlprop_free(name);
  elit = add_enum_lit(CLDR_locale_ext, ename)->val;
  if(*ename == 'u' && !xmlHasProp(n, (const xmlChar *)"alias")) {
    /* u exts can be given after @ with alias */
    /* or using raw kw if no alias */
    uni_valueof_t *v = add_enum_lit(CLDR_locale_ext, ename + 2);
    v->val = elit;
    num_val_aliases[CLDR_locale_ext]--;
  }
  free(ename);
}
@

<<Add enum lit aliases for (@prop) node (@n)>>=
{
  char *alias = xml_prop(<<@n>>, "alias");
  if(alias) {
    /* alias may be multiple space-separated aliases */
    /* adding to val's alias list may overflow, so just add enum lits */
    s = alias;
    char *e;
    for(e = s; *e && *e != ' '; e++)
      *e = tolower(*e);
    while(1) {
      if(*e)
        *e = 0;
      else
        e = NULL;
      int na = num_val_aliases[<<@prop>>];
      uni_valueof_t *v = add_enum_lit(<<@prop>>, s);
      if(na != num_val_aliases[<<@prop>>]) {
        v->val = elit;
	num_val_aliases[<<@prop>>]--;
      } else {
        /* I've been assuming alias=x means x is an alias, but */
	/* calendar e.g. uses this field to say "this is an alias of x" */
      }
      if(e) {
	for(*e++ = ' ', s = e; *e && *e != ' '; e++)
	  *e = tolower(*e);
      } else
        break;
    }
    xmlprop_free(alias);
  }
}
@

<<UCD parser local definitions>>=
#define LEXT_ATTR    -1
#define LEXT_BOOL    -2
#define LEXT_CP      -3
#define LEXT_REORDER -4
#define LEXT_GENERIC -5
@

<<[[uni_CLDR_locale_ext_desc]]>>=
/** Descriptor for known locale extensions (u_* and t_*) */
typedef struct {
  int type_len; /**< >0: enum defined by lookup; <0: see below */
  const uni_valueof_t *lookup; /**< enum lookup table; len == type_len */
  const uni_alias_t *names; /**< enum name table; len unnecessary if vals valid */
} uni_CLDR_locale_ext_desc_t;

#define UNI_LOCALE_EXT_BOOL  -1 /**< Boolean: true=yes/false=no */
#define UNI_LOCALE_EXT_CP    -2 /**< Code point(s): 4-6 hex digits */
#define UNI_LOCALE_EXT_REORD -3 /**< Reordering block identifier(s) */
#define UNI_LOCALE_EXT_ATTR  -4 /**< Attribute */
#define UNI_LOCALE_EXT_PRIV  -5 /**< Private use: anything goes */
@

<<Unicode locale exports>>=
<<[[uni_CLDR_locale_ext_desc]]>>
/** A table of all known locale extensions, indexed by the
  * \ref uni_CLDR_locale_ext_t enumeration literal.  */
extern const uni_CLDR_locale_ext_desc_t uni_CLDR_locale_ext_desc[];
@

<<Dump character information as C code>>=
{
  open_wf(of, "uni_CLDR_locale_ext_desc.gen.c");
  fputs("#include \"uni_locale.h\"\n\n"
        "const uni_CLDR_locale_ext_desc_t uni_CLDR_locale_ext_desc[] = {\n", of);
  for(i = 0; i < CLDR_locale_ext_desc_len; i++) {
    int desc = CLDR_locale_ext_desc[i];
    if(desc >= 0)
      fprintf(of, "\t{ uni_%s_valueof_len, ", parsed_props[desc].name);
    else
      fprintf(of, "\t{ UNI_LOCALE_EXT_%s, ",
                  desc == LEXT_BOOL ? "BOOL" : desc == LEXT_CP ? "CP" :
		  desc == LEXT_REORDER ? "REORD" : desc == LEXT_GENERIC ? "PRIV" :
		  "ATTR");
    if(desc >= 0)
      fprintf(of, "uni_%s_valueof, uni_%s_nameof}", parsed_props[desc].name,
                  parsed_props[desc].name);
    else
      fputs("NULL}", of);
    if(i < CLDR_locale_ext_desc_len - 1)
      putc(',', of);
    putc('\n', of);
  }
  fputs("};\n", of);
  fclose(of);
}
@

<<Additional parse-ucd includes>>=
#include <dirent.h>
@

<<Library [[uni]] Members>>=
uni_CLDR_locale_ext_desc.gen.o
@

This is now enough data to allow parsing of locale names.  I will put
no effort into trying to support every single obsolete locale
identifier.  Instead, only those which can be parsed using the above
data are supported.  After all, the only purpose of this is to select
the correct locale data for the algorithms already implemented.

The return value for this function is an array of integers indicating
the locale.   The first is the base triplet, as described above.  The
remaining words are variants and locale extensions.  Unknown or
missing language, script and territory tags are returned as
zero.  Unknown variant and locale extension tags are silently dropped.

<<Library [[uni]] Members>>=
uni_locale.o
@

<<uni_locale.c>>=
<<Common C Header>>
#include "uni_locale.h"
// static_proto

<<Unicode locale local functions>>

<<Unicode locale functions>>
@

<<Unicode locale exports>>=
/** Parse textual locale into a binary descriptor.
  * Parses \p locale_str.  See \ref uni_return32_buf32 for information
  * on how the descriptor is returned.  The descriptor format consists
  * of the locale triplet, follwed by optional variant identifiers and
  * extension tags.  Parse errors are silently ignored. */
<<Additional doxymentation for [[uni_parse_locale]]>>
int uni_parse_locale(const char *locale_str,
                     <<Buffer return parameters for UTF-[[32]]>>);
@

<<Unicode locale functions>>=
int uni_parse_locale(const char *locale_str,
                     <<Buffer return parameters for UTF-[[32]]>>)
{
  <<Parse CLDR/Unicode Locale string>>
}
@

The first thing to do is convert the string to lower-case, since all
comparisons are case-insensitive.  Unfortunately, this means
duplicating the input every time.  The only quick case not requiring
this is an empty string.  Actually, conversion can take place while
parsing, so only the tag currently being extracted is converted. The
first tag is always the language identifier, or it should be; some
legacy language identifiers also have a leading ``i'' tag, and there
is also a leading ``x'' tag for private use language identifiers.  If
there are other special cases, I probably don't care.

<<Parse CLDR/Unicode Locale string>>=
if(!*locale_str)
  return 0;
const char *ls = locale_str;
char *str = malloc(strlen(ls) + 1), *s, *ltag = str;
uint32_t lang = ~0;

for(s = str; *ls; ls++) {
  if(!isalpha(*ls))
    break;
  *s++ = tolower(*ls);
}
if(s == str + 1 && (s[-1] == 'i' || s[-1] == 'x') && (*ls == '-' || *ls == '_'))
  for(ls++, *s++ = '_'; isalpha(*ls); ls++)
    *s++ = tolower(*ls);
*s++ = 0;
@

The IETF allows up to 3 more 3-letter language tags after this.  I go
ahead and skip any 3-letter tags, without counting.  However, if the
3-letter tag matches a region code, it is assumed to be one, and is
not skipped.

<<Parse CLDR/Unicode Locale string>>=
int tl = 0;
uni_valueof_t me, *v;
if(*ls == '-' || *ls == '_')
  for(ls++; isalnum(*ls); ls++, tl++)
    *s++ = tolower(*ls);
while(tl == 3) {
  me.name = s - tl;
  v = bsearch(&me, uni_CLDR_territory_valueof, uni_CLDR_territory_valueof_len,
              sizeof(me), uni_cmp_valueof);
  if(v)
    break;
  <<Advance to next locale tag>>
}
@

<<Advance to next locale tag>>=
*s++ = 0;
tl = 0;
if(*ls == '-' || *ls == '_')
  for(ls++; isalnum(*ls); ls++, tl++)
    *s++ = tolower(*ls);
@

The next tag, if present, may be the script tag.  If so, it is always
4 characters long.  It is also always alphabetic, but only the first
character needs to be checked, since the only other 4-letter tag is a
variant starting with a digit.

<<Parse CLDR/Unicode Locale string>>=
char *stag = NULL;
uint32_t scr = ~0;
if(tl == 4 && isalpha(s[-tl])) {
  stag = s - tl;
  <<Advance to next locale tag>>
}
@

The next tag, if present, may be the territory (region) tag.  If so,
it is always 2-3 alphanumeric characters long.  After that may be the
variant tags, which are always 4 or more characters long.  It has a
maximum limit, but that is irrelevant, as no other tags can be that
long in this position.  It also only occupies 4 characters if the
first character is numeric, but that is checked sufficiently well by
the script check above.

<<Parse CLDR/Unicode Locale string>>=
char *ttag = NULL, *vtag = NULL;
uint32_t ter = ~0;
if(tl >= 2 && tl <= 3) {
  ttag = s - tl;
  <<Advance to next locale tag>>
}
if(tl >= 4) {
  vtag = s - tl;
  <<Advance to next locale tag>>
}
@

All remaining tags are part of the locale extension syntax.  Before
getting into those, it's time to look up the tags already extracted.

First, the language tag can be looked up.  One complication is that
some aliases consist of a language tag combined with script, territory
and/or variant tags.  To deal with this, a lookup must be followed by
a loop which checks for these, whether the lookup fails or not.

<<Parse CLDR/Unicode Locale string>>=
int l = 0, h = uni_CLDR_language_valueof_len - 1, m, c;
while(l <= h) {
  m = (l + h) / 2;
  c = strcmp(ltag, uni_CLDR_language_valueof[m].name);
  if(c < 0)
    h = m - 1;
  else if(c > 0)
    l = m + 1;
  else {
    lang = uni_CLDR_language_valueof[m].val;
    l = m + 1; /* start alias search after lang-only match */
    break;
  }
}
/* check for aliases */
if(l < uni_CLDR_language_valueof_len) {
  m = l;
  l = strlen(ltag);
  for(;!memcmp(ltag, uni_CLDR_language_valueof[m].name, l) &&
       uni_CLDR_language_valueof[m].name[l] == '_'; m++) {
    /* indeed, it is.  see if anything matches */
    const char *rt = uni_CLDR_language_valueof[m].name + l + 1,
               *et = strchr(rt, '_');
    h = et ? et - rt : strlen(rt);
    c = 1;
    char *ostag = stag, *ottag = ttag, *ovtag = vtag;
    if(stag && h == 4) {
      if(!(c = memcmp(rt, stag, 4)))
        stag = NULL;
      <<Advance [[rt]] to next tag if match>>
    }
    if(ttag && h >= 2 && h <= 3) {
      if(!(c = memcmp(rt, ttag, h) && !ttag[h]))
        ttag = NULL;
      <<Advance [[rt]] to next tag if match>>
    }
    /* Since variants are mostly harmless, I'll not worry about FIXMEs below */
    if(vtag && h >= 4) {
      if(!(c = memcmp(rt, vtag, h) && !vtag[h]))
        vtag = NULL;
      <<Advance [[rt]] to next tag if match>>
      if(c && tl >= 4) { /* more variant tags */
        const char *ols = ls;
	char *os = s;
	int otl = tl;
        do {
	  c = tl == h && memcmp(rt, s - tl, h);
	  /* FIXME: how do I delete this? */
	  <<Advance [[rt]] to next tag if match>>
	  if(!c)
	    break;
	  <<Advance to next locale tag>>
	} while(tl >= 4);
	ls = ols;
	s = os;
	tl = otl;
      }
    }
    if(!c && !et) {
      lang = uni_CLDR_language_valueof[m].val;
      break;
    }
    stag = ostag;  ttag = ottag;  vtag = ovtag;
  }
}
if(!vtag && tl >= 4) {
  vtag = s - tl;  /* FIXME: what if there are more of the same? */
  <<Advance to next locale tag>>
}
@

<<Advance [[rt]] to next tag if match>>=
if(!c && et) {
  rt = et + 1;
  et = strchr(rt, '_');
  h = et ? et - rt : strlen(rt);
} else
  h = 0;
@

Another complication for language tags is that an alias may convert it
to a language tag with associated default script and/or territory
tags.  Since a specific tag will override it later, this can be
handled by just setting all associated tags.

<<Parse CLDR/Unicode Locale string>>=
if(lang != ~0) {
  uint8_t b0 = (uint8_t)uni_CLDR_language_nameof[lang].short_name[0];
  if(b0 <= 3) {
    /* multi-part alias; extract all parts */
    const uint8_t *p =
          (const uint8_t *)uni_CLDR_language_nameof[lang].short_name + 1;
    unsigned int cl;
    lang = uni_valid_utf8_decode(p, &cl) - 1;
    p += cl;
    if(b0 & 1) {
      scr = uni_valid_utf8_decode(p, &cl) - 1; /* may be overridden below */
      p += cl;
    }
    if(b0 & 2)
      ter = uni_valid_utf8_decode(p, NULL) - 1; /* may be overridden below */
  }
}
@

Script tags can simply be looked up directly.

<<Parse CLDR/Unicode Locale string>>=
if(stag) {
  me.name = stag;
  v = bsearch(&me, uni_CLDR_script_valueof, uni_CLDR_script_valueof_len,
              sizeof(me), uni_cmp_valueof);
  if(v)
    scr = v->val;
}
@

One variant alias was dropped above, because it translates into a
territory.  This is handled manually here, before looking up the
territory tag, since that overrides this.  There is no need to discard
the variant; it will be discarded later due to lookup failure.

<<Parse CLDR/Unicode Locale string>>=
if(vtag && ter != UNI_CLDR_territory_ax) {
  if(!strcmp(vtag, "aaland"))
    ter = UNI_CLDR_territory_ax;
  else if(tl >= 4) { /* more variant tags */
    const char *ols = ls;
    char *os = s;
    int otl = tl;
    do {
      if(!strcmp(s - tl, "aaland")) {
        ter = UNI_CLDR_territory_ax;
	break;
      }
      <<Advance to next locale tag>>
    } while(tl >= 4);
    ls = ols;
    s = os;
    tl = otl;
  }
}
@

Territories are complicated by multiple-choice aliases.  If such an
alias is found, the ``most likely'' territory must be chosen if
present, and if not present, or the ``most likely'' territory can't be
found, the first must be chosen.  Without guidelines on how to
determine what is ``most likely,'' the CLDR's likely subtags table is
queried by script and language, and then language.  There are no
entries keyed on variant, so that is ignored.

<<Parse CLDR/Unicode Locale string>>=
if(ttag) {
  me.name = ttag;
  v = bsearch(&me, uni_CLDR_territory_valueof, uni_CLDR_territory_valueof_len,
              sizeof(me), uni_cmp_valueof);
  if(v) {
    ter = v->val;
    uint8_t b0 = (uint8_t)uni_CLDR_territory_nameof[ter].short_name[0];
    if(b0 < '0') {
      /* multiple choice; pick most popular answer */
      /* first by lang+script (or just lang if script == ~0) */
      uint32_t ch = uni_locale_triplet(lang, scr, ~0);
      uni_cp_val_t *likely = lookup_likely(ch);
      if(!likely && scr < UNI_NUM_CLDR_script) {
        /* then by lang only, if not already done above */
        ch = uni_locale_triplet(lang, ~0, ~0);
	likely = lookup_likely(ch);
      }
      const uint8_t *p =
	(const uint8_t *)uni_CLDR_territory_nameof[ter].short_name + 1;
      if(likely) {
        uint32_t lt = uni_locale_extract_territory(likely->val);
	unsigned int cl;
	while(b0 > 0) {
	  uint32_t t = uni_valid_utf8_decode(p, &cl) - 1;
	  p += cl;
	  if(t == lt) {
	    ter = t;
	    break;
	  }
	  --b0;
	}
	if(!b0) {
          /* or, if most popular not available, pick first */
	  p = (const uint8_t *)uni_CLDR_territory_nameof[lang].short_name + 1;
	  ter = uni_valid_utf8_decode(p, NULL) - 1;
	}
      } else
        /* or, if most popular not known, pick first */
	ter = uni_valid_utf8_decode(p, NULL) - 1;
    }
  }
}
@

<<Unicode locale local functions>>=
#define lookup_likely(t) \
  bsearch(&t, uni_CLDR_likely_subtags, uni_CLDR_likely_subtags_len, \
          sizeof(uni_cp_val_t), uni_cmp_cp)
@

Variants are complicated by the fact that there may be more than one.
Also, the [[POSIX]] variant must be converted into [[u-va-posix]].

<<Parse CLDR/Unicode Locale string>>=
uint32_t *exts = NULL, exts_len = 0, had_posix = 0;
while(vtag) {
  me.name = vtag;
  v = bsearch(&me, uni_CLDR_variant_valueof, uni_CLDR_variant_valueof_len,
              sizeof(me), uni_cmp_valueof);
  if(v) {
    if(v->val == UNI_CLDR_variant_posix) {
      had_posix = 1;
    } else {
      grow_size(exts, exts_len, exts_len + 1);
      exts[exts_len - 1] = v->val + (1UL << 31);
    }
  }
  if(tl >= 4) {
    vtag = s - tl;
    <<Advance to next locale tag>>
  } else
    vtag = NULL;
}
if(had_posix) {
  <<Append [[u-va-posix]] into result>>
}
@

<<Additional doxymentation for [[uni_parse_locale]]>>=
/** Variant tags are \ref uni_CLDR_variant_t enumeration values with
  * thier high bit set. */
@

Now that the main tags have been parsed, the locale extensions can be
processed.  Each locale extension begins with a single character, and
ends either before the start of the next extension tag, or upon
encountering invalid characters.  This implies that value aliases
which have invalid characters in them are ignored, and are only used
for the alternate extension syntax, parsed later.  Any unknown tags
of any kind are simply ignored.  While I allow multiple extensions of
the same type, the actual syntax only allows for one each.

Unicode u-type extensions consist of 3-8 letter attributes, followed
by groups of keyword/value pairs.  Each keyword is 2 letters, and each
value is more than 2 letters, so they are easily distinguished.

Unicode t-type extensions consinst of one or both of a language code
(without extensions) and field specifiers.  Field specifiers are the
same as keyword/value pairs in the u-type extension.  The only way to
distinguish the beginning of the field specifiers from a locale name
is to compare it with valid tags (always a letter followed by a digit,
but that is irrelevant when just comparing).

IETF x-type extensions consist of all remaining tags, interpreted in
an application-specific manner.  They are ignored.

Any other unknown extension is skipped until the next single-character
tag.

<<Parse CLDR/Unicode Locale string>>=
while(tl > 1) { /* skip illegal tags */
  <<Advance to next locale tag>>
}
while(tl == 1) {
  char ext_type = s[-1];
  if(ext_type == 'x') { /* all subsequent tags are private use/unsupported */
    do {
      <<Advance to next locale tag>>
    } while(tl);
    break;
  } else if(ext_type != 'u' && ext_type != 't') {
    do {
      <<Advance to next locale tag>>
    } while(tl > 1);
    continue;
  }
  <<Advance to next locale tag>>
  if(tl < 2)
    continue; /* no subtags */
  if(ext_type == 't') {
    /* may have initial locale tag */
    const char *locale_start = s - tl;
    s[-tl - 1] = '_';
    do {
      while(tl > 2) {
        <<Advance to next locale tag>>
	s[-tl - 1] = '_';
      }
      if(tl == 2) {
        char c0 = s[-tl - 2];
        s[-tl - 2] = 't';
        me.name = s - tl - 2;
        v = bsearch(&me, uni_CLDR_locale_ext_valueof, uni_CLDR_locale_ext_valueof_len,
                    sizeof(me), uni_cmp_valueof);
        s[-tl - 2] = c0;
        if(v) {
	  s[-tl - 1] = 0;
          break;
        }
	<<Advance to next locale tag>>
	s[-tl - 1] = '_';
      }
    } while(tl > 1);
    if(s - tl != locale_start) {
      <<Encode [[t]] extension locale>>
    }
    if(tl < 2)
      continue; /* no field specs */
  }
  /* there are currently no attributes, but if there were, they'd go here */
  if(ext_type == 'u') {
    while(tl > 2) {
      s[-tl - 2] = 'u';
      s[-tl - 1] = '_';
      me.name = s - tl - 2;
      v = bsearch(&me, uni_CLDR_locale_ext_valueof, uni_CLDR_locale_ext_valueof_len,
                  sizeof(me), uni_cmp_valueof);
      <<Advance to next locale tag>>
      if(!v || uni_CLDR_locale_ext_desc[v->val].type_len != UNI_LOCALE_EXT_ATTR)
        continue;
      <<Append locale ext attribute [[v->val]] into result>>
    }
    if(tl < 2)
      continue;
  }
  /* now there are just 2-letter keywords, followed by 3-8 letter parms */
  do {
    s[-tl - 2] = ext_type;
    s[-tl - 1] = '_';
    me.name = s - tl - 2;
    v = bsearch(&me, uni_CLDR_locale_ext_valueof, uni_CLDR_locale_ext_valueof_len,
                sizeof(me), uni_cmp_valueof);
    <<Advance to next locale tag>>
    if(!v) {
      <<Skip to next locale ext kw>>
      continue;
    }
    uint32_t ext = v->val;
    const uni_CLDR_locale_ext_desc_t *desc = &uni_CLDR_locale_ext_desc[ext];
    switch(desc->type_len) {
      <<Type cases for locale ext keyword [[ext]]>>
    }
  } while(tl == 2);
}
@

<<Skip to next locale ext kw>>=
while(tl > 2) {
  <<Advance to next locale tag>>
}
@

While there are no attributes in the current CLDR, if there were, they
would be simplest:  just the keyword ID itself.

<<Append locale ext attribute [[v->val]] into result>>=
grow_size(exts, exts_len, exts_len + 1);
exts[exts_len - 1] = v->val;
@

<<Additional doxymentation for [[uni_parse_locale]]>>=
/** Extensions of type \ref UNI_LOCALE_EXT_ATTR are encoded as their
  * descriptor array index.  */
@

If they have parameters, they are invalid, and simply ignored.
Likewise, private extensions have no known format, and are ignored.

<<Type cases for locale ext keyword [[ext]]>>=
case UNI_LOCALE_EXT_ATTR:
  /* none */
  /* ignore: attrs may not have parms */
  /* fall through */
case UNI_LOCALE_EXT_PRIV:
  /*
    t-x0 private use; not useful
   */
  /* ignore: x0 not supported.  do it yourself. */
  <<Skip to next locale ext kw>>
  continue;
@

The locale for the [[t]] tag is encoded just like the main locale, but
has bit 30 set for each word.  It is parsed by recursion, but there
will never be more than one recursion (there are no extension tags in
the string).

<<Encode [[t]] extension locale>>=
unsigned int max_len = 0, len;
uint32_t *tloc = NULL, *p;
len = uni_parse_locale(locale_start, &tloc, 0, &max_len);
if(len > 0) {
  grow_size(exts, exts_len, exts_len + len);
  p = exts + exts_len - len;
  while(len-- > 0)
    p[len] = tloc[len] | (1UL << 30);
}
if(tloc)
  free(tloc);
@

<<Additional doxymentation for [[uni_parse_locale]]>>=
/** t-locale-id extensions are encoded just like locale-id, but with
  * bit 30 set on every word. */
@

Enumerations are stored with the keyword code in the low 8 bits, and
the looked-up enumeration code in the upper 24 bits.  The parser
supports multiple parameters, mainly because the [[t]] extension
implies that multiple parameters may be necessary.  The [[u]]
extension's documentation explicitly states that enumerated parameters
are ``final and used alone,'' but once again, this routine is not a
validity checker.  Multiple parameters are encoded as if they were
independent keyword/value pairs.

<<Type cases for locale ext keyword [[ext]]>>=
default:
  if(tl <= 2)
    continue; /* at lest one parameter required */
  do {
    me.name = s - tl;
    v = bsearch(&me, desc->lookup, desc->type_len, sizeof(me), uni_cmp_valueof);
    <<Advance to next locale tag>>
    if(!v) {
      /* this is just for u-ca */
      if(tl > 2) {
        s[-tl - 1] = '-';
        v = bsearch(&me, desc->lookup, desc->type_len, sizeof(me), uni_cmp_valueof);
        if(v) {
          <<Advance to next locale tag>>
        }
      }
    }
    if(v) {
      grow_size(exts, exts_len, exts_len + 1);
      exts[exts_len - 1] = ext + (v->val << 8);
    }
  } while(tl > 2);
  break;
@

<<Append [[u-va-posix]] into result>>=
grow_size(exts, exts_len, exts_len + 1);
exts[exts_len - 1] = UNI_CLDR_locale_ext_u_va + (UNI_CLDR_locale_u_va_posix << 8);
@

<<Additional doxymentation for [[uni_parse_locale]]>>=
/** Enumerated extension tags are stored with their descriptor index
  * in the low byte, and the enumeration value in the remaining bytes
  * (sans the top two bits). */
@

Booleans are a special form of enumerations, and are stored similarly.
However, [[false]] always maps to zero, and [[true]] always maps to
one.  While it is not documented as such, some boolean variables are
shown in examples as if they were attributes:  their presence means
truth.  This is supported here as well.

<<Type cases for locale ext keyword [[ext]]>>=
case UNI_LOCALE_EXT_BOOL: {
  int val;
  if(tl < 3)
    val = 1; /* if just present, same as "true" */
  else {
    /* 2-letter "no" is impossible */
    if(!strcmp(s - tl, "false") /* || !strcmp(s - tl, "no") */ )
      val= 0;
    /* "yes" is probably meant for @-syntax only, but I'll allow it */
    else if(!strcmp(s - tl, "true") || !strcmp(s - tl, "yes"))
      val = 1;
    else { /* invalid; skip to next kw */
      <<Skip to next locale ext kw>>
      continue;
    }
    <<Advance to next locale tag>>
    if(tl > 2) {
      /* only supports 0 or 1 parms, so invalid */
      <<Skip to next locale ext kw>>
      continue;
    }
  }
  grow_size(exts, exts_len, exts_len + 1);
  exts[exts_len - 1] = ext + (val << 8);
  break;
}
@

<<Additional doxymentation for [[uni_parse_locale]]>>=
/** Extensions of type \ref UNI_LOCALE_EXT_BOOL are encoded as their
  * descriptor index in the low byte, and the truth flag in bit 9. */
@

Code point lists are encoded as multiple words.  The first word has
the extension identifier and the length, and the remaining words are
the raw code points.

<<Type cases for locale ext keyword [[ext]]>>=
case UNI_LOCALE_EXT_CP: {
  if(tl <= 2)
    continue; /* at lest one parameter required */
  uint32_t lenoff = exts_len; 
  grow_size(exts, exts_len, exts_len + 1);
  do {
    char *en;
    uint32_t n = strtoul(s - tl, &en, 16);
    /* technically, also an error if size is not 4 or 6, but I'll allow */
    if(*en || tl > 6 || n > UNI_MAX_CP) {
      exts_len = lenoff;
      if(!exts_len) {
        free(exts);
	exts = NULL;
      }
      lenoff = ~0;
      break;
    }
    grow_size(exts, exts_len, exts_len + 1);
    exts[exts_len - 1] = n;
    <<Advance to next locale tag>>
  } while(tl > 2);
  if(lenoff == (uint32_t)~0)
    continue;
  exts[lenoff] = ext + ((exts_len - lenoff - 1) << 8);
  break;
}
@

<<Additional doxymentation for [[uni_parse_locale]]>>=
/** Extensions of type \ref UNI_LOCALE_EXT_CP are encoded as their
  * descriptor index in the low byte, and the number of code points
  * in the remining bytes (sans the top two bits).  This is followed by
  * the code points themselves, in order.  */
@

Reordering codes are encoded the same way, except that the words
following the initial word identify reordering blocks.

<<Type cases for locale ext keyword [[ext]]>>=
case UNI_LOCALE_EXT_REORD: {
  if(tl <= 2)
    continue; /* at lest one parameter required */
  uint32_t lenoff = exts_len; 
  grow_size(exts, exts_len, exts_len + 1);
  do {
    uint32_t b = uni_parse_locale_reorder_block(s - tl);
    if(b == (uint32_t)~0) {
      exts_len = lenoff;
      if(!exts_len) {
        free(exts);
	exts = NULL;
      }
      lenoff = ~0;
      break;
    }
    grow_size(exts, exts_len, exts_len + 1);
    exts[exts_len - 1] = b;
    <<Advance to next locale tag>>
  } while(tl > 2);
  if(lenoff == (uint32_t)~0)
    continue;
  exts[lenoff] = ext + ((exts_len - lenoff - 1) << 8);
  break;
}
@

<<Additional doxymentation for [[uni_parse_locale]]>>=
/** Extensions of type \ref UNI_LOCALE_EXT_REORD are encoded as their
  * descriptor index in the low byte, and the number of reordering
  * block identifiers in the remining bytes (sans the top two bits).  This
  * is followed by by the reordering block identifiers themselves, in
  * order.  An identifier is a code returned by
  * \ref uni_parse_locale_reorder_block. */
@

Reordering block names are not available in any easy enumeration table
for parsing.  There are really only 6 fixed strings, though, and the
rest are the 4-character script names.  One of the fixed strings is
equivalent to a script, and the other 5 are given codes beyond the
maximum script code.

<<Unicode locale exports>>=
/** Reordering block: gc = Z|Cc */
#define UNI_REORDER_SPACE    (UNI_NUM_sc)
/** Reordering block: gc = P */
#define UNI_REORDER_PUNCT    (UNI_NUM_sc + 1)
/** Reordering block: gc = Sk|Sm|So */
#define UNI_REORDER_SYMBOL   (UNI_NUM_sc + 2)
/** Reordering block: gc = Sc */
#define UNI_REORDER_CURRENCY (UNI_NUM_sc + 3)
/** Reordering block: gc = Nd */
#define UNI_REORDER_DIGIT    (UNI_NUM_sc + 4)
/** Reordering block: other (sc = Zzzz) */
#define UNI_REORDER_OTHER    UNI_sc_Zzzz

/** Parse a normalized, lower-case reordering block identifier.
  * This is either a script code (represented as a \ref uni_sc_t
  * return value) or a special reordering block (represented as
  * one of the UNI_REORDER_* symbols) */
uint32_t uni_parse_locale_reorder_block(const char *s);
@

<<Unicode locale functions>>=
/* assumes s is already lower-case */
uint32_t uni_parse_locale_reorder_block(const char *s)
{
  uint32_t len = strlen(s);
  if(len == 4) {
    uni_valueof_t me, *v;
    me.name = s;
    v = bsearch(&me, uni_sc_valueof_approx, uni_sc_valueof_len, sizeof(me),
                uni_cmp_valueof);
    if(v)
      return v->val;
    else
      return ~0;
  }
  /* linear search for rest, but that's probably OK since there are so few */
  if(!strcmp(s, "space"))
    return UNI_REORDER_SPACE;
  if(!strcmp(s, "punct"))
    return UNI_REORDER_PUNCT;
  if(!strcmp(s, "symbol"))
    return UNI_REORDER_SYMBOL;
  if(!strcmp(s, "currency"))
    return UNI_REORDER_CURRENCY;
  if(!strcmp(s, "digit"))
    return UNI_REORDER_DIGIT;
  if(!strcmp(s, "other"))
    return UNI_REORDER_OTHER;
  return ~0;
}
@

Now that all of the tags have been processed, there may be more text
afterwards.  The only supported text is an at-sign, followed by
old-style locale extensions.  It is also legal to have a dot, followed
by a text encoding identifier, but everything other than an at-sign is
ignored.  The dot is still used to potentially terminate the old-style
locale extensions, though, and if a dot is encountered first, an
at-sign may still be present afterwards (I am unsure of the required
order, but POSIX appears to like dot first, then at-sign).

<<Parse CLDR/Unicode Locale string>>=
if(*ls == '.')
  while(*++ls && *ls != '@');
if(*ls == '@') {
  do {
    ls++;
    me.name = s;
    for(; *ls && *ls != '=' && *ls != ';' && *ls != '.'; ls++)
      *s++ = tolower(*ls);
    if(*ls != '=')
      continue;
    *s = 0;
    v = bsearch(&me, uni_CLDR_locale_ext_valueof_approx,
                uni_CLDR_locale_ext_valueof_len, sizeof(me), uni_cmp_valueof);
    if(!v) {
      while(*ls && *ls != '.' && *ls != ';')
        ls++;
      continue;
    }
    uint32_t ext = v->val;
    const uni_CLDR_locale_ext_desc_t *desc = &uni_CLDR_locale_ext_desc[ext];
    me.name = s;
    for(ls++; *ls && *ls != ';' && *ls != '.'; ls++)
      /* if(*ls != '-' && *ls != '_') */ /* this makes - as sep below impossible */
        *s++ = tolower(*ls);
    *s = 0;
    switch(desc->type_len) {
      default:
        v = bsearch(&me, desc->lookup, desc->type_len, sizeof(me), uni_cmp_valueof);
        if(!v)
          continue;
        grow_size(exts, exts_len, exts_len + 1);
        exts[exts_len - 1] = ext + (v->val << 8);
        continue;
      case UNI_LOCALE_EXT_BOOL: {
        v = bsearch(&me, uni_locale_bool_approx, uni_locale_bool_approx_len,
	            sizeof(me), uni_cmp_valueof);
        if(!v)
          continue; /* invalid */
        grow_size(exts, exts_len, exts_len + 1);
        exts[exts_len - 1] = ext + (v->val << 8);
        break;
      }
      case UNI_LOCALE_EXT_ATTR:
        continue; /* ignore: attrs may not have parms */
      case UNI_LOCALE_EXT_PRIV:
        continue; /* ignore: not supported.  do it yourself. */
      case UNI_LOCALE_EXT_CP: {
        char *en;
	if(me.name[0] == 'u')
	  me.name++;
	int startoff = exts_len;
	uint32_t n = strtoul(me.name, &en, 16);
	if((*en && *en != 'u') || !me.name[0] || me.name[0] == 'u')
	  continue;
        grow_size(exts, exts_len, exts_len + 2);
	exts[startoff] = ext + ((1 << 8));
	exts[exts_len - 1] = n;
	while(*en == 'u') {
	  me.name = en + 1;
	  n = strtoul(me.name, &en, 16);
	  if((*en && *en != 'u') || !me.name[0] || me.name[0] == 'u') {
	    exts_len = startoff;
	    break;
	  }
          grow_size(exts, exts_len, exts_len + 1);
	  exts[startoff] += 1 << 8;
	  exts[exts_len - 1] = n;
	}
        break;
      }
      case UNI_LOCALE_EXT_REORD: {
        /* FIXME: support > 1 (what separates? -? ,?) */
	/* not too important; introduced after u- syntax, so use u- */
        uint32_t b = uni_parse_locale_reorder_block(me.name);
	if(b == (uint32_t)~0)
	  continue;
        grow_size(exts, exts_len, exts_len + 2);
	exts[exts_len - 2] = ext + ((1 << 8));
	exts[exts_len - 1] = b;
	break;
      }
    }
  } while(*ls && *ls != '.');
}
@

<<Unicode locale exports>>=
/** A lookup table for approximate boolean value matching.
  * The associated value is 0 for false and 1 for true. */
extern const uni_valueof_t uni_locale_bool_approx[];
/** The length of \ref uni_locale_bool_approx */
#define uni_locale_bool_approx_len 6
/** Look up truth value for name \p s using loose matching.
  * See \ref uni_x_valueof_approx for details.
  * If lookup fails, ~0 is returned. */
#define uni_locale_bool_lookup(s) \
  uni_x_valueof_approx(s, uni_locale_bool_approx, uni_locale_bool_approx_len, ~0)
@

<<Unicode locale functions>>=
/* note: added on/off for XML support */
const uni_valueof_t uni_locale_bool_approx[] = {
  { "false", 0 },
  { "no", 0 },
  { "off", 0 },
  { "on", 1 },
  { "true", 1 },
  { "yes", 1 }
};
@

The only thing left to do is return a value, after freeing any
temporary storage.

<<Parse CLDR/Unicode Locale string>>=
uint32_t ret = uni_locale_triplet(lang, scr, ter);
uni_return32_buf32(&ret, 1, buf, off, buf_len);
if(off < 0) { /* fixed-len buffer of length *buf_len */
  if(buf_len && buf && *buf_len > 1) {
    uint32_t *b = *buf + 1;
    unsigned int len = *buf_len - 1;
    uni_return32_buf32(exts, exts_len, &b, off, &len);
  }
} else {
  if(*buf)
    uni_return32_buf32(exts, exts_len, buf, off + 1, buf_len);
}
free(str);
if(exts)
  free(exts);
return 1 + exts_len;
@

The likely subtags table can also be used to fill in missing pieces,
or to remove redundant pieces.

<<Unicode locale exports>>=
/** Strip redundant tags from locale triplet.
  * If a script or territory is a likely subtag, remove it from
  * \p triplet.  Explicit uknowns (und/Zzzzz/ZZ) are used when
  * matching, unless \p force is true.  */
uint32_t uni_locale_triplet_strip(uint32_t triplet, int force);
/** Fill in missing tags of locale triplet.
  * Fills in missing tags with likely subtags.  Explicit unknowns
  * (und/Zzzz/ZZ) are left alone, unless \p force is true. */
uint32_t uni_locale_triplet_fill(uint32_t triplet, int force);
@

<<Unicode locale functions>>=
uint32_t uni_locale_triplet_strip(uint32_t triplet, int force)
{
  if(!triplet)
    return triplet;
  if(uni_locale_extract_language(triplet) == UNI_CLDR_language_root)
    return uni_locale_triplet(UNI_CLDR_language_root, ~0, ~0);
  triplet = uni_locale_triplet_fill(triplet, force);
  uni_CLDR_language_t l = uni_locale_extract_language(triplet);
  uni_CLDR_script_t s = uni_locale_extract_script(triplet);
  uni_CLDR_territory_t t = uni_locale_extract_territory(triplet);
  if(l == UNI_CLDR_language_root)
    return triplet;
  /* try just language first */
  uint32_t loc = uni_locale_triplet(l, ~0, ~0);
  uni_cp_val_t *v = lookup_likely(loc);
  if(v) {
    if(s == uni_locale_extract_script(v->val))
      s = ~0;
    if(t == uni_locale_extract_territory(v->val))
      t = ~0;
    return uni_locale_triplet(l, s, t);
  }
  /* try dropping script first, since it's longer */
  loc = uni_locale_triplet(l, ~0, t);
  v = lookup_likely(loc);
  if(v) {
    if(s == uni_locale_extract_script(v->val))
      s = ~0;
    return uni_locale_triplet(l, s, t);
  }
  /* finally, try dropping territory */
  loc = uni_locale_triplet(l, s, ~0);
  v = lookup_likely(loc);
  if(v && t == uni_locale_extract_territory(v->val))
    t = ~0;
  return uni_locale_triplet(l, s, t);
}
@

<<Unicode locale functions>>=
uint32_t uni_locale_triplet_fill(uint32_t triplet, int force)
{
  uni_CLDR_language_t l = uni_locale_extract_language(triplet);
  uni_CLDR_script_t s = uni_locale_extract_script(triplet);
  uni_CLDR_territory_t t = uni_locale_extract_territory(triplet);
  if(l == UNI_CLDR_language_root) /* root has no script or territory */
    return uni_locale_triplet(l, ~0, ~0);
  if(force && s == UNI_CLDR_script_zzzz)
    s = UNI_NUM_CLDR_script;
  if(force && t == UNI_CLDR_territory_zz)
    t = UNI_NUM_CLDR_territory;
  if(force && l == UNI_CLDR_language_und)
    l = UNI_NUM_CLDR_language;
  triplet = uni_locale_triplet(l, s, t);
  /* if nothing to do, just return as is */
  if(l != UNI_NUM_CLDR_language && s != UNI_NUM_CLDR_script &&
     t != UNI_NUM_CLDR_territory)
    return triplet;
  /* try filling the blanks direcly */
  uni_cp_val_t *v = lookup_likely(triplet);
  if(v)
    return v->val;
  /* try filling in at least the language blank */
  if(l == UNI_NUM_CLDR_language) {
    /* some entries use und instead of a blank language */
    l = UNI_CLDR_language_und;
    triplet = uni_locale_triplet(l, s, t);
    v = lookup_likely(triplet);
    if(v)
      return v->val;
  }
  /* next, try language only if script or territory missing */
  if((t == UNI_NUM_CLDR_territory && s != UNI_NUM_CLDR_script) ||
     (t != UNI_NUM_CLDR_territory && s == UNI_NUM_CLDR_script)) {
    triplet = uni_locale_triplet(l, ~0, ~0);
    v = lookup_likely(triplet);
    if(v) {
      if(t == UNI_NUM_CLDR_territory)
        t = uni_locale_extract_territory(v->val);
      else
        s = uni_locale_extract_script(v->val);
    }
  }
  /* otherwise, no clue; just fill in with undefineds */
  if(s == UNI_NUM_CLDR_script)
    s = UNI_CLDR_script_zzzz;
  if(t == UNI_NUM_CLDR_territory)
    t = UNI_CLDR_territory_zz;
  return uni_locale_triplet(l, s, t);
}
@

For fully canonical locale descriptors, duplicate tags must be removed
(retaining the first one, according to RFC6067, although I'd prefer to
keep the last one), and tags and variants must be sorted
alphabetically, with variants first, then [[u]] extensions, with the
attributes (none of which currently exist%
\footnote{Maybe they also meant booleans whose value is true, since,
like attributes, they do not need parameters.}%
) first, and finally [[t]] extensions.  [[x]] extensions were
dropped above, or they would come next.  This is not, strictly
speaking, the canonical locale descriptor, because the parser dropped
unknowns and ignored errors while generating it.  Also, multiple
enumeration groups are combined into one, if that makes a difference
(for this libary, only [[t]] extensions support multiple enumeration
values, and they are pretty much ignored).

<<Unicode locale exports>>=
/** Sort and filter extensions in a locale descriptor.
  * Canonicalizes order of variants and extension tags in descriptor array
  * \p locale/\p locale_len.  Duplicate tags are removed.
  * The updated length is returned.  */
int uni_locale_sort_var_exts(uint32_t *locale, int locale_len);
@

<<Unicode locale functions>>=
static int cmp_locale_ext_ind(const void *_a, const void *_b)
{
  const uint32_t *a = *(uint32_t **)_a, *b = *(uint32_t **)_b;
  uint32_t isvara = *a & (1UL << 31), isvarb = *b & (1UL << 31);
  
  if(isvara && !isvarb)
    return -1;
  if(isvarb && !isvara)
    return 1;
  if(isvara)
    return strcmp(uni_CLDR_variant_nameof[*a & 0xff].short_name,
                  uni_CLDR_variant_nameof[*b & 0xff].short_name);

  uint32_t istloca = *a & (1UL << 30), istlocb = *b & (1UL << 30);
  if(istloca && istlocb)
    return a > b ? 1 : -1; /* safe to assume a != b */
  const char *namea, *nameb;
  namea = istloca ? "t" : uni_CLDR_locale_ext_nameof[*a & 0xff].short_name;
  nameb = istlocb ? "t" : uni_CLDR_locale_ext_nameof[*b & 0xff].short_name;
  if(*namea == 't' && *nameb == 'u')
    return 1;
  if(*namea == 'u' && *nameb == 't')
    return -1;
  /* no attrs at present, but maybe condition should be: */
  /* ... == UNI_LOCALE_EXT_BOOL && (*x) >> 8 */
  int ata = *namea == 'u' &&
        uni_CLDR_locale_ext_desc[*a & 0xff].type_len == UNI_LOCALE_EXT_ATTR;
  int atb = *nameb == 'u' &&
        uni_CLDR_locale_ext_desc[*b & 0xff].type_len == UNI_LOCALE_EXT_ATTR;
  if(ata && !atb)
    return -1;
  if(atb && !ata)
    return 1;
  int c = strcmp(namea, nameb);
  if(c)
    return c;
  return a > b ? 1 : -1; /* safe to assume a != b */
}

int uni_locale_sort_var_exts(uint32_t *locale, int locale_len)
{
  if(locale_len < 2)
    return locale_len;
  /* convert array w/ varying-length entries into fixed-length array */
  uint32_t *sort_array[locale_len - 1], sort_array_len = 0, i;
  for(i = 1; i < locale_len; i++) {
    sort_array[sort_array_len++] = &locale[i];
    if(locale[i] & (1UL << 31))
      continue; /* 1 word variant */
    if(locale[i] & (1UL << 30)) { /* t locale w/ var-length variants */
      while(i < locale_len - 1 && (locale[i + 1] & (1UL << 30)) &&
            ((locale[i + 1] & (1UL << 31)) ||
	     ((locale[i + 1] & 0xff) == UNI_CLDR_locale_ext_u_va &&
	      ((locale[i + 1] >> 8) & 0x3fffff) < UNI_NUM_CLDR_locale_u_va)))
        i++;
      continue;
    }
    const uni_CLDR_locale_ext_desc_t *desc =
                                  &uni_CLDR_locale_ext_desc[locale[i] & 0xff];
    switch(desc->type_len) {
      case UNI_LOCALE_EXT_BOOL: /* val in upper bytes */
      case UNI_LOCALE_EXT_ATTR: /* no val */
      case UNI_LOCALE_EXT_PRIV: /* can't ever happen */
      default: /* val in upper bytes */
        continue;
      case UNI_LOCALE_EXT_CP: /* len in upper bytes */
      case UNI_LOCALE_EXT_REORD: /* len in upper bytes */
        i += locale[i] >> 8;
	continue;
    }
  }
  /* sort fixed-length array */
  qsort(sort_array, sort_array_len, sizeof(*sort_array), cmp_locale_ext_ind);
  /* remove invalid duplicates */
  int sorted = 1;
  for(i = 0; i < sort_array_len - 1; i++) {
    if(sort_array[i + 1] < sort_array[i])
      sorted = 0;
    /* it's ok if 1st cmp picks up some t-langs */
    if((*sort_array[i] & 0xff) == (*sort_array[i + 1] & 0xff) ||
    /* since there can't be more than one t-lang anyway */
       ((*sort_array[i] & (1UL << 30)) && (*sort_array[i + 1] & (1UL << 30)))) {
      if((*sort_array[i] & (3UL << 30)) ||
         uni_CLDR_locale_ext_nameof[*sort_array[i] & 0xff].short_name[0] != 't') {
	/* always keep first one */
	movebuf(sort_array + i + 1, sort_array + i + 2, sort_array_len - i - 2);
	--sort_array_len;
	--i; /* recheck */
	sorted = 0;
      }
    }
  }
  /* shuffle data around if not already sorted */
  if(sorted)
    return locale_len;
  /* using a temporary array.. */
  uint32_t new_loc[locale_len - 1], *p = new_loc, *s;
  for(i = 0; i < sort_array_len; i++) {
    s = sort_array[i];
    *p++ = *s;
    if(*s & (1UL << 30)) {
      while(s < locale + locale_len - 1 && (s[1] & (1UL << 30)) &&
            ((s[1] & (1UL << 31)) ||
	     ((s[1] & 0xff) == UNI_CLDR_locale_ext_u_va &&
	      ((s[1] >> 8) & 0x3fffff) < UNI_NUM_CLDR_locale_u_va)))
        *p++ = *++s;
    } else if(!(*s & (1UL << 31))) {
      const uni_CLDR_locale_ext_desc_t *desc =
                                         &uni_CLDR_locale_ext_desc[*s & 0xff];
      if(desc->type_len == UNI_LOCALE_EXT_CP ||
         desc->type_len == UNI_LOCALE_EXT_REORD) {
        cpybuf(p, s + 1, *s >> 8);
	p += *s >> 8;
      }
    }
  }
  locale_len = p - new_loc;
  cpybuf(locale + 1, new_loc, locale_len);
  return locale_len + 1;
}
@

\lstset{language=txt}
<<FIXME>>=
set invalid flag for any bad lookup, and propagate to user
test aliasing:
  cmn-TW -> zh-TW
  sh -> sr-Latn
  sh-Cyrl -> sr-Cyrl
  sr-CS -> sr-RS
  hy-SU -> hy-AM
convert to canonical string:
  lang, ext = lc
  scr = tc
  ter = uc
  var = uc or lc (rec. lc, I guess)
convert to uni_uca_opts_t and various tables
  triplet, variants, u-va -> select locale
    break tabs: WB SB lb
    locale selects full default uni_uca_opts_t (including some unimpl.)
                        due to "settings" tags in locale data
  t-localeid t-k0 t-i0 t-t0 t-m0 -> ignore translation extension
  u-cu u-tz u-ca u-nu -> ignore non-collation, non-break tags
  u-lb - line break type (strict normal loose)
    unimplemented - see css3-text above
  u-co - collation type (ducet standard search ...) (default=standard)
     ducet means use original DUCET table
     root/standard is the unmodified CLDR table
     search* becomes search if not available in this locale
     or default for locale if not search* or search not available
     or standard if none of above available
     or root/standard if none of the above avail.
  u-ka - collation alt handling (noignore shifted) (default noignore)
    var_mode = UNI_UCA_VAR_MODE_NON_IGNORABLE
               UNI_UCA_VAR_MODE_SHIFTED
  u-kf - collation case folding (upper lower false) (default false)
    unimplemented; see caseFirst comment above
  u-ks - collation strength (level1 level2 level3 level4 identic)
    max_level = 1/2/3/4/5  (5 gets auto-converted to 4+do_literal)
  u-kv - collation maxvar (space punct symbol currency)
           set vartop to top of given reordering block
  u-kb - collation 2nd level backwards
    reverse_lev2
  u-kc - collation insert case level
    unimplemented; see caseLevel comment above
  u-kh - collation hiragana handling
    unimplemented; see hiraganaQuaternary comment aboe
  u-kk - collation normalization
    needs to be implemented at a higher level
    default for UCA is true, but for CLDR is false
  u-kn - collation numeric handling
    unimplemented; see numeric comment above
  u-vt - collation vartop
    var_top, but must set to top cp in cp's reordering block
  u-kr - collation reordering
    unimplemented; see reorder comment above
  u-va - variant (posix)
    not sure if affects collation
@

The remainder of the locale customization data is covered in another
document ([[uni-locale.nw]]).

\chapter{Code Index}
\nowebchunks

% Some might prefer having the Code Index in an appendix as well.
% For this document, I'm making the appendix the "user documentation"
\appendix

% Begin-doc Users-Guide
\chapter{Building}

Before starting, ensure that all prerequisites are present.  To build
this, build.nw and tjm-ext.nw are required.  If you obtained the PDF
or HTML version of this document, these are included.  Otherwise, they
may be obtained the same place you got this.  In addition, tjm-ext.nw
must be built (using the same build instructions as below, although
the command \texttt{make~libtjm-supt.a} is probably sufficient) in a
separate directory (i.e., without uni.nw in the same directory).  The
other prerequisites are the Unicode data files.  Make sure the file
locations are also set correctly in the config file.

\begin{itemize}
\item The Unicode Character Database; this library was tested using
versions 6.0.0, 6.2.0, 6.3.0 and 7.0.0.\\
\url{http://www.unicode.org/Public/zipped/}\emph{version}/UCD.zip
\item The Unicode Han Database; this should match the UCD version.\\
\url{http://www.unicode.org/Public/zipped/}\emph{version}/Unihan.zip
\item The Default Unicode Collation Element Table; this should match
the UCD version.  It should be placed in the same directory as the UCD.\\
\url{http://www.unicode.org/Public/UCA/}\emph{version}/*
\item The Unicode Common Locale Data Repository; make sure that the
CLDR version is compatible with the UCD version.\\
\url{http://www.unicode.org/Public/cldr/latest/core.zip}
\item The XML entity database; the only version currently supported is
the one at the following URL:\\
\url{http://www.w3.org/2003/entities/2007xml/unicode.xml}
\item The Unicode IDNA compatibility database (optional); this should
match the UCD version.  If present, it should be in the same directory
as the UCD, or in a subdirectory or sibling directory named idna.\\
\url{http://www.unicode.org/Public/idna/}\emph{version}/*
\item The Unicode security information database (optional); for recent
versions, this should match the UCD version, and for older versions,
it should at least be compatible with the UCD version.  If present, it
should be in the same directory as the UCD, or in a subdirectory or
sibling directory named security.\\
\url{http://www.unicode.org/Public/security/latest/}uts39-data-\emph{version}.zip\\
or, if the zip file is not present, all of the text files in that
directory.
\end{itemize}

\input{build-doc.tex} %%% doc
The full, documented [[makefile.config]] is reproduced here for
convenience.  In particular, the non-generic configuration parameters
start at [[UCD_LOC]].

\input{makefile.config.tex} % make

\chapter{API Documentation}

The easiest way to include this library's definitions is using
[[#include "uni_all.h"]].  Linking requires [[-luni]].  When using the
[[cmph]]-related symbols, linking requires [[-luni_cmph]] and
[[-lcmph]]; compiling requires [[-DUSE_CMPH]] as well as any include
path and other options required by [[cmph]].

Since this library is primarily a C version of the Unicode data files,
the versions of those files used to generate the code may be queried
using preprocessor macros ([[UNI_UCD_VER]] and [[UNI_CLDR_VER]]) or
using variables ([[const unsigned int uni_ucd_version, uni_cldr_version]]).

\section{Unicode Encodings}

\input{unitypes.tex} %%% doc

To support using valid UTF-8 and UTF-16 for internal storage, macros
are provided to navigate the string.  For consistency, UTF-32
navigation functions are provided as well.  The [[nextc]] and
[[prevc]] macros take a pointer to the start of a character, and
return the offset (in words) to the next or previous character,
respectively. The [[startc]] macro takes a pointer to an arbitrary
word in the string, and returns the offset (in words) to the start of
the character this word belongs to.  Since they are macros, [[buf]]
may be evaluated more than once.

% uni_utf32_startc prototype
% uni_utf32_nextc prototype
% uni_utf32_prevc prototype
% uni_utf16_startc prototype
% uni_utf16_nextc prototype
% uni_utf16_prevc prototype
% uni_utf8_startc prototype
% uni_utf8_nextc prototype
% uni_utf8_prevc prototype

For indexed navigation, functions are provided to convert an offset
into a word array to/from an index into the string.  Again, the
behavior is undefined if the UTF-8 or UTF-16 encoding is not valid up
to (but not necessarily including) the desired offset/index.  These
functions are expensive in that they scan the string from the
beginning for every call; use the macros above for small index
adjustments.  The 32-bit versions are dummy functions that return
[[n]], since offset and index are always identical.  They are mainly
for writing generic code using macros.

% uni_utf32_offset_of prototype
% uni_utf32_index_of prototype
% uni_utf16_offset_of prototype
% uni_utf16_index_of prototype
% uni_utf8_offset_of prototype
% uni_utf8_index_of prototype

To determine the length (in code points) of a string of fixed word
length, the [[index_of]] function for the appropriate type can be used
with the word length as the offset.  To determine the length of a
zero-terminated string, the following functions are provided.

% uni_utf32_strlen prototype
% uni_utf16_strlen prototype
% uni_utf8_strlen prototype

For truly random access to Unicode code points, they should be
converted to UTF-32 first.  For internally generated, guaranteed valid
strings, the following functions can be used to do that.  They return
the code point starting at the valid UTF-8/UTF-16 encoding at [[s]]. 
The optional [[nread]] parameter returns the number of words actually
read, although the [[nextc]] macros above could be used instead.

% uni_valid_utf8_decode prototype
% uni_valid_utf16_decode prototype

In fact, they should always be stored in the architecture's native
endianness.  For this reason, a set of functions without endianness
control is provided.  The fact that they are for internal use only is
emphasized by the ``int'' in the name.  While the UTF-32 versions do
mostly nothing, and the UTF-8 versions just copy other functions, a
complete set is provided to make creation of parallel routines for
differing inputs using preprocessor macros easier.  Each encoder takes
a pointer to a sufficiently large buffer (1 word for UTF-32, 2 for
UTF-16, and 4 for UTF-8) and the code point to encode, and returns the
number of words actually written.  Each decoder takes a pointer to
valid data, and returns the code point stored at that pointer.
Optionally, int can also return the number of words to skip if
[[nread]] is non-[[NULL]].

% uni_int_utf8_decode prototype
% uni_int_utf16_decode prototype
% uni_int_utf32_decode prototype
% uni_int_utf8_encode prototype
% uni_int_utf16_encode prototype
% uni_int_utf32_encode prototype

If the required output buffer length is needed for a character, one of
the following macros may be used.  They may evaluate their arguments
more than once.  These compute the output length of the encoder
functions.  To compute the amount consumed by the decoder functions,
use the [[nextc]] macros above.

% uni_utf8_enclen prototype
% uni_utf16_enclen prototype
% uni_utf32_enclen prototype

Another way to compute the length of a UTF-16 string is to just check
for surrogates.  A macro is provided for that.  The macro may evaluate
its parameter more than once, but is as fast as a simple comparison.

% uni_is_surrogate prototype

To directly compare two strings in a valid UTF format, [[memcmp]] can
be used, given the computed length.  For convenience, the following
functions can be used instead.  Note that these do not use the Unicode
Collation Algorithm or perform any locale-specific transformations;
they simply compare code point values.  Each string length may be
positive to indicate the number of words in the string (not
necessarily the number of characters), and it may be negative to
indicate zero termination.

% uni_int_utf32_strcmp prototype
% uni_int_utf16_strcmp prototype
% uni_int_utf8_strcmp prototype

To transform UTF-32 data to and from UTF-32BE and UTF-32LE, the
following functions are used.  The endianness is specified by
[[bige]]; zero specifies little-endian and non-zero big-endian.  The
encoder takes a [[cp]] and stores its result in a buffer; the stored
length is returned (0 on error).  The decoder takes a character from a
buffer, and returns the decoded character.  The return value is a
signed integer; a negative result indicates an error (i.e., an invalid
Unicode code point).

% uni_utf32_encode prototype
% uni_utf32_decode prototype

Similarly, conversion of UTF-32 to and from UTF-16BE and UTF-16LE is
done using the following functions.  At least two words must be
available in the encoder's return buffer, unless [[uni_utf16_enclen]]
is used to compute the output buffer length.  If the input buffer to
the decoder may be invalid, ensure that at least two words are
available as well (padding with zero is safe).  The results and
parameters have the same meaning as for the utf32 functions above,
except that the decoder also takes the length of [[s]] (in words), and
returns the number of words it read from the buffer in [[*nread]]
(optionally, if [[nread]] is non-[[NULL]]).  If [[s]] is too short,
[[*nread]] is always zero.

% uni_utf16_encode prototype
% uni_utf16_decode prototype

Similarly, conversion of UTF-32 to and from UTF-8 is done using the
following functions.  At least 4 bytes must be available in the
encoder's return buffer, unless [[uni_utf8_enclen]] is used to compute
the output buffer length.  The results and parameters have the same
meaning as for the utf16 functions above.

% uni_utf8_encode prototype
% uni_utf8_decode prototype

The above functions generally convert to and from UTF-32, one
code point at a time.  To convert internal (i.e., valid) strings from
one encoding to another in bulk, the following functions are provided.
The number after [[return]] indicates the input ([[str]]/[[len]]) type.
% Begin-doc UTF buffer return
Their return type is indicated by the trailing number in the function
name: UTF-32 (32), UTF-16 (16), or UTF-8 (8).  The return buffer is
pointed to by [[buf]].
% Begin-doc buffer return
If [[off]] is less than zero, [[buf]] points to a fixed-length buffer
of size [[*buf_len]].  In that case, if [[*buf_len]] is zero, [[buf]]
may be NULL; in fact, [[buf_len]] may be [[NULL]] as well to indicate
a zero-length return buffer.  If [[off]] is zero or more, the results
are placed into a dynamically resizable buffer starting at [[off]].
[[*buf]] is newly allocated using [[malloc]] or expanded using
[[realloc]] as needed; the current allocated length is in
[[*buf_len]]. A memory allocation failure causes [[NULL]] to be
returned in [[*buf]] along with a non-zero length.  The return value
of the function is the number of words which would have been written
to the output buffer if it had enough space; on successful dynamic
resizing, or if given a large enough fixed-sized buffer, this is the
actual number of words written. Otherwise, for fixed-sized buffers, at
most [[*buf_len]] words are written, and the function must be called
again with a larger buffer for the full results.

% End-doc buffer return
% End-doc UTF buffer return
When the input and output types match, this basically degenerates to a
straight copy, except that the buffer is resized if possible and
needed, and the length is restricted if needed.

% uni_return8_buf8 prototype
% uni_return8_buf16 prototype
% uni_return8_buf32 prototype
% uni_return16_buf8 prototype
% uni_return16_buf16 prototype
% uni_return16_buf32 prototype
% uni_return32_buf8 prototype
% uni_return32_buf16 prototype
% uni_return32_buf32 prototype

To read and write these types directly from/to a file, the following
functions may be used.  The writers return the number of words in the
target format (e.g. 32-bit words for UTF-32) written to the file on
success, and zero or less on failure.  If the failure was due to a
short write, the return value is the negative of the number of words
actually written.  Use [[ferror]] to distinguish between invalid input
and read/write errors.  The readers return
% Begin-doc utf-reader-ret
$-1$ if no characters could be read, $-2$ on a short (less than a full
code point) read, $-3$ on invalid code points, and otherwise the read
character.
% End-doc utf-reader-ret

Note that due to limitations of the C [[FILE]] API, it may not be
possible to recover from errors reading UTF-16 input.  A read of an
initial surrogate character, followed by anything but a final
surrogate character will result in the file's read pointer advanced
past the second word, which can no longer be returned to the input
stream.  However, the [[nread]] parameter, if non-[[NULL]], can be
used to determine the number of actual bytes read from the input
stream.  If that is 4, and an error occurred, then recovery could be
done by seeking backwards in the input stream (assuming it is
seekable).

% uni_utf32_putc prototype
% uni_utf16_putc prototype
% uni_utf8_putc prototype
% uni_utf32_getc prototype
% uni_utf16_getc prototype
% uni_utf8_getc prototype

For convenience, UTF-32 strings may also be written to UTF-8 files.  A
UTF-32 buffer [[buf]] of length [[len]] is written to UTF-8 file
[[f]].  Like the single-character output functions above, the return
value is the number of words (bytes in this case) written on success,
or less than or equal to zero on error.  If less than zero, this is
the (short) number of bytes written.

% uni_utf8_fputs prototype

Reading files whose encoding is unknown (e.g. UTF-16 and UTF-32, whose
endianness is determined by the file contents) is supported by a
separate mechanism.  Instead of using the standard C [[FILE]] type, it
uses the opaque type [[uni_file_t]].  Functions to open, close, and
read from such files are provided.

The [[uni_fopen]] function supports multiple input file encodings, as
specified by the [[encoding]] parameter.  If the library was compiled
with [[iconv]] support, all available input encodings for [[iconv]]
are supported.  Otherwise, only the raw input formats, listed below,
are supported.  The default (if [[encoding]] is [[NULL]]) is to use
the current locale's encoding if [[iconv]] support is compiled in and
the current locale's encoding can be obtained, or \verb|UTF|
(generic Unicode) if not.

\paragraph*{Built-in Unicode encodings (case-sensitive):}
\begin{quote}
\begin{description}
\item[ANSI\_X3.4-1968] --- ASCII (i.e., raw bytes; no range checking)
\item[ISO-8859-1] --- ISO Latin-1 (i.e., raw bytes)
\item[UTF] --- Automatically detected UTF mode
\item[UTF-8] --- UTF-8
\item[UTF-16] --- UTF-16; if first character in file is U+FEFF in
inverse native byte order, use that order; otherwise, use native byte order
\item[UTF-16LE] --- Little-endian UTF-16
\item[UTF-16BE] --- Big-endian UTF-16
\item[UTF-32] --- UTF-32; if first character in file is U+FEFF in
inverse native byte order, use that order; otherwise, use native byte order
\item[UTF-32LE] --- Little-endian UTF-32
\item[UTF-32BE] --- Big-endian UTF-32
\end{description}
\end{quote}

Note that unlike [[uni_utf16_getc]], all of the internal encoding
types consume the longest erroneous sequence, and no more, when an
encoding error is encountered.

For generic Unicode input ([["UTF"]]), if the first character could be
Byte Order Mark (U+FEFF) in any supported format, it is considered to
be one, and the format of the file is the format of that Byte Order
Mark:

\begin{quote}
\begin{tabular}{ll}
First bytes of file&Format\\
\texttt{00 00 FE FF}&UTF-32BE\\
\texttt{FF FE 00 00}&UTF-32LE\\
\texttt{FE FF}&UTF-16BE\\
\texttt{FF FE}&UTF-16LE\\
Other&UTF-8\\
\end{tabular}
\end{quote}

Note that a UTF-16LE file whose first character after the Byte Order
Mark is zero is determined to be a UTF-32LE file, instead.  While a
potential conflict exists between all other formats and ``Other'',
there is no valid UTF-8 character which starts with FF or FE, so there
should be no conflict.  Also, the Byte Order Mark is part of the file
data, so it is returned as the first character.

The return value for [[uni_fopen]] is [[NULL]] on error, or a pointer
to the file structure.  This structure is freed by [[uni_fclose]].
The [[uni_fgetc]] return value is
\input{utf-reader-ret.tex} %%% doc

% uni_fopen prototype
% uni_fclose prototype
% uni_fgetc prototype

Since normalization is a common requirement for user input, a function
to read a single normalized character is provided as well.  The [[nt]]
parameter takes a normalization type, corresponding to the standard
normalization type names:  [[UNI_NORM_NFD]], [[UNI_NORM_NFKD]],
[[UNI_NORM_NFC]], [[UNI_NORM_NFKC]], and [[UNI_NORM_NFKC_CF]].  For
completeness, no normalization can be specified using
[[UNI_NORM_NONE]], although it is cheaper to just use [[uni_fgetc]].

The entire file must be read using this function in order for it to
work properly.  Changing the normalization type mid-stream is also
generally not supported, since any normalization in progress will not
be affected.

Errors in the input stream are ignored.  Encoding errors and invalid
Unicode characters (surrogates and code points larger than
[[0x10FFFF]]) are converted to U+FFFD, REPLACEMENT CHARACTER.  There
is no way to determine the difference between the presence of an
actual REPLACEMENT CHARACTER and invalid text.

% uni_fgetc_norm prototype

\section{Data Types for Property Storage}

\subsection{Bit Sets}

The following functions support storage of Unicode boolean properties
in simple bit sets.  [[a]] is an array of any unsigned integer data
type.  The [[UNI_BSET_ELT]] and [[UNI_BSET_BIT]] convert an index
[[i]] into an array index [[e]] and bit number within that array
element [[b]].  The [[UNI_BSET_ENTRY]] function performs the reverse
operation, giving an index [[i]] from an element index [[e]] and bit
index [[b]].  The [[UNI_BSET_MASK]] function converts the bit index
into a mask.  Several common uses for this mask are taken care of:
[[UNI_BSET_IS_SET]] tests if a given index is set, and
[[UNI_BSET_SET]] and [[UNI_BSET_CLEAR]] modify a specific bit.  The
final useful function is [[UNI_BSET_AELT]], which returns the element
at index [[UNI_BSET_ELT]].  This is an addressable lvalue; that is,
you can take its address or assign a value to it.

Normally, extra code would be added to shorten the array by indicating
a lower and upper bound.  For convenience, the common upper bound,
which is the largest valid Unicode code point, is provided as a
preprocessor symbol ([[UNI_MAX_CP]]).

% UNI_BSET_ELT prototype
% UNI_BSET_BIT prototype
% UNI_BSET_ENTRY prototype
% UNI_BSET_MASK prototype
% UNI_BSET_IS_SET prototype
% UNI_BSET_SET prototype
% UNI_BSET_CLEAR prototype
% UNI_BSET_AELT prototype

\subsection{Sorted Code Point Arrays}

An array of any data type whose first member is a 32-bit signed
integer can be searched and sorted using the standard C [[bsearch]]
and [[qsort]] functions with the following comparison routine:

% uni_cmp_cp prototype

Note that valid Unicode code points are always positive when stored in
a signed 32-bit integer.  A convenience wrapper for [[bsearch]] is
provided, but in general a hand-written binary search can perform
significantly faster.

% uni_is_cp prototype

For convenience, a structure is defined for code point arrays with
32-bit values.  As the [[cp]] member is first, [[uni_cmp_cp]] works
with arrays of this type as well, if [[cp]] is a valid Unicode code
point.  If not, care must be taken that the value never exceeds the
maximum [[int32_t]] value ([[0x7fffffff]]).

\input{[[uni_cp_val_t]].tex} % C

\subsection{Sorted Code Point Range Arrays}

Sorted code point range arrays are arrays of structures whose first
two members define an (inclusive) range; the ranges do not overlap, so
sorting can be done using only the low element.

\input{[[uni_chrrng_t]].tex} % C

An array of any data type whose first two members are 32-bit signed
integers indicating a contiguous range of values, as per the
[[uni_chrrng_t]] data type (if the [[low]] and [[high]] values do not
exceed the maximum [[int32_t]] value), may be searched and sorted
using the standard C [[bsearch]] and [[qsort]] functions with the
following comparison routine:

% uni_cmprng prototype

A convenience wrapper for binary searching is provided.  It uses a
hand-written binary search for significant performance improvement
over using [[bsearch]].  However, unlike the macro provided for single
code point arrays, this function can only be used for arrays of the raw
[[uni_chrrng_t]] type.  Since it uses the raw structure, values
exceeding the maximum [[int32_t]] value are treated as unsigned values
still, so care must be taken that sorting either uses a different
comparison function than above or that the values never exceed this
value.

% uni_is_cp_chrrng prototype

In addition to plain ranges, ranges may store data.  One variant,
[[uni_chrrng_dat8_t]], stores a byte of data along side a
24-bit high range value.  This keeps the structure the same size, but
makes it a bit less efficient for searching:

\input{[[uni_chrrng_dat8_t]].tex} % C

The comparison function for [[qsort]] and [[bsearch]] is different,
since it needs to mask out the value:

% uni_cmprng_dat8 prototype

Since the return value is a byte rather than a flag, the wrapper for
binary searching (again, using a hand-written implementation rather
than [[bsearch]]) returns the byte value, or a given default ([[def]])
if the code point is not in the array.

% uni_chrrng_dat8 prototype

16-bit and 32-bit variants are provided as well.  Each attempts to
keep the size of the structure down to 16 bytes.  The 16-bit version
does this the same way as the 8-bit version:  by storing the code
points as 24-bit numbers, and using the remaining 8 for storage.  The
32-bit version uses a low+length method rather than low-high method,
and disallows ranges longer than 256.  Again, special comparison
functions for [[qsort]] and [[bsearch]] are provided, along with
optimized binary searches.

\input{[[uni_chrrng_dat16_t]].tex} % C
\input{[[uni_chrrng_dat32_t]].tex} % C
% uni_cmprng_dat16 prototype
% uni_chrrng_dat16 prototype
% uni_cmprng_dat32 prototype
% uni_chrrng_dat32 prototype

A special variant of the 32-bit type above interprets the 32-bit value
as a rational number with a 24-bit signed numerator and an 8-bit
unsigned denominator.  The 32-bit number is directly cast to this data
type.  Technically, any pair of numbers could be stored in this
structure; the only thing the functions do to support this
interpretation is to set the denominator to one if the numerator is
zero, so that zero can be more conveniently stored as zero over zero.
Also, the default return value is always zero (over one).

\input{[[uni_frac_t]].tex} % C

While the [[uni_cmprng_dat32]] function could be used, the following
function returns the correctly interpreted numerator and denominator.

% uni_chrrng_val prototype

\subsection{Multilevel Tables}

Multi-level tables are bit arrays split into smaller chunks, with
pointers to those chunks in a table at the next level.  Each level
except the top is also split into smaller chunks with the next level
being pointers to that level.  Duplicates in lower levels are removed
by having only one copy with multiple pointers to that copy.  All-zero
data and all-one data are also stored with special pointer values,
making sparse and repetitive bit arrays fairly storage-efficient.

The tables are stored as an opaque sequence of 32-bit words which are
interpreted dynamically.  When reading such a data structure, only the
pointer to the first word is needed.  When creating such a structure,
both the sequence of words and the total length are returned.

Use the following function to convert a plain bit array to a
multi-level table.  It may take a long time to run, so generally it
should be used to pre-generate tables.  The bit array is described by
[[bits]], which is an array of [[len]] bytes.  The [[low]] and
[[high]] numbers are the valid bit numbers; it is up to the user to
ensure that [[high]] - [[low]] $<$ [[len]] times 8.  The data need
not be a bit array; arbitrary data structures are supported.  The
[[minwidth]] parameter is the number of bytes in the data structure;
this is ignored if less than 4.  It is possible to make values outside
of the [[low]] to [[high]] range be either all zeroes or all ones;
[[def]] should be one or zero to indicate which.  The return value,
[[*ml]], is always allocated using [[malloc]], and contains [[*ml_len]]
32-bit words.

% uni_bits_to_multi prototype

Once a table [[dat]] is generated, the first byte of data for a code
point [[val]] can be found using [[uni_multi_tab_lookup]].  For data
structures whose size is not one byte, the code point must be
multiplied by the number of bytes.  For bit arrays, this means
dividing by 8.  For bit arrays, the actual bit must then be obtained
using a bit mask.

The return value from the function is zero for all zeroes, [[~0]] for all
ones, and 1 for anything else.  [[*ret]] is [[NULL]] for all zeroes or
all ones, and a pointer to the first byte of the result otherwise.  If
the code point is outside the range defined by the table ([[low]] and
[[high]] during creation), and [[def]] is zero, the default provided
during table creation is returned (i.e., all zeroes or all ones).  If
[[def]] is non-zero, [[def]] is returned by the function, but [[*ret]]
is [[NULL]].  Note that if [[def]] is [[~0]], there is no way to tell
the difference between out-of-range and all ones.

% uni_multi_tab_lookup prototype

The above lookup function returns a pointer to raw data.  Since they
are most common, bit array tables can be queried using a simpler
function, which takes the unscaled code point and returns a boolean
flag.

% uni_is_x prototype

To get a list of all members of a multi-level table bit array, it can
be converted into a sorted range list.  The result ([[*ret]]) is always
allocated using [[malloc]].

% uni_multi_bit_to_range prototype

The inverse operation is also possible, but it consumes a large amount
of time and memory.  Internally, the range list is first converted
into a plain bit array, which is then converted using
[[uni_bits_to_multi]] above.  The return value is the multi-level table,
allocated using [[malloc]].  If [[ml_len]] is non-NULL, the storage
size of the table is returned as well.

% uni_rng_to_multi_bit prototype

The multi-level tables can store more information, as well.  The
lookup function merely returns a pointer to a byte.  This byte can be
a collection of bits, or it can be a byte value.  For the latter case,
a separate generic lookup function is provided.  This takes a
byte-sized default value, and returns this if the byte is out-of-range
or zero.  Otherwise it returns the stored value, minus one.  This way,
the default value is stored most efficiently (as a zero), and the rest
only require minor adjustment on return.

% uni_x_of prototype

A function is provided to convert a [[uni_chrrng_dat8_t]] sorted range
list to an offsetted multi-level byte array.  However, no inverse
function is provided.  This operation is not really intended for end
users, but for the static table generator.  Note that while the
default value must be provided, it is not stored, but instead is used
to remove the default value from the table by converting it to zero.

% uni_rng_dat8_to_multi prototype

By multiplying the value size with the lookup index, larger values can
be stored as well.  Raw 16-bit or 32-bit values are directly
supported.  Both lookup and conversion from a [[uni_chrrng_dat16_t]]
type or a [[uni_chrrng_dat32_t]] type may be performed.  However, the
inverse conversion is not supported.

% uni_x_dat16 prototype
% uni_rng_dat16_to_multi prototype
% uni_x_dat32 prototype
% uni_rng_dat32_to_multi prototype

Another implementation is to store the numerator and denominator of a
rational number as a 32-bit value: 24 bits for a signed numerator, and
8 bits for an unsigned denominator.  A special lookup function is
provided to split out the numerator and denominator from the 32-bit
value; otherwise use the 32-bit functions above.

% uni_x_val prototype

For values where ranges are inappropriate, a function is provided to
convert a sorted code point array with 32-bit values (nominally
[[uni_cp_val_t]]) into a multi-level table.  No inverse function is
provided.  No generic lookup function is provided, either; remember to
multiply code point values by 4 when using [[uni_multi_tab_lookup]].

% uni_cp_val_to_multi prototype

\section{All Properties}

Information on all currently supported properties is provided by
several arrays.  They are intended to be used to scan for desired
properties, find property names and their aliases, and generate data
based on that.  However, there is no harm in using it if you intended
to support every single property, anyway.  Including a reference to
any of these symbols will likely defeat the static linking property of
this library, pulling every single property's data in.

\begin{itemize}
\item [[const uni_propdesc_t uni_propdesc[]]] is an array whose length
is given by the preprocessor symbol [[uni_propdesc_len]].  Each entry
describes a property, and points to its data.  See below for a
description of the elements' data structure.
\item [[uni_propdesc_valueof]] is an array whose length is given by
the preprocessor symbol [[uni_propdesc_valueof_len]].  It is a sorted
array of names for lookup of properties by name rather than sequence
number.  See Enumeration Properties (section \ref{sec:enum}) for
details on the structure and how to use it.
\item [[uni_propdesc_valueof_approx]] is an array whose length is
given by the preprocessor symbol [[uni_propdesc_valueof_len]].
Its usage is similar to the previous item; see Enumeration Properties
(section \ref{sec:enum}) for details.
\end{itemize}


The [[name]] field contains the canonical name and all known aliases
for the property name; see Enumeration Properties (section
\ref{sec:enum}) for details on the [[uni_alias_t]] structure and how
to use it..  The [[mtab]] and [[mtab_len]] fields describe the
multi-level table for the property.  The [[tab]] and [[tab_len]]
fields describe the sorted range table or sorted code point table for
the property.  See the sections below for details on what these tables
contain.  The [[type]] field determines which section describes the
tables:

\begin{itemize}
\item [[UNI_PROP_TYPE_BOOL]]: Boolean Properties
\item [[UNI_PROP_TYPE_ENUM]]: Enumeration Properties; in addition, the
name translation tables are in [[nameof]], [[valueof]],
[[valueof_approx]], [[nameof_len]], and [[valueof_len]].  The default
is in [[def]].
\item [[UNI_PROP_TYPE_NUM]]: Numeric Properties
\item [[UNI_PROP_TYPE_STR]]: String Properties; in addition, the
length of the string table is [[strs_len]], and the string table is in
either [[strs8]], [[strs16]] or [[strs32]], depending on the string
data type.
\item [[UNI_PROP_TYPE_DAT16]]: The FCD property (sec. \ref{FCD})
\end{itemize}

\input{[[uni_propdesc_t]].tex} % C

\section{Boolean Properties}

The supported boolean properties are listed in table
\ref{tab:boolprop}.  For each supported boolean property \emph{BP},
the following symbols are defined:

\begin{itemize}
\item [[const uni_chrrng_t uni_]]\emph{BP}[[_rng[]]] is a sorted range
table with entries for that property.
\item [[uni_]]\emph{BP}[[_rng_len]] is the number of entries in that
array (a preprocessor symbol).
\item [[const uint32_t uni_]]\emph{BP}[[_mtab[]]] is a multi-level
table for that property.
\item [[uni_]]\emph{BP}[[_mtab_len]] is the length of that table (a
preprocessor symbol), in case it is to be written out to a file.
\item [[int uni_is_]]\emph{BP}[[(uint32_t cp)]] is a boolean lookup
function for that property.  It is actually a preprocessor macro which
uses the multi-level table and [[uni_is_x]] internally.
\end{itemize}

For static linking, each property is in a separate object file, and
the sorted range table and multi-level table are stored separately, as
well.

\begin{table}
\begin{centering}
\begin{tabular}{lllll}
AHex&ASSIGNED&Alpha&Bidi\_C&Bidi\_M\\
CE&CI&CWCF&CWCM&CWKCF\\
CWL&CWT&CWU&Cased&Comp\_Ex\\
DI&Dash&Dep&Dia&Ext\\
Gr\_Base&Gr\_Ext&Hex&IDC&IDS\\
IDSB&IDST&Ideo&Join\_C&LOE\\
Lower&Math&NChar&NFD\_QC&NFKD\_QC\\
OAlpha&ODI&OIDC&OIDS&OLower\\
OMath&OUpper&OGr\_Ext&Pat\_Syn&Pat\_WS\\
QMark&Radical&SD&STerm&Term\\
UIdeo&Upper&VS&WSpace&XIDC\\
XIDS\\
\end{tabular}
\end{centering}

~

{\small Note: NFD\_QC and NFKD\_QC are actually enumerated properties
with only two values: Y and N.}

\caption{\label{tab:boolprop}Boolean properties (Unicode canonical short name)}
\end{table}

\subsection{Bit Set Operations}

The following functions perform a generic bit operation (see table
\ref{tab:setop2} for a complete list of operations).

\input{op-table.tex} %%% doc

\caption{\label{tab:setop2}Set Operations}
\end{table}

Bit arrays consisting of a single unsigned integer can be operated on
using the following macro:

% UNI_BIT_SET_OP prototype

The [[op]] argument is intended to be one of the [[uni_setop_t]]
enumeration constants.

\input{[[uni_setop_t]].tex} % C

Arbitrary bit arrays of standard C unsigned integer types can be
operated on using the following functions.  The inputs are bit arrays
[[a]] and [[b]], which start at [[a_low]] and [[b_low]], respectively,
and contain [[a_len]] and [[b_len]] elements of the particular bit
size, respectively.  Although the endianness of each element of an
array is irrelevant, the array itself is assumed to be in
little-endian order if the arrays are not of identical size and
offset.  The result ([[res]]) is allocated from new memory using
[[malloc]] if the pointed-to pointer is NULL, or resized using
[[realloc]] if non-NULL and the reported [[res_max]] is too small. 
Its low bit is [[res_low]].  The number of relevant elements of the
bit size is returned in [[res_len]], and the actual number of elements
allocated is returned in [[res_max]].  All return pointers must be
non-[[NULL]].

% uni_setop_bit64 prototype
% uni_setop_bit32 prototype
% uni_setop_bit16 prototype
% uni_setop_bit8 prototype

Since sorted code point arrays are generally inefficient for storing
Unicode boolean properties (sets), no set operations are provided for
this data type.  However, the following sample code may be used if
needed:

\input{Perform set ops on cp array.tex} % C

For sorted code point range arrays, a single function is provided.  It
always allocates its results using [[malloc]].

% uni_chrrng_setop prototype

No set operations are provided for multi-level tables at this time.
Implementation would be insanely complex given the current
implementation, and should not be performed frequently, anyway.  The
only way to perform these operations right now is to convert the
multi-level tables to range arrays first, and then convert the result
back.  Alternately, the lookup function can simply be called for each
table, and the lookup results combined instead, using
[[UNI_BIT_SET_OP]].  For example:

\begin{quote}
\begin{verbatim}
res = UNI_BIT_SET_OP(uni_is_X(cp), op, uni_is_Y(cp)) & 1;
\end{verbatim}
\end{quote}

\section{\label{sec:enum}Enumerated Properties}

For lists of names, there are both the canonical names and aliases.
These are stored in an array of [[uni_alias_t]] structures.  Any
undefined names are [[NULL]].

\input{[[uni_alias_t]].tex} % C

For reverse lookups, the [[uni_valueof_t]] structure's [[val]] field
is the index of that name in the alias array.

\input{[[uni_valueof_t]].tex} % C

An array of [[uni_valueof_t]] structures can be searched or sorted
using the generic comparison function.

% uni_cmp_valueof prototype

If the name entry is stripped of separators and converted to
lower-case, then the Unicode approximate matching technique can be
applied.  This converts the input string to lower-case, strips out
separators (hyphens, underscores, and spaces), and, if the raw string
is not found, strips off a leading ``is''.

% uni_x_valueof_approx prototype

It performs its function by first stripping the name; manual stripping
can be done with the following function.  After stripping, the name
can be looked up directly using [[uni_cmp_valueof]] twice:  first
without further modification, and second after striping any leading
``is''.

% uni_enum_name_strip prototype

The list of supported enumeration properties is in table
\ref{tab:enumprop}.  For each enumerated property \emph{EP}, the
following symbols are defined:

\begin{itemize}
\item [[uni_]]\emph{EP}[[_t]] is a C enumeration type whose literals
correspond to the property's values.
\item For each canonical enumeration value name \emph{EN}, the symbol
[[UNI_]]\emph{EP}[[_]]\emph{EN} is defined, which is an enumeration
literal of type [[uni_]]\emph{EP}[[_t]].  Enumeration literals
corresponding to canonical aliases are defined as well, as long as they
do not contain a minus sign or a decimal point.
\item [[UNI_NUM_]]\emph{EP} is another enumeration literal of type
[[uni_]]\emph{EP}[[_t]], which corresponds to the number of unique
values.  Note that where canonical aliases are present, this number
is smaller than the actual number of enumeration symbols.  Also, where
numerical aliases exist, this may be larger than the number of symbols
present.
\item [[const uni_alias_t uni_]]\emph{EP}[[_nameof[]]] is an array of
names corresponding with the enumeration literals.  The index is
simply a valid enumeration literal value, or any positive integer less
than [[UNI_NUM_]]\emph{EP}.
\item [[const uni_valueof_t uni_]]\emph{EP}[[_valueof[]]] is a sorted
array of all unique names (including aliases) for this property; the
[[val]] field in each entry is the associated enumeration literal.
\item [[uni_valueof_t uni_]]\emph{EP}[[_valueof_approx[]]] is a sorted
array of all unique names (including aliases) for this property,
stripped of underscores and converted to lower-case for loose
matching.  Like the [[valueof]] array, the [[val]] field in each entry
is the associated enumeration literal.
\item [[uni_]]\emph{EP}[[_valueof_len]] is the length of the
[[valueof]] and [[valueof_approx]] arrays (a preprocessor symbol).
\item [[uni_]]\emph{EP}[[_t]] [[uni_]]\emph{EP}[[_lookup(const char *n)]]
is a lookup function which will strip underscores, hyphens, and spaces
from the lookup string, optionaly strip off any Is, and return the
enumeration constant matching that string.  This is actually a
preprocessor macro which uses the [[uni_x_valueof_approx]] function
with the [[valueof_approx]] array internally.
\item [[const uni_chrrng_dat8_t uni_]]\emph{EP}[[_rng[]]] is a sorted
array of ranges.  The default value is not provided here.
\item [[uni_]]\emph{EP}[[_rng_len]] is the length of the range
array (a preprocessor symbol).
\item [[uint32_t *uni_]]\emph{EP}[[_mtab]] is a multi-level table whose
byte-length values correspond to the enumeration literal values.
\item [[uni_]]\emph{EP}[[_mtab_len]] is the size of the multi-level
table (in words), in case the table is to transferred or written out.
\item [[int uni_]]\emph{EP}[[_of]] is a preprocessor macro which
returns the enumeration value for any code point.  The canonical
default is returned when the table has no entry.  Internally, the
multi-level table and [[uni_x_of]] are used for lookup.
\end{itemize}

For static linking, each property is in a separate object file.  In
addition, the range tables and multi-level tables are stored
separately.  The two plain name tables are stored together in order to
share constant name strings, but are stored separately from the range and
multi-level tables.  The approximate name table is stored separately,
as well.

\begin{table}
\begin{centering}
\begin{tabular}{llll}
age&bc&blk&ccc\\
dt&ea&gc&GCB\\
hst&jg&jt&lb\\
NFC\_QC&NFKC\_QC&nt&sc\\
SB&WB&IDNA\_Status&ID\_Restrict\_Status\\
ID\_Restrict\_Type&bpt&&\\
\end{tabular}
\end{centering}

~

{\small Note: age and ccc are technically numeric properties.}

\caption{\label{tab:enumprop}Enumerated properties (Unicode canonical short name)}
\end{table}

The IDNA\_Status property will not be available if the IDNA data files
were not present when this library was compiled.  The
ID\_Restricted\_Status and ID\_Restricted\_Type fields will not be
available if the security data files were not available when this
library was compiled.  The ID\_Restrict\_Type enumeration literals
change minus signs to underscores; this is the only current
enumeration that supports enumeration names with minus signs.

In addition, the gc property defines a symbol to support aliases which
cover more than one value.  For example, the enumeration literal
[[UNI_gc_Z]] is actually an alias for [[UNI_gc_Zl]], [[UNI_gc_Zp]],
and [[UNI_gc_Zs]].  The [[const uint64_t uni_gc_trans[]]] table takes
an enumeration literal as index, and returns a bit mask with all
appropriate base classes included with this literal set.  The base
classes themselves only have one bit set; to check if a class is a
base class, simply check if the bit is set for itself.

There are two additional support macros for the gc property.
[[uni_is_nl]] checks if a character is any valid newline-type character,
and [[uni_is_bs]] checks if a character is any valid backslash-type
character.  [[uni_is_bs]] is not necessary for text already processed
using NFKC\_CF.

% uni_is_nl prototype
% uni_is_bs prototype

\subsection{Boundary Detection}

Several of the enumerated properties are designed for boundary
detection.  These require an additional set of rules to be useful.
Those rules are provided in the string section, but require an
ICU-like regular expression engine to use.  Instead, the following
functions may be used.

A grapheme cluster generally corresponds to a single user-visible
character (glyph).  The following function can tell you if a grapheme
cluster boundary exists between any two characters.  For improved
performance, the return value can be passed in to [[prevret]] when
scanning a sequential string; that is, when [[next]] from the previous
call is passed in as [[prev]].  However, other than improved
performance, there is no reason not to pass zero for this parameter
every time:  the function retains and requires no state.

Unicode defines two types of grapheme cluster:  legacy and extended.
By default the extended form is detected.  Setting [[legacy]] to
non-zero selects the legacy form.

The return value is less than zero if there is no break between
[[prev]] and [[next]], and greater than zero otherwise.

% uni_gc_brk prototype

What constitutes a word in text is locale and context dependent.
Often detecting words requires a dictionary as well.  Unicode provides
two methods of detecting words, though:  a simple one based mostly on
the character being looked at, and a more complex one which detects
the boundaries between words.

The following function returns zero if a character is definitely not a
word character, more than zero if the character definitely is a word
character, and less than zero if it is a word character only if either
the previous character is also a word character or there is no
previous character.  Words are then defined as the longest contiguous
string of word characters.

No state is used or saved, so this function can be applied anywhere.
Manual backtracking is, however, required when a negative value is
returned.

% uni_is_simple_word prototype

Unicode also has an algorithm similar to the grapheme cluster
algorithm:  Given two characters, find if they belong to the same word
or not.  Non-word characters may be included in a word by this
definition, so additional filtering may be necessary.

Explicit backtracking is necessary.  It is not generally possible to
begin detecting a word in the middle of a string.  Instead, detection
must start either at the beginning of a string or at the last known
good word boundary.  If the last boundary was set by backtracking, it
is better to start one earlier.  The safest is to just always start
two words behind the current location.  At the start of a scan, pass
in zero for [[prevret]].  Otherwise, always pass in the previous
return value.

The return value is one of three different ranges:  less than $-1$
indicates [[prev]] and [[next]] are in the same word; greater than $1$
indicates [[prev]] and [[next]] are not in the same word, and $1$ and
$-1$ indicate that the relationship is not yet known.

When the relationship is not yet known, the same value is returned
every time until the relationship is known.  For example, call the
first pair of characters [[prev1]] and [[next1]].  After the first
time $-1$ or $1$ is returned, any subsequent $-1$ or $1$ indicates
that [[prev]] and [[next]] are definitely in the same word, but still
says nothing about [[prev1]] and [[next1]].  Once a value outside of
the $-1$ to $1$ range is returned, that value indicates the
relationship between [[next]] and [[prev]], as well as the
relationship between [[next1]] and [[prev1]].  If the end of string is
reached before anything but $1$ and $-1$ has been returned, [[next1]]
and [[prev1]] are not in the same word.

For example, given the string [[c1 c2 c3 c4 c5 c6]], and passing in
[[c1]] and [[c2]], and then [[c2]] and [[c3]], etc., to produce the
return values [[-5 -1 -1 -1 3]] indicates that [[c1]] and [[c2]] are
in the same word ($-5$ indicates no break).  [[c3]] starts a new word
($-1$ is deferred until $3$ is encontered, indicating a break),
containing [[c3]], [[c4]], and [[c5]] (subsequent $-1$ values always
indicate no break).  [[c6]] once again starts a new word ($3$
indicates a break).  Note that there will never be more than one
backtrack position at once, and that the backtrack position is no
longer needed once it is resolved.  The next $-1$ or $1$ starts a new
backtracking session.

% uni_word_brk prototype

Currently, locale dependencies are hard-coded, and enabled by using
locale-dependent word break property tables ([[tab]]).

What constitutes a sentence in text is even more context and locale
dependent.  Unicode tries to provide a standard means to do so,
though.  Like the word boundary checker, the sentence boundary checker
determines if a sentence break exists between two characters.

Explicit backtracking is necessary.  It is not generally possible to
begin detecting a sentence in the middle of a string.  Instead,
detection must start either at the beginning of a string or at the
last known good sentence boundary.  If the last boundary was set by
backtracking, it is better to start one earlier.  The safest is to
just always start two sentences behind the current location.  At the
start of a scan, pass in zero for [[prevret]].  Otherwise, always pass
in the previous return value.

As with words, values less than $-1$ always indicate that [[prev]] and
[[next]] are in the same sentence, and values greater than $1$ always
indicate that they are not.  As with words, $1$ or $-1$, when first
encountered, indicates an indeterminate (backtracking) position.
Unlike words, the subsequent return values must be exactly the same
($1$ or $-1$) to indicate that they are part of the same sentence as
the first character.  If the return value is not the same, but also
not $1$ or $-1$, then, as with words, the original indeterminate
location gets the same treatment as the pair which returned different
values.  On the other hand, if the return value is not the same, but
it is still $1$ or $-1$, then it indicates that no break is permitted
between the characters which returned this value, but a break
\emph{is} required at the original indeterminate location.  Also
unlike the word boundary detector, if no change is made before the end
of the string, the indeterminate location's handling is determined by
the sign of the return value.  $1$ indicates a break, and $-1$
indicates no break.

For example, given the string [[c1 c2 c3 c4 c5 c6]], and passing in
[[c1]] and [[c2]], and then [[c2]] and [[c3]], etc., to produce the
return values [[-5 1 1 1 -1]] indicates that [[c1]] and [[c2]] are
in the same sentence ($-5$ indicates no break).  [[c3]] starts a new
sentence ($1$ is deferred until $-1$ is encontered, indicating a
break),  containing [[c3]], [[c4]], and [[c5]] (subsequent $1$ values
always indicate no break), as well as [[c6]] (just changing $1$ to
$-1$ indicates end of backtracking and no break).  Note that there
will never be more than one backtrack position at once, and that the
backtrack position is no longer needed once it is resolved.  The next
$-1$ or $1$ starts a new backtracking session.

% uni_sentence_brk prototype

In addition to a property table override for locale support, this
function also supports a break suppression list.  This list must be a
sorted array of UTF-8 encoded strings (sorted by simple byte value)
normalized the same way as text to be scanned.  Any text matching a
suppression never breaks a sentence after the next character.  No
attempts are made to ensure matches start at a word boundary, and no
transformations are done to the list or to text being scanned prior to
matching.  If the override list is given, it is only enabled if an
additional state parameter ([[sup_state]]) is supplied as well.

\input{[[uni_sb_locale_t]].tex} % C

Unicode assumes in the above function that paragraphs are ended by
either explicit end-of-line characters or paragraph separators.  This
means that to use this function, if you are using doubled end-of-line
characters to separate paragraphs, you will have to first strip the
paragraph of all explicit newlines before detecting sentences.

\subsection{Word Wrapping}

Given a paragraph of text, Unicode provides an algorithm to narrow
that text (if possible) by inserting soft line breaks within the
paragraph.  These soft line breaks may be placed at any potential
break point between two characters.  As mentioned above, Unicode
assumes that paragraphs are delimited by single hard end-of-line
characters or explicit paragraph separators, so if this algorithm is
being used to take text that uses double-newlines to separate
paragraphs, the explicit line separators within the paragraph must be
stripped, first.

Explicit backtracking is necessary.  It is not generally possible to
begin detecting a potential line break in the middle of a string.
Instead, detection must start either at the beginning of a string or
at the last known good break point.  If the last break point was set by
backtracking, it is better to start one earlier.  The safest is to
just always start two break points behind the current location.  At the
start of a scan, pass in zero for [[prevret]].  Otherwise, always pass
in the previous return value.

The return values work almost exactly like with sentences.  The only
difference is that a return value of zero indicates that [[prev]] and
[[next]] must always be split.  Values less than $-1$ always indicate
that [[prev]] and [[next]] should remain together, and values greater
than $1$ always indicate that they may be split if desired.  $1$ or
$-1$, when first encountered, indicates an indeterminate
(backtracking) position. The subsequent return values must be exactly
the same ($1$ or $-1$) to indicate that they are part of the same
sentence as the first character.  If the return value is not the same,
but also not $1$ or $-1$, then the original indeterminate location
gets the same treatment as the pair which returned different values.
On the other hand, if the return value is not the same, but it is
still $1$ or $-1$, then it indicates that no break is permitted
between the characters which returned this value, but a break
\emph{is} permitted at the original indeterminate location. If no
change is made before the end of the string, the indeterminate
location's handling is determined by the sign of the return value. $1$
indicates a break is permitted, and $-1$ indicates no break is
permitted.  See [[uni_sentence_brk]] for an example.

% uni_line_brk prototype

As with the above two functions, [[tab]] may be set for
locale-specific property table overrides.

\section{Numeric Properties}

Each numeric property \emph{NP} is supported by the following symbols:

\begin{itemize}
\item [[const uni_chrrng_dat32_t uni_]]\emph{NP}[[_rng[]]] is a sorted
range table whose value is a rational representation of the numeric
value.  You can cast a pointer to the [[dat]] field to a pointer to
[[uni_frac_t]] to retrieve the value.
\item [[int uni_]]\emph{NP}[[_rng_len]] is the length of the sorted
range table.
\item [[const uint32_t *uni_]]\emph{NP}[[_mtab[]]] is a multi-level
table with the same data.
\item [[uni_]]\emph{NP}[[_mtab_len]] is the length of that table (a
preprocessor symbol), in case it is to be written out to a file.
\item [[void uni_]]\emph{NP}[[_of(uint32_t cp, int32_t *num, uint8_t *denom)]]
is a simple wrapper lookup function that uses the multi-level table to
retrieve the numerator and denominator.
\end{itemize}

The age property is officially numeric, but not supported as such. 
Instead, it is only supported as an enumerated property.  The ccc
property is similarly only enumerated, but the value of the
enumeration literal is equal to the numeric value of the property as
well.

The slc, suc, stc, scf, bmg and bpb properties are technically string
properties, not numeric properties.  However, since they always map to
exactly one character (or nothing), they can be considered numeric
properties whose value is the translated code point (or zero if nothing).

In addition, the value for the cjkRSUnicode field is interpreted as a
series of dotted numbers, with optional exclamation points.  This is
all encoded into the numerator and denominator fields, and can be
extracted using decoder macros.  The [[l]] parameter is set to the
number before the decimal point, and the [[r]] is set to the number
after the decimal point.  If both are zero, the number is not present.
If [[r]] is negative, then an exclamation point precedes the decimal
point, and the right side is the absolute value of [[r]].  When there
is more than one value, the [[val2]] macro will return a non-zero value.

% uni_cjkRSUnicode_val1 prototype
% uni_cjkRSUnicode_val2 prototype

\begin{table}
\begin{centering}
\begin{tabular}{llll}
nv&cjkRSUnicode&slc&suc\\
stc&scf&bmg&bpb\\
\end{tabular}
\end{centering}

~

{\small Note: slc, suc, stc, scf, bmg and bpb are technically string
properties.}

\caption{\label{tab:numprop}Numeric properties (Unicode canonical short name)}
\end{table}

\section{String Properties}

Properties with string values are stored in two parts: a constant
string table, and tables which have offsets and lengths of strings
within that table.  Multiple properties may use the same string table.
The offset and length tables are either sorted code point arrays of
type [[uni_str_arr_t]], or multi-level tables of 32-bit values, which
are interpreted as (and can be cast into) [[uni_str_ptr_t]].

\input{[[uni_str_arr_t]].tex} % C
\input{[[uni_str_ptr_t]].tex} % C

Note that the [[uni_str_arr_t]] data structure can also be cast to the
[[uni_cp_val_t]] type, whose [[val]] element can then be cast to
the [[uni_str_ptr_t]] type.

Searching the multi-level table for a string can be done generically
using the following function; it is used internally by the
per-property lookup functions.  The returned pointer is always valid;
a lookup failure is detected by checking the offset and length.  If
both are zero, then a lookup failure occurred.  Zero-length strings
have non-zero offsets to avoid looking like a failure.

% uni_x_str_of prototype

The supported string properties are listed in table
\ref{tab:stringprop}.  Unless otherwise stated in the subsequent
sections, for each string property \emph{SP}, the following symbols
are defined:

\begin{itemize}
\item [[const uint32_t uni_]]\emph{SP}[[_strs[]]] is a string table
containing all string values.
\item [[uni_]]\emph{SP}[[_strs_len]] is the length of the string table (a
preprocessor symbol), in case it needs to be written out to a file.
\item [[const uni_str_arr_t uni_]]\emph{SP}[[_arr[]]] is a sorted list
of code points with their associated string offsets and lengths.
\item [[uni_]]\emph{SP}[[_arr_len]] is the length of the above array
(a preprocessor symbol).
\item [[const uint32_t uni_]]\emph{SP}[[_mtab[]]] is a multi-level
table of 32-bit [[uni_str_ptr_t]] values.
\item [[uni_]]\emph{SP}[[_mtab_len]] is the length of that table (a
preprocessor symbol), in case it is to be written out to a file.
\item [[const uni_str_ptr_t *uni_]]\emph{SP}[[_of(uint32_t cp)]]
is a simple wrapper lookup function that uses the multi-level table to
retrieve string information as an offset and length in the string
table.  A zero-length string is distinguished from a missed lookup by
the offset field:  only missed lookups have both offset and length of
zero.
\end{itemize}

For static linking, the string table, sorted code point table, and
multi-level table are all in separate object files.

\begin{table}
\begin{centering}
\begin{tabular}{llll}
dm&\emph{canon\_decomp}&\emph{compat\_decomp}&\emph{canon\_comp}\\
lc&uc&tc&cf\\
\emph{NFD\_CF}&\emph{NFKD\_CF}&NFKC\_CF&cjkTraditionalVariant\\
cjkSimplifiedVariant&cjkCompatibilityVariant&IDNA\_Mapping&\emph{DUCET}\\
\emph{DUCET\_CLDR}&\emph{rev\_DUCET}&\emph{rev\_DUCET\_CLDR}&scx\\
JSN&na&Name\_Alias&\emph{na\_seq}\\
\emph{na\_xml}&\emph{na\_xml\_seq}&&\\
\end{tabular}
\end{centering}

~

\small{Property names in italics are non-standard properties.}

\small{na, Name\_Alias, and scx are technically ``other'' properties.}

\caption{\label{tab:stringprop}String Properties (Unicode canonical short name)}
\end{table}

\subsection{Names}

The name properties are technically ``other'' properties, but are
treated by the library as string properties with auxiliary tables and
support functions.

To look up the name associated with a sequence of code points, use the
following functions.  [[uni_cp_to_name]] is for looking up standard
Unicode names, and [[uni_cp_to_xml_name]] is for looking up XML entity
names.  Note that [[uni_cp_to_name]] also generates Unicode labels for
unnamed code points, with less-than at the start and greater-than at
the end (e.g. <control-0000>).  If a lookup is meant to not support
labels, it is up to the caller to filter out these results.  In some
cases, valid names do exist for code points with just labels, in the
form of aliases.

They take a code point sequence [[cp]] of length [[len]].  The name
matching the longest initial sequence is returned, along with the
length of that sequence if [[seq_len]] is not [[NULL]]. If, given more
input, a different name may result, [[*seq_len]] is the negative
length of the actual returned sequence.  If [[alias]] is [[NULL]] or
[[*alias]] is zero, the first known name for the code point is
returned.  If [[*alias]] is zero, the returned [[*alias]] will be
largest alias index.  Passing in a non-zero [[*alias]] will retrieve
that alias, and return the next lower index in [[*alias]].  An
out-of-range [[*alias]] returns an empty string, and sets [[*alias]]
to [[-1]].

The returned name is placed in [[*buf]].
\input{buffer return.tex} %%% doc

% uni_cp_to_name prototype
% uni_cp_to_xml_name prototype

Looking up a code point given its name is not meant to be done
directly using the library.  Instead, the tables are meant to be
dumped in a format most useful for the particular application.
However, a few lookup functions are provided below.  These return a
code point or sequence ID given a name and the name's length (negative
to use [[strlen]]).  Both functions require normalized 8-bit input.
Additionally, the [[uni_name_to_cp]] function requires case-folded
input.  The return value is negative on lookup failure.

Note that these functions also accept Unicode labels for unnamed code
points, with less-than at the start and greater-than at the end (e.g.
<control-0000> or <surrogate-D816>).  If a lookup is meant to not
support labels, it is up to the caller to filter out these inputs.

A sequence ID is a code point larger than [[UNI_MAX_CP]].  To convert
it to a string, use the [[na_seq_id]] or [[na_xml_seq_id]] tables,
depending on the function being used.  For each of these two ID arrays
\emph{I}, two arrays are defined:

\begin{itemize}
\item [[const uni_str_ptr_t *uni_]]\emph{I}[[_arr[]]] is the main
lookup table.  Its index is the code point minus $([[UNI_MAX_CP]] + 1)$.
Its length is the preprocessor symbol [[uni_]]\emph{I}[[_arr_len]] in
case the table is to be written to a file.  Its contents are offsets
and lengths for the assocaited string table.
\item [[const uint16_t *uni_]]\emph{I}[[_strs[]]] is the associated
string table, of length [[uni_]]\emph{I}[[_strs_len]].  The string
value is the full UTF-16-encoded sequence.
\end{itemize}

% uni_name_to_cp prototype
% uni_xml_name_to_cp prototype

Both of the above two functions use binary searching for the primary
name lookup.  If [[cmph]] support is enabled, two additional functions
are provided; they perform the same function, except that they use
[[cmph]] hash tables instead of binary searching.

The [[cmph]] functions provide and use hash tables.  These are 
[[const cmph_t uni_rev_na_mph]] and [[const cmph_t uni_rev_na_xml_mph]].
As mentioned earlier, these require linking with the [[cmph]] library,
and may need parameter adjustment to find the include files.  The are
also in a separate library, so using this requires linking with
[[-luni_cmph]].

In addition to the hash tables, some hash algortihms require
translation tables to turn the hash result into an array index in the
reverse name table.  The are
[[const uint32_t uni_rev_na_mph_xlate[]]] and
[[const uint32_t un_rev_na_xml_mph_xlate[]]].  The size of those
tables is normally the same as the associated reverse name lookup
table, but some algorithms produce sparse hash functions, so they
require the explicit translation table size:
[[const uint32_t uni_rev_na_max_mph_res]] and
[[const uint32_t uni_rev_na_xml_max_mph_res]], respectively.  For
sparse tables, unused entries are always [[~0]].  There is currently
no way to query the library to determine if these are, in fact,
necessary, but the current fastest lookup algorithm always needs both,
and using them does not affect the results (only the timing), so it is
probably not a bad idea to assume that both are necessary.

% uni_cmph_name_to_cp prototype
% uni_cmph_xml_name_to_cp prototype

The na property's string value is the name for that code point, with
words encoded to reduce space usage.  The strings are 32-bit words,
but may be viewed as a 32-bit-aligned 8-bit character string padded
with zeroes. That is, the string pointer must be cast to an 8-bit
character type after adding the offset, and the string length must be
multiplied by four and all trailing zeroes removed to find the actual
string length.

The Name\_Alias property's string value is a concatenation of all
aliases for that code point, each with words encoded for reduced space
usage, and each name preceded by its length minus one in a single
byte.  Named sequences, which are not really a property, are provided
by the pseudo-property na\_seq.  The index code point for this
property is the first character of a named sequence.  The value is a
concatenation of all named sequences starting with this name.  Each
name is followed by an optional zero to pad the total entry (with
length byte) to an even number, followed by a length byte.  The low 6
bits of the length byte are the length of the name (minus one), and
the upper 2 bits are the length of the remainder of the sequence that
starts with the index code point.  The sequence extension length is
the number of 16-bit words minus one; the sequence extension is UTF-16
encoded.  So, to find an entry, read the last byte as a length byte.
Skip the preceding byte if it is a zero.  The name is then all
characters preceding that, up to the given name length.  Preceding
that is the sequence extension, with the given number of words (but
possibly fewer characters due to the encoding).  Note that although
the list must be read in reverse, the contents are not reversed.  Like
the na and Name\_Alias properties, the names in na\_seq have words
encoded to reduce space usage.

The rev\_na property is the reverse (name-to-code-point) lookup
property.  It combines the contents of the above three tables,
converts all names to lower-case, and strips them of spaces and
hyphens.  The array is sorted by the stripped name, and contains
multiple entries with the same code point (but no duplicated string
values).  No multi-level table lookup support is provided.  It is
intended for binary searching or linear traversal to print the names
for a different lookup mechanism.  Like the na tables, it uses 32-bit
string tables and encodes words for more compact storage.

The word encodings replace words with two-character sequences with the
first character's high bit set.  These two form a 15-bit index into
the associated word array.  There are two word arrays:  na\_words for
the code-point-to-name arrays, and rev\_na\_words for the rev\_na
array.  For each word array \emph{W}, the following are defined:

\begin{itemize}
\item [[const uni_str_ptr_t * uni_]]\emph{W}[[_arr[]]] is the main array;
it provides offsets and lengths of strings in the strings array.  Its
length is [[uni_]]\emph{W}[[_arr_len]].
\item [[const uint8_t uni_]]\emph{W}[[_strs[]]] is the associated
string array, of length [[uni_]]\emph{W}[[_strs_len]].  The
two-character index should be replaced by this string.
\end{itemize}

Several functions are provided to support this encoding.  First, a set
of functions simply expands word pointers to their associated string.
The generic function takes a set of word arrays, and the specific
functions already use the correct arrays.

% uni_naX_expand_words prototype
% uni_na_expand_words prototype
% uni_rev_na_expand_words prototype

Direct comparison between strings which may or may not contain word
pointers may be done using the following functions.  Since this is
commonly for reverse lookup, only that specific case is implemented.
The optional [[*eqlen]] parameter is set to the last character that
matched if the strings mismatch.

% uni_cmp_word_str prototype
% uni_cmp_rev_na_str prototype

Likewise, computing the expanded length of a string can be done using
one of the following functions.

% uni_word_strlen prototype
% uni_rev_na_strlen prototype

The above tables are not sufficient to fully process Unicode names.
First, some names in the rev\_na table map to multiple code points.
These code points require a hyphen before a particular character in
the stripped name.  The returned code point can be checked for this
condition by the following preprocessor macro (which evaluates is
arguments more than once):

% uni_rev_na_needs_hyphen prototype

If the above macro returns true, the
[[const uni_chrrng_dat32_t uni_na_hyphen_rng[]]] table (of length
[[uni_na_hyphen_rng_len]], but not sorted) may be consulted to
determine what to return instead if a hyphen is present, and where
that hyphen should be.  The [[low]] field contains the return value
from the rev\_na table.  The [[dat32]] field's lower 8 bits are an
index into the stripped name.  If a hyphen immediately preceded the
character at that position before stripping, then the return value
should be transformed to the [[dat32]] field shifted right 8 bits.

In addition to the hyphen problem, the above tables are missing
synthetic names.  Some ranges of code points are assigned
algorithmically generated names.  To find the name of a code point in
one of these ranges, a few tables and functions are provided.  The
na\_rng and rev\_na\_rng tables (string properties without multi-level
tables) consist of pairs of entries; the first in each pair maps the
prefix of a synthetic name to the first code point in its range, and
the second maps to the last code point in its range.  Both use word
encoding for the names, using the appropriate word table for its
function.  The only other differences are the order (forward by code
point, and reverse by stripped name) and the fact that the reverse
table uses stripped names.  Looking up in the forward table cannot be
done using [[uni_cmp_cp]], so the following function is provided.  It
always returns the lower end of the range, or [[NULL]] if not found.

% uni_na_rng_of prototype

These tables are not easy to use, and say nothing about how to
synthesize the names.  To synthesize a name, use the
[[uni_cp_to_name]] function above, with a code point that lies within
a synthetic range.  To look up a synthesized name, the following
function is provided.  While the main name table is better served with
a different encoding, it is recommended that this function be used for
synthetic code points, either after a main name lookup failure or to
test before even doing the main name lookup.

% uni_name_rng_to_cp prototype

The above function also takes advantage of the rev\_JSN
pseudo-property, which is the JSN table (without multi-level table
support), sorted by type (hst) first, and then by name.  Additionally,
a byte is inserted in front of all non-empty strings to indicate the
hst (but not the hst enumeration value itself):  0 for L, 1 for V, and
2 for T.

\subsection{Normalization}

Normalization data includes the decomposition, composition, full case
folding, and character combining class information.  This is all
combined into convenient functions for normalization.  Each
Unicode-defined normalization method has its own functions:  canonical
decomposition (NFD), canonical composition (NFC), compatibility
decomposition (NFKD), compatibility composition (NFKC), and
compatibility decomposition with case folding (NFKC\_CF).

There are three functions for each normalization type.
\input{UTF buffer return.tex} %%% doc

Overlap of the input buffer ([[ibuf]]) and the output buffer
([[*buf]]) is not generally supported; if the input is overwritten
during conversion, the output may be affected.  However, if the input
and output buffer are identical, [[NULL]] may be specified for
[[ibuf]], which indicates that the output position (including offset,
if specified) is also the location of the input.  This is usually done
by allocating new memory and copying the input buffer first.

% uni_NFD32 prototype
% uni_NFD16 prototype
% uni_NFD8 prototype
% uni_NFC32 prototype
% uni_NFC16 prototype
% uni_NFC8 prototype
% uni_NFKD32 prototype
% uni_NFKD16 prototype
% uni_NFKD8 prototype
% uni_NFKC32 prototype
% uni_NFKC16 prototype
% uni_NFKC8 prototype
% uni_NFKC_CF32 prototype
% uni_NFKC_CF16 prototype
% uni_NFKC_CF8 prototype

Most normalization procedures above perform canonical ordering.  This
can also be done outside of those functions.  The following function
takes a buffer ([[buf]]) of a given length ([[ilen]]), and sorts the
characters in-place.  It will never need to expand the length of the
array.  However, it may need more characters to fully sort the input.
The [[last]] parameter indicates that the function must assume there
are no more characters available. Otherwise, it will return the full
length of the input if no more characters are needed, or less than the
full length if more are needed.  If more are needed, the return value
indicates the number of characters which have already been sorted;
these can safely be removed before the next pass.

% uni_Canon_Order32 prototype
% uni_Canon_Order16 prototype
% uni_Canon_Order8 prototype

\label{FCD}In some cases, a chain of transformations begins with NFD
normalization.  However, subsequent steps may succeed even without
fully normalized text.  In particular, if the next step is canonically
closed, it can perform the normalization as part of its
transformation.  For this case, the data only needs to be in what is
called Fast C or D form, or FCD.  A pseudo-property is provided to
perform this test.  The following function performs this test on a
string, returning true if no normalization needs to be performed:

% uni_is_FCD32 prototype
% uni_is_FCD16 prototype
% uni_is_FCD8 prototype

The following symbols are defined for accessing the raw FCD data:

\begin{itemize}
\item [[const uni_chrrng_dat16_t uni_FCD_rng[]]] is a sorted range
table whose 16-bit unsigned data is a ccc pair for the FCD test.
\item [[uni_FCD_rng_len]] is the length of that table (a preprocessor
symbol).
\item [[const uint32_t uni_FCD_mtab[]]] is a multi-level table with
the same data.
\item [[uni_FCD_mtab_len]] is the length of that table, in case it is
to be written out to a file.
\item [[uint16_t uni_FCD_of(uint32_t cp)]] is a function
which returns the 16-bit ccc pair for the given [[cp]].  Internally,
this is a preprocessor macro which uses the multi-level table and
[[uni_x_dat16]].
\end{itemize}

Each 16-bit word has the ccc of the first character of any potential
canonical decomposition as its high byte, and the ccc of the last
character of any potential canonical decomposition as its low byte.
See Unicode Technical Note \#5 for details on the algorithm.  Also see
that technical note for a brief description of what it means for a
transformation to be canonically closed.  The only property currently
claiming to meet this requirement is the DUCET.%
\footnote{However, the supplied CLDR tests have several cases where
passing FCD is not enough to produce a valid sort key.  This may be a
bug in either the UCA or the UCD.}
The UCA is not always guaranteed to meet this requirement; if the
literal text is to be appended to the key, then the literal text must
be in NFD form first.

\subsection{Decomposition}

The raw decomposition data is not meant to be used directly.  Instead,
the following wrapper functions are provided.  They perform just the
decomposition step for the decomposition normal forms; canonical
ordering needs to be performed manually afterwards.  Like the full
normalization functions, they do not support input and output overlap,
but do support [[NULL]] for [[ibuf]] to indicate that input and output
locations are identical.  The return value is the length after
transformation.

% uni_NFD_dec32 prototype
% uni_NFD_dec16 prototype
% uni_NFD_dec8 prototype
% uni_NFKD_dec32 prototype
% uni_NFKD_dec16 prototype
% uni_NFKD_dec8 prototype

The NFKC\_CF property does not describe the full NFKC\_CF
transformation.  Instead, it provides only the steps up to and
including decomposition.  The full transformation requires canonical
ordering and composition, which are performed by the [[uni_NFKC_CF]]
functions listed in the previous section.  For just the case folding
and decomposition steps, the following functions are similar to the
[[_dec]] functions above.

% uni_NFKC_Casefold32 prototype
% uni_NFKC_Casefold16 prototype
% uni_NFKC_Casefold8 prototype

The functions listed so far in this section use a low-level routine
which takes the raw decomposition data as additional parameters to
perform their function.

% uni_do_any_dec32 prototype
% uni_do_any_dec16 prototype
% uni_do_any_dec8 prototype

Raw decomposition data (except for NFKC\_CF, which is a normal string
property) is stored in [[uni_dm_strs]].  This includes the raw dm
property, as well as two pseudo properties.  The canon\_decomp
property contains only full (recursive) canonical decompositions, and
the compat\_decomp property contains only full (recursive)
compatibility decompositions when they differ from the canon\_decomp
result.  The raw dm property is not recursively/fully decomposed.  The
length field cannot be used raw; a flag is ored into the upper bit.
This flag eliminates the need for a dt property lookup to determine if
an entry is canonical or compatibility: compatibilty decompositions
have this flag set.

Lookup with the multi-level tables is simplified with some
preprocessor macros, which return the associated string's offset and
length.  For the compatibility lookups, an optional pointer to a flag
([[compat]]) can be used to check if the returned decomposition was
not canonical.  Unlike the other string properties, the
[[uni_]]\emph{SP}[[_of]] function is not supported for these
properties, in order to force usage of these functions.

% uni_find_canon_decomp prototype
% uni_find_compat_decomp prototype
% uni_find_dm prototype

These macros use the internal-use only [[uni_x_dec]] function to do
their work.  For Hangul syllable strings, the [[h]] flag determines
behavior.  If positive, it acts as the [[full]] flag below, and the
returned offset is always $-1$, but the length is correct.  If
negative, the returned offset is always 1, and the length is zero.

% uni_x_dec prototype

For the Hangul syllables, a separate function must be use to
decompose.  A Hangul syllable string in the form LVT or LV can be
singularly or completely decomposed (dependent on the [[full]] flag)
using the following function.  The function will return nothing (-1)
if the input is not a decomposable Hangul syllable string, so it is
safe to call whenver no decompositionmapping is found in the tables.
Otherwise, it fills in the return buffer ([[res]]) and returns the
number of filled-in values (always 2 or 3).  The return buffer
must contain space for at least 2 characters if the [[full]] flag is
not set, or 3 if it is set.

% uni_hangul_syllable_decomp prototype

To make this easier, a wrapper macro is provided.  This assumes that
at least [[len]] words are available in the passed-in buffer
([[buf]]), and copies out the result.  This is meant to be called
after [[uni_find_]]\emph{xxx}, using its results combined with the
[[uni_hangul_syllable_decomp]] results.

% uni_get_decomp prototype

\subsection{Composition}

The raw composition data is not meant to be used directly.  Instead,
the following wrapper functions are provided.  They perform just the
composition step for the composition normal forms; canonical
decomposition needs to be performed manually beforehand.  Like the
full normalization functions, they modify buffers in-place.  However,
composition never extends the length of the input, so no extra space
needs to be available in the buffer.  However, more characters may be
needed to determine if composition is possible.  if [[nok]] is NULL,
the input buffer is assumed to be complete.  Otherwise, if the
returned [[*nok]] is less than the returned (updated) buffer length,
then the first [[*nok]] characters have been composed successfully,
but the remainder needs additional input before completion.  In any
case, the return value is the updated length of the buffer after
composition.

% uni_NFC_comp32 prototype
% uni_NFC_comp16 prototype
% uni_NFC_comp8 prototype

The raw composition tables, derived from the dm and Comp\_Ex
properties, are designed for multi-step lookup as well. In addition to
having a gap for the Hangul syllable strings, composition takes two
code points for input rather than just one.  The latter problem is
solved by looking up a ``string'' using the first input, which is
actually a sub-table that can be searched using the second input.  The
format of the sub-table is like a [[uni_cp_val_t]] structure; the
value is the composed character.  These sub-tables are stored in the
string table [[uni_canon_comp_strs]] (i.e., \emph{ST} is canon\_comp).
Only one pseudo-property is defined for this: canon\_comp.

The following macro takes the first element of the composition pair
and returns the offset and length of the sub-table by using
[[uni_x_dec]] and the multi-level table.

% uni_find_canon_comp prototype

To look up values in the sub-table, a separate search function is
provided.  Again, a hand-coded binary search is used instead of
[[bsearch]] for speed.

% uni_lookup_compent prototype

To look up Hangul syllable compositions, another separate function is
provided.  It returns zero if there is no valid composition; otherwise
it returns the result of composition.

% uni_hangul_syllable_comp prototype

Combining the above two steps can be done using a wrapper macro.  Thus
the preferred method of looking up compositions is to call
[[uni_find_canon_comp]] for a code point, and then to use the results
for all candidates for the second input to composition with
[[uni_canon_comp]].

% uni_canon_comp prototype

\subsection{Collation Tables}

The Default Unicode Collation Element Table is available as the DUCET
pseudo-property.  The CLDR version of the DUCET is available as the
DUCET\_CLDR pseudo property.  Both of these properties also define the
variable uni\_\emph{property}\_var\_top, which is the default variable
top parameter for that table.  As with the normalization tables, raw
lookup is not encouraged.

To perform a straight string comparison without fully generating keys,
use the following functions.  The [[opts]] parameter may be NULL, or
it may be a set of collation options.  Each string's length may be
either a positive number, indicating absolute length in words, or a
negative number, indicating that the string is zero-terminated.

% uni_uca_strcmp32 prototype
% uni_uca_strcmp16 prototype
% uni_uca_strcmp8 prototype

Collation options are set by the following structure.  The defaults
are selected by zeroing the options structure out first.

\input{[[uni_uca_opts_t]].tex} % C

The [[tab]] and [[strs]] options must be set together, and specify
the static data for the DUCET.  By default, the standard Unicode DUCET
is used.  Normally, the [[var_top]] option would be set at the same
time; by default it is the standard Unicode DUCET's variable top.

The [[max_level]] option is the maximum significant key level; by
default, it is 3.  The second level, if enabled, may be returned in
reverse order using the [[reverse_lev2]] flag. If it is larger than 4,
[[do_literal]] is implied. The [[do_literal]] option, if non-zero,
appends the literal input string to the key.  The UCA requires that
the string be in NFD form for this to work.

Keys whose primary component is non-zero and equal to or less than
[[var_top]] are variable weight keys.  The behavior of variable
weights is controlled by the [[var_mode]] parameter, whose values
correspond to the standard methods.

\input{UCA variable modes.tex} % C

There is no function to directly generate something like the UCA sort
key.  Users wishing to store these keys are encouraged to encode them
in a more efficient manner for comparison.  However, the raw keys may
be looked up and compared as if they were UCA sort keys.  To retrieve
the sort key for a string, use the following functions.  The string
length is negative for zero-terminated strings, or positive for
strings with an absolute length.  The function returns a sort key
allocated with [[malloc]], whose length is returned in [[*rlen]].  If
[[llen]] is non-[[NULL]], it points to a 4-element array into which to
place the number of non-zero elements at each level of the key.
Remember that the string input should be at least FCD form.%
\footnote{At least one test case for the CLDR DUCET fails when in FCD
form, but not NFD form, so it's safest to just use NFD form.}

% uni_str_uca_raw32 prototype
% uni_str_uca_raw16 prototype
% uni_str_uca_raw8 prototype

To compare the keys returned by the above function, use the following
functions.  Pass in the same options as passed into the key creation
routine; passing different options for different keys or different
options for keys and comparison will result in undefined behavior.
This combination of functions respects all the same options
as the string comparison routines, except for the [[do_literal]]
option.  To obtain the effect of this option, use something equivalent
to [[uni_int_utf32_strcmp]] on the normalized strings if
[[uni_uca_raw_cmp]] returns zero.

% uni_uca_raw_cmp prototype

Alternately, the keys can be directly examined.  Each key element is
returned as one or two 32-bit integers.  The first contains a
combination of the first three levels using shifts and masks.  If
[[max_level]] was larger than three when the key was created, a second
word for each key element is the fourth level.

\input{DUCET lookup format defs.tex} % C

Raw DUCET lookups for characters without the variable weight and level
processing can be performed using the following function.  The
[[lev123]] parameter is formatted just like the first word of each
pair returned by [[uni_str_uca_raw]].  Since there are DUCET entries
for strings of more than one character, the lookup requires string
input. This is done one character at a time, using a state structure
to keep track of progress.  Pass in the next character in [[c]],
unless at end of input, in which case pass in [[UNI_UCA_LOOKUP_END]].
The return value is one of [[UNI_UCA_LOOKUP_AGAIN]],
[[UNI_UCA_LOOKUP_OK]], or [[UNI_UCA_LOOKUP_NONE]].  If the return code
was [[NONE]], [[lev123]] and [[lev4]] must be ignored.  Otherwise,
they contain the next DUCET sort key element.  If the passed-in
character was not [[END]], or the return value was [[AGAIN]], the
function must always be called again to finish the lookup.  The [[c]]
parameter should be the next character (or [[END]]) for the next call,
unless the return value was [[AGAIN]], in which case [[c]] is ignored.

The [[state]] parameter is a pointer to a pointer to an opaque state
structure.  It must initially point to [[NULL]].  After the last
lookup returns ([[c]] is [[UNI_UCA_LOOKUP_END]] and the return value
was not [[UNI_UCA_LOOKUP_AGAIN]]), the structure is automatically freed
and reset to [[NULL]].  It should never be freed manually.

Note that the [[SHIFT_TRIMMED]] variable weight processing method can
be implemented more cheaply in an outer function by using [[SHIFTED]]
instead, and trimming after the entire key has been built.

% uni_uca_lookup prototype

The initial state can also be used to set options.  Currently, the
[[reverse_lev2]] and [[do_literal]] options are not supported; use the
higher-level functions above, instead.  The options may only be set
once; any further attempts will be ignored.

% uni_uca_lookup_opts prototype

While [[uni_uca_lookup]] is meant for mostly raw lookup, it does
support options, and automatically adds DUCET entries which were
suppressed in the raw data.  For truly raw lookup, the following
function is provided.  It returns the best key out of the table
specified by [[tab]] and [[strs]] (defaults to the standard DUCET if
[[NULL]] is passed in) for the passed-in string ([[buf]],
[[buf_len]]).  It returns a flag which is true if passing in a longer
buffer might give different results.  Other values may be returned in
the remaining parameters, if non-NULL.  [[ce_len]] contains the number
of characters actually used to generate the key. [[key]] is
[[key_len]] long words of raw key information.  If the lookup fails,
[[key]] is set to [[NULL]], and [[key_len]] is set to zero.

The raw data pointed to by [[key]] is similar to, but not quite the
same as the raw data returned by the functions above.  The first word
contains the first three levels, with nothing masked out.  Its low 2
bits indicate the value of level 4: 0 means 0, 1 means that it is the
first code point of [[buf]], and 2 means that it is the following word
in [[key]], shifted left 2 bits.

A lookup failure indicates that the first character in [[buf]] is not
in the table.  This can mean one of three things: either it is illegal
(such as a surrogate), or it is not in NFD form (in which case the
character should be normalized and passed in again), or it has a
synthetically generated value.  The [[uni_uca_synth_key]] can be used
to generate the correct synthetic value in the latter case; it is
passed in a 2-word [[key]] buffer into which to place the synthetic
levels 1-3 for [[c]].  Level 4 is officially [[c]] for the first, and
0 for the second, but it normally doesn't matter, and in fact the
routines above and older versions of the DUCET use [[c]] always, and
recent versions of the DUCET use 0 for both.

Note that there are some strings which this routine may identify as a
collation element, but which are not.  For example, there are entries
which are merely the reordering of the keys for multiple collation
elements.  There are also some strings which could be considered
collation elements, but are not, because there are fully ignorable
characters in the string.  The former may be mitigated by examining
the keys, and only accepting keys of a known single-element format,
but no assistance is provided by this library for that.  The latter
can be mitigated by simply ignoring returned zeroes.

One type of long collation element handled by [[uni_uca_lookup]] which
[[uni_DUCET_lookup]] can't is reordering of grapheme cluster elements
(S2.1).  This may be fixed in a future release.

% uni_DUCET_lookup prototype
% uni_uca_synth_key prototype

Using the raw tables is possible as well.  They are string arrays
indexed on the first character of the collation element, and the data
format is as described above, but each key is preceded by zero or more
``index extensions'' --- additional code points to append to the index
for the actual index of this key, each shifted left two bits and with
both low bits set.  The actual array element string consists of every
key with the same starting character, ordered by the index extension
string.

It may also be useful to do reverse lookups: given the sort of data
returned by the above functions, find a set of collation elements
which match.  Normally, this would be done by giving a collation
element, and finding its collation class set.  This is performed by
the following functions.  They take a collation element ([[ch]],
[[chlen]]), perform a lookup using [[opts]], and then a reverse lookup
(using the [[rev_DUCET]] and [[rev_strs]] parameters, or, if NULL, the
standard UCA DUCET), and return a string containing the results.
\input{UTF buffer return.tex} %%% doc

If the result string is of length zero, the input string is not a
collation element. Otherwise, the result string has collation elements
in order of increasing length, starting with one.  Each length group
consists of a count word (possibly zero), followed by that many
strings of the group's length (in code points, not words); if this
count is more than the maximum which can be stored in a word (e.g. 255
for 8-bit words), the count will silently and incorrectly overflow
(this may be fixed in a future release).  Other than being sorted by
length, there is no order imposed on the results.  The [[max_level]]
field of [[opts]] can be used to determine how accurate comparisons
are.  The [[do_literal]] field is ignored, and all values of
[[max_level]] over four are identical to four.  As above, the default
level is three.

% uni_uca_char_class32 prototype
% uni_uca_char_class16 prototype
% uni_uca_char_class8 prototype

The reverse lookups are done using the psuedo-properties [[rev_DUCET]]
and [[rev_DUCET_CLDR]].  These are stored in a string array indexed on
the level 1 value (shifted right 16 bits from the combined format).
The value of each array element is a string of key groups, each
followed by the corresponding collation element.  The keys are encoded
as described for the raw data above, and the collation element is
simlply all of its characters, shifted right 2 bits and with both low
bits set.  The array element is sorted by key for easier lookup of
detailed entries.

\subsection{Case Conversion}

The slc, suc, and stc properties are provided as if they were numeric
properties; the numerator of the result is the single character to
which the input maps.  A lookup failure in the stc table has the
result of a suc lookup as its default.  A lookup failure in either the
slc or suc tables has the input code point itself as its default.
None of these defaults is actually implemented in the generic lookup
functions.

The lc, uc, and sc properties, however, provide not only a mapping,
but also a condition under which the mapping takes place.  The
properties are each stored in separate string tables, so their
\emph{ST} name is the same as their \emph{SP} name.  All currently
possible conditions are mapped to a flag:

\input{Special casing conditions.tex} % C

A flag in [[UNI_SC_FL_LOCALE]] indicates that the translation only
applies in specific locales.  The others ([[UNI_SC_FL_CONTEXT]])
depend on context: generally the entire containing combining character
sequence, and possibly the next character must be known before
deciding on a translation.

Since each input code point may map to multiple translations
(depending on the condition),  the encoding of the string is to
provide all possible translations, each preceded by its length.  The
condition flags for that interpretation are then or-ed into the length.

The actual lc, uc, and tc properties combine the slc, suc, and stc
lookups with the contents of the lc, uc, and tc string tables, and may
vary depending on context.  To do a lookup, the following functions
are provided.  A [[cond]] of [[~0]] indicates that the caller does not
know the current context conditions.  The return is $-1$ if the
condition is unknown, but may affect the result.  In that case, the
function should be called again, but with valid [[cond]] flags.
Otherwise, it is the length of the result, with the output placed in
the output buffer.

There are three functions per case conversion type.
\input{UTF buffer return.tex} %%% doc

Note that the passed-in condition flags should never set
[[UNI_SC_FL_NOT]]; just leave a flag unset if it is not in effect.
Also, if the title casing functions ([[tc]]) return the input code
point, the user must manually call the upper-case ([[uc]]) function
for the final result.

% uni_lc32 prototype
% uni_uc32 prototype
% uni_tc32 prototype
% uni_lc16 prototype
% uni_uc16 prototype
% uni_tc16 prototype
% uni_lc8 prototype
% uni_uc8 prototype
% uni_tc8 prototype

These functions are actually preprocessor macros which use a generic
function that takes the tables to query:

% uni_case_convert32 prototype
% uni_case_convert16 prototype
% uni_case_convert8 prototype

Case folding converts to a canonical desired case, usually lower-case.
Like the plain case conversion properties, there are two cases:
simple and full.  In addition, some of the full folding is
conditional.  In this case, though, the only condition is whether or
not the tr or az locale is in use.  Encoding is identical to the case
conversion properties.  The scf property is numeric, and the cf
property can be derived using the [[uni_case_convert]] functions, and
their associated wrapper macros:

% uni_cf32 prototype
% uni_cf16 prototype
% uni_cf8 prototype

The pseudo-property tcf, which includes the Turkic language exceptions
(i.e., the mappings when in the tr or az locale), can be obtained
using a wrapper macro as well:

% uni_tcf32 prototype
% uni_tcf16 prototype
% uni_tcf8 prototype

Note that the cf and tcf properties are actually the same, and are
stored and treated the same way as the other case properties.  The
wrapper functions above filter the low-level cf property based on the
desired locale information.

The case conversion functions above are for one character at a time.
To convert a full string, use one of the following functions.  Behavior
of these functions is undefined if [[s]] overlaps the output buffer
([[*buf]]).  If an in-place conversion is desired, pass in [[NULL]]
for [[s]]; in that case, the input starts at the same place as the
output.  All but the case folding functions take a [[cond]] parameter
to provide context.

Unlike the character conversion functions, the title case conversion
function uses the upper-case and lower-case conversions as necessary
according to the full title case conversion algorithm.  That is, words
only have their first character capitalized, and everything else is
converted to lower-case.  This requires word boundary detection, which
can be localized with a property table override ([[WB_tab]]).

Also unlike the character conversion functions, the special value
[[~0]] is not supported for [[cond]]; it is not possible to query
whether or not context would alter the result.  In fact, only
locale-related flags are respected, and all others are filtered out
and generated internally based on the string contents.

% uni_str_lc32 prototype
% uni_str_lc16 prototype
% uni_str_lc8 prototype
% uni_str_uc32 prototype
% uni_str_uc16 prototype
% uni_str_uc8 prototype
% uni_str_tc32 prototype
% uni_str_tc16 prototype
% uni_str_tc8 prototype
% uni_str_cf32 prototype
% uni_str_cf16 prototype
% uni_str_cf8 prototype
% uni_str_tcf32 prototype
% uni_str_tcf16 prototype
% uni_str_tcf8 prototype

All but the title-case functions above are actually preprocessor
macros which use generic functions to perform their task, given the
tables to query:

% uni_str_case_convert32 prototype
% uni_str_case_convert16 prototype
% uni_str_case_convert8 prototype

Since a common operation to perform after case folding is comparison,
a function is provided to do both steps at the same time.  This is
equivalent to [[strcasecmp]] if the inputs are sufficiently
normalized.  The return value is $1$, $0$, or $-1$ on successful
comparison, indicating [[a]] is greater than, equal to, or less than
[[b]], respectively.  If there is an encoding error in [[a]], $2$ is
returned.  If there is an encoding error in [[b]], $-2$ is returned.

% uni_cf_strcmp32 prototype
% uni_cf_strcmp16 prototype
% uni_cf_strcmp8 prototype
% uni_tcf_strcmp32 prototype
% uni_tcf_strcmp16 prototype
% uni_tcf_strcmp8 prototype

These are all preprocessor macros using a more generic routine which
takes the locale flags as a parameter.

% uni_either_cf_strcmp32 prototype
% uni_either_cf_strcmp16 prototype
% uni_either_cf_strcmp8 prototype

If the inputs are not sufficiently normalized, the following functions
perform the comparison instead.  The return values have the same
values and interpretations, except that in addition, $3$ indicates a
memory allocation error while processing [[a]], and $-3$ indicates a
memory allocation error while processing [[b]].  Memory is allocated
for long canonical character sequences.

% uni_NFD_cf_strcmp32 prototype
% uni_NFD_cf_strcmp16 prototype
% uni_NFD_cf_strcmp8 prototype
% uni_NFD_tcf_strcmp32 prototype
% uni_NFD_tcf_strcmp16 prototype
% uni_NFD_tcf_strcmp8 prototype
% uni_NFKD_cf_strcmp32 prototype
% uni_NFKD_cf_strcmp16 prototype
% uni_NFKD_cf_strcmp8 prototype
% uni_NFKD_tcf_strcmp32 prototype
% uni_NFKD_tcf_strcmp16 prototype
% uni_NFKD_tcf_strcmp8 prototype

Again, these are preprocessor macros using more generic functions
which take the locale flags and NFD\_CF/NFKD\_CF pseudo-property
tables as parameters.

% uni_norm_cf_strcmp32 prototype
% uni_norm_cf_strcmp16 prototype
% uni_norm_cf_strcmp8 prototype

Finally, using the NFKC\_Casefold property instead of simple case
folding can be done using the following functions.  The capitalization
of CF should be enough to indicate that their function does not
involve the cf property directly.  Like the other normalizing
comparison functions, these may return memory errors as well when long
canonical character sequences are present.

% uni_NFKC_CF_strcmp32 prototype
% uni_NFKC_CF_strcmp16 prototype
% uni_NFKC_CF_strcmp8 prototype

There are also test functions for determining if a character or string
would change if converted to a case.  Normally, it would be more
efficient to check the gc property for the character, but this does
not necessarily match Unicode's strict definition of casing.  The
following functions opreate with one input character, and its full
context.  They return $-1$ if the context is insufficient, $0$ if the
character would change if the conversion function would apply, and $1$
othewise.  The [[uni_is_cased]] function, on the other hand, returns
$1$ if any of the conversion functions might change the character, and
$0$ (or $-1$, if unknown) otherwise.

Unlike the conversion functions, the title casing ([[tc]]) function
properly falls back to the upper-case ([[uc]]) table if there is no
direct title-casing mapping.  However, the user must manually switch
between title-casing and lower-casing ([[lc]]) depending on the
location within a word.

These are actually preprocessor macros, and the title-casing ([[tc]])
and any-casing ([[cased]]) functions may evaluate their parameters
more than once.

% uni_is_lc prototype
% uni_is_uc prototype
% uni_is_tc prototype
% uni_is_cf prototype
% uni_is_tcf prototype
% uni_is_cased prototype

The preprocessor macros use the following generic function with the
appropriate tables.

% uni_case_check prototype

The string equivalents of the above functions check if any part of the
string would change if a conversion function were applied.  Anything
doing title case checks needs to know about word boundaries, so a
locale word boundary property override can be specified for them
([[WB_tab]]).

% uni_str_is_lc8 prototype
% uni_str_is_lc16 prototype
% uni_str_is_lc32 prototype
% uni_str_is_uc8 prototype
% uni_str_is_uc16 prototype
% uni_str_is_uc32 prototype
% uni_str_is_tc8 prototype
% uni_str_is_tc16 prototype
% uni_str_is_tc32 prototype
% uni_str_is_cf8 prototype
% uni_str_is_cf16 prototype
% uni_str_is_cf32 prototype
% uni_str_is_tcf8 prototype
% uni_str_is_tcf16 prototype
% uni_str_is_tcf32 prototype
% uni_str_is_cased8 prototype
% uni_str_is_cased16 prototype
% uni_str_is_cased32 prototype

Again, the lower-case, upper-case and case folding functions are
actually macros, which use the generic functions taking a table:

% uni_str_case_check8 prototype
% uni_str_case_check16 prototype
% uni_str_case_check32 prototype

The only other case conversion string property is NFKC\_CF, which is a
normal string property.  See the normalization and decompostion
sections for additional functions related to this property.

\subsection{Other String Properties}

The scx property is officially not a string property, but it is stored
internally as a string property whose strings are sequences of
enumeration literal values of the [[uni_sc_t]] type.  For example, the
code point U+0363 has the scx property ``Arab Syrc'', which is stored
as string of length two.  The first character of the string is
[[UNI_sc_Arab]], and the second is [[UNI_sc_Syrc]].  The default value
for a code point's scx property if the lookup fails is the code
point's sc property value.

% End-doc Users-Guide

\lstset{language=txt}
<<FIXME>>=
Document or remove:
  enums w/o names:
    DUCET_dt
  strings:
    DUCET_dm
@

% Begin-doc Users-Guide
\section{Locale Support}

Locale support was introduced in various functions above via hooks and
flags.  The following functions and tables help to set those hooks and
flags.

A locale is identified by language, script, territory, and variant.  
These are represented numerically via the partial enumeration
properties [[uni_CLDR_language]], [[uni_CLDR_script]],
[[uni_CLDR_territory]] and [[uni_CLDR_variant]], respectively.  These
have enumeration types and literals and name and value arrays like
enumerated properties, but do not have range or multi-level tables, as
they are not associated with code points.  A more compact form is a
triplet of language, script, and territory in a single 32-bit integer.
These are manipulated using the following macros, which consider
anything beyond the normal value range of each enumeration to mean
that the field is undefined:

% uni_locale_extract_language prototype
% uni_locale_extract_script prototype
% uni_locale_extract_territory prototype
% uni_locale_triplet prototype
% uni_locale_set_language prototype
% uni_locale_set_script prototype
% uni_locale_set_territory prototype

A locale identifier may also have extensions.  The extensions and
their keys at least partially supported by this library are
represented by the partial enumeration property
[[uni_CLDR_locale_ext]].  Each extension key also has a descriptor,
indexed by its enumeration literal, in the
[[uni_CLDR_locale_ext_desc]] array.  This array contains the extension
key's parameter type and, in the case of enumerated parameters, a
pointer to the matching array for that enumerated parameter.  Each
enumerated parameter is available as a partial enumeration property as
well, and is named [[uni_CLDR_locale_]]\emph{extension}, where
\emph{extension} is the single-letter extension group ([[u]] or
[[t]]), followed by an underscore, followed by the two-letter
canonical key (e.g. [[uni_CLDR_locale_u_ca]]).

\input{[[uni_CLDR_locale_ext_desc]].tex} % C

Normally, locales are provided in text form.  The following function
parses a string to produce values described above.  It is not meant to
determine the validity of a locale identifier; it silently skips
anything it doesn't understand.  Its purpose is to convert a valid
locale identifier into something usable for setting locale-specific
options.

Its return is an array of 32-bit integers.  The first is the
language-script-territory triplet.  All remaining words are variant
and extension codes.  Variant codes are encoded as enumeration literal
values, with the high bit set.  Translation locale codes from [[t]]
extensions contain the triplet and variant codes encoded the same way,
but with bit 30 set.  Extension codes have the extension enumeration
literal in the low byte, and other information in the upper 3 bytes,
and, in some cases, following words.  The interpretation of the upper
3 bytes (shifted down 8 bits) is determined by the extension type:

\begin{itemize}
\item Enumeration ([[type_len > 0]]) --- the enumeration literal value
corresponding to the value tag.  If more than one value was present,
each will be encoded in the same way, as if there were multiple
key-value pairs.
\item [[UNI_LOCALE_EXT_BOOL]] --- 0 if false, 1 if true.
\item [[UNI_LOCALE_EXT_CP]] --- a count; this many words follow giving
the specified code points.
\item [[UNI_LOCALE_EXT_REORD]] --- a count; this many words follow
giving the specified reordering blocks.  Reordering blocks are script
codes (i.e., [[uni_sc_t]] enumeration values), plus
[[UNI_REORDER_SPACE]], [[UNI_REORDER_PUNCT]], [[UNI_REORDER_SYMBOL]],
[[UNI_REORDER_CURRENCY]], [[UNI_REORDER_DIGIT]] and
[[UNI_REORDER_OTHER]] to indicate the special fixed block names
([[UNI_REORDER_OTHER]] is actually just an alias for the script code
[[UNI_sc_Zzzz]]).
\item [[UNI_LOCALE_EXT_ATTR]] --- unused.
\item [[UNI_LOCALE_EXT_PRIV]] --- unsupported.
\end{itemize}

% uni_parse_locale prototype

The reordering code can be separately parsed using the following
function, which takes an all-lower-case string as input.  It returns
all ones ([[~0]]) on lookup failure.

% uni_parse_locale_reorder_block prototype

In addtion to the partial enumerations specified above, the locale
parser function uses one more support table to resolve territory
aliases ([[const uni_cp_val_t uni_CLDR_likely_subtags[]]] of length
[[uni_CLDR_likely_subtags_len]]).  This support table indicates the
most likely full locale triplet given a partial triplet.  It is stored
as a [[uni_cp_val_t]], but the ``code point'' is actually the partial
triplet, and the value is the associated full triplet.  It is sorted
by ``code point'' field.  This table could also be used after parsing
to fill in missing script and territory values, or during printing to
suppress redundant script and territory tags.  To help with this, the
following two functions are provided.  [[uni_locale_triplet_strip]]
deletes redundant script or territory tags, and
[[uni_locale_triplet_fill]] attempts to fill in any missing tags.
Both take a [[force]] option to cause explicitly undefined tags to be
considered missing, instead.

% uni_locale_triplet_strip prototype
% uni_locale_triplet_fill prototype

The [[uni_locale_triplet_strip]] function is one step towards
producing the canonical form of a locale descriptor.  The next step is
to sort the tags alphabetically.  The following function does this. 
Note that it also silently strips out invalid duplicates, and may make
other alterations that change the intended meaning of ill-formed
locale descriptors. Since they were ill-formed to begin with, this
should not be a great concern.

There is no rule stating whether [[t]] or [[u]] extension information
should come first.  Alphabetizing is only necessary on keywords, not
the extensions themselves.  Since the rest of this library ignores the
[[t]] extension anyway, the output of this routine places [[u]] first.

% uni_locale_sort_var_exts prototype

\section{Named Unicode Algorithms}

Table \ref{tab:unialg} lists named Unicode algorithms, and the API
functions which implement them.  The Bidirectional Algorithm (UBA) and
the Standard Compression Scheme for Unicode (SCSU) are not
implemented, and there are no plans to ever implement them.

% End-doc Users-Guide

<<FIXME>>=
Should probably implement UBA anyway, to ensure that data files are
usable, and to exercise BidiTest.txt and BidiCharacterTest.txt (see tr9)
@

% Begin-doc Users-Guide
\begin{table}[htb]
\begin{centering}
\begin{tabular}{ll}
\bf{Name}&\bf{Implementation}\\
Canonical Ordering&[[uni_Canon_Order]]\{[[8]],[[16]],[[32]]\}\\
Canonical Composition&[[uni_NFC_comp]]\{[[8]],[[16]],[[32]]\}\\
Normalization&[[uni_]]\{[[NFD]],[[NFKD]],[[NFC]],[[NFKC]]\}\{[[8]],[[16]],[[32]]\}\\
Hangul Syllable Composition&[[uni_hangul_syllable_comp]]\\
Hangul Syllable Decomposition&[[uni_hangul_syllable_decomp]]\\
Hangul Syllable Name Generation&[[uni_cp_to_name]], [[uni_name_rng_to_cp]]\\
Default Case Conversion*&[[uni_str_]]\{[[lc]],[[uc]],[[tc]],[[cf]]\}\{[[8]],[[16]],[[32]]\},\\
&[[uni_NFKC_CF]]\{[[8]],[[16]],[[32]]\}\\
Default Case Detection*&[[uni_str_is_]]\{[[lc]],[[uc]],[[tc]],[[cf]],[[cased]]\}\{[[8]],[[16]],[[32]]\}\\
Default Caseless Matching&[[uni_]]\{,[[NFD_]],[[NFKD_]],[[NFKC_]]\}[[cf_strcmp]]\\
Line Breaking Algorithm*&[[uni_line_brk]]\\
Character Segmentation&[[uni_gc_brk]]\\
Word Segmentation*&[[uni_word_brk]]\\
Sentence Segmentation*&[[uni_sentence_brk]]\\
Hangul Syllable Boundary Determination&[[uni_gc_brk]]\\
Default Identifier Determination&[[uni_is_IDS]], [[uni_is_IDC]]\\
Alternative Identifier Determination&[[uni_is_XIDS]], [[uni_is_XIDC]]\\
Pattern Syntax Determination&[[uni_is_Pat_WS]], [[uni_is_Pat_Syn]]\\
Identifier Normalization&[[uni_NFKC_CF]]\{[[8]],[[16]],[[32]]\}, [[uni_is_DI]]\\
Identifier Case Folding&[[uni_NFKC_CF]]\{[[8]],[[16]],[[32]]\}\\
Unicode Collation Algorithm (UCA)*&[[uni_uca_strcmp]]\{[[8]],[[16]],[[32]]\},\\
&[[uni_str_uca_raw]]\{[[8]],[[16]],[[32]]\},\\
&[[uni_uca_raw_cmp]]\\
\end{tabular}
\end{centering}

~

\small{* With localization support}

\small{Note that [[_brk]]-style functions are not provided for
identifiers and pattern syntax; the properties described by the
[[is_]]-style functions are mutually exclusive and
context-independent.}

\small{Note that identifier normalization is application-dependent,
so no specific function is provided.  Anything but NFKC\_CF requires
removal of default ignorable (DI) code points in addition to
normalization.}

\caption{\label{tab:unialg}Named Unicode Algorithms}
\end{table}

\section{Doxygen support}

The header files contain comments which can be parsed by doxygen%
\footnote{\url{http://www.doxygen.org/}}%
, for those who prefer that style of documentation over the
hand-written reference manual.  I do not like the way it organizes
and presents things, but it does at least provide a cheap (but flawed) 
way to generate man pages.  Running doxygen is entirely manual, but I
have included the [[Doxyfile]] I used during testing for reference.
Just extract it ([[make misc]] will do), maybe also manually make the
headers, and run [[doxygen]].  I may one day decide to drop this and
use some other in-line documentation method instead.

\lstset{language=make}
<<Doxyfile>>=
PROJECT_NAME           = "Unicode Support Routines"
JAVADOC_AUTOBRIEF      = YES
TAB_SIZE               = 8
OPTIMIZE_OUTPUT_FOR_C  = YES
HIDE_UNDOC_MEMBERS     = YES
SHOW_INCLUDE_FILES     = NO
MAX_INITIALIZER_LINES  = 0
INPUT                  = uni_all.h uni_io.h uni_locale.h uni_norm.h \
                         uni_prop.gen.h uni_prop.h
VERBATIM_HEADERS       = NO
SORT_BRIEF_DOCS        = YES
SORT_GROUP_NAMES       = YES
QUIET                  = YES
IGNORE_PREFIX          = uni_ UNI_
GENERATE_MAN           = YES
MAN_LINKS              = YES
INLINE_GROUPED_CLASSES = YES
PAPER_TYPE             = letter
LATEX_BATCHMODE        = YES
# EXTRACT_ALL            = YES
@

<<Plain Files>>=
Doxyfile \
@

% End-doc Users-Guide

\end{document}

\input{begin-hidden.tex} %%% doc
Completely untouched & undocumented files:
  IDNA:
    IdnaTest.txt (IDNA algorithm not implemented, so no testing)

  IVD: (doesn't seem all that useful except to the supported vendors)
    IVD_Collections.txt
    IVD_Sequences.txt
    IVD_Stats.txt
\input{end-hidden.tex} %%% doc
